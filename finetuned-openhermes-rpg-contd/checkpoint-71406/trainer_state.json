{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.9999159734476093,
  "eval_steps": 500,
  "global_step": 71406,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005601770159370361,
      "grad_norm": 37.009212493896484,
      "learning_rate": 1.8e-05,
      "loss": 5.6996,
      "step": 10
    },
    {
      "epoch": 0.0011203540318740722,
      "grad_norm": 14.342145919799805,
      "learning_rate": 3.8e-05,
      "loss": 4.2782,
      "step": 20
    },
    {
      "epoch": 0.0016805310478111082,
      "grad_norm": 4.0707316398620605,
      "learning_rate": 5.8e-05,
      "loss": 3.2727,
      "step": 30
    },
    {
      "epoch": 0.0022407080637481445,
      "grad_norm": 2.325505256652832,
      "learning_rate": 7.800000000000001e-05,
      "loss": 2.5385,
      "step": 40
    },
    {
      "epoch": 0.0028008850796851807,
      "grad_norm": 2.6656556129455566,
      "learning_rate": 9.8e-05,
      "loss": 2.5217,
      "step": 50
    },
    {
      "epoch": 0.0033610620956222165,
      "grad_norm": 2.5321240425109863,
      "learning_rate": 9.998738718538036e-05,
      "loss": 2.3748,
      "step": 60
    },
    {
      "epoch": 0.003921239111559253,
      "grad_norm": 3.160191535949707,
      "learning_rate": 9.997337294691407e-05,
      "loss": 2.3493,
      "step": 70
    },
    {
      "epoch": 0.004481416127496289,
      "grad_norm": 2.1491963863372803,
      "learning_rate": 9.995935870844779e-05,
      "loss": 2.2037,
      "step": 80
    },
    {
      "epoch": 0.005041593143433325,
      "grad_norm": 2.3314921855926514,
      "learning_rate": 9.99453444699815e-05,
      "loss": 2.2534,
      "step": 90
    },
    {
      "epoch": 0.005601770159370361,
      "grad_norm": 1.8330025672912598,
      "learning_rate": 9.993133023151522e-05,
      "loss": 2.1279,
      "step": 100
    },
    {
      "epoch": 0.006161947175307397,
      "grad_norm": 3.4647879600524902,
      "learning_rate": 9.991731599304895e-05,
      "loss": 2.1533,
      "step": 110
    },
    {
      "epoch": 0.006722124191244433,
      "grad_norm": 2.5065126419067383,
      "learning_rate": 9.990330175458266e-05,
      "loss": 2.1515,
      "step": 120
    },
    {
      "epoch": 0.00728230120718147,
      "grad_norm": 1.9326280355453491,
      "learning_rate": 9.988928751611638e-05,
      "loss": 2.1226,
      "step": 130
    },
    {
      "epoch": 0.007842478223118506,
      "grad_norm": 2.982295036315918,
      "learning_rate": 9.987527327765009e-05,
      "loss": 2.0959,
      "step": 140
    },
    {
      "epoch": 0.008402655239055541,
      "grad_norm": 2.2013299465179443,
      "learning_rate": 9.986125903918382e-05,
      "loss": 2.1253,
      "step": 150
    },
    {
      "epoch": 0.008962832254992578,
      "grad_norm": 1.9018995761871338,
      "learning_rate": 9.984724480071753e-05,
      "loss": 2.0531,
      "step": 160
    },
    {
      "epoch": 0.009523009270929614,
      "grad_norm": 2.092149496078491,
      "learning_rate": 9.983323056225126e-05,
      "loss": 1.9007,
      "step": 170
    },
    {
      "epoch": 0.01008318628686665,
      "grad_norm": 2.30824875831604,
      "learning_rate": 9.981921632378498e-05,
      "loss": 1.9579,
      "step": 180
    },
    {
      "epoch": 0.010643363302803686,
      "grad_norm": 2.9232192039489746,
      "learning_rate": 9.980520208531868e-05,
      "loss": 1.9287,
      "step": 190
    },
    {
      "epoch": 0.011203540318740723,
      "grad_norm": 2.5912206172943115,
      "learning_rate": 9.97911878468524e-05,
      "loss": 2.105,
      "step": 200
    },
    {
      "epoch": 0.011763717334677758,
      "grad_norm": 2.4044082164764404,
      "learning_rate": 9.977717360838612e-05,
      "loss": 2.0061,
      "step": 210
    },
    {
      "epoch": 0.012323894350614794,
      "grad_norm": 2.423166275024414,
      "learning_rate": 9.976315936991985e-05,
      "loss": 2.068,
      "step": 220
    },
    {
      "epoch": 0.012884071366551831,
      "grad_norm": 2.4986612796783447,
      "learning_rate": 9.974914513145356e-05,
      "loss": 2.0323,
      "step": 230
    },
    {
      "epoch": 0.013444248382488866,
      "grad_norm": 3.3660051822662354,
      "learning_rate": 9.973513089298728e-05,
      "loss": 1.9583,
      "step": 240
    },
    {
      "epoch": 0.014004425398425903,
      "grad_norm": 2.5098211765289307,
      "learning_rate": 9.9721116654521e-05,
      "loss": 1.9841,
      "step": 250
    },
    {
      "epoch": 0.01456460241436294,
      "grad_norm": 2.7109005451202393,
      "learning_rate": 9.970710241605472e-05,
      "loss": 1.9797,
      "step": 260
    },
    {
      "epoch": 0.015124779430299974,
      "grad_norm": 3.1447174549102783,
      "learning_rate": 9.969308817758844e-05,
      "loss": 1.9198,
      "step": 270
    },
    {
      "epoch": 0.015684956446237013,
      "grad_norm": 2.418846368789673,
      "learning_rate": 9.967907393912216e-05,
      "loss": 1.9133,
      "step": 280
    },
    {
      "epoch": 0.016245133462174047,
      "grad_norm": 3.3731539249420166,
      "learning_rate": 9.966505970065587e-05,
      "loss": 1.9024,
      "step": 290
    },
    {
      "epoch": 0.016805310478111082,
      "grad_norm": 2.7498972415924072,
      "learning_rate": 9.965104546218958e-05,
      "loss": 1.9624,
      "step": 300
    },
    {
      "epoch": 0.01736548749404812,
      "grad_norm": 3.6063966751098633,
      "learning_rate": 9.963703122372331e-05,
      "loss": 1.9128,
      "step": 310
    },
    {
      "epoch": 0.017925664509985156,
      "grad_norm": 3.535525321960449,
      "learning_rate": 9.962301698525702e-05,
      "loss": 1.7975,
      "step": 320
    },
    {
      "epoch": 0.01848584152592219,
      "grad_norm": 2.7684519290924072,
      "learning_rate": 9.960900274679075e-05,
      "loss": 1.8681,
      "step": 330
    },
    {
      "epoch": 0.01904601854185923,
      "grad_norm": 3.5750582218170166,
      "learning_rate": 9.959498850832447e-05,
      "loss": 1.8586,
      "step": 340
    },
    {
      "epoch": 0.019606195557796264,
      "grad_norm": 3.357976198196411,
      "learning_rate": 9.958097426985818e-05,
      "loss": 1.7309,
      "step": 350
    },
    {
      "epoch": 0.0201663725737333,
      "grad_norm": 2.916567325592041,
      "learning_rate": 9.95669600313919e-05,
      "loss": 1.7886,
      "step": 360
    },
    {
      "epoch": 0.020726549589670337,
      "grad_norm": 3.203514337539673,
      "learning_rate": 9.955294579292562e-05,
      "loss": 1.8564,
      "step": 370
    },
    {
      "epoch": 0.021286726605607372,
      "grad_norm": 3.0998220443725586,
      "learning_rate": 9.953893155445934e-05,
      "loss": 1.7782,
      "step": 380
    },
    {
      "epoch": 0.021846903621544407,
      "grad_norm": 4.268044471740723,
      "learning_rate": 9.952491731599305e-05,
      "loss": 1.7809,
      "step": 390
    },
    {
      "epoch": 0.022407080637481445,
      "grad_norm": 4.0391364097595215,
      "learning_rate": 9.951090307752677e-05,
      "loss": 1.9151,
      "step": 400
    },
    {
      "epoch": 0.02296725765341848,
      "grad_norm": 3.975538730621338,
      "learning_rate": 9.949688883906048e-05,
      "loss": 1.7676,
      "step": 410
    },
    {
      "epoch": 0.023527434669355515,
      "grad_norm": 3.4411861896514893,
      "learning_rate": 9.948287460059421e-05,
      "loss": 1.7517,
      "step": 420
    },
    {
      "epoch": 0.024087611685292554,
      "grad_norm": 3.1163649559020996,
      "learning_rate": 9.946886036212793e-05,
      "loss": 1.7144,
      "step": 430
    },
    {
      "epoch": 0.02464778870122959,
      "grad_norm": 3.443187952041626,
      "learning_rate": 9.945484612366165e-05,
      "loss": 1.5901,
      "step": 440
    },
    {
      "epoch": 0.025207965717166624,
      "grad_norm": 3.7671408653259277,
      "learning_rate": 9.944083188519536e-05,
      "loss": 1.7713,
      "step": 450
    },
    {
      "epoch": 0.025768142733103662,
      "grad_norm": 3.8126590251922607,
      "learning_rate": 9.942681764672908e-05,
      "loss": 1.7347,
      "step": 460
    },
    {
      "epoch": 0.026328319749040697,
      "grad_norm": 3.863736867904663,
      "learning_rate": 9.94128034082628e-05,
      "loss": 1.7502,
      "step": 470
    },
    {
      "epoch": 0.026888496764977732,
      "grad_norm": 3.5719292163848877,
      "learning_rate": 9.939878916979651e-05,
      "loss": 1.7963,
      "step": 480
    },
    {
      "epoch": 0.02744867378091477,
      "grad_norm": 3.565642833709717,
      "learning_rate": 9.938477493133024e-05,
      "loss": 1.6948,
      "step": 490
    },
    {
      "epoch": 0.028008850796851805,
      "grad_norm": 2.9562642574310303,
      "learning_rate": 9.937076069286396e-05,
      "loss": 1.6695,
      "step": 500
    },
    {
      "epoch": 0.02856902781278884,
      "grad_norm": 4.0359954833984375,
      "learning_rate": 9.935674645439767e-05,
      "loss": 1.4964,
      "step": 510
    },
    {
      "epoch": 0.02912920482872588,
      "grad_norm": 3.106057643890381,
      "learning_rate": 9.934273221593139e-05,
      "loss": 1.6488,
      "step": 520
    },
    {
      "epoch": 0.029689381844662913,
      "grad_norm": 4.571928977966309,
      "learning_rate": 9.932871797746511e-05,
      "loss": 1.6069,
      "step": 530
    },
    {
      "epoch": 0.030249558860599948,
      "grad_norm": 4.0893073081970215,
      "learning_rate": 9.931470373899883e-05,
      "loss": 1.669,
      "step": 540
    },
    {
      "epoch": 0.030809735876536987,
      "grad_norm": 6.693032741546631,
      "learning_rate": 9.930068950053256e-05,
      "loss": 1.6327,
      "step": 550
    },
    {
      "epoch": 0.031369912892474025,
      "grad_norm": 3.846813440322876,
      "learning_rate": 9.928667526206626e-05,
      "loss": 1.6827,
      "step": 560
    },
    {
      "epoch": 0.031930089908411056,
      "grad_norm": 3.2180542945861816,
      "learning_rate": 9.927266102359997e-05,
      "loss": 1.8556,
      "step": 570
    },
    {
      "epoch": 0.032490266924348095,
      "grad_norm": 7.303603172302246,
      "learning_rate": 9.92586467851337e-05,
      "loss": 1.4897,
      "step": 580
    },
    {
      "epoch": 0.03305044394028513,
      "grad_norm": 4.524498462677002,
      "learning_rate": 9.924463254666742e-05,
      "loss": 1.5792,
      "step": 590
    },
    {
      "epoch": 0.033610620956222165,
      "grad_norm": 4.98999547958374,
      "learning_rate": 9.923061830820114e-05,
      "loss": 1.6091,
      "step": 600
    },
    {
      "epoch": 0.0341707979721592,
      "grad_norm": 6.293428421020508,
      "learning_rate": 9.921660406973486e-05,
      "loss": 1.3673,
      "step": 610
    },
    {
      "epoch": 0.03473097498809624,
      "grad_norm": 4.412165641784668,
      "learning_rate": 9.920258983126857e-05,
      "loss": 1.4255,
      "step": 620
    },
    {
      "epoch": 0.03529115200403327,
      "grad_norm": 6.436214447021484,
      "learning_rate": 9.918857559280229e-05,
      "loss": 1.607,
      "step": 630
    },
    {
      "epoch": 0.03585132901997031,
      "grad_norm": 3.7964134216308594,
      "learning_rate": 9.917456135433602e-05,
      "loss": 1.5336,
      "step": 640
    },
    {
      "epoch": 0.03641150603590735,
      "grad_norm": 3.8639373779296875,
      "learning_rate": 9.916054711586973e-05,
      "loss": 1.6665,
      "step": 650
    },
    {
      "epoch": 0.03697168305184438,
      "grad_norm": 4.480119705200195,
      "learning_rate": 9.914653287740345e-05,
      "loss": 1.5443,
      "step": 660
    },
    {
      "epoch": 0.03753186006778142,
      "grad_norm": 5.674130439758301,
      "learning_rate": 9.913251863893716e-05,
      "loss": 1.537,
      "step": 670
    },
    {
      "epoch": 0.03809203708371846,
      "grad_norm": 6.662449836730957,
      "learning_rate": 9.911850440047088e-05,
      "loss": 1.3778,
      "step": 680
    },
    {
      "epoch": 0.03865221409965549,
      "grad_norm": 3.4288082122802734,
      "learning_rate": 9.91044901620046e-05,
      "loss": 1.5297,
      "step": 690
    },
    {
      "epoch": 0.03921239111559253,
      "grad_norm": 4.1526312828063965,
      "learning_rate": 9.909047592353832e-05,
      "loss": 1.3979,
      "step": 700
    },
    {
      "epoch": 0.039772568131529566,
      "grad_norm": 3.6225433349609375,
      "learning_rate": 9.907646168507205e-05,
      "loss": 1.5224,
      "step": 710
    },
    {
      "epoch": 0.0403327451474666,
      "grad_norm": 4.846179962158203,
      "learning_rate": 9.906244744660575e-05,
      "loss": 1.6991,
      "step": 720
    },
    {
      "epoch": 0.040892922163403636,
      "grad_norm": 4.775014877319336,
      "learning_rate": 9.904843320813948e-05,
      "loss": 1.4237,
      "step": 730
    },
    {
      "epoch": 0.041453099179340674,
      "grad_norm": 5.0051774978637695,
      "learning_rate": 9.903441896967319e-05,
      "loss": 1.4941,
      "step": 740
    },
    {
      "epoch": 0.042013276195277706,
      "grad_norm": 3.904973030090332,
      "learning_rate": 9.90204047312069e-05,
      "loss": 1.557,
      "step": 750
    },
    {
      "epoch": 0.042573453211214744,
      "grad_norm": 4.662972927093506,
      "learning_rate": 9.900639049274063e-05,
      "loss": 1.5434,
      "step": 760
    },
    {
      "epoch": 0.04313363022715178,
      "grad_norm": 4.889822483062744,
      "learning_rate": 9.899237625427435e-05,
      "loss": 1.4569,
      "step": 770
    },
    {
      "epoch": 0.043693807243088814,
      "grad_norm": 4.858367443084717,
      "learning_rate": 9.897836201580806e-05,
      "loss": 1.3474,
      "step": 780
    },
    {
      "epoch": 0.04425398425902585,
      "grad_norm": 4.356265544891357,
      "learning_rate": 9.896434777734178e-05,
      "loss": 1.4771,
      "step": 790
    },
    {
      "epoch": 0.04481416127496289,
      "grad_norm": 3.7707266807556152,
      "learning_rate": 9.895033353887551e-05,
      "loss": 1.4522,
      "step": 800
    },
    {
      "epoch": 0.04537433829089992,
      "grad_norm": 5.19972562789917,
      "learning_rate": 9.893631930040922e-05,
      "loss": 1.4408,
      "step": 810
    },
    {
      "epoch": 0.04593451530683696,
      "grad_norm": 6.005987167358398,
      "learning_rate": 9.892230506194295e-05,
      "loss": 1.4234,
      "step": 820
    },
    {
      "epoch": 0.046494692322774,
      "grad_norm": 4.14546537399292,
      "learning_rate": 9.890829082347665e-05,
      "loss": 1.4578,
      "step": 830
    },
    {
      "epoch": 0.04705486933871103,
      "grad_norm": 5.7506632804870605,
      "learning_rate": 9.889427658501037e-05,
      "loss": 1.4465,
      "step": 840
    },
    {
      "epoch": 0.04761504635464807,
      "grad_norm": 4.365988254547119,
      "learning_rate": 9.88802623465441e-05,
      "loss": 1.3809,
      "step": 850
    },
    {
      "epoch": 0.04817522337058511,
      "grad_norm": 4.937405109405518,
      "learning_rate": 9.886624810807781e-05,
      "loss": 1.3981,
      "step": 860
    },
    {
      "epoch": 0.04873540038652214,
      "grad_norm": 5.889217376708984,
      "learning_rate": 9.885223386961154e-05,
      "loss": 1.3328,
      "step": 870
    },
    {
      "epoch": 0.04929557740245918,
      "grad_norm": 3.7035481929779053,
      "learning_rate": 9.883821963114525e-05,
      "loss": 1.4242,
      "step": 880
    },
    {
      "epoch": 0.049855754418396216,
      "grad_norm": 4.606056213378906,
      "learning_rate": 9.882420539267897e-05,
      "loss": 1.4288,
      "step": 890
    },
    {
      "epoch": 0.05041593143433325,
      "grad_norm": 3.382689952850342,
      "learning_rate": 9.881019115421268e-05,
      "loss": 1.4341,
      "step": 900
    },
    {
      "epoch": 0.050976108450270285,
      "grad_norm": 6.381376266479492,
      "learning_rate": 9.879617691574641e-05,
      "loss": 1.2924,
      "step": 910
    },
    {
      "epoch": 0.051536285466207324,
      "grad_norm": 4.729197978973389,
      "learning_rate": 9.878216267728012e-05,
      "loss": 1.323,
      "step": 920
    },
    {
      "epoch": 0.052096462482144355,
      "grad_norm": 5.823989391326904,
      "learning_rate": 9.876814843881384e-05,
      "loss": 1.2643,
      "step": 930
    },
    {
      "epoch": 0.052656639498081394,
      "grad_norm": 6.203653812408447,
      "learning_rate": 9.875413420034755e-05,
      "loss": 1.2406,
      "step": 940
    },
    {
      "epoch": 0.05321681651401843,
      "grad_norm": 4.059416770935059,
      "learning_rate": 9.874011996188127e-05,
      "loss": 1.3238,
      "step": 950
    },
    {
      "epoch": 0.053776993529955464,
      "grad_norm": 5.718772888183594,
      "learning_rate": 9.8726105723415e-05,
      "loss": 1.1926,
      "step": 960
    },
    {
      "epoch": 0.0543371705458925,
      "grad_norm": 4.279841423034668,
      "learning_rate": 9.871209148494871e-05,
      "loss": 1.3583,
      "step": 970
    },
    {
      "epoch": 0.05489734756182954,
      "grad_norm": 7.039615631103516,
      "learning_rate": 9.869807724648244e-05,
      "loss": 1.3956,
      "step": 980
    },
    {
      "epoch": 0.05545752457776657,
      "grad_norm": 5.883831977844238,
      "learning_rate": 9.868406300801614e-05,
      "loss": 1.2755,
      "step": 990
    },
    {
      "epoch": 0.05601770159370361,
      "grad_norm": 4.045046329498291,
      "learning_rate": 9.867004876954987e-05,
      "loss": 1.4196,
      "step": 1000
    },
    {
      "epoch": 0.05657787860964065,
      "grad_norm": 5.288678169250488,
      "learning_rate": 9.865603453108358e-05,
      "loss": 1.3131,
      "step": 1010
    },
    {
      "epoch": 0.05713805562557768,
      "grad_norm": 4.820218086242676,
      "learning_rate": 9.86420202926173e-05,
      "loss": 1.3208,
      "step": 1020
    },
    {
      "epoch": 0.05769823264151472,
      "grad_norm": 5.130753040313721,
      "learning_rate": 9.862800605415103e-05,
      "loss": 1.314,
      "step": 1030
    },
    {
      "epoch": 0.05825840965745176,
      "grad_norm": 4.7090630531311035,
      "learning_rate": 9.861399181568474e-05,
      "loss": 1.3159,
      "step": 1040
    },
    {
      "epoch": 0.05881858667338879,
      "grad_norm": 5.597872734069824,
      "learning_rate": 9.859997757721846e-05,
      "loss": 1.1646,
      "step": 1050
    },
    {
      "epoch": 0.05937876368932583,
      "grad_norm": 5.097334861755371,
      "learning_rate": 9.858596333875217e-05,
      "loss": 1.4521,
      "step": 1060
    },
    {
      "epoch": 0.059938940705262865,
      "grad_norm": 6.272392749786377,
      "learning_rate": 9.85719491002859e-05,
      "loss": 1.163,
      "step": 1070
    },
    {
      "epoch": 0.060499117721199897,
      "grad_norm": 5.157864570617676,
      "learning_rate": 9.855793486181962e-05,
      "loss": 1.3248,
      "step": 1080
    },
    {
      "epoch": 0.061059294737136935,
      "grad_norm": 5.730712890625,
      "learning_rate": 9.854392062335333e-05,
      "loss": 1.3182,
      "step": 1090
    },
    {
      "epoch": 0.06161947175307397,
      "grad_norm": 4.8562912940979,
      "learning_rate": 9.852990638488704e-05,
      "loss": 1.3064,
      "step": 1100
    },
    {
      "epoch": 0.062179648769011005,
      "grad_norm": 4.175216197967529,
      "learning_rate": 9.851589214642076e-05,
      "loss": 1.3246,
      "step": 1110
    },
    {
      "epoch": 0.06273982578494805,
      "grad_norm": 6.487740516662598,
      "learning_rate": 9.850187790795449e-05,
      "loss": 1.3004,
      "step": 1120
    },
    {
      "epoch": 0.06330000280088507,
      "grad_norm": 5.112025737762451,
      "learning_rate": 9.84878636694882e-05,
      "loss": 1.3336,
      "step": 1130
    },
    {
      "epoch": 0.06386017981682211,
      "grad_norm": 5.2483320236206055,
      "learning_rate": 9.847384943102193e-05,
      "loss": 1.4562,
      "step": 1140
    },
    {
      "epoch": 0.06442035683275915,
      "grad_norm": 6.2273945808410645,
      "learning_rate": 9.845983519255563e-05,
      "loss": 1.1483,
      "step": 1150
    },
    {
      "epoch": 0.06498053384869619,
      "grad_norm": 5.831747055053711,
      "learning_rate": 9.844582095408936e-05,
      "loss": 1.2136,
      "step": 1160
    },
    {
      "epoch": 0.06554071086463323,
      "grad_norm": 8.292613983154297,
      "learning_rate": 9.843180671562307e-05,
      "loss": 1.197,
      "step": 1170
    },
    {
      "epoch": 0.06610088788057027,
      "grad_norm": 6.13753604888916,
      "learning_rate": 9.84177924771568e-05,
      "loss": 1.0468,
      "step": 1180
    },
    {
      "epoch": 0.06666106489650729,
      "grad_norm": 5.9057440757751465,
      "learning_rate": 9.840377823869052e-05,
      "loss": 1.2259,
      "step": 1190
    },
    {
      "epoch": 0.06722124191244433,
      "grad_norm": 5.495423316955566,
      "learning_rate": 9.838976400022423e-05,
      "loss": 1.1672,
      "step": 1200
    },
    {
      "epoch": 0.06778141892838137,
      "grad_norm": 5.240584850311279,
      "learning_rate": 9.837574976175795e-05,
      "loss": 1.2161,
      "step": 1210
    },
    {
      "epoch": 0.0683415959443184,
      "grad_norm": 6.803238868713379,
      "learning_rate": 9.836173552329166e-05,
      "loss": 1.2827,
      "step": 1220
    },
    {
      "epoch": 0.06890177296025544,
      "grad_norm": 4.885364055633545,
      "learning_rate": 9.834772128482539e-05,
      "loss": 1.2691,
      "step": 1230
    },
    {
      "epoch": 0.06946194997619248,
      "grad_norm": 7.312310218811035,
      "learning_rate": 9.83337070463591e-05,
      "loss": 1.2395,
      "step": 1240
    },
    {
      "epoch": 0.07002212699212951,
      "grad_norm": 5.039008140563965,
      "learning_rate": 9.831969280789283e-05,
      "loss": 1.1268,
      "step": 1250
    },
    {
      "epoch": 0.07058230400806655,
      "grad_norm": 5.578150272369385,
      "learning_rate": 9.830567856942653e-05,
      "loss": 1.1232,
      "step": 1260
    },
    {
      "epoch": 0.07114248102400358,
      "grad_norm": 4.563378810882568,
      "learning_rate": 9.829166433096026e-05,
      "loss": 1.1641,
      "step": 1270
    },
    {
      "epoch": 0.07170265803994062,
      "grad_norm": 4.39313268661499,
      "learning_rate": 9.827765009249398e-05,
      "loss": 1.0822,
      "step": 1280
    },
    {
      "epoch": 0.07226283505587766,
      "grad_norm": 5.8265485763549805,
      "learning_rate": 9.826363585402769e-05,
      "loss": 1.0776,
      "step": 1290
    },
    {
      "epoch": 0.0728230120718147,
      "grad_norm": 5.077713966369629,
      "learning_rate": 9.824962161556142e-05,
      "loss": 1.2046,
      "step": 1300
    },
    {
      "epoch": 0.07338318908775172,
      "grad_norm": 6.831062316894531,
      "learning_rate": 9.823560737709514e-05,
      "loss": 1.3061,
      "step": 1310
    },
    {
      "epoch": 0.07394336610368876,
      "grad_norm": 6.200860977172852,
      "learning_rate": 9.822159313862885e-05,
      "loss": 1.0039,
      "step": 1320
    },
    {
      "epoch": 0.0745035431196258,
      "grad_norm": 4.162482738494873,
      "learning_rate": 9.820757890016256e-05,
      "loss": 1.0956,
      "step": 1330
    },
    {
      "epoch": 0.07506372013556284,
      "grad_norm": 5.5700836181640625,
      "learning_rate": 9.81935646616963e-05,
      "loss": 1.1297,
      "step": 1340
    },
    {
      "epoch": 0.07562389715149988,
      "grad_norm": 5.679996013641357,
      "learning_rate": 9.817955042323001e-05,
      "loss": 0.9875,
      "step": 1350
    },
    {
      "epoch": 0.07618407416743692,
      "grad_norm": 6.067718029022217,
      "learning_rate": 9.816553618476372e-05,
      "loss": 1.1886,
      "step": 1360
    },
    {
      "epoch": 0.07674425118337394,
      "grad_norm": 6.1328301429748535,
      "learning_rate": 9.815152194629744e-05,
      "loss": 0.9907,
      "step": 1370
    },
    {
      "epoch": 0.07730442819931098,
      "grad_norm": 5.422970294952393,
      "learning_rate": 9.813750770783117e-05,
      "loss": 0.9849,
      "step": 1380
    },
    {
      "epoch": 0.07786460521524802,
      "grad_norm": 6.833364009857178,
      "learning_rate": 9.812349346936488e-05,
      "loss": 1.1125,
      "step": 1390
    },
    {
      "epoch": 0.07842478223118506,
      "grad_norm": 5.57279634475708,
      "learning_rate": 9.81094792308986e-05,
      "loss": 1.2084,
      "step": 1400
    },
    {
      "epoch": 0.0789849592471221,
      "grad_norm": 6.584819316864014,
      "learning_rate": 9.809546499243232e-05,
      "loss": 1.1262,
      "step": 1410
    },
    {
      "epoch": 0.07954513626305913,
      "grad_norm": 6.5941572189331055,
      "learning_rate": 9.808145075396602e-05,
      "loss": 1.0313,
      "step": 1420
    },
    {
      "epoch": 0.08010531327899616,
      "grad_norm": 4.518508434295654,
      "learning_rate": 9.806743651549975e-05,
      "loss": 1.2063,
      "step": 1430
    },
    {
      "epoch": 0.0806654902949332,
      "grad_norm": 7.238914489746094,
      "learning_rate": 9.805342227703347e-05,
      "loss": 1.1791,
      "step": 1440
    },
    {
      "epoch": 0.08122566731087023,
      "grad_norm": 4.986719131469727,
      "learning_rate": 9.80394080385672e-05,
      "loss": 1.3197,
      "step": 1450
    },
    {
      "epoch": 0.08178584432680727,
      "grad_norm": 6.673000812530518,
      "learning_rate": 9.802539380010091e-05,
      "loss": 1.0249,
      "step": 1460
    },
    {
      "epoch": 0.08234602134274431,
      "grad_norm": 4.5204644203186035,
      "learning_rate": 9.801137956163463e-05,
      "loss": 1.0091,
      "step": 1470
    },
    {
      "epoch": 0.08290619835868135,
      "grad_norm": 4.4658942222595215,
      "learning_rate": 9.799736532316834e-05,
      "loss": 1.2751,
      "step": 1480
    },
    {
      "epoch": 0.08346637537461837,
      "grad_norm": 6.80431604385376,
      "learning_rate": 9.798335108470206e-05,
      "loss": 1.1091,
      "step": 1490
    },
    {
      "epoch": 0.08402655239055541,
      "grad_norm": 6.618100643157959,
      "learning_rate": 9.796933684623578e-05,
      "loss": 0.7226,
      "step": 1500
    },
    {
      "epoch": 0.08458672940649245,
      "grad_norm": 6.370776176452637,
      "learning_rate": 9.79553226077695e-05,
      "loss": 0.8647,
      "step": 1510
    },
    {
      "epoch": 0.08514690642242949,
      "grad_norm": 4.465889930725098,
      "learning_rate": 9.794130836930323e-05,
      "loss": 1.1724,
      "step": 1520
    },
    {
      "epoch": 0.08570708343836653,
      "grad_norm": 5.474150657653809,
      "learning_rate": 9.792729413083693e-05,
      "loss": 1.159,
      "step": 1530
    },
    {
      "epoch": 0.08626726045430357,
      "grad_norm": 6.6206955909729,
      "learning_rate": 9.791327989237066e-05,
      "loss": 0.9947,
      "step": 1540
    },
    {
      "epoch": 0.08682743747024059,
      "grad_norm": 5.193179130554199,
      "learning_rate": 9.789926565390437e-05,
      "loss": 1.1219,
      "step": 1550
    },
    {
      "epoch": 0.08738761448617763,
      "grad_norm": 4.486810207366943,
      "learning_rate": 9.78852514154381e-05,
      "loss": 0.9687,
      "step": 1560
    },
    {
      "epoch": 0.08794779150211467,
      "grad_norm": 5.586259841918945,
      "learning_rate": 9.787123717697181e-05,
      "loss": 0.8968,
      "step": 1570
    },
    {
      "epoch": 0.0885079685180517,
      "grad_norm": 6.199566841125488,
      "learning_rate": 9.785722293850553e-05,
      "loss": 1.0923,
      "step": 1580
    },
    {
      "epoch": 0.08906814553398874,
      "grad_norm": 5.390232563018799,
      "learning_rate": 9.784320870003924e-05,
      "loss": 1.1165,
      "step": 1590
    },
    {
      "epoch": 0.08962832254992578,
      "grad_norm": 6.771603107452393,
      "learning_rate": 9.782919446157296e-05,
      "loss": 1.1301,
      "step": 1600
    },
    {
      "epoch": 0.0901884995658628,
      "grad_norm": 5.261183261871338,
      "learning_rate": 9.781518022310669e-05,
      "loss": 0.8869,
      "step": 1610
    },
    {
      "epoch": 0.09074867658179984,
      "grad_norm": 6.2975687980651855,
      "learning_rate": 9.78011659846404e-05,
      "loss": 0.8601,
      "step": 1620
    },
    {
      "epoch": 0.09130885359773688,
      "grad_norm": 5.8633904457092285,
      "learning_rate": 9.778715174617412e-05,
      "loss": 0.8799,
      "step": 1630
    },
    {
      "epoch": 0.09186903061367392,
      "grad_norm": 6.8475751876831055,
      "learning_rate": 9.777313750770783e-05,
      "loss": 1.0796,
      "step": 1640
    },
    {
      "epoch": 0.09242920762961096,
      "grad_norm": 6.680342674255371,
      "learning_rate": 9.775912326924156e-05,
      "loss": 0.9722,
      "step": 1650
    },
    {
      "epoch": 0.092989384645548,
      "grad_norm": 4.5413994789123535,
      "learning_rate": 9.774510903077527e-05,
      "loss": 1.1258,
      "step": 1660
    },
    {
      "epoch": 0.09354956166148502,
      "grad_norm": 4.862964630126953,
      "learning_rate": 9.773109479230899e-05,
      "loss": 0.9282,
      "step": 1670
    },
    {
      "epoch": 0.09410973867742206,
      "grad_norm": 3.5305919647216797,
      "learning_rate": 9.771708055384272e-05,
      "loss": 0.9327,
      "step": 1680
    },
    {
      "epoch": 0.0946699156933591,
      "grad_norm": 8.837746620178223,
      "learning_rate": 9.770306631537642e-05,
      "loss": 1.0115,
      "step": 1690
    },
    {
      "epoch": 0.09523009270929614,
      "grad_norm": 6.3503522872924805,
      "learning_rate": 9.768905207691015e-05,
      "loss": 0.9275,
      "step": 1700
    },
    {
      "epoch": 0.09579026972523318,
      "grad_norm": 6.664885520935059,
      "learning_rate": 9.767503783844386e-05,
      "loss": 0.7607,
      "step": 1710
    },
    {
      "epoch": 0.09635044674117021,
      "grad_norm": 4.475496292114258,
      "learning_rate": 9.766102359997759e-05,
      "loss": 1.0468,
      "step": 1720
    },
    {
      "epoch": 0.09691062375710724,
      "grad_norm": 6.494497299194336,
      "learning_rate": 9.76470093615113e-05,
      "loss": 1.1168,
      "step": 1730
    },
    {
      "epoch": 0.09747080077304428,
      "grad_norm": 5.3354597091674805,
      "learning_rate": 9.763299512304502e-05,
      "loss": 1.2016,
      "step": 1740
    },
    {
      "epoch": 0.09803097778898132,
      "grad_norm": 7.316672325134277,
      "learning_rate": 9.761898088457873e-05,
      "loss": 1.0479,
      "step": 1750
    },
    {
      "epoch": 0.09859115480491835,
      "grad_norm": 5.177597999572754,
      "learning_rate": 9.760496664611245e-05,
      "loss": 0.9842,
      "step": 1760
    },
    {
      "epoch": 0.09915133182085539,
      "grad_norm": 5.9864115715026855,
      "learning_rate": 9.759095240764618e-05,
      "loss": 0.874,
      "step": 1770
    },
    {
      "epoch": 0.09971150883679243,
      "grad_norm": 6.599603176116943,
      "learning_rate": 9.757693816917989e-05,
      "loss": 1.0128,
      "step": 1780
    },
    {
      "epoch": 0.10027168585272946,
      "grad_norm": 4.792365550994873,
      "learning_rate": 9.75629239307136e-05,
      "loss": 0.9426,
      "step": 1790
    },
    {
      "epoch": 0.1008318628686665,
      "grad_norm": 6.635513782501221,
      "learning_rate": 9.754890969224732e-05,
      "loss": 0.9127,
      "step": 1800
    },
    {
      "epoch": 0.10139203988460353,
      "grad_norm": 5.246753215789795,
      "learning_rate": 9.753489545378105e-05,
      "loss": 0.7598,
      "step": 1810
    },
    {
      "epoch": 0.10195221690054057,
      "grad_norm": 5.869290351867676,
      "learning_rate": 9.752088121531476e-05,
      "loss": 0.9878,
      "step": 1820
    },
    {
      "epoch": 0.10251239391647761,
      "grad_norm": 7.271116733551025,
      "learning_rate": 9.750686697684849e-05,
      "loss": 1.0873,
      "step": 1830
    },
    {
      "epoch": 0.10307257093241465,
      "grad_norm": 5.284729480743408,
      "learning_rate": 9.74928527383822e-05,
      "loss": 1.0545,
      "step": 1840
    },
    {
      "epoch": 0.10363274794835167,
      "grad_norm": 5.682661533355713,
      "learning_rate": 9.747883849991591e-05,
      "loss": 0.9498,
      "step": 1850
    },
    {
      "epoch": 0.10419292496428871,
      "grad_norm": 5.58426570892334,
      "learning_rate": 9.746482426144964e-05,
      "loss": 0.9532,
      "step": 1860
    },
    {
      "epoch": 0.10475310198022575,
      "grad_norm": 6.083011627197266,
      "learning_rate": 9.745081002298335e-05,
      "loss": 1.168,
      "step": 1870
    },
    {
      "epoch": 0.10531327899616279,
      "grad_norm": 7.407041072845459,
      "learning_rate": 9.743679578451708e-05,
      "loss": 0.9674,
      "step": 1880
    },
    {
      "epoch": 0.10587345601209983,
      "grad_norm": 8.35856819152832,
      "learning_rate": 9.74227815460508e-05,
      "loss": 0.8626,
      "step": 1890
    },
    {
      "epoch": 0.10643363302803686,
      "grad_norm": 5.459451198577881,
      "learning_rate": 9.740876730758451e-05,
      "loss": 0.7757,
      "step": 1900
    },
    {
      "epoch": 0.1069938100439739,
      "grad_norm": 4.607298851013184,
      "learning_rate": 9.739475306911822e-05,
      "loss": 1.1083,
      "step": 1910
    },
    {
      "epoch": 0.10755398705991093,
      "grad_norm": 5.686107158660889,
      "learning_rate": 9.738073883065195e-05,
      "loss": 0.7956,
      "step": 1920
    },
    {
      "epoch": 0.10811416407584797,
      "grad_norm": 4.769740581512451,
      "learning_rate": 9.736672459218567e-05,
      "loss": 0.9195,
      "step": 1930
    },
    {
      "epoch": 0.108674341091785,
      "grad_norm": 7.803099632263184,
      "learning_rate": 9.735271035371938e-05,
      "loss": 0.9867,
      "step": 1940
    },
    {
      "epoch": 0.10923451810772204,
      "grad_norm": 5.524913311004639,
      "learning_rate": 9.733869611525311e-05,
      "loss": 1.0434,
      "step": 1950
    },
    {
      "epoch": 0.10979469512365908,
      "grad_norm": 6.024078369140625,
      "learning_rate": 9.732468187678681e-05,
      "loss": 0.9033,
      "step": 1960
    },
    {
      "epoch": 0.11035487213959612,
      "grad_norm": 5.995643615722656,
      "learning_rate": 9.731066763832054e-05,
      "loss": 0.9637,
      "step": 1970
    },
    {
      "epoch": 0.11091504915553314,
      "grad_norm": 4.398583889007568,
      "learning_rate": 9.729665339985425e-05,
      "loss": 0.8176,
      "step": 1980
    },
    {
      "epoch": 0.11147522617147018,
      "grad_norm": 5.699228763580322,
      "learning_rate": 9.728263916138798e-05,
      "loss": 1.0573,
      "step": 1990
    },
    {
      "epoch": 0.11203540318740722,
      "grad_norm": 6.1274733543396,
      "learning_rate": 9.72686249229217e-05,
      "loss": 0.8468,
      "step": 2000
    },
    {
      "epoch": 0.11259558020334426,
      "grad_norm": 5.815851211547852,
      "learning_rate": 9.725461068445541e-05,
      "loss": 1.1513,
      "step": 2010
    },
    {
      "epoch": 0.1131557572192813,
      "grad_norm": 7.506139755249023,
      "learning_rate": 9.724059644598913e-05,
      "loss": 0.9391,
      "step": 2020
    },
    {
      "epoch": 0.11371593423521834,
      "grad_norm": 5.249773025512695,
      "learning_rate": 9.722658220752284e-05,
      "loss": 0.9582,
      "step": 2030
    },
    {
      "epoch": 0.11427611125115536,
      "grad_norm": 3.89041805267334,
      "learning_rate": 9.721256796905657e-05,
      "loss": 0.9478,
      "step": 2040
    },
    {
      "epoch": 0.1148362882670924,
      "grad_norm": 5.419809818267822,
      "learning_rate": 9.719855373059028e-05,
      "loss": 0.7893,
      "step": 2050
    },
    {
      "epoch": 0.11539646528302944,
      "grad_norm": 5.64909553527832,
      "learning_rate": 9.7184539492124e-05,
      "loss": 0.8812,
      "step": 2060
    },
    {
      "epoch": 0.11595664229896648,
      "grad_norm": 5.402641773223877,
      "learning_rate": 9.717052525365771e-05,
      "loss": 0.9731,
      "step": 2070
    },
    {
      "epoch": 0.11651681931490351,
      "grad_norm": 6.314830780029297,
      "learning_rate": 9.715651101519144e-05,
      "loss": 0.7956,
      "step": 2080
    },
    {
      "epoch": 0.11707699633084055,
      "grad_norm": 7.1264142990112305,
      "learning_rate": 9.714249677672516e-05,
      "loss": 0.9138,
      "step": 2090
    },
    {
      "epoch": 0.11763717334677758,
      "grad_norm": 7.403558254241943,
      "learning_rate": 9.712848253825888e-05,
      "loss": 0.8393,
      "step": 2100
    },
    {
      "epoch": 0.11819735036271461,
      "grad_norm": 5.83328914642334,
      "learning_rate": 9.71144682997926e-05,
      "loss": 0.8501,
      "step": 2110
    },
    {
      "epoch": 0.11875752737865165,
      "grad_norm": 7.617031097412109,
      "learning_rate": 9.71004540613263e-05,
      "loss": 0.8688,
      "step": 2120
    },
    {
      "epoch": 0.11931770439458869,
      "grad_norm": 6.791628837585449,
      "learning_rate": 9.708643982286003e-05,
      "loss": 0.9459,
      "step": 2130
    },
    {
      "epoch": 0.11987788141052573,
      "grad_norm": 4.508634567260742,
      "learning_rate": 9.707242558439374e-05,
      "loss": 0.5548,
      "step": 2140
    },
    {
      "epoch": 0.12043805842646277,
      "grad_norm": 7.227697372436523,
      "learning_rate": 9.705841134592747e-05,
      "loss": 0.9241,
      "step": 2150
    },
    {
      "epoch": 0.12099823544239979,
      "grad_norm": 8.798869132995605,
      "learning_rate": 9.704439710746119e-05,
      "loss": 0.7208,
      "step": 2160
    },
    {
      "epoch": 0.12155841245833683,
      "grad_norm": 6.412347793579102,
      "learning_rate": 9.70303828689949e-05,
      "loss": 0.8424,
      "step": 2170
    },
    {
      "epoch": 0.12211858947427387,
      "grad_norm": 12.311246871948242,
      "learning_rate": 9.701636863052862e-05,
      "loss": 0.7705,
      "step": 2180
    },
    {
      "epoch": 0.12267876649021091,
      "grad_norm": 7.43512487411499,
      "learning_rate": 9.700235439206234e-05,
      "loss": 0.9264,
      "step": 2190
    },
    {
      "epoch": 0.12323894350614795,
      "grad_norm": 5.013436317443848,
      "learning_rate": 9.698834015359606e-05,
      "loss": 1.0788,
      "step": 2200
    },
    {
      "epoch": 0.12379912052208498,
      "grad_norm": 4.526104927062988,
      "learning_rate": 9.697432591512977e-05,
      "loss": 0.9267,
      "step": 2210
    },
    {
      "epoch": 0.12435929753802201,
      "grad_norm": 4.380791187286377,
      "learning_rate": 9.69603116766635e-05,
      "loss": 0.8248,
      "step": 2220
    },
    {
      "epoch": 0.12491947455395905,
      "grad_norm": 4.081552505493164,
      "learning_rate": 9.69462974381972e-05,
      "loss": 0.8944,
      "step": 2230
    },
    {
      "epoch": 0.1254796515698961,
      "grad_norm": 5.909390449523926,
      "learning_rate": 9.693228319973093e-05,
      "loss": 0.8362,
      "step": 2240
    },
    {
      "epoch": 0.12603982858583312,
      "grad_norm": 5.303002834320068,
      "learning_rate": 9.691826896126465e-05,
      "loss": 1.0401,
      "step": 2250
    },
    {
      "epoch": 0.12660000560177015,
      "grad_norm": 5.9542236328125,
      "learning_rate": 9.690425472279837e-05,
      "loss": 0.9992,
      "step": 2260
    },
    {
      "epoch": 0.1271601826177072,
      "grad_norm": 4.182840824127197,
      "learning_rate": 9.689024048433209e-05,
      "loss": 0.803,
      "step": 2270
    },
    {
      "epoch": 0.12772035963364423,
      "grad_norm": 6.248558044433594,
      "learning_rate": 9.68762262458658e-05,
      "loss": 0.8549,
      "step": 2280
    },
    {
      "epoch": 0.12828053664958128,
      "grad_norm": 3.0266973972320557,
      "learning_rate": 9.686221200739952e-05,
      "loss": 0.8839,
      "step": 2290
    },
    {
      "epoch": 0.1288407136655183,
      "grad_norm": 5.80146598815918,
      "learning_rate": 9.684819776893323e-05,
      "loss": 0.8854,
      "step": 2300
    },
    {
      "epoch": 0.12940089068145533,
      "grad_norm": 7.738363742828369,
      "learning_rate": 9.683418353046696e-05,
      "loss": 0.9054,
      "step": 2310
    },
    {
      "epoch": 0.12996106769739238,
      "grad_norm": 7.1219162940979,
      "learning_rate": 9.682016929200068e-05,
      "loss": 0.7285,
      "step": 2320
    },
    {
      "epoch": 0.1305212447133294,
      "grad_norm": 4.991413593292236,
      "learning_rate": 9.680615505353439e-05,
      "loss": 0.7741,
      "step": 2330
    },
    {
      "epoch": 0.13108142172926646,
      "grad_norm": 5.541952133178711,
      "learning_rate": 9.67921408150681e-05,
      "loss": 0.7313,
      "step": 2340
    },
    {
      "epoch": 0.13164159874520348,
      "grad_norm": 5.988865375518799,
      "learning_rate": 9.677812657660183e-05,
      "loss": 0.8821,
      "step": 2350
    },
    {
      "epoch": 0.13220177576114053,
      "grad_norm": 6.709191799163818,
      "learning_rate": 9.676411233813555e-05,
      "loss": 0.8386,
      "step": 2360
    },
    {
      "epoch": 0.13276195277707756,
      "grad_norm": 5.096182346343994,
      "learning_rate": 9.675009809966928e-05,
      "loss": 0.6446,
      "step": 2370
    },
    {
      "epoch": 0.13332212979301458,
      "grad_norm": 5.637604236602783,
      "learning_rate": 9.673608386120299e-05,
      "loss": 0.7371,
      "step": 2380
    },
    {
      "epoch": 0.13388230680895163,
      "grad_norm": 5.6561360359191895,
      "learning_rate": 9.67220696227367e-05,
      "loss": 0.7843,
      "step": 2390
    },
    {
      "epoch": 0.13444248382488866,
      "grad_norm": 10.364019393920898,
      "learning_rate": 9.670805538427042e-05,
      "loss": 1.0412,
      "step": 2400
    },
    {
      "epoch": 0.1350026608408257,
      "grad_norm": 6.588565826416016,
      "learning_rate": 9.669404114580414e-05,
      "loss": 0.8779,
      "step": 2410
    },
    {
      "epoch": 0.13556283785676274,
      "grad_norm": 4.876647472381592,
      "learning_rate": 9.668002690733786e-05,
      "loss": 0.8869,
      "step": 2420
    },
    {
      "epoch": 0.13612301487269976,
      "grad_norm": 6.358173847198486,
      "learning_rate": 9.666601266887158e-05,
      "loss": 0.8658,
      "step": 2430
    },
    {
      "epoch": 0.1366831918886368,
      "grad_norm": 5.88951301574707,
      "learning_rate": 9.66519984304053e-05,
      "loss": 0.9219,
      "step": 2440
    },
    {
      "epoch": 0.13724336890457384,
      "grad_norm": 5.984091758728027,
      "learning_rate": 9.663798419193901e-05,
      "loss": 0.7163,
      "step": 2450
    },
    {
      "epoch": 0.1378035459205109,
      "grad_norm": 6.063173770904541,
      "learning_rate": 9.662396995347274e-05,
      "loss": 0.9466,
      "step": 2460
    },
    {
      "epoch": 0.1383637229364479,
      "grad_norm": 5.394069194793701,
      "learning_rate": 9.660995571500645e-05,
      "loss": 0.7565,
      "step": 2470
    },
    {
      "epoch": 0.13892389995238497,
      "grad_norm": 4.50253438949585,
      "learning_rate": 9.659594147654018e-05,
      "loss": 0.9532,
      "step": 2480
    },
    {
      "epoch": 0.139484076968322,
      "grad_norm": 6.4207024574279785,
      "learning_rate": 9.658192723807388e-05,
      "loss": 0.995,
      "step": 2490
    },
    {
      "epoch": 0.14004425398425902,
      "grad_norm": 6.225033760070801,
      "learning_rate": 9.65679129996076e-05,
      "loss": 0.8357,
      "step": 2500
    },
    {
      "epoch": 0.14060443100019607,
      "grad_norm": 7.632905006408691,
      "learning_rate": 9.655389876114132e-05,
      "loss": 0.8573,
      "step": 2510
    },
    {
      "epoch": 0.1411646080161331,
      "grad_norm": 5.38047456741333,
      "learning_rate": 9.653988452267504e-05,
      "loss": 0.8799,
      "step": 2520
    },
    {
      "epoch": 0.14172478503207014,
      "grad_norm": 6.450940132141113,
      "learning_rate": 9.652587028420877e-05,
      "loss": 0.758,
      "step": 2530
    },
    {
      "epoch": 0.14228496204800717,
      "grad_norm": 7.064465045928955,
      "learning_rate": 9.651185604574248e-05,
      "loss": 0.8325,
      "step": 2540
    },
    {
      "epoch": 0.1428451390639442,
      "grad_norm": 8.859368324279785,
      "learning_rate": 9.64978418072762e-05,
      "loss": 0.9101,
      "step": 2550
    },
    {
      "epoch": 0.14340531607988125,
      "grad_norm": 4.670359134674072,
      "learning_rate": 9.648382756880991e-05,
      "loss": 0.9473,
      "step": 2560
    },
    {
      "epoch": 0.14396549309581827,
      "grad_norm": 7.833426475524902,
      "learning_rate": 9.646981333034364e-05,
      "loss": 0.8247,
      "step": 2570
    },
    {
      "epoch": 0.14452567011175532,
      "grad_norm": 5.380515098571777,
      "learning_rate": 9.645579909187736e-05,
      "loss": 0.6772,
      "step": 2580
    },
    {
      "epoch": 0.14508584712769235,
      "grad_norm": 4.965718746185303,
      "learning_rate": 9.644178485341107e-05,
      "loss": 0.95,
      "step": 2590
    },
    {
      "epoch": 0.1456460241436294,
      "grad_norm": 5.417000770568848,
      "learning_rate": 9.642777061494478e-05,
      "loss": 0.9127,
      "step": 2600
    },
    {
      "epoch": 0.14620620115956642,
      "grad_norm": 3.919165849685669,
      "learning_rate": 9.64137563764785e-05,
      "loss": 0.7716,
      "step": 2610
    },
    {
      "epoch": 0.14676637817550345,
      "grad_norm": 5.502227306365967,
      "learning_rate": 9.639974213801223e-05,
      "loss": 0.8461,
      "step": 2620
    },
    {
      "epoch": 0.1473265551914405,
      "grad_norm": 6.7798991203308105,
      "learning_rate": 9.638572789954594e-05,
      "loss": 0.7816,
      "step": 2630
    },
    {
      "epoch": 0.14788673220737752,
      "grad_norm": 6.3647847175598145,
      "learning_rate": 9.637171366107967e-05,
      "loss": 0.886,
      "step": 2640
    },
    {
      "epoch": 0.14844690922331458,
      "grad_norm": 7.580420017242432,
      "learning_rate": 9.635769942261339e-05,
      "loss": 0.7168,
      "step": 2650
    },
    {
      "epoch": 0.1490070862392516,
      "grad_norm": 4.467341899871826,
      "learning_rate": 9.63436851841471e-05,
      "loss": 0.796,
      "step": 2660
    },
    {
      "epoch": 0.14956726325518863,
      "grad_norm": 5.688685894012451,
      "learning_rate": 9.632967094568081e-05,
      "loss": 0.8104,
      "step": 2670
    },
    {
      "epoch": 0.15012744027112568,
      "grad_norm": 6.6773905754089355,
      "learning_rate": 9.631565670721453e-05,
      "loss": 0.9884,
      "step": 2680
    },
    {
      "epoch": 0.1506876172870627,
      "grad_norm": 6.507796287536621,
      "learning_rate": 9.630164246874826e-05,
      "loss": 0.7376,
      "step": 2690
    },
    {
      "epoch": 0.15124779430299976,
      "grad_norm": 5.127806663513184,
      "learning_rate": 9.628762823028197e-05,
      "loss": 0.7297,
      "step": 2700
    },
    {
      "epoch": 0.15180797131893678,
      "grad_norm": 5.574933052062988,
      "learning_rate": 9.627361399181569e-05,
      "loss": 0.7493,
      "step": 2710
    },
    {
      "epoch": 0.15236814833487383,
      "grad_norm": 6.734964370727539,
      "learning_rate": 9.62595997533494e-05,
      "loss": 0.681,
      "step": 2720
    },
    {
      "epoch": 0.15292832535081086,
      "grad_norm": 5.569135665893555,
      "learning_rate": 9.624558551488313e-05,
      "loss": 0.7156,
      "step": 2730
    },
    {
      "epoch": 0.15348850236674788,
      "grad_norm": 10.008038520812988,
      "learning_rate": 9.623157127641685e-05,
      "loss": 0.7689,
      "step": 2740
    },
    {
      "epoch": 0.15404867938268493,
      "grad_norm": 7.750551223754883,
      "learning_rate": 9.621755703795057e-05,
      "loss": 0.8211,
      "step": 2750
    },
    {
      "epoch": 0.15460885639862196,
      "grad_norm": 5.80321741104126,
      "learning_rate": 9.620354279948427e-05,
      "loss": 0.6765,
      "step": 2760
    },
    {
      "epoch": 0.155169033414559,
      "grad_norm": 5.881166458129883,
      "learning_rate": 9.618952856101799e-05,
      "loss": 0.8346,
      "step": 2770
    },
    {
      "epoch": 0.15572921043049603,
      "grad_norm": 7.04649543762207,
      "learning_rate": 9.617551432255172e-05,
      "loss": 0.7773,
      "step": 2780
    },
    {
      "epoch": 0.15628938744643306,
      "grad_norm": 4.727505683898926,
      "learning_rate": 9.616150008408543e-05,
      "loss": 1.0597,
      "step": 2790
    },
    {
      "epoch": 0.1568495644623701,
      "grad_norm": 5.593252658843994,
      "learning_rate": 9.614748584561916e-05,
      "loss": 0.6515,
      "step": 2800
    },
    {
      "epoch": 0.15740974147830714,
      "grad_norm": 6.063333034515381,
      "learning_rate": 9.613347160715288e-05,
      "loss": 0.7156,
      "step": 2810
    },
    {
      "epoch": 0.1579699184942442,
      "grad_norm": 5.827224254608154,
      "learning_rate": 9.611945736868659e-05,
      "loss": 0.9288,
      "step": 2820
    },
    {
      "epoch": 0.1585300955101812,
      "grad_norm": 6.44426965713501,
      "learning_rate": 9.61054431302203e-05,
      "loss": 0.8559,
      "step": 2830
    },
    {
      "epoch": 0.15909027252611826,
      "grad_norm": 4.577105522155762,
      "learning_rate": 9.609142889175403e-05,
      "loss": 0.7397,
      "step": 2840
    },
    {
      "epoch": 0.1596504495420553,
      "grad_norm": 6.6695685386657715,
      "learning_rate": 9.607741465328775e-05,
      "loss": 0.8039,
      "step": 2850
    },
    {
      "epoch": 0.16021062655799231,
      "grad_norm": 5.0413594245910645,
      "learning_rate": 9.606340041482146e-05,
      "loss": 0.8624,
      "step": 2860
    },
    {
      "epoch": 0.16077080357392937,
      "grad_norm": 4.933169841766357,
      "learning_rate": 9.604938617635518e-05,
      "loss": 0.6045,
      "step": 2870
    },
    {
      "epoch": 0.1613309805898664,
      "grad_norm": 6.039483070373535,
      "learning_rate": 9.603537193788889e-05,
      "loss": 0.8828,
      "step": 2880
    },
    {
      "epoch": 0.16189115760580344,
      "grad_norm": 6.247369289398193,
      "learning_rate": 9.602135769942262e-05,
      "loss": 0.691,
      "step": 2890
    },
    {
      "epoch": 0.16245133462174047,
      "grad_norm": 5.600979804992676,
      "learning_rate": 9.600734346095634e-05,
      "loss": 0.7308,
      "step": 2900
    },
    {
      "epoch": 0.16301151163767752,
      "grad_norm": 6.485385894775391,
      "learning_rate": 9.599332922249006e-05,
      "loss": 0.5611,
      "step": 2910
    },
    {
      "epoch": 0.16357168865361454,
      "grad_norm": 4.639223575592041,
      "learning_rate": 9.597931498402378e-05,
      "loss": 0.548,
      "step": 2920
    },
    {
      "epoch": 0.16413186566955157,
      "grad_norm": 11.075472831726074,
      "learning_rate": 9.596530074555749e-05,
      "loss": 0.8905,
      "step": 2930
    },
    {
      "epoch": 0.16469204268548862,
      "grad_norm": 5.689300537109375,
      "learning_rate": 9.595128650709121e-05,
      "loss": 0.6813,
      "step": 2940
    },
    {
      "epoch": 0.16525221970142565,
      "grad_norm": 11.741033554077148,
      "learning_rate": 9.593727226862492e-05,
      "loss": 0.6478,
      "step": 2950
    },
    {
      "epoch": 0.1658123967173627,
      "grad_norm": 5.536905765533447,
      "learning_rate": 9.592325803015865e-05,
      "loss": 0.6281,
      "step": 2960
    },
    {
      "epoch": 0.16637257373329972,
      "grad_norm": 5.438888072967529,
      "learning_rate": 9.590924379169237e-05,
      "loss": 0.676,
      "step": 2970
    },
    {
      "epoch": 0.16693275074923675,
      "grad_norm": 6.698936939239502,
      "learning_rate": 9.589522955322608e-05,
      "loss": 0.9736,
      "step": 2980
    },
    {
      "epoch": 0.1674929277651738,
      "grad_norm": 10.574823379516602,
      "learning_rate": 9.58812153147598e-05,
      "loss": 0.6575,
      "step": 2990
    },
    {
      "epoch": 0.16805310478111082,
      "grad_norm": 3.4254980087280273,
      "learning_rate": 9.586720107629352e-05,
      "loss": 0.6224,
      "step": 3000
    },
    {
      "epoch": 0.16861328179704788,
      "grad_norm": 6.863877773284912,
      "learning_rate": 9.585318683782724e-05,
      "loss": 0.7098,
      "step": 3010
    },
    {
      "epoch": 0.1691734588129849,
      "grad_norm": 5.48552942276001,
      "learning_rate": 9.583917259936097e-05,
      "loss": 0.6839,
      "step": 3020
    },
    {
      "epoch": 0.16973363582892195,
      "grad_norm": 5.964648246765137,
      "learning_rate": 9.582515836089467e-05,
      "loss": 0.7034,
      "step": 3030
    },
    {
      "epoch": 0.17029381284485898,
      "grad_norm": 5.498782634735107,
      "learning_rate": 9.581114412242838e-05,
      "loss": 0.9101,
      "step": 3040
    },
    {
      "epoch": 0.170853989860796,
      "grad_norm": 6.448442459106445,
      "learning_rate": 9.579712988396211e-05,
      "loss": 0.578,
      "step": 3050
    },
    {
      "epoch": 0.17141416687673305,
      "grad_norm": 6.607870578765869,
      "learning_rate": 9.578311564549583e-05,
      "loss": 0.6398,
      "step": 3060
    },
    {
      "epoch": 0.17197434389267008,
      "grad_norm": 5.7341203689575195,
      "learning_rate": 9.576910140702955e-05,
      "loss": 0.6324,
      "step": 3070
    },
    {
      "epoch": 0.17253452090860713,
      "grad_norm": 5.051809310913086,
      "learning_rate": 9.575508716856327e-05,
      "loss": 0.5925,
      "step": 3080
    },
    {
      "epoch": 0.17309469792454416,
      "grad_norm": 7.904265403747559,
      "learning_rate": 9.574107293009698e-05,
      "loss": 0.6723,
      "step": 3090
    },
    {
      "epoch": 0.17365487494048118,
      "grad_norm": 4.759622573852539,
      "learning_rate": 9.57270586916307e-05,
      "loss": 0.9721,
      "step": 3100
    },
    {
      "epoch": 0.17421505195641823,
      "grad_norm": 6.541490077972412,
      "learning_rate": 9.571304445316443e-05,
      "loss": 0.7768,
      "step": 3110
    },
    {
      "epoch": 0.17477522897235526,
      "grad_norm": 7.447837829589844,
      "learning_rate": 9.569903021469814e-05,
      "loss": 0.681,
      "step": 3120
    },
    {
      "epoch": 0.1753354059882923,
      "grad_norm": 5.520007133483887,
      "learning_rate": 9.568501597623186e-05,
      "loss": 0.7811,
      "step": 3130
    },
    {
      "epoch": 0.17589558300422933,
      "grad_norm": 6.501302242279053,
      "learning_rate": 9.567100173776557e-05,
      "loss": 0.5524,
      "step": 3140
    },
    {
      "epoch": 0.17645576002016639,
      "grad_norm": 7.354224681854248,
      "learning_rate": 9.565698749929929e-05,
      "loss": 0.7113,
      "step": 3150
    },
    {
      "epoch": 0.1770159370361034,
      "grad_norm": 5.31137228012085,
      "learning_rate": 9.564297326083301e-05,
      "loss": 0.7428,
      "step": 3160
    },
    {
      "epoch": 0.17757611405204043,
      "grad_norm": 7.71731424331665,
      "learning_rate": 9.562895902236673e-05,
      "loss": 0.795,
      "step": 3170
    },
    {
      "epoch": 0.1781362910679775,
      "grad_norm": 6.536055088043213,
      "learning_rate": 9.561494478390046e-05,
      "loss": 0.6397,
      "step": 3180
    },
    {
      "epoch": 0.1786964680839145,
      "grad_norm": 4.593129634857178,
      "learning_rate": 9.560093054543416e-05,
      "loss": 0.8234,
      "step": 3190
    },
    {
      "epoch": 0.17925664509985156,
      "grad_norm": 6.037144660949707,
      "learning_rate": 9.558691630696789e-05,
      "loss": 0.5957,
      "step": 3200
    },
    {
      "epoch": 0.1798168221157886,
      "grad_norm": 5.9332990646362305,
      "learning_rate": 9.55729020685016e-05,
      "loss": 0.6945,
      "step": 3210
    },
    {
      "epoch": 0.1803769991317256,
      "grad_norm": 5.255030155181885,
      "learning_rate": 9.555888783003532e-05,
      "loss": 0.7127,
      "step": 3220
    },
    {
      "epoch": 0.18093717614766266,
      "grad_norm": 5.854579925537109,
      "learning_rate": 9.554487359156904e-05,
      "loss": 0.691,
      "step": 3230
    },
    {
      "epoch": 0.1814973531635997,
      "grad_norm": 6.19287633895874,
      "learning_rate": 9.553085935310276e-05,
      "loss": 0.6704,
      "step": 3240
    },
    {
      "epoch": 0.18205753017953674,
      "grad_norm": 5.655822277069092,
      "learning_rate": 9.551684511463647e-05,
      "loss": 0.8443,
      "step": 3250
    },
    {
      "epoch": 0.18261770719547377,
      "grad_norm": 6.71558952331543,
      "learning_rate": 9.550283087617019e-05,
      "loss": 0.5608,
      "step": 3260
    },
    {
      "epoch": 0.18317788421141082,
      "grad_norm": 5.94559383392334,
      "learning_rate": 9.548881663770392e-05,
      "loss": 0.4887,
      "step": 3270
    },
    {
      "epoch": 0.18373806122734784,
      "grad_norm": 8.250198364257812,
      "learning_rate": 9.547480239923763e-05,
      "loss": 0.6823,
      "step": 3280
    },
    {
      "epoch": 0.18429823824328487,
      "grad_norm": 5.656453609466553,
      "learning_rate": 9.546078816077136e-05,
      "loss": 0.6305,
      "step": 3290
    },
    {
      "epoch": 0.18485841525922192,
      "grad_norm": 6.196000099182129,
      "learning_rate": 9.544677392230506e-05,
      "loss": 0.6591,
      "step": 3300
    },
    {
      "epoch": 0.18541859227515894,
      "grad_norm": 5.135025501251221,
      "learning_rate": 9.543275968383878e-05,
      "loss": 0.4491,
      "step": 3310
    },
    {
      "epoch": 0.185978769291096,
      "grad_norm": 7.53110408782959,
      "learning_rate": 9.54187454453725e-05,
      "loss": 0.7727,
      "step": 3320
    },
    {
      "epoch": 0.18653894630703302,
      "grad_norm": 4.297775745391846,
      "learning_rate": 9.540473120690622e-05,
      "loss": 0.6653,
      "step": 3330
    },
    {
      "epoch": 0.18709912332297005,
      "grad_norm": 5.541874408721924,
      "learning_rate": 9.539071696843995e-05,
      "loss": 0.6053,
      "step": 3340
    },
    {
      "epoch": 0.1876593003389071,
      "grad_norm": 4.761140823364258,
      "learning_rate": 9.537670272997366e-05,
      "loss": 0.6707,
      "step": 3350
    },
    {
      "epoch": 0.18821947735484412,
      "grad_norm": 3.77854061126709,
      "learning_rate": 9.536268849150738e-05,
      "loss": 0.486,
      "step": 3360
    },
    {
      "epoch": 0.18877965437078117,
      "grad_norm": 5.13075065612793,
      "learning_rate": 9.534867425304109e-05,
      "loss": 0.8676,
      "step": 3370
    },
    {
      "epoch": 0.1893398313867182,
      "grad_norm": 8.841245651245117,
      "learning_rate": 9.533466001457482e-05,
      "loss": 0.7145,
      "step": 3380
    },
    {
      "epoch": 0.18990000840265525,
      "grad_norm": 6.603890895843506,
      "learning_rate": 9.532064577610853e-05,
      "loss": 0.738,
      "step": 3390
    },
    {
      "epoch": 0.19046018541859228,
      "grad_norm": 5.570810317993164,
      "learning_rate": 9.530663153764225e-05,
      "loss": 0.7563,
      "step": 3400
    },
    {
      "epoch": 0.1910203624345293,
      "grad_norm": 6.158350944519043,
      "learning_rate": 9.529261729917596e-05,
      "loss": 0.586,
      "step": 3410
    },
    {
      "epoch": 0.19158053945046635,
      "grad_norm": 7.65217924118042,
      "learning_rate": 9.527860306070968e-05,
      "loss": 0.8516,
      "step": 3420
    },
    {
      "epoch": 0.19214071646640338,
      "grad_norm": 4.993403434753418,
      "learning_rate": 9.52645888222434e-05,
      "loss": 0.7944,
      "step": 3430
    },
    {
      "epoch": 0.19270089348234043,
      "grad_norm": 3.9803621768951416,
      "learning_rate": 9.525057458377712e-05,
      "loss": 0.699,
      "step": 3440
    },
    {
      "epoch": 0.19326107049827745,
      "grad_norm": 5.9004316329956055,
      "learning_rate": 9.523656034531085e-05,
      "loss": 0.7218,
      "step": 3450
    },
    {
      "epoch": 0.19382124751421448,
      "grad_norm": 7.095420837402344,
      "learning_rate": 9.522254610684455e-05,
      "loss": 0.9347,
      "step": 3460
    },
    {
      "epoch": 0.19438142453015153,
      "grad_norm": 6.117526531219482,
      "learning_rate": 9.520853186837828e-05,
      "loss": 0.6519,
      "step": 3470
    },
    {
      "epoch": 0.19494160154608856,
      "grad_norm": 4.910381317138672,
      "learning_rate": 9.5194517629912e-05,
      "loss": 0.668,
      "step": 3480
    },
    {
      "epoch": 0.1955017785620256,
      "grad_norm": 6.550877571105957,
      "learning_rate": 9.518050339144572e-05,
      "loss": 0.5638,
      "step": 3490
    },
    {
      "epoch": 0.19606195557796263,
      "grad_norm": 6.472121238708496,
      "learning_rate": 9.516648915297944e-05,
      "loss": 0.7853,
      "step": 3500
    },
    {
      "epoch": 0.19662213259389968,
      "grad_norm": 8.87335205078125,
      "learning_rate": 9.515247491451315e-05,
      "loss": 0.6371,
      "step": 3510
    },
    {
      "epoch": 0.1971823096098367,
      "grad_norm": 4.683090686798096,
      "learning_rate": 9.513846067604687e-05,
      "loss": 0.6393,
      "step": 3520
    },
    {
      "epoch": 0.19774248662577373,
      "grad_norm": 4.329620838165283,
      "learning_rate": 9.512444643758058e-05,
      "loss": 0.6348,
      "step": 3530
    },
    {
      "epoch": 0.19830266364171079,
      "grad_norm": 5.033609867095947,
      "learning_rate": 9.511043219911431e-05,
      "loss": 0.8169,
      "step": 3540
    },
    {
      "epoch": 0.1988628406576478,
      "grad_norm": 4.399466514587402,
      "learning_rate": 9.509641796064802e-05,
      "loss": 0.73,
      "step": 3550
    },
    {
      "epoch": 0.19942301767358486,
      "grad_norm": 5.132119178771973,
      "learning_rate": 9.508240372218174e-05,
      "loss": 0.5362,
      "step": 3560
    },
    {
      "epoch": 0.1999831946895219,
      "grad_norm": 6.689167499542236,
      "learning_rate": 9.506838948371545e-05,
      "loss": 0.6826,
      "step": 3570
    },
    {
      "epoch": 0.2005433717054589,
      "grad_norm": 6.200316905975342,
      "learning_rate": 9.505437524524918e-05,
      "loss": 0.7315,
      "step": 3580
    },
    {
      "epoch": 0.20110354872139596,
      "grad_norm": 4.501186847686768,
      "learning_rate": 9.50403610067829e-05,
      "loss": 0.5711,
      "step": 3590
    },
    {
      "epoch": 0.201663725737333,
      "grad_norm": 6.980241775512695,
      "learning_rate": 9.502634676831661e-05,
      "loss": 0.8252,
      "step": 3600
    },
    {
      "epoch": 0.20222390275327004,
      "grad_norm": 5.730686187744141,
      "learning_rate": 9.501233252985034e-05,
      "loss": 0.5731,
      "step": 3610
    },
    {
      "epoch": 0.20278407976920707,
      "grad_norm": 5.934609889984131,
      "learning_rate": 9.499831829138405e-05,
      "loss": 0.5702,
      "step": 3620
    },
    {
      "epoch": 0.20334425678514412,
      "grad_norm": 6.2403740882873535,
      "learning_rate": 9.498430405291777e-05,
      "loss": 0.5884,
      "step": 3630
    },
    {
      "epoch": 0.20390443380108114,
      "grad_norm": 4.341139793395996,
      "learning_rate": 9.497028981445148e-05,
      "loss": 0.5559,
      "step": 3640
    },
    {
      "epoch": 0.20446461081701817,
      "grad_norm": 4.897307872772217,
      "learning_rate": 9.495627557598521e-05,
      "loss": 0.755,
      "step": 3650
    },
    {
      "epoch": 0.20502478783295522,
      "grad_norm": 4.510342121124268,
      "learning_rate": 9.494226133751893e-05,
      "loss": 0.7434,
      "step": 3660
    },
    {
      "epoch": 0.20558496484889224,
      "grad_norm": 4.934434413909912,
      "learning_rate": 9.492824709905264e-05,
      "loss": 0.6548,
      "step": 3670
    },
    {
      "epoch": 0.2061451418648293,
      "grad_norm": 7.054708957672119,
      "learning_rate": 9.491423286058636e-05,
      "loss": 0.5235,
      "step": 3680
    },
    {
      "epoch": 0.20670531888076632,
      "grad_norm": 6.181090354919434,
      "learning_rate": 9.490021862212007e-05,
      "loss": 0.4287,
      "step": 3690
    },
    {
      "epoch": 0.20726549589670334,
      "grad_norm": 6.160691738128662,
      "learning_rate": 9.48862043836538e-05,
      "loss": 0.552,
      "step": 3700
    },
    {
      "epoch": 0.2078256729126404,
      "grad_norm": 5.203263282775879,
      "learning_rate": 9.487219014518751e-05,
      "loss": 0.6682,
      "step": 3710
    },
    {
      "epoch": 0.20838584992857742,
      "grad_norm": 6.826504230499268,
      "learning_rate": 9.485817590672124e-05,
      "loss": 0.6894,
      "step": 3720
    },
    {
      "epoch": 0.20894602694451447,
      "grad_norm": 5.193792343139648,
      "learning_rate": 9.484416166825494e-05,
      "loss": 0.8262,
      "step": 3730
    },
    {
      "epoch": 0.2095062039604515,
      "grad_norm": 5.98356294631958,
      "learning_rate": 9.483014742978867e-05,
      "loss": 0.667,
      "step": 3740
    },
    {
      "epoch": 0.21006638097638855,
      "grad_norm": 5.526452541351318,
      "learning_rate": 9.481613319132239e-05,
      "loss": 0.6017,
      "step": 3750
    },
    {
      "epoch": 0.21062655799232557,
      "grad_norm": 4.106739044189453,
      "learning_rate": 9.480211895285611e-05,
      "loss": 0.5016,
      "step": 3760
    },
    {
      "epoch": 0.2111867350082626,
      "grad_norm": 7.40471076965332,
      "learning_rate": 9.478810471438983e-05,
      "loss": 0.6443,
      "step": 3770
    },
    {
      "epoch": 0.21174691202419965,
      "grad_norm": 6.675721645355225,
      "learning_rate": 9.477409047592354e-05,
      "loss": 0.7393,
      "step": 3780
    },
    {
      "epoch": 0.21230708904013668,
      "grad_norm": 4.215205669403076,
      "learning_rate": 9.476007623745726e-05,
      "loss": 0.7227,
      "step": 3790
    },
    {
      "epoch": 0.21286726605607373,
      "grad_norm": 7.558437347412109,
      "learning_rate": 9.474606199899097e-05,
      "loss": 0.6617,
      "step": 3800
    },
    {
      "epoch": 0.21342744307201075,
      "grad_norm": 5.037989139556885,
      "learning_rate": 9.47320477605247e-05,
      "loss": 0.7366,
      "step": 3810
    },
    {
      "epoch": 0.2139876200879478,
      "grad_norm": 4.892114162445068,
      "learning_rate": 9.471803352205842e-05,
      "loss": 0.5302,
      "step": 3820
    },
    {
      "epoch": 0.21454779710388483,
      "grad_norm": 5.007972717285156,
      "learning_rate": 9.470401928359213e-05,
      "loss": 0.4404,
      "step": 3830
    },
    {
      "epoch": 0.21510797411982185,
      "grad_norm": 5.885674953460693,
      "learning_rate": 9.469000504512585e-05,
      "loss": 0.8755,
      "step": 3840
    },
    {
      "epoch": 0.2156681511357589,
      "grad_norm": 4.084500312805176,
      "learning_rate": 9.467599080665957e-05,
      "loss": 0.5432,
      "step": 3850
    },
    {
      "epoch": 0.21622832815169593,
      "grad_norm": 5.98789644241333,
      "learning_rate": 9.466197656819329e-05,
      "loss": 0.6609,
      "step": 3860
    },
    {
      "epoch": 0.21678850516763298,
      "grad_norm": 6.049936771392822,
      "learning_rate": 9.4647962329727e-05,
      "loss": 0.6711,
      "step": 3870
    },
    {
      "epoch": 0.21734868218357,
      "grad_norm": 6.3577446937561035,
      "learning_rate": 9.463394809126073e-05,
      "loss": 0.6055,
      "step": 3880
    },
    {
      "epoch": 0.21790885919950703,
      "grad_norm": 6.128110885620117,
      "learning_rate": 9.461993385279443e-05,
      "loss": 0.7125,
      "step": 3890
    },
    {
      "epoch": 0.21846903621544408,
      "grad_norm": 7.553182601928711,
      "learning_rate": 9.460591961432816e-05,
      "loss": 0.486,
      "step": 3900
    },
    {
      "epoch": 0.2190292132313811,
      "grad_norm": 5.643334865570068,
      "learning_rate": 9.459190537586188e-05,
      "loss": 0.4626,
      "step": 3910
    },
    {
      "epoch": 0.21958939024731816,
      "grad_norm": 5.728572368621826,
      "learning_rate": 9.45778911373956e-05,
      "loss": 0.7002,
      "step": 3920
    },
    {
      "epoch": 0.22014956726325519,
      "grad_norm": 7.520909786224365,
      "learning_rate": 9.456387689892932e-05,
      "loss": 0.59,
      "step": 3930
    },
    {
      "epoch": 0.22070974427919224,
      "grad_norm": 3.888789415359497,
      "learning_rate": 9.454986266046303e-05,
      "loss": 0.4474,
      "step": 3940
    },
    {
      "epoch": 0.22126992129512926,
      "grad_norm": 7.370704174041748,
      "learning_rate": 9.453584842199675e-05,
      "loss": 0.6188,
      "step": 3950
    },
    {
      "epoch": 0.2218300983110663,
      "grad_norm": 5.15719747543335,
      "learning_rate": 9.452183418353046e-05,
      "loss": 0.5974,
      "step": 3960
    },
    {
      "epoch": 0.22239027532700334,
      "grad_norm": 5.95607852935791,
      "learning_rate": 9.450781994506419e-05,
      "loss": 0.4675,
      "step": 3970
    },
    {
      "epoch": 0.22295045234294036,
      "grad_norm": 6.821089744567871,
      "learning_rate": 9.449380570659791e-05,
      "loss": 0.7272,
      "step": 3980
    },
    {
      "epoch": 0.22351062935887742,
      "grad_norm": 6.972287654876709,
      "learning_rate": 9.447979146813164e-05,
      "loss": 0.6428,
      "step": 3990
    },
    {
      "epoch": 0.22407080637481444,
      "grad_norm": 5.967155933380127,
      "learning_rate": 9.446577722966534e-05,
      "loss": 0.7598,
      "step": 4000
    },
    {
      "epoch": 0.22463098339075147,
      "grad_norm": 6.604753017425537,
      "learning_rate": 9.445176299119906e-05,
      "loss": 0.4661,
      "step": 4010
    },
    {
      "epoch": 0.22519116040668852,
      "grad_norm": 5.581344127655029,
      "learning_rate": 9.443774875273278e-05,
      "loss": 0.5824,
      "step": 4020
    },
    {
      "epoch": 0.22575133742262554,
      "grad_norm": 5.757679462432861,
      "learning_rate": 9.442373451426651e-05,
      "loss": 0.6866,
      "step": 4030
    },
    {
      "epoch": 0.2263115144385626,
      "grad_norm": 4.84496545791626,
      "learning_rate": 9.440972027580022e-05,
      "loss": 0.6239,
      "step": 4040
    },
    {
      "epoch": 0.22687169145449962,
      "grad_norm": 6.706318378448486,
      "learning_rate": 9.439570603733394e-05,
      "loss": 0.6862,
      "step": 4050
    },
    {
      "epoch": 0.22743186847043667,
      "grad_norm": 7.071230411529541,
      "learning_rate": 9.438169179886765e-05,
      "loss": 0.5386,
      "step": 4060
    },
    {
      "epoch": 0.2279920454863737,
      "grad_norm": 7.466179370880127,
      "learning_rate": 9.436767756040137e-05,
      "loss": 0.7046,
      "step": 4070
    },
    {
      "epoch": 0.22855222250231072,
      "grad_norm": 5.719091415405273,
      "learning_rate": 9.43536633219351e-05,
      "loss": 0.6768,
      "step": 4080
    },
    {
      "epoch": 0.22911239951824777,
      "grad_norm": 4.801708221435547,
      "learning_rate": 9.433964908346881e-05,
      "loss": 0.6571,
      "step": 4090
    },
    {
      "epoch": 0.2296725765341848,
      "grad_norm": 7.023825168609619,
      "learning_rate": 9.432563484500252e-05,
      "loss": 0.4852,
      "step": 4100
    },
    {
      "epoch": 0.23023275355012185,
      "grad_norm": 5.829925537109375,
      "learning_rate": 9.431162060653624e-05,
      "loss": 0.5715,
      "step": 4110
    },
    {
      "epoch": 0.23079293056605887,
      "grad_norm": 4.635905742645264,
      "learning_rate": 9.429760636806997e-05,
      "loss": 0.4638,
      "step": 4120
    },
    {
      "epoch": 0.2313531075819959,
      "grad_norm": 4.281765937805176,
      "learning_rate": 9.428359212960368e-05,
      "loss": 0.6111,
      "step": 4130
    },
    {
      "epoch": 0.23191328459793295,
      "grad_norm": 4.962350845336914,
      "learning_rate": 9.42695778911374e-05,
      "loss": 0.663,
      "step": 4140
    },
    {
      "epoch": 0.23247346161386997,
      "grad_norm": 3.9035115242004395,
      "learning_rate": 9.425556365267113e-05,
      "loss": 0.5249,
      "step": 4150
    },
    {
      "epoch": 0.23303363862980703,
      "grad_norm": 6.841507911682129,
      "learning_rate": 9.424154941420483e-05,
      "loss": 0.6391,
      "step": 4160
    },
    {
      "epoch": 0.23359381564574405,
      "grad_norm": 5.575904369354248,
      "learning_rate": 9.422753517573855e-05,
      "loss": 0.8737,
      "step": 4170
    },
    {
      "epoch": 0.2341539926616811,
      "grad_norm": 5.7580246925354,
      "learning_rate": 9.421352093727227e-05,
      "loss": 0.707,
      "step": 4180
    },
    {
      "epoch": 0.23471416967761813,
      "grad_norm": 6.022554874420166,
      "learning_rate": 9.4199506698806e-05,
      "loss": 0.6712,
      "step": 4190
    },
    {
      "epoch": 0.23527434669355515,
      "grad_norm": 6.558125972747803,
      "learning_rate": 9.418549246033971e-05,
      "loss": 0.4936,
      "step": 4200
    },
    {
      "epoch": 0.2358345237094922,
      "grad_norm": 5.571966648101807,
      "learning_rate": 9.417147822187343e-05,
      "loss": 0.6809,
      "step": 4210
    },
    {
      "epoch": 0.23639470072542923,
      "grad_norm": 7.962458610534668,
      "learning_rate": 9.415746398340714e-05,
      "loss": 0.8324,
      "step": 4220
    },
    {
      "epoch": 0.23695487774136628,
      "grad_norm": 4.523128986358643,
      "learning_rate": 9.414344974494086e-05,
      "loss": 0.5112,
      "step": 4230
    },
    {
      "epoch": 0.2375150547573033,
      "grad_norm": 5.202476501464844,
      "learning_rate": 9.412943550647459e-05,
      "loss": 0.6873,
      "step": 4240
    },
    {
      "epoch": 0.23807523177324033,
      "grad_norm": 5.459936618804932,
      "learning_rate": 9.41154212680083e-05,
      "loss": 0.5072,
      "step": 4250
    },
    {
      "epoch": 0.23863540878917738,
      "grad_norm": 6.260300159454346,
      "learning_rate": 9.410140702954201e-05,
      "loss": 0.5536,
      "step": 4260
    },
    {
      "epoch": 0.2391955858051144,
      "grad_norm": 6.623355388641357,
      "learning_rate": 9.408739279107573e-05,
      "loss": 0.5904,
      "step": 4270
    },
    {
      "epoch": 0.23975576282105146,
      "grad_norm": 6.593900203704834,
      "learning_rate": 9.407337855260946e-05,
      "loss": 0.5088,
      "step": 4280
    },
    {
      "epoch": 0.24031593983698848,
      "grad_norm": 4.577538967132568,
      "learning_rate": 9.405936431414317e-05,
      "loss": 0.5683,
      "step": 4290
    },
    {
      "epoch": 0.24087611685292554,
      "grad_norm": 7.108421325683594,
      "learning_rate": 9.40453500756769e-05,
      "loss": 0.7979,
      "step": 4300
    },
    {
      "epoch": 0.24143629386886256,
      "grad_norm": 6.706874847412109,
      "learning_rate": 9.403133583721062e-05,
      "loss": 0.5326,
      "step": 4310
    },
    {
      "epoch": 0.24199647088479959,
      "grad_norm": 7.148072242736816,
      "learning_rate": 9.401732159874433e-05,
      "loss": 0.7084,
      "step": 4320
    },
    {
      "epoch": 0.24255664790073664,
      "grad_norm": 6.0903096199035645,
      "learning_rate": 9.400330736027804e-05,
      "loss": 0.644,
      "step": 4330
    },
    {
      "epoch": 0.24311682491667366,
      "grad_norm": 4.438959121704102,
      "learning_rate": 9.398929312181176e-05,
      "loss": 0.615,
      "step": 4340
    },
    {
      "epoch": 0.24367700193261072,
      "grad_norm": 4.979217052459717,
      "learning_rate": 9.397527888334549e-05,
      "loss": 0.4875,
      "step": 4350
    },
    {
      "epoch": 0.24423717894854774,
      "grad_norm": 4.254646301269531,
      "learning_rate": 9.39612646448792e-05,
      "loss": 0.5981,
      "step": 4360
    },
    {
      "epoch": 0.24479735596448476,
      "grad_norm": 6.6684651374816895,
      "learning_rate": 9.394725040641292e-05,
      "loss": 0.5214,
      "step": 4370
    },
    {
      "epoch": 0.24535753298042182,
      "grad_norm": 4.706252574920654,
      "learning_rate": 9.393323616794663e-05,
      "loss": 0.6264,
      "step": 4380
    },
    {
      "epoch": 0.24591770999635884,
      "grad_norm": 5.430851459503174,
      "learning_rate": 9.391922192948036e-05,
      "loss": 0.7384,
      "step": 4390
    },
    {
      "epoch": 0.2464778870122959,
      "grad_norm": 5.241798400878906,
      "learning_rate": 9.390520769101408e-05,
      "loss": 0.3869,
      "step": 4400
    },
    {
      "epoch": 0.24703806402823292,
      "grad_norm": 5.597668647766113,
      "learning_rate": 9.389119345254779e-05,
      "loss": 0.5566,
      "step": 4410
    },
    {
      "epoch": 0.24759824104416997,
      "grad_norm": 4.390691757202148,
      "learning_rate": 9.387717921408152e-05,
      "loss": 0.6011,
      "step": 4420
    },
    {
      "epoch": 0.248158418060107,
      "grad_norm": 4.65843391418457,
      "learning_rate": 9.386316497561522e-05,
      "loss": 0.6776,
      "step": 4430
    },
    {
      "epoch": 0.24871859507604402,
      "grad_norm": 6.005733966827393,
      "learning_rate": 9.384915073714895e-05,
      "loss": 0.6181,
      "step": 4440
    },
    {
      "epoch": 0.24927877209198107,
      "grad_norm": 6.1999688148498535,
      "learning_rate": 9.383513649868266e-05,
      "loss": 0.5573,
      "step": 4450
    },
    {
      "epoch": 0.2498389491079181,
      "grad_norm": 7.290282726287842,
      "learning_rate": 9.382112226021639e-05,
      "loss": 0.6305,
      "step": 4460
    },
    {
      "epoch": 0.2503991261238551,
      "grad_norm": 5.8329057693481445,
      "learning_rate": 9.38071080217501e-05,
      "loss": 0.5501,
      "step": 4470
    },
    {
      "epoch": 0.2509593031397922,
      "grad_norm": 7.541443824768066,
      "learning_rate": 9.379309378328382e-05,
      "loss": 0.5224,
      "step": 4480
    },
    {
      "epoch": 0.2515194801557292,
      "grad_norm": 3.5008089542388916,
      "learning_rate": 9.377907954481754e-05,
      "loss": 0.6551,
      "step": 4490
    },
    {
      "epoch": 0.25207965717166625,
      "grad_norm": 3.490020513534546,
      "learning_rate": 9.376506530635125e-05,
      "loss": 0.5845,
      "step": 4500
    },
    {
      "epoch": 0.2526398341876033,
      "grad_norm": 6.208795547485352,
      "learning_rate": 9.375105106788498e-05,
      "loss": 0.7159,
      "step": 4510
    },
    {
      "epoch": 0.2532000112035403,
      "grad_norm": 6.224464416503906,
      "learning_rate": 9.373703682941869e-05,
      "loss": 0.5784,
      "step": 4520
    },
    {
      "epoch": 0.2537601882194774,
      "grad_norm": 9.049826622009277,
      "learning_rate": 9.372302259095241e-05,
      "loss": 0.5474,
      "step": 4530
    },
    {
      "epoch": 0.2543203652354144,
      "grad_norm": 4.812115669250488,
      "learning_rate": 9.370900835248612e-05,
      "loss": 0.5212,
      "step": 4540
    },
    {
      "epoch": 0.2548805422513514,
      "grad_norm": 7.7862443923950195,
      "learning_rate": 9.369499411401985e-05,
      "loss": 0.6116,
      "step": 4550
    },
    {
      "epoch": 0.25544071926728845,
      "grad_norm": 4.644922733306885,
      "learning_rate": 9.368097987555357e-05,
      "loss": 0.5685,
      "step": 4560
    },
    {
      "epoch": 0.2560008962832255,
      "grad_norm": 6.351377010345459,
      "learning_rate": 9.36669656370873e-05,
      "loss": 0.4945,
      "step": 4570
    },
    {
      "epoch": 0.25656107329916256,
      "grad_norm": 4.841292858123779,
      "learning_rate": 9.365295139862101e-05,
      "loss": 0.4506,
      "step": 4580
    },
    {
      "epoch": 0.2571212503150996,
      "grad_norm": 5.625032424926758,
      "learning_rate": 9.363893716015472e-05,
      "loss": 0.5815,
      "step": 4590
    },
    {
      "epoch": 0.2576814273310366,
      "grad_norm": 7.007001876831055,
      "learning_rate": 9.362492292168844e-05,
      "loss": 0.5499,
      "step": 4600
    },
    {
      "epoch": 0.25824160434697363,
      "grad_norm": 5.34930944442749,
      "learning_rate": 9.361090868322215e-05,
      "loss": 0.6025,
      "step": 4610
    },
    {
      "epoch": 0.25880178136291065,
      "grad_norm": 4.562114715576172,
      "learning_rate": 9.359689444475588e-05,
      "loss": 0.6983,
      "step": 4620
    },
    {
      "epoch": 0.25936195837884773,
      "grad_norm": 4.777335166931152,
      "learning_rate": 9.35828802062896e-05,
      "loss": 0.6074,
      "step": 4630
    },
    {
      "epoch": 0.25992213539478476,
      "grad_norm": 4.329226970672607,
      "learning_rate": 9.356886596782331e-05,
      "loss": 0.4341,
      "step": 4640
    },
    {
      "epoch": 0.2604823124107218,
      "grad_norm": 7.085482597351074,
      "learning_rate": 9.355485172935703e-05,
      "loss": 0.6869,
      "step": 4650
    },
    {
      "epoch": 0.2610424894266588,
      "grad_norm": 5.398390293121338,
      "learning_rate": 9.354083749089075e-05,
      "loss": 0.4766,
      "step": 4660
    },
    {
      "epoch": 0.26160266644259583,
      "grad_norm": 7.223499298095703,
      "learning_rate": 9.352682325242447e-05,
      "loss": 0.591,
      "step": 4670
    },
    {
      "epoch": 0.2621628434585329,
      "grad_norm": 5.796248912811279,
      "learning_rate": 9.35128090139582e-05,
      "loss": 0.5374,
      "step": 4680
    },
    {
      "epoch": 0.26272302047446994,
      "grad_norm": 4.052753925323486,
      "learning_rate": 9.349879477549191e-05,
      "loss": 0.4094,
      "step": 4690
    },
    {
      "epoch": 0.26328319749040696,
      "grad_norm": 6.190073013305664,
      "learning_rate": 9.348478053702561e-05,
      "loss": 0.4248,
      "step": 4700
    },
    {
      "epoch": 0.263843374506344,
      "grad_norm": 8.829206466674805,
      "learning_rate": 9.347076629855934e-05,
      "loss": 0.5058,
      "step": 4710
    },
    {
      "epoch": 0.26440355152228107,
      "grad_norm": 10.229350090026855,
      "learning_rate": 9.345675206009306e-05,
      "loss": 0.3753,
      "step": 4720
    },
    {
      "epoch": 0.2649637285382181,
      "grad_norm": 4.688973903656006,
      "learning_rate": 9.344273782162678e-05,
      "loss": 0.5878,
      "step": 4730
    },
    {
      "epoch": 0.2655239055541551,
      "grad_norm": 3.417825937271118,
      "learning_rate": 9.34287235831605e-05,
      "loss": 0.6337,
      "step": 4740
    },
    {
      "epoch": 0.26608408257009214,
      "grad_norm": 4.1097211837768555,
      "learning_rate": 9.341470934469421e-05,
      "loss": 0.3696,
      "step": 4750
    },
    {
      "epoch": 0.26664425958602916,
      "grad_norm": 5.457359790802002,
      "learning_rate": 9.340069510622793e-05,
      "loss": 0.6018,
      "step": 4760
    },
    {
      "epoch": 0.26720443660196624,
      "grad_norm": 6.396565914154053,
      "learning_rate": 9.338668086776166e-05,
      "loss": 0.564,
      "step": 4770
    },
    {
      "epoch": 0.26776461361790327,
      "grad_norm": 6.608458995819092,
      "learning_rate": 9.337266662929537e-05,
      "loss": 0.6291,
      "step": 4780
    },
    {
      "epoch": 0.2683247906338403,
      "grad_norm": 4.47183084487915,
      "learning_rate": 9.335865239082909e-05,
      "loss": 0.5907,
      "step": 4790
    },
    {
      "epoch": 0.2688849676497773,
      "grad_norm": 4.930412292480469,
      "learning_rate": 9.33446381523628e-05,
      "loss": 0.488,
      "step": 4800
    },
    {
      "epoch": 0.26944514466571434,
      "grad_norm": 4.955471992492676,
      "learning_rate": 9.333062391389652e-05,
      "loss": 0.4385,
      "step": 4810
    },
    {
      "epoch": 0.2700053216816514,
      "grad_norm": 4.5622358322143555,
      "learning_rate": 9.331660967543024e-05,
      "loss": 0.4225,
      "step": 4820
    },
    {
      "epoch": 0.27056549869758845,
      "grad_norm": 4.854282379150391,
      "learning_rate": 9.330259543696396e-05,
      "loss": 0.4068,
      "step": 4830
    },
    {
      "epoch": 0.27112567571352547,
      "grad_norm": 4.149511337280273,
      "learning_rate": 9.328858119849769e-05,
      "loss": 0.652,
      "step": 4840
    },
    {
      "epoch": 0.2716858527294625,
      "grad_norm": 7.588200569152832,
      "learning_rate": 9.32745669600314e-05,
      "loss": 0.6774,
      "step": 4850
    },
    {
      "epoch": 0.2722460297453995,
      "grad_norm": 4.789336681365967,
      "learning_rate": 9.326055272156512e-05,
      "loss": 0.7091,
      "step": 4860
    },
    {
      "epoch": 0.2728062067613366,
      "grad_norm": 4.670085430145264,
      "learning_rate": 9.324653848309883e-05,
      "loss": 0.4802,
      "step": 4870
    },
    {
      "epoch": 0.2733663837772736,
      "grad_norm": 5.4863362312316895,
      "learning_rate": 9.323252424463255e-05,
      "loss": 0.4134,
      "step": 4880
    },
    {
      "epoch": 0.27392656079321065,
      "grad_norm": 2.873448610305786,
      "learning_rate": 9.321851000616627e-05,
      "loss": 0.6596,
      "step": 4890
    },
    {
      "epoch": 0.2744867378091477,
      "grad_norm": 10.59245491027832,
      "learning_rate": 9.320449576769999e-05,
      "loss": 0.5777,
      "step": 4900
    },
    {
      "epoch": 0.27504691482508475,
      "grad_norm": 8.713812828063965,
      "learning_rate": 9.31904815292337e-05,
      "loss": 0.4602,
      "step": 4910
    },
    {
      "epoch": 0.2756070918410218,
      "grad_norm": 7.232385635375977,
      "learning_rate": 9.317646729076742e-05,
      "loss": 0.5888,
      "step": 4920
    },
    {
      "epoch": 0.2761672688569588,
      "grad_norm": 5.042770862579346,
      "learning_rate": 9.316245305230115e-05,
      "loss": 0.5501,
      "step": 4930
    },
    {
      "epoch": 0.2767274458728958,
      "grad_norm": 6.53922700881958,
      "learning_rate": 9.314843881383486e-05,
      "loss": 0.5848,
      "step": 4940
    },
    {
      "epoch": 0.27728762288883285,
      "grad_norm": 3.9975197315216064,
      "learning_rate": 9.313442457536859e-05,
      "loss": 0.5588,
      "step": 4950
    },
    {
      "epoch": 0.27784779990476993,
      "grad_norm": 6.912024021148682,
      "learning_rate": 9.312041033690229e-05,
      "loss": 0.5119,
      "step": 4960
    },
    {
      "epoch": 0.27840797692070696,
      "grad_norm": 3.5940961837768555,
      "learning_rate": 9.3106396098436e-05,
      "loss": 0.3672,
      "step": 4970
    },
    {
      "epoch": 0.278968153936644,
      "grad_norm": 4.435728073120117,
      "learning_rate": 9.309238185996973e-05,
      "loss": 0.7615,
      "step": 4980
    },
    {
      "epoch": 0.279528330952581,
      "grad_norm": 5.688167095184326,
      "learning_rate": 9.307836762150345e-05,
      "loss": 0.4939,
      "step": 4990
    },
    {
      "epoch": 0.28008850796851803,
      "grad_norm": 5.809770107269287,
      "learning_rate": 9.306435338303718e-05,
      "loss": 0.5366,
      "step": 5000
    },
    {
      "epoch": 0.2806486849844551,
      "grad_norm": 4.846129894256592,
      "learning_rate": 9.305033914457089e-05,
      "loss": 0.4965,
      "step": 5010
    },
    {
      "epoch": 0.28120886200039213,
      "grad_norm": 8.422286987304688,
      "learning_rate": 9.30363249061046e-05,
      "loss": 0.4233,
      "step": 5020
    },
    {
      "epoch": 0.28176903901632916,
      "grad_norm": 5.3484978675842285,
      "learning_rate": 9.302231066763832e-05,
      "loss": 0.5566,
      "step": 5030
    },
    {
      "epoch": 0.2823292160322662,
      "grad_norm": 6.03839111328125,
      "learning_rate": 9.300829642917205e-05,
      "loss": 0.6051,
      "step": 5040
    },
    {
      "epoch": 0.2828893930482032,
      "grad_norm": 7.3912672996521,
      "learning_rate": 9.299428219070576e-05,
      "loss": 0.5762,
      "step": 5050
    },
    {
      "epoch": 0.2834495700641403,
      "grad_norm": 5.606418132781982,
      "learning_rate": 9.298026795223948e-05,
      "loss": 0.6099,
      "step": 5060
    },
    {
      "epoch": 0.2840097470800773,
      "grad_norm": 4.767765045166016,
      "learning_rate": 9.29662537137732e-05,
      "loss": 0.4526,
      "step": 5070
    },
    {
      "epoch": 0.28456992409601434,
      "grad_norm": 4.7057085037231445,
      "learning_rate": 9.295223947530691e-05,
      "loss": 0.4462,
      "step": 5080
    },
    {
      "epoch": 0.28513010111195136,
      "grad_norm": 6.961928367614746,
      "learning_rate": 9.293822523684064e-05,
      "loss": 0.4773,
      "step": 5090
    },
    {
      "epoch": 0.2856902781278884,
      "grad_norm": 5.969535827636719,
      "learning_rate": 9.292421099837435e-05,
      "loss": 0.6426,
      "step": 5100
    },
    {
      "epoch": 0.28625045514382547,
      "grad_norm": 5.047726631164551,
      "learning_rate": 9.291019675990808e-05,
      "loss": 0.467,
      "step": 5110
    },
    {
      "epoch": 0.2868106321597625,
      "grad_norm": 4.722653388977051,
      "learning_rate": 9.28961825214418e-05,
      "loss": 0.3692,
      "step": 5120
    },
    {
      "epoch": 0.2873708091756995,
      "grad_norm": 3.6693506240844727,
      "learning_rate": 9.288216828297551e-05,
      "loss": 0.5284,
      "step": 5130
    },
    {
      "epoch": 0.28793098619163654,
      "grad_norm": 6.326022624969482,
      "learning_rate": 9.286815404450922e-05,
      "loss": 0.5396,
      "step": 5140
    },
    {
      "epoch": 0.2884911632075736,
      "grad_norm": 4.814908981323242,
      "learning_rate": 9.285413980604294e-05,
      "loss": 0.3568,
      "step": 5150
    },
    {
      "epoch": 0.28905134022351064,
      "grad_norm": 6.738729476928711,
      "learning_rate": 9.284012556757667e-05,
      "loss": 0.5251,
      "step": 5160
    },
    {
      "epoch": 0.28961151723944767,
      "grad_norm": 5.34044885635376,
      "learning_rate": 9.282611132911038e-05,
      "loss": 0.5781,
      "step": 5170
    },
    {
      "epoch": 0.2901716942553847,
      "grad_norm": 6.3639020919799805,
      "learning_rate": 9.28120970906441e-05,
      "loss": 0.6744,
      "step": 5180
    },
    {
      "epoch": 0.2907318712713217,
      "grad_norm": 4.721602439880371,
      "learning_rate": 9.279808285217781e-05,
      "loss": 0.4695,
      "step": 5190
    },
    {
      "epoch": 0.2912920482872588,
      "grad_norm": 7.801524639129639,
      "learning_rate": 9.278406861371154e-05,
      "loss": 0.6,
      "step": 5200
    },
    {
      "epoch": 0.2918522253031958,
      "grad_norm": 9.253652572631836,
      "learning_rate": 9.277005437524525e-05,
      "loss": 0.4205,
      "step": 5210
    },
    {
      "epoch": 0.29241240231913285,
      "grad_norm": 4.772884368896484,
      "learning_rate": 9.275604013677898e-05,
      "loss": 0.3978,
      "step": 5220
    },
    {
      "epoch": 0.29297257933506987,
      "grad_norm": 6.63217306137085,
      "learning_rate": 9.274202589831268e-05,
      "loss": 0.5294,
      "step": 5230
    },
    {
      "epoch": 0.2935327563510069,
      "grad_norm": 4.286041736602783,
      "learning_rate": 9.27280116598464e-05,
      "loss": 0.4903,
      "step": 5240
    },
    {
      "epoch": 0.294092933366944,
      "grad_norm": 5.221353054046631,
      "learning_rate": 9.271399742138013e-05,
      "loss": 0.5912,
      "step": 5250
    },
    {
      "epoch": 0.294653110382881,
      "grad_norm": 6.487731456756592,
      "learning_rate": 9.269998318291384e-05,
      "loss": 0.4862,
      "step": 5260
    },
    {
      "epoch": 0.295213287398818,
      "grad_norm": 7.836594581604004,
      "learning_rate": 9.268596894444757e-05,
      "loss": 0.5115,
      "step": 5270
    },
    {
      "epoch": 0.29577346441475505,
      "grad_norm": 4.435122966766357,
      "learning_rate": 9.267195470598128e-05,
      "loss": 0.6549,
      "step": 5280
    },
    {
      "epoch": 0.2963336414306921,
      "grad_norm": 6.683497428894043,
      "learning_rate": 9.2657940467515e-05,
      "loss": 0.4783,
      "step": 5290
    },
    {
      "epoch": 0.29689381844662915,
      "grad_norm": 6.334778785705566,
      "learning_rate": 9.264392622904871e-05,
      "loss": 0.6887,
      "step": 5300
    },
    {
      "epoch": 0.2974539954625662,
      "grad_norm": 6.853166103363037,
      "learning_rate": 9.262991199058244e-05,
      "loss": 0.4844,
      "step": 5310
    },
    {
      "epoch": 0.2980141724785032,
      "grad_norm": 4.660783290863037,
      "learning_rate": 9.261589775211616e-05,
      "loss": 0.3615,
      "step": 5320
    },
    {
      "epoch": 0.2985743494944402,
      "grad_norm": 5.445628643035889,
      "learning_rate": 9.260188351364987e-05,
      "loss": 0.6538,
      "step": 5330
    },
    {
      "epoch": 0.29913452651037725,
      "grad_norm": 5.497294902801514,
      "learning_rate": 9.258786927518359e-05,
      "loss": 0.4118,
      "step": 5340
    },
    {
      "epoch": 0.29969470352631433,
      "grad_norm": 4.035777568817139,
      "learning_rate": 9.25738550367173e-05,
      "loss": 0.4083,
      "step": 5350
    },
    {
      "epoch": 0.30025488054225136,
      "grad_norm": 4.877675533294678,
      "learning_rate": 9.255984079825103e-05,
      "loss": 0.4459,
      "step": 5360
    },
    {
      "epoch": 0.3008150575581884,
      "grad_norm": 5.655750274658203,
      "learning_rate": 9.254582655978474e-05,
      "loss": 0.4449,
      "step": 5370
    },
    {
      "epoch": 0.3013752345741254,
      "grad_norm": 7.393283367156982,
      "learning_rate": 9.253181232131847e-05,
      "loss": 0.608,
      "step": 5380
    },
    {
      "epoch": 0.3019354115900625,
      "grad_norm": 7.81001091003418,
      "learning_rate": 9.251779808285219e-05,
      "loss": 0.4638,
      "step": 5390
    },
    {
      "epoch": 0.3024955886059995,
      "grad_norm": 5.703793048858643,
      "learning_rate": 9.25037838443859e-05,
      "loss": 0.4907,
      "step": 5400
    },
    {
      "epoch": 0.30305576562193653,
      "grad_norm": 4.913744926452637,
      "learning_rate": 9.248976960591962e-05,
      "loss": 0.5337,
      "step": 5410
    },
    {
      "epoch": 0.30361594263787356,
      "grad_norm": 9.212055206298828,
      "learning_rate": 9.247575536745333e-05,
      "loss": 0.4179,
      "step": 5420
    },
    {
      "epoch": 0.3041761196538106,
      "grad_norm": 4.000342845916748,
      "learning_rate": 9.246174112898706e-05,
      "loss": 0.7149,
      "step": 5430
    },
    {
      "epoch": 0.30473629666974766,
      "grad_norm": 4.897300720214844,
      "learning_rate": 9.244772689052077e-05,
      "loss": 0.611,
      "step": 5440
    },
    {
      "epoch": 0.3052964736856847,
      "grad_norm": 6.6947479248046875,
      "learning_rate": 9.243371265205449e-05,
      "loss": 0.3724,
      "step": 5450
    },
    {
      "epoch": 0.3058566507016217,
      "grad_norm": 6.457979202270508,
      "learning_rate": 9.24196984135882e-05,
      "loss": 0.3997,
      "step": 5460
    },
    {
      "epoch": 0.30641682771755874,
      "grad_norm": 8.086344718933105,
      "learning_rate": 9.240568417512193e-05,
      "loss": 0.5889,
      "step": 5470
    },
    {
      "epoch": 0.30697700473349576,
      "grad_norm": 5.531128406524658,
      "learning_rate": 9.239166993665565e-05,
      "loss": 0.5558,
      "step": 5480
    },
    {
      "epoch": 0.30753718174943284,
      "grad_norm": 5.560390472412109,
      "learning_rate": 9.237765569818938e-05,
      "loss": 0.7207,
      "step": 5490
    },
    {
      "epoch": 0.30809735876536987,
      "grad_norm": 3.9958741664886475,
      "learning_rate": 9.236364145972308e-05,
      "loss": 0.4493,
      "step": 5500
    },
    {
      "epoch": 0.3086575357813069,
      "grad_norm": 6.280058860778809,
      "learning_rate": 9.234962722125679e-05,
      "loss": 0.4891,
      "step": 5510
    },
    {
      "epoch": 0.3092177127972439,
      "grad_norm": 6.077534198760986,
      "learning_rate": 9.233561298279052e-05,
      "loss": 0.4332,
      "step": 5520
    },
    {
      "epoch": 0.30977788981318094,
      "grad_norm": 4.019337177276611,
      "learning_rate": 9.232159874432423e-05,
      "loss": 0.3841,
      "step": 5530
    },
    {
      "epoch": 0.310338066829118,
      "grad_norm": 3.960660696029663,
      "learning_rate": 9.230758450585796e-05,
      "loss": 0.5501,
      "step": 5540
    },
    {
      "epoch": 0.31089824384505504,
      "grad_norm": 5.749591827392578,
      "learning_rate": 9.229357026739168e-05,
      "loss": 0.4515,
      "step": 5550
    },
    {
      "epoch": 0.31145842086099207,
      "grad_norm": 4.176102638244629,
      "learning_rate": 9.227955602892539e-05,
      "loss": 0.5577,
      "step": 5560
    },
    {
      "epoch": 0.3120185978769291,
      "grad_norm": 5.518866062164307,
      "learning_rate": 9.22655417904591e-05,
      "loss": 0.5151,
      "step": 5570
    },
    {
      "epoch": 0.3125787748928661,
      "grad_norm": 5.507330894470215,
      "learning_rate": 9.225152755199284e-05,
      "loss": 0.4905,
      "step": 5580
    },
    {
      "epoch": 0.3131389519088032,
      "grad_norm": 7.233740329742432,
      "learning_rate": 9.223751331352655e-05,
      "loss": 0.5854,
      "step": 5590
    },
    {
      "epoch": 0.3136991289247402,
      "grad_norm": 3.8147716522216797,
      "learning_rate": 9.222349907506026e-05,
      "loss": 0.5607,
      "step": 5600
    },
    {
      "epoch": 0.31425930594067725,
      "grad_norm": 5.07426118850708,
      "learning_rate": 9.220948483659398e-05,
      "loss": 0.496,
      "step": 5610
    },
    {
      "epoch": 0.31481948295661427,
      "grad_norm": 6.114715576171875,
      "learning_rate": 9.21954705981277e-05,
      "loss": 0.4568,
      "step": 5620
    },
    {
      "epoch": 0.31537965997255135,
      "grad_norm": 6.568970680236816,
      "learning_rate": 9.218145635966142e-05,
      "loss": 0.6745,
      "step": 5630
    },
    {
      "epoch": 0.3159398369884884,
      "grad_norm": 4.327103137969971,
      "learning_rate": 9.216744212119514e-05,
      "loss": 0.4368,
      "step": 5640
    },
    {
      "epoch": 0.3165000140044254,
      "grad_norm": 5.032463550567627,
      "learning_rate": 9.215342788272887e-05,
      "loss": 0.3945,
      "step": 5650
    },
    {
      "epoch": 0.3170601910203624,
      "grad_norm": 6.43375301361084,
      "learning_rate": 9.213941364426257e-05,
      "loss": 0.6086,
      "step": 5660
    },
    {
      "epoch": 0.31762036803629945,
      "grad_norm": 5.8080034255981445,
      "learning_rate": 9.21253994057963e-05,
      "loss": 0.4274,
      "step": 5670
    },
    {
      "epoch": 0.31818054505223653,
      "grad_norm": 10.688252449035645,
      "learning_rate": 9.211138516733001e-05,
      "loss": 0.427,
      "step": 5680
    },
    {
      "epoch": 0.31874072206817355,
      "grad_norm": 7.4136152267456055,
      "learning_rate": 9.209737092886374e-05,
      "loss": 0.554,
      "step": 5690
    },
    {
      "epoch": 0.3193008990841106,
      "grad_norm": 4.453333854675293,
      "learning_rate": 9.208335669039745e-05,
      "loss": 0.439,
      "step": 5700
    },
    {
      "epoch": 0.3198610761000476,
      "grad_norm": 2.7874460220336914,
      "learning_rate": 9.206934245193117e-05,
      "loss": 0.5108,
      "step": 5710
    },
    {
      "epoch": 0.32042125311598463,
      "grad_norm": 3.9800469875335693,
      "learning_rate": 9.205532821346488e-05,
      "loss": 0.5331,
      "step": 5720
    },
    {
      "epoch": 0.3209814301319217,
      "grad_norm": 4.7400078773498535,
      "learning_rate": 9.20413139749986e-05,
      "loss": 0.4115,
      "step": 5730
    },
    {
      "epoch": 0.32154160714785873,
      "grad_norm": 4.469903945922852,
      "learning_rate": 9.202729973653233e-05,
      "loss": 0.5115,
      "step": 5740
    },
    {
      "epoch": 0.32210178416379576,
      "grad_norm": 4.755577564239502,
      "learning_rate": 9.201328549806604e-05,
      "loss": 0.5721,
      "step": 5750
    },
    {
      "epoch": 0.3226619611797328,
      "grad_norm": 5.17359733581543,
      "learning_rate": 9.199927125959977e-05,
      "loss": 0.6027,
      "step": 5760
    },
    {
      "epoch": 0.3232221381956698,
      "grad_norm": 5.489262104034424,
      "learning_rate": 9.198525702113347e-05,
      "loss": 0.531,
      "step": 5770
    },
    {
      "epoch": 0.3237823152116069,
      "grad_norm": 4.547565937042236,
      "learning_rate": 9.19712427826672e-05,
      "loss": 0.5995,
      "step": 5780
    },
    {
      "epoch": 0.3243424922275439,
      "grad_norm": 8.933249473571777,
      "learning_rate": 9.195722854420091e-05,
      "loss": 0.5721,
      "step": 5790
    },
    {
      "epoch": 0.32490266924348093,
      "grad_norm": 7.141083717346191,
      "learning_rate": 9.194321430573463e-05,
      "loss": 0.5535,
      "step": 5800
    },
    {
      "epoch": 0.32546284625941796,
      "grad_norm": 4.3059587478637695,
      "learning_rate": 9.192920006726836e-05,
      "loss": 0.4892,
      "step": 5810
    },
    {
      "epoch": 0.32602302327535504,
      "grad_norm": 7.992267608642578,
      "learning_rate": 9.191518582880207e-05,
      "loss": 0.482,
      "step": 5820
    },
    {
      "epoch": 0.32658320029129206,
      "grad_norm": 6.720706939697266,
      "learning_rate": 9.190117159033578e-05,
      "loss": 0.4566,
      "step": 5830
    },
    {
      "epoch": 0.3271433773072291,
      "grad_norm": 6.340776443481445,
      "learning_rate": 9.18871573518695e-05,
      "loss": 0.4908,
      "step": 5840
    },
    {
      "epoch": 0.3277035543231661,
      "grad_norm": 5.603524208068848,
      "learning_rate": 9.187314311340323e-05,
      "loss": 0.3985,
      "step": 5850
    },
    {
      "epoch": 0.32826373133910314,
      "grad_norm": 4.299073219299316,
      "learning_rate": 9.185912887493694e-05,
      "loss": 0.4418,
      "step": 5860
    },
    {
      "epoch": 0.3288239083550402,
      "grad_norm": 5.969333648681641,
      "learning_rate": 9.184511463647066e-05,
      "loss": 0.3373,
      "step": 5870
    },
    {
      "epoch": 0.32938408537097724,
      "grad_norm": 4.7769975662231445,
      "learning_rate": 9.183110039800437e-05,
      "loss": 0.4928,
      "step": 5880
    },
    {
      "epoch": 0.32994426238691427,
      "grad_norm": 4.0056681632995605,
      "learning_rate": 9.181708615953809e-05,
      "loss": 0.3854,
      "step": 5890
    },
    {
      "epoch": 0.3305044394028513,
      "grad_norm": 7.02137565612793,
      "learning_rate": 9.180307192107182e-05,
      "loss": 0.432,
      "step": 5900
    },
    {
      "epoch": 0.3310646164187883,
      "grad_norm": 5.514407157897949,
      "learning_rate": 9.178905768260553e-05,
      "loss": 0.5027,
      "step": 5910
    },
    {
      "epoch": 0.3316247934347254,
      "grad_norm": 4.846617698669434,
      "learning_rate": 9.177504344413926e-05,
      "loss": 0.3945,
      "step": 5920
    },
    {
      "epoch": 0.3321849704506624,
      "grad_norm": 5.56121301651001,
      "learning_rate": 9.176102920567296e-05,
      "loss": 0.504,
      "step": 5930
    },
    {
      "epoch": 0.33274514746659944,
      "grad_norm": 3.528916358947754,
      "learning_rate": 9.174701496720669e-05,
      "loss": 0.3394,
      "step": 5940
    },
    {
      "epoch": 0.33330532448253647,
      "grad_norm": 3.1268885135650635,
      "learning_rate": 9.17330007287404e-05,
      "loss": 0.3349,
      "step": 5950
    },
    {
      "epoch": 0.3338655014984735,
      "grad_norm": 5.524848937988281,
      "learning_rate": 9.171898649027413e-05,
      "loss": 0.4378,
      "step": 5960
    },
    {
      "epoch": 0.3344256785144106,
      "grad_norm": 5.953398704528809,
      "learning_rate": 9.170497225180785e-05,
      "loss": 0.5359,
      "step": 5970
    },
    {
      "epoch": 0.3349858555303476,
      "grad_norm": 7.308740615844727,
      "learning_rate": 9.169095801334156e-05,
      "loss": 0.4542,
      "step": 5980
    },
    {
      "epoch": 0.3355460325462846,
      "grad_norm": 4.254311561584473,
      "learning_rate": 9.167694377487528e-05,
      "loss": 0.5259,
      "step": 5990
    },
    {
      "epoch": 0.33610620956222165,
      "grad_norm": 4.685874938964844,
      "learning_rate": 9.166292953640899e-05,
      "loss": 0.4379,
      "step": 6000
    },
    {
      "epoch": 0.33666638657815867,
      "grad_norm": 4.3820295333862305,
      "learning_rate": 9.164891529794272e-05,
      "loss": 0.5548,
      "step": 6010
    },
    {
      "epoch": 0.33722656359409575,
      "grad_norm": 7.363259792327881,
      "learning_rate": 9.163490105947643e-05,
      "loss": 0.4154,
      "step": 6020
    },
    {
      "epoch": 0.3377867406100328,
      "grad_norm": 7.7604079246521,
      "learning_rate": 9.162088682101016e-05,
      "loss": 0.4188,
      "step": 6030
    },
    {
      "epoch": 0.3383469176259698,
      "grad_norm": 6.408723831176758,
      "learning_rate": 9.160687258254386e-05,
      "loss": 0.5885,
      "step": 6040
    },
    {
      "epoch": 0.3389070946419068,
      "grad_norm": 6.170376300811768,
      "learning_rate": 9.159285834407759e-05,
      "loss": 0.3814,
      "step": 6050
    },
    {
      "epoch": 0.3394672716578439,
      "grad_norm": 7.785187244415283,
      "learning_rate": 9.15788441056113e-05,
      "loss": 0.4105,
      "step": 6060
    },
    {
      "epoch": 0.34002744867378093,
      "grad_norm": 4.675727367401123,
      "learning_rate": 9.156482986714502e-05,
      "loss": 0.2771,
      "step": 6070
    },
    {
      "epoch": 0.34058762568971795,
      "grad_norm": 6.174314022064209,
      "learning_rate": 9.155081562867875e-05,
      "loss": 0.512,
      "step": 6080
    },
    {
      "epoch": 0.341147802705655,
      "grad_norm": 5.318835735321045,
      "learning_rate": 9.153680139021246e-05,
      "loss": 0.5552,
      "step": 6090
    },
    {
      "epoch": 0.341707979721592,
      "grad_norm": 6.558431625366211,
      "learning_rate": 9.152278715174618e-05,
      "loss": 0.518,
      "step": 6100
    },
    {
      "epoch": 0.3422681567375291,
      "grad_norm": 4.3730950355529785,
      "learning_rate": 9.150877291327989e-05,
      "loss": 0.522,
      "step": 6110
    },
    {
      "epoch": 0.3428283337534661,
      "grad_norm": 5.6948347091674805,
      "learning_rate": 9.149475867481362e-05,
      "loss": 0.5332,
      "step": 6120
    },
    {
      "epoch": 0.34338851076940313,
      "grad_norm": 4.217782020568848,
      "learning_rate": 9.148074443634734e-05,
      "loss": 0.4705,
      "step": 6130
    },
    {
      "epoch": 0.34394868778534016,
      "grad_norm": 7.600268840789795,
      "learning_rate": 9.146673019788105e-05,
      "loss": 0.6633,
      "step": 6140
    },
    {
      "epoch": 0.3445088648012772,
      "grad_norm": 6.935818672180176,
      "learning_rate": 9.145271595941477e-05,
      "loss": 0.5746,
      "step": 6150
    },
    {
      "epoch": 0.34506904181721426,
      "grad_norm": 4.861332893371582,
      "learning_rate": 9.143870172094848e-05,
      "loss": 0.6203,
      "step": 6160
    },
    {
      "epoch": 0.3456292188331513,
      "grad_norm": 10.169635772705078,
      "learning_rate": 9.142468748248221e-05,
      "loss": 0.4959,
      "step": 6170
    },
    {
      "epoch": 0.3461893958490883,
      "grad_norm": 6.000551700592041,
      "learning_rate": 9.141067324401592e-05,
      "loss": 0.3772,
      "step": 6180
    },
    {
      "epoch": 0.34674957286502534,
      "grad_norm": 5.689356327056885,
      "learning_rate": 9.139665900554965e-05,
      "loss": 0.4675,
      "step": 6190
    },
    {
      "epoch": 0.34730974988096236,
      "grad_norm": 3.134565830230713,
      "learning_rate": 9.138264476708335e-05,
      "loss": 0.431,
      "step": 6200
    },
    {
      "epoch": 0.34786992689689944,
      "grad_norm": 6.555062770843506,
      "learning_rate": 9.136863052861708e-05,
      "loss": 0.3609,
      "step": 6210
    },
    {
      "epoch": 0.34843010391283646,
      "grad_norm": 4.169127464294434,
      "learning_rate": 9.13546162901508e-05,
      "loss": 0.5219,
      "step": 6220
    },
    {
      "epoch": 0.3489902809287735,
      "grad_norm": 6.3368024826049805,
      "learning_rate": 9.134060205168452e-05,
      "loss": 0.4158,
      "step": 6230
    },
    {
      "epoch": 0.3495504579447105,
      "grad_norm": 7.663022518157959,
      "learning_rate": 9.132658781321824e-05,
      "loss": 0.4928,
      "step": 6240
    },
    {
      "epoch": 0.35011063496064754,
      "grad_norm": 4.7107834815979,
      "learning_rate": 9.131257357475195e-05,
      "loss": 0.4209,
      "step": 6250
    },
    {
      "epoch": 0.3506708119765846,
      "grad_norm": 4.779041767120361,
      "learning_rate": 9.129855933628567e-05,
      "loss": 0.3692,
      "step": 6260
    },
    {
      "epoch": 0.35123098899252164,
      "grad_norm": 2.7485406398773193,
      "learning_rate": 9.128454509781938e-05,
      "loss": 0.5452,
      "step": 6270
    },
    {
      "epoch": 0.35179116600845867,
      "grad_norm": 5.999172687530518,
      "learning_rate": 9.127053085935311e-05,
      "loss": 0.5986,
      "step": 6280
    },
    {
      "epoch": 0.3523513430243957,
      "grad_norm": 6.551212787628174,
      "learning_rate": 9.125651662088683e-05,
      "loss": 0.4709,
      "step": 6290
    },
    {
      "epoch": 0.35291152004033277,
      "grad_norm": 5.9098801612854,
      "learning_rate": 9.124250238242054e-05,
      "loss": 0.4492,
      "step": 6300
    },
    {
      "epoch": 0.3534716970562698,
      "grad_norm": 4.470277786254883,
      "learning_rate": 9.122848814395426e-05,
      "loss": 0.3979,
      "step": 6310
    },
    {
      "epoch": 0.3540318740722068,
      "grad_norm": 5.91141939163208,
      "learning_rate": 9.121447390548798e-05,
      "loss": 0.3115,
      "step": 6320
    },
    {
      "epoch": 0.35459205108814384,
      "grad_norm": 5.810764312744141,
      "learning_rate": 9.12004596670217e-05,
      "loss": 0.621,
      "step": 6330
    },
    {
      "epoch": 0.35515222810408087,
      "grad_norm": 4.374896049499512,
      "learning_rate": 9.118644542855541e-05,
      "loss": 0.3997,
      "step": 6340
    },
    {
      "epoch": 0.35571240512001795,
      "grad_norm": 5.441088676452637,
      "learning_rate": 9.117243119008914e-05,
      "loss": 0.4519,
      "step": 6350
    },
    {
      "epoch": 0.356272582135955,
      "grad_norm": 6.57719612121582,
      "learning_rate": 9.115841695162284e-05,
      "loss": 0.3282,
      "step": 6360
    },
    {
      "epoch": 0.356832759151892,
      "grad_norm": 6.0142621994018555,
      "learning_rate": 9.114440271315657e-05,
      "loss": 0.59,
      "step": 6370
    },
    {
      "epoch": 0.357392936167829,
      "grad_norm": 5.606424331665039,
      "learning_rate": 9.113038847469029e-05,
      "loss": 0.6233,
      "step": 6380
    },
    {
      "epoch": 0.35795311318376605,
      "grad_norm": 5.465934753417969,
      "learning_rate": 9.111637423622401e-05,
      "loss": 0.4328,
      "step": 6390
    },
    {
      "epoch": 0.3585132901997031,
      "grad_norm": 4.696808338165283,
      "learning_rate": 9.110235999775773e-05,
      "loss": 0.3615,
      "step": 6400
    },
    {
      "epoch": 0.35907346721564015,
      "grad_norm": 5.910569190979004,
      "learning_rate": 9.108834575929144e-05,
      "loss": 0.5053,
      "step": 6410
    },
    {
      "epoch": 0.3596336442315772,
      "grad_norm": 5.814670085906982,
      "learning_rate": 9.107433152082516e-05,
      "loss": 0.6093,
      "step": 6420
    },
    {
      "epoch": 0.3601938212475142,
      "grad_norm": 8.021029472351074,
      "learning_rate": 9.106031728235887e-05,
      "loss": 0.3176,
      "step": 6430
    },
    {
      "epoch": 0.3607539982634512,
      "grad_norm": 5.7820892333984375,
      "learning_rate": 9.10463030438926e-05,
      "loss": 0.4905,
      "step": 6440
    },
    {
      "epoch": 0.3613141752793883,
      "grad_norm": 5.48992395401001,
      "learning_rate": 9.103228880542632e-05,
      "loss": 0.4453,
      "step": 6450
    },
    {
      "epoch": 0.36187435229532533,
      "grad_norm": 5.56408166885376,
      "learning_rate": 9.101827456696004e-05,
      "loss": 0.5538,
      "step": 6460
    },
    {
      "epoch": 0.36243452931126235,
      "grad_norm": 5.5692524909973145,
      "learning_rate": 9.100426032849375e-05,
      "loss": 0.3502,
      "step": 6470
    },
    {
      "epoch": 0.3629947063271994,
      "grad_norm": 2.6924540996551514,
      "learning_rate": 9.099024609002747e-05,
      "loss": 0.3943,
      "step": 6480
    },
    {
      "epoch": 0.3635548833431364,
      "grad_norm": 5.282948970794678,
      "learning_rate": 9.097623185156119e-05,
      "loss": 0.3681,
      "step": 6490
    },
    {
      "epoch": 0.3641150603590735,
      "grad_norm": 5.341891288757324,
      "learning_rate": 9.096221761309492e-05,
      "loss": 0.4389,
      "step": 6500
    },
    {
      "epoch": 0.3646752373750105,
      "grad_norm": 5.944712162017822,
      "learning_rate": 9.094820337462863e-05,
      "loss": 0.4308,
      "step": 6510
    },
    {
      "epoch": 0.36523541439094753,
      "grad_norm": 4.704965591430664,
      "learning_rate": 9.093418913616235e-05,
      "loss": 0.6091,
      "step": 6520
    },
    {
      "epoch": 0.36579559140688456,
      "grad_norm": 7.220698833465576,
      "learning_rate": 9.092017489769606e-05,
      "loss": 0.5309,
      "step": 6530
    },
    {
      "epoch": 0.36635576842282164,
      "grad_norm": 6.780267238616943,
      "learning_rate": 9.090616065922978e-05,
      "loss": 0.3576,
      "step": 6540
    },
    {
      "epoch": 0.36691594543875866,
      "grad_norm": 4.25126314163208,
      "learning_rate": 9.08921464207635e-05,
      "loss": 0.4807,
      "step": 6550
    },
    {
      "epoch": 0.3674761224546957,
      "grad_norm": 8.573945999145508,
      "learning_rate": 9.087813218229722e-05,
      "loss": 0.6204,
      "step": 6560
    },
    {
      "epoch": 0.3680362994706327,
      "grad_norm": 5.046725273132324,
      "learning_rate": 9.086411794383093e-05,
      "loss": 0.4327,
      "step": 6570
    },
    {
      "epoch": 0.36859647648656974,
      "grad_norm": 2.358581304550171,
      "learning_rate": 9.085010370536465e-05,
      "loss": 0.511,
      "step": 6580
    },
    {
      "epoch": 0.3691566535025068,
      "grad_norm": 8.718305587768555,
      "learning_rate": 9.083608946689838e-05,
      "loss": 0.6108,
      "step": 6590
    },
    {
      "epoch": 0.36971683051844384,
      "grad_norm": 6.185424327850342,
      "learning_rate": 9.082207522843209e-05,
      "loss": 0.6027,
      "step": 6600
    },
    {
      "epoch": 0.37027700753438086,
      "grad_norm": 5.421446800231934,
      "learning_rate": 9.08080609899658e-05,
      "loss": 0.4967,
      "step": 6610
    },
    {
      "epoch": 0.3708371845503179,
      "grad_norm": 6.718355178833008,
      "learning_rate": 9.079404675149953e-05,
      "loss": 0.5977,
      "step": 6620
    },
    {
      "epoch": 0.3713973615662549,
      "grad_norm": 6.683266639709473,
      "learning_rate": 9.078003251303324e-05,
      "loss": 0.5824,
      "step": 6630
    },
    {
      "epoch": 0.371957538582192,
      "grad_norm": 4.135459899902344,
      "learning_rate": 9.076601827456696e-05,
      "loss": 0.4403,
      "step": 6640
    },
    {
      "epoch": 0.372517715598129,
      "grad_norm": 6.3215460777282715,
      "learning_rate": 9.075200403610068e-05,
      "loss": 0.5503,
      "step": 6650
    },
    {
      "epoch": 0.37307789261406604,
      "grad_norm": 4.196010112762451,
      "learning_rate": 9.073798979763441e-05,
      "loss": 0.5571,
      "step": 6660
    },
    {
      "epoch": 0.37363806963000307,
      "grad_norm": 4.81089973449707,
      "learning_rate": 9.072397555916812e-05,
      "loss": 0.3519,
      "step": 6670
    },
    {
      "epoch": 0.3741982466459401,
      "grad_norm": 4.819289684295654,
      "learning_rate": 9.070996132070184e-05,
      "loss": 0.395,
      "step": 6680
    },
    {
      "epoch": 0.37475842366187717,
      "grad_norm": 5.804813385009766,
      "learning_rate": 9.069594708223555e-05,
      "loss": 0.3979,
      "step": 6690
    },
    {
      "epoch": 0.3753186006778142,
      "grad_norm": 6.710035800933838,
      "learning_rate": 9.068193284376928e-05,
      "loss": 0.3406,
      "step": 6700
    },
    {
      "epoch": 0.3758787776937512,
      "grad_norm": 4.95994234085083,
      "learning_rate": 9.0667918605303e-05,
      "loss": 0.4256,
      "step": 6710
    },
    {
      "epoch": 0.37643895470968824,
      "grad_norm": 5.233926773071289,
      "learning_rate": 9.065390436683671e-05,
      "loss": 0.6232,
      "step": 6720
    },
    {
      "epoch": 0.3769991317256253,
      "grad_norm": 4.331101417541504,
      "learning_rate": 9.063989012837044e-05,
      "loss": 0.3212,
      "step": 6730
    },
    {
      "epoch": 0.37755930874156235,
      "grad_norm": 4.054474830627441,
      "learning_rate": 9.062587588990414e-05,
      "loss": 0.4033,
      "step": 6740
    },
    {
      "epoch": 0.3781194857574994,
      "grad_norm": 4.195172309875488,
      "learning_rate": 9.061186165143787e-05,
      "loss": 0.5821,
      "step": 6750
    },
    {
      "epoch": 0.3786796627734364,
      "grad_norm": 6.692166328430176,
      "learning_rate": 9.059784741297158e-05,
      "loss": 0.34,
      "step": 6760
    },
    {
      "epoch": 0.3792398397893734,
      "grad_norm": 5.224448204040527,
      "learning_rate": 9.058383317450531e-05,
      "loss": 0.4429,
      "step": 6770
    },
    {
      "epoch": 0.3798000168053105,
      "grad_norm": 5.644906520843506,
      "learning_rate": 9.056981893603902e-05,
      "loss": 0.4839,
      "step": 6780
    },
    {
      "epoch": 0.3803601938212475,
      "grad_norm": 7.841599941253662,
      "learning_rate": 9.055580469757274e-05,
      "loss": 0.4944,
      "step": 6790
    },
    {
      "epoch": 0.38092037083718455,
      "grad_norm": 6.409174919128418,
      "learning_rate": 9.054179045910645e-05,
      "loss": 0.3798,
      "step": 6800
    },
    {
      "epoch": 0.3814805478531216,
      "grad_norm": 3.484362840652466,
      "learning_rate": 9.052777622064017e-05,
      "loss": 0.3584,
      "step": 6810
    },
    {
      "epoch": 0.3820407248690586,
      "grad_norm": 3.9712584018707275,
      "learning_rate": 9.05137619821739e-05,
      "loss": 0.3062,
      "step": 6820
    },
    {
      "epoch": 0.3826009018849957,
      "grad_norm": 5.193147659301758,
      "learning_rate": 9.049974774370761e-05,
      "loss": 0.4651,
      "step": 6830
    },
    {
      "epoch": 0.3831610789009327,
      "grad_norm": 7.165138244628906,
      "learning_rate": 9.048573350524133e-05,
      "loss": 0.5905,
      "step": 6840
    },
    {
      "epoch": 0.38372125591686973,
      "grad_norm": 5.512772560119629,
      "learning_rate": 9.047171926677504e-05,
      "loss": 0.3861,
      "step": 6850
    },
    {
      "epoch": 0.38428143293280675,
      "grad_norm": 8.41601848602295,
      "learning_rate": 9.045770502830877e-05,
      "loss": 0.4958,
      "step": 6860
    },
    {
      "epoch": 0.3848416099487438,
      "grad_norm": 5.093718528747559,
      "learning_rate": 9.044369078984248e-05,
      "loss": 0.2956,
      "step": 6870
    },
    {
      "epoch": 0.38540178696468086,
      "grad_norm": 6.915393352508545,
      "learning_rate": 9.042967655137621e-05,
      "loss": 0.3713,
      "step": 6880
    },
    {
      "epoch": 0.3859619639806179,
      "grad_norm": 5.236873626708984,
      "learning_rate": 9.041566231290993e-05,
      "loss": 0.4589,
      "step": 6890
    },
    {
      "epoch": 0.3865221409965549,
      "grad_norm": 5.3539581298828125,
      "learning_rate": 9.040164807444363e-05,
      "loss": 0.5244,
      "step": 6900
    },
    {
      "epoch": 0.38708231801249193,
      "grad_norm": 5.5640645027160645,
      "learning_rate": 9.038763383597736e-05,
      "loss": 0.4662,
      "step": 6910
    },
    {
      "epoch": 0.38764249502842896,
      "grad_norm": 6.155187129974365,
      "learning_rate": 9.037361959751107e-05,
      "loss": 0.4307,
      "step": 6920
    },
    {
      "epoch": 0.38820267204436604,
      "grad_norm": 6.00343656539917,
      "learning_rate": 9.03596053590448e-05,
      "loss": 0.5087,
      "step": 6930
    },
    {
      "epoch": 0.38876284906030306,
      "grad_norm": 8.312047958374023,
      "learning_rate": 9.034559112057851e-05,
      "loss": 0.5472,
      "step": 6940
    },
    {
      "epoch": 0.3893230260762401,
      "grad_norm": 5.798617839813232,
      "learning_rate": 9.033157688211223e-05,
      "loss": 0.409,
      "step": 6950
    },
    {
      "epoch": 0.3898832030921771,
      "grad_norm": 6.090715408325195,
      "learning_rate": 9.031756264364594e-05,
      "loss": 0.4627,
      "step": 6960
    },
    {
      "epoch": 0.3904433801081142,
      "grad_norm": 3.529085397720337,
      "learning_rate": 9.030354840517967e-05,
      "loss": 0.4118,
      "step": 6970
    },
    {
      "epoch": 0.3910035571240512,
      "grad_norm": 5.084840774536133,
      "learning_rate": 9.028953416671339e-05,
      "loss": 0.4732,
      "step": 6980
    },
    {
      "epoch": 0.39156373413998824,
      "grad_norm": 3.976984739303589,
      "learning_rate": 9.02755199282471e-05,
      "loss": 0.4985,
      "step": 6990
    },
    {
      "epoch": 0.39212391115592526,
      "grad_norm": 6.434272766113281,
      "learning_rate": 9.026150568978082e-05,
      "loss": 0.4984,
      "step": 7000
    },
    {
      "epoch": 0.3926840881718623,
      "grad_norm": 4.324981689453125,
      "learning_rate": 9.024749145131453e-05,
      "loss": 0.3625,
      "step": 7010
    },
    {
      "epoch": 0.39324426518779937,
      "grad_norm": 5.834137916564941,
      "learning_rate": 9.023347721284826e-05,
      "loss": 0.4396,
      "step": 7020
    },
    {
      "epoch": 0.3938044422037364,
      "grad_norm": 5.013125419616699,
      "learning_rate": 9.021946297438197e-05,
      "loss": 0.3971,
      "step": 7030
    },
    {
      "epoch": 0.3943646192196734,
      "grad_norm": 6.133862495422363,
      "learning_rate": 9.02054487359157e-05,
      "loss": 0.3338,
      "step": 7040
    },
    {
      "epoch": 0.39492479623561044,
      "grad_norm": 6.475732326507568,
      "learning_rate": 9.019143449744942e-05,
      "loss": 0.4578,
      "step": 7050
    },
    {
      "epoch": 0.39548497325154747,
      "grad_norm": 6.490638256072998,
      "learning_rate": 9.017742025898313e-05,
      "loss": 0.5536,
      "step": 7060
    },
    {
      "epoch": 0.39604515026748455,
      "grad_norm": 5.405982971191406,
      "learning_rate": 9.016340602051685e-05,
      "loss": 0.2574,
      "step": 7070
    },
    {
      "epoch": 0.39660532728342157,
      "grad_norm": 5.33982515335083,
      "learning_rate": 9.014939178205056e-05,
      "loss": 0.4072,
      "step": 7080
    },
    {
      "epoch": 0.3971655042993586,
      "grad_norm": 5.224324703216553,
      "learning_rate": 9.013537754358429e-05,
      "loss": 0.5014,
      "step": 7090
    },
    {
      "epoch": 0.3977256813152956,
      "grad_norm": 6.959578990936279,
      "learning_rate": 9.0121363305118e-05,
      "loss": 0.468,
      "step": 7100
    },
    {
      "epoch": 0.39828585833123265,
      "grad_norm": 5.983896732330322,
      "learning_rate": 9.010734906665172e-05,
      "loss": 0.5698,
      "step": 7110
    },
    {
      "epoch": 0.3988460353471697,
      "grad_norm": 6.762679100036621,
      "learning_rate": 9.009333482818543e-05,
      "loss": 0.3092,
      "step": 7120
    },
    {
      "epoch": 0.39940621236310675,
      "grad_norm": 5.351594924926758,
      "learning_rate": 9.007932058971916e-05,
      "loss": 0.3393,
      "step": 7130
    },
    {
      "epoch": 0.3999663893790438,
      "grad_norm": 5.662403583526611,
      "learning_rate": 9.006530635125288e-05,
      "loss": 0.4666,
      "step": 7140
    },
    {
      "epoch": 0.4005265663949808,
      "grad_norm": 4.451973915100098,
      "learning_rate": 9.00512921127866e-05,
      "loss": 0.4281,
      "step": 7150
    },
    {
      "epoch": 0.4010867434109178,
      "grad_norm": 6.820905685424805,
      "learning_rate": 9.003727787432032e-05,
      "loss": 0.4292,
      "step": 7160
    },
    {
      "epoch": 0.4016469204268549,
      "grad_norm": 7.735071659088135,
      "learning_rate": 9.002326363585402e-05,
      "loss": 0.3773,
      "step": 7170
    },
    {
      "epoch": 0.4022070974427919,
      "grad_norm": 3.0407629013061523,
      "learning_rate": 9.000924939738775e-05,
      "loss": 0.4341,
      "step": 7180
    },
    {
      "epoch": 0.40276727445872895,
      "grad_norm": 5.37416410446167,
      "learning_rate": 8.999523515892146e-05,
      "loss": 0.4099,
      "step": 7190
    },
    {
      "epoch": 0.403327451474666,
      "grad_norm": 5.29777193069458,
      "learning_rate": 8.998122092045519e-05,
      "loss": 0.5261,
      "step": 7200
    },
    {
      "epoch": 0.40388762849060306,
      "grad_norm": 4.036816120147705,
      "learning_rate": 8.996720668198891e-05,
      "loss": 0.39,
      "step": 7210
    },
    {
      "epoch": 0.4044478055065401,
      "grad_norm": 3.4097509384155273,
      "learning_rate": 8.995319244352262e-05,
      "loss": 0.4716,
      "step": 7220
    },
    {
      "epoch": 0.4050079825224771,
      "grad_norm": 3.3230080604553223,
      "learning_rate": 8.993917820505634e-05,
      "loss": 0.3968,
      "step": 7230
    },
    {
      "epoch": 0.40556815953841413,
      "grad_norm": 4.221322536468506,
      "learning_rate": 8.992516396659007e-05,
      "loss": 0.4885,
      "step": 7240
    },
    {
      "epoch": 0.40612833655435115,
      "grad_norm": 7.9494500160217285,
      "learning_rate": 8.991114972812378e-05,
      "loss": 0.3233,
      "step": 7250
    },
    {
      "epoch": 0.40668851357028823,
      "grad_norm": 3.9340434074401855,
      "learning_rate": 8.98971354896575e-05,
      "loss": 0.351,
      "step": 7260
    },
    {
      "epoch": 0.40724869058622526,
      "grad_norm": 5.5597028732299805,
      "learning_rate": 8.988312125119121e-05,
      "loss": 0.3676,
      "step": 7270
    },
    {
      "epoch": 0.4078088676021623,
      "grad_norm": 7.08686637878418,
      "learning_rate": 8.986910701272492e-05,
      "loss": 0.497,
      "step": 7280
    },
    {
      "epoch": 0.4083690446180993,
      "grad_norm": 6.781666278839111,
      "learning_rate": 8.985509277425865e-05,
      "loss": 0.4689,
      "step": 7290
    },
    {
      "epoch": 0.40892922163403633,
      "grad_norm": 6.830296516418457,
      "learning_rate": 8.984107853579237e-05,
      "loss": 0.3267,
      "step": 7300
    },
    {
      "epoch": 0.4094893986499734,
      "grad_norm": 5.52593469619751,
      "learning_rate": 8.98270642973261e-05,
      "loss": 0.5388,
      "step": 7310
    },
    {
      "epoch": 0.41004957566591044,
      "grad_norm": 6.471059799194336,
      "learning_rate": 8.981305005885981e-05,
      "loss": 0.3098,
      "step": 7320
    },
    {
      "epoch": 0.41060975268184746,
      "grad_norm": 4.1999335289001465,
      "learning_rate": 8.979903582039352e-05,
      "loss": 0.2978,
      "step": 7330
    },
    {
      "epoch": 0.4111699296977845,
      "grad_norm": 4.459240913391113,
      "learning_rate": 8.978502158192724e-05,
      "loss": 0.3931,
      "step": 7340
    },
    {
      "epoch": 0.4117301067137215,
      "grad_norm": 3.7152464389801025,
      "learning_rate": 8.977100734346095e-05,
      "loss": 0.4558,
      "step": 7350
    },
    {
      "epoch": 0.4122902837296586,
      "grad_norm": 5.4580206871032715,
      "learning_rate": 8.975699310499468e-05,
      "loss": 0.3501,
      "step": 7360
    },
    {
      "epoch": 0.4128504607455956,
      "grad_norm": 4.914402484893799,
      "learning_rate": 8.97429788665284e-05,
      "loss": 0.3767,
      "step": 7370
    },
    {
      "epoch": 0.41341063776153264,
      "grad_norm": 6.98958158493042,
      "learning_rate": 8.972896462806211e-05,
      "loss": 0.392,
      "step": 7380
    },
    {
      "epoch": 0.41397081477746966,
      "grad_norm": 6.806053638458252,
      "learning_rate": 8.971495038959583e-05,
      "loss": 0.4388,
      "step": 7390
    },
    {
      "epoch": 0.4145309917934067,
      "grad_norm": 7.55412483215332,
      "learning_rate": 8.970093615112956e-05,
      "loss": 0.5778,
      "step": 7400
    },
    {
      "epoch": 0.41509116880934377,
      "grad_norm": 4.432553291320801,
      "learning_rate": 8.968692191266327e-05,
      "loss": 0.3911,
      "step": 7410
    },
    {
      "epoch": 0.4156513458252808,
      "grad_norm": 7.553475379943848,
      "learning_rate": 8.9672907674197e-05,
      "loss": 0.4733,
      "step": 7420
    },
    {
      "epoch": 0.4162115228412178,
      "grad_norm": 4.156644821166992,
      "learning_rate": 8.965889343573071e-05,
      "loss": 0.3742,
      "step": 7430
    },
    {
      "epoch": 0.41677169985715484,
      "grad_norm": 5.03482723236084,
      "learning_rate": 8.964487919726441e-05,
      "loss": 0.3985,
      "step": 7440
    },
    {
      "epoch": 0.4173318768730919,
      "grad_norm": 5.472269058227539,
      "learning_rate": 8.963086495879814e-05,
      "loss": 0.212,
      "step": 7450
    },
    {
      "epoch": 0.41789205388902895,
      "grad_norm": 5.339580535888672,
      "learning_rate": 8.961685072033186e-05,
      "loss": 0.6298,
      "step": 7460
    },
    {
      "epoch": 0.41845223090496597,
      "grad_norm": 3.9189603328704834,
      "learning_rate": 8.960283648186559e-05,
      "loss": 0.3545,
      "step": 7470
    },
    {
      "epoch": 0.419012407920903,
      "grad_norm": 7.7794928550720215,
      "learning_rate": 8.95888222433993e-05,
      "loss": 0.4714,
      "step": 7480
    },
    {
      "epoch": 0.41957258493684,
      "grad_norm": 4.2426300048828125,
      "learning_rate": 8.957480800493302e-05,
      "loss": 0.3631,
      "step": 7490
    },
    {
      "epoch": 0.4201327619527771,
      "grad_norm": 6.119311332702637,
      "learning_rate": 8.956079376646673e-05,
      "loss": 0.397,
      "step": 7500
    },
    {
      "epoch": 0.4206929389687141,
      "grad_norm": 4.2628302574157715,
      "learning_rate": 8.954677952800046e-05,
      "loss": 0.4466,
      "step": 7510
    },
    {
      "epoch": 0.42125311598465115,
      "grad_norm": 7.180028915405273,
      "learning_rate": 8.953276528953417e-05,
      "loss": 0.4394,
      "step": 7520
    },
    {
      "epoch": 0.4218132930005882,
      "grad_norm": 6.732794761657715,
      "learning_rate": 8.951875105106789e-05,
      "loss": 0.4025,
      "step": 7530
    },
    {
      "epoch": 0.4223734700165252,
      "grad_norm": 6.127355098724365,
      "learning_rate": 8.95047368126016e-05,
      "loss": 0.3154,
      "step": 7540
    },
    {
      "epoch": 0.4229336470324623,
      "grad_norm": 5.826642036437988,
      "learning_rate": 8.949072257413532e-05,
      "loss": 0.4708,
      "step": 7550
    },
    {
      "epoch": 0.4234938240483993,
      "grad_norm": 5.801959991455078,
      "learning_rate": 8.947670833566905e-05,
      "loss": 0.4308,
      "step": 7560
    },
    {
      "epoch": 0.42405400106433633,
      "grad_norm": 7.539649963378906,
      "learning_rate": 8.946269409720276e-05,
      "loss": 0.4918,
      "step": 7570
    },
    {
      "epoch": 0.42461417808027335,
      "grad_norm": 5.480340480804443,
      "learning_rate": 8.944867985873649e-05,
      "loss": 0.4691,
      "step": 7580
    },
    {
      "epoch": 0.4251743550962104,
      "grad_norm": 6.130782604217529,
      "learning_rate": 8.94346656202702e-05,
      "loss": 0.3438,
      "step": 7590
    },
    {
      "epoch": 0.42573453211214746,
      "grad_norm": 5.5420122146606445,
      "learning_rate": 8.942065138180392e-05,
      "loss": 0.3334,
      "step": 7600
    },
    {
      "epoch": 0.4262947091280845,
      "grad_norm": 5.0389018058776855,
      "learning_rate": 8.940663714333763e-05,
      "loss": 0.4032,
      "step": 7610
    },
    {
      "epoch": 0.4268548861440215,
      "grad_norm": 6.838226795196533,
      "learning_rate": 8.939262290487135e-05,
      "loss": 0.5352,
      "step": 7620
    },
    {
      "epoch": 0.42741506315995853,
      "grad_norm": 9.65968132019043,
      "learning_rate": 8.937860866640508e-05,
      "loss": 0.5282,
      "step": 7630
    },
    {
      "epoch": 0.4279752401758956,
      "grad_norm": 6.233204364776611,
      "learning_rate": 8.936459442793879e-05,
      "loss": 0.4257,
      "step": 7640
    },
    {
      "epoch": 0.42853541719183263,
      "grad_norm": 4.1011810302734375,
      "learning_rate": 8.93505801894725e-05,
      "loss": 0.4642,
      "step": 7650
    },
    {
      "epoch": 0.42909559420776966,
      "grad_norm": 6.166236400604248,
      "learning_rate": 8.933656595100622e-05,
      "loss": 0.589,
      "step": 7660
    },
    {
      "epoch": 0.4296557712237067,
      "grad_norm": 5.317063808441162,
      "learning_rate": 8.932255171253995e-05,
      "loss": 0.385,
      "step": 7670
    },
    {
      "epoch": 0.4302159482396437,
      "grad_norm": 4.509130001068115,
      "learning_rate": 8.930853747407366e-05,
      "loss": 0.5571,
      "step": 7680
    },
    {
      "epoch": 0.4307761252555808,
      "grad_norm": 5.771889686584473,
      "learning_rate": 8.929452323560739e-05,
      "loss": 0.3482,
      "step": 7690
    },
    {
      "epoch": 0.4313363022715178,
      "grad_norm": 9.06895637512207,
      "learning_rate": 8.928050899714109e-05,
      "loss": 0.4692,
      "step": 7700
    },
    {
      "epoch": 0.43189647928745484,
      "grad_norm": 4.292830944061279,
      "learning_rate": 8.926649475867481e-05,
      "loss": 0.2756,
      "step": 7710
    },
    {
      "epoch": 0.43245665630339186,
      "grad_norm": 5.039727210998535,
      "learning_rate": 8.925248052020854e-05,
      "loss": 0.4227,
      "step": 7720
    },
    {
      "epoch": 0.4330168333193289,
      "grad_norm": 4.910205364227295,
      "learning_rate": 8.923846628174225e-05,
      "loss": 0.438,
      "step": 7730
    },
    {
      "epoch": 0.43357701033526597,
      "grad_norm": 6.173360824584961,
      "learning_rate": 8.922445204327598e-05,
      "loss": 0.4089,
      "step": 7740
    },
    {
      "epoch": 0.434137187351203,
      "grad_norm": 6.338089466094971,
      "learning_rate": 8.92104378048097e-05,
      "loss": 0.3685,
      "step": 7750
    },
    {
      "epoch": 0.43469736436714,
      "grad_norm": 5.5031585693359375,
      "learning_rate": 8.919642356634341e-05,
      "loss": 0.3995,
      "step": 7760
    },
    {
      "epoch": 0.43525754138307704,
      "grad_norm": 4.073190689086914,
      "learning_rate": 8.918240932787712e-05,
      "loss": 0.4209,
      "step": 7770
    },
    {
      "epoch": 0.43581771839901406,
      "grad_norm": 5.145447254180908,
      "learning_rate": 8.916839508941085e-05,
      "loss": 0.3972,
      "step": 7780
    },
    {
      "epoch": 0.43637789541495114,
      "grad_norm": 3.7512094974517822,
      "learning_rate": 8.915438085094457e-05,
      "loss": 0.4066,
      "step": 7790
    },
    {
      "epoch": 0.43693807243088817,
      "grad_norm": 5.120223045349121,
      "learning_rate": 8.91403666124783e-05,
      "loss": 0.4018,
      "step": 7800
    },
    {
      "epoch": 0.4374982494468252,
      "grad_norm": 6.3165717124938965,
      "learning_rate": 8.9126352374012e-05,
      "loss": 0.5343,
      "step": 7810
    },
    {
      "epoch": 0.4380584264627622,
      "grad_norm": 5.572029113769531,
      "learning_rate": 8.911233813554571e-05,
      "loss": 0.468,
      "step": 7820
    },
    {
      "epoch": 0.43861860347869924,
      "grad_norm": 3.79897403717041,
      "learning_rate": 8.909832389707944e-05,
      "loss": 0.4539,
      "step": 7830
    },
    {
      "epoch": 0.4391787804946363,
      "grad_norm": 6.181830406188965,
      "learning_rate": 8.908430965861315e-05,
      "loss": 0.3334,
      "step": 7840
    },
    {
      "epoch": 0.43973895751057335,
      "grad_norm": 6.020521640777588,
      "learning_rate": 8.907029542014688e-05,
      "loss": 0.4902,
      "step": 7850
    },
    {
      "epoch": 0.44029913452651037,
      "grad_norm": 6.207592487335205,
      "learning_rate": 8.90562811816806e-05,
      "loss": 0.3558,
      "step": 7860
    },
    {
      "epoch": 0.4408593115424474,
      "grad_norm": 5.45481014251709,
      "learning_rate": 8.904226694321431e-05,
      "loss": 0.4064,
      "step": 7870
    },
    {
      "epoch": 0.4414194885583845,
      "grad_norm": 5.106573104858398,
      "learning_rate": 8.902825270474803e-05,
      "loss": 0.3577,
      "step": 7880
    },
    {
      "epoch": 0.4419796655743215,
      "grad_norm": 7.743322372436523,
      "learning_rate": 8.901423846628175e-05,
      "loss": 0.4517,
      "step": 7890
    },
    {
      "epoch": 0.4425398425902585,
      "grad_norm": 5.334752559661865,
      "learning_rate": 8.900022422781547e-05,
      "loss": 0.2952,
      "step": 7900
    },
    {
      "epoch": 0.44310001960619555,
      "grad_norm": 5.44179630279541,
      "learning_rate": 8.898620998934918e-05,
      "loss": 0.4119,
      "step": 7910
    },
    {
      "epoch": 0.4436601966221326,
      "grad_norm": 3.879754066467285,
      "learning_rate": 8.89721957508829e-05,
      "loss": 0.3406,
      "step": 7920
    },
    {
      "epoch": 0.44422037363806965,
      "grad_norm": 5.173646926879883,
      "learning_rate": 8.895818151241661e-05,
      "loss": 0.5703,
      "step": 7930
    },
    {
      "epoch": 0.4447805506540067,
      "grad_norm": 4.035154342651367,
      "learning_rate": 8.894416727395034e-05,
      "loss": 0.3737,
      "step": 7940
    },
    {
      "epoch": 0.4453407276699437,
      "grad_norm": 2.474893808364868,
      "learning_rate": 8.893015303548406e-05,
      "loss": 0.2321,
      "step": 7950
    },
    {
      "epoch": 0.44590090468588073,
      "grad_norm": 4.943004608154297,
      "learning_rate": 8.891613879701778e-05,
      "loss": 0.3885,
      "step": 7960
    },
    {
      "epoch": 0.44646108170181775,
      "grad_norm": 6.240809440612793,
      "learning_rate": 8.890212455855149e-05,
      "loss": 0.4141,
      "step": 7970
    },
    {
      "epoch": 0.44702125871775483,
      "grad_norm": 3.4933767318725586,
      "learning_rate": 8.888811032008521e-05,
      "loss": 0.3382,
      "step": 7980
    },
    {
      "epoch": 0.44758143573369186,
      "grad_norm": 5.094577312469482,
      "learning_rate": 8.887409608161893e-05,
      "loss": 0.2874,
      "step": 7990
    },
    {
      "epoch": 0.4481416127496289,
      "grad_norm": 5.983728885650635,
      "learning_rate": 8.886008184315264e-05,
      "loss": 0.2954,
      "step": 8000
    },
    {
      "epoch": 0.4487017897655659,
      "grad_norm": 5.45638370513916,
      "learning_rate": 8.884606760468637e-05,
      "loss": 0.1978,
      "step": 8010
    },
    {
      "epoch": 0.44926196678150293,
      "grad_norm": 4.715855121612549,
      "learning_rate": 8.883205336622009e-05,
      "loss": 0.4798,
      "step": 8020
    },
    {
      "epoch": 0.44982214379744,
      "grad_norm": 6.632680416107178,
      "learning_rate": 8.88180391277538e-05,
      "loss": 0.3465,
      "step": 8030
    },
    {
      "epoch": 0.45038232081337704,
      "grad_norm": 6.808085918426514,
      "learning_rate": 8.880402488928752e-05,
      "loss": 0.2252,
      "step": 8040
    },
    {
      "epoch": 0.45094249782931406,
      "grad_norm": 3.6216886043548584,
      "learning_rate": 8.879001065082124e-05,
      "loss": 0.3276,
      "step": 8050
    },
    {
      "epoch": 0.4515026748452511,
      "grad_norm": 5.359127998352051,
      "learning_rate": 8.877599641235496e-05,
      "loss": 0.2893,
      "step": 8060
    },
    {
      "epoch": 0.4520628518611881,
      "grad_norm": 4.12723445892334,
      "learning_rate": 8.876198217388869e-05,
      "loss": 0.3977,
      "step": 8070
    },
    {
      "epoch": 0.4526230288771252,
      "grad_norm": 5.498823642730713,
      "learning_rate": 8.874796793542239e-05,
      "loss": 0.4264,
      "step": 8080
    },
    {
      "epoch": 0.4531832058930622,
      "grad_norm": 5.908260345458984,
      "learning_rate": 8.87339536969561e-05,
      "loss": 0.3996,
      "step": 8090
    },
    {
      "epoch": 0.45374338290899924,
      "grad_norm": 7.702681064605713,
      "learning_rate": 8.871993945848983e-05,
      "loss": 0.3521,
      "step": 8100
    },
    {
      "epoch": 0.45430355992493626,
      "grad_norm": 4.33509635925293,
      "learning_rate": 8.870592522002355e-05,
      "loss": 0.4784,
      "step": 8110
    },
    {
      "epoch": 0.45486373694087334,
      "grad_norm": 5.304431438446045,
      "learning_rate": 8.869191098155727e-05,
      "loss": 0.455,
      "step": 8120
    },
    {
      "epoch": 0.45542391395681037,
      "grad_norm": 3.0579192638397217,
      "learning_rate": 8.867789674309099e-05,
      "loss": 0.3084,
      "step": 8130
    },
    {
      "epoch": 0.4559840909727474,
      "grad_norm": 4.157310962677002,
      "learning_rate": 8.86638825046247e-05,
      "loss": 0.3324,
      "step": 8140
    },
    {
      "epoch": 0.4565442679886844,
      "grad_norm": 6.358582019805908,
      "learning_rate": 8.864986826615842e-05,
      "loss": 0.3895,
      "step": 8150
    },
    {
      "epoch": 0.45710444500462144,
      "grad_norm": 4.42545223236084,
      "learning_rate": 8.863585402769215e-05,
      "loss": 0.2649,
      "step": 8160
    },
    {
      "epoch": 0.4576646220205585,
      "grad_norm": 6.8616251945495605,
      "learning_rate": 8.862183978922586e-05,
      "loss": 0.4228,
      "step": 8170
    },
    {
      "epoch": 0.45822479903649554,
      "grad_norm": 3.5392544269561768,
      "learning_rate": 8.860782555075958e-05,
      "loss": 0.2834,
      "step": 8180
    },
    {
      "epoch": 0.45878497605243257,
      "grad_norm": 11.297140121459961,
      "learning_rate": 8.859381131229329e-05,
      "loss": 0.3794,
      "step": 8190
    },
    {
      "epoch": 0.4593451530683696,
      "grad_norm": 5.2251691818237305,
      "learning_rate": 8.8579797073827e-05,
      "loss": 0.4798,
      "step": 8200
    },
    {
      "epoch": 0.4599053300843066,
      "grad_norm": 3.1194896697998047,
      "learning_rate": 8.856578283536073e-05,
      "loss": 0.3444,
      "step": 8210
    },
    {
      "epoch": 0.4604655071002437,
      "grad_norm": 5.350400924682617,
      "learning_rate": 8.855176859689445e-05,
      "loss": 0.4371,
      "step": 8220
    },
    {
      "epoch": 0.4610256841161807,
      "grad_norm": 6.1618733406066895,
      "learning_rate": 8.853775435842818e-05,
      "loss": 0.5338,
      "step": 8230
    },
    {
      "epoch": 0.46158586113211775,
      "grad_norm": 4.712030410766602,
      "learning_rate": 8.852374011996188e-05,
      "loss": 0.4173,
      "step": 8240
    },
    {
      "epoch": 0.46214603814805477,
      "grad_norm": 6.614065170288086,
      "learning_rate": 8.85097258814956e-05,
      "loss": 0.341,
      "step": 8250
    },
    {
      "epoch": 0.4627062151639918,
      "grad_norm": 3.818047523498535,
      "learning_rate": 8.849571164302932e-05,
      "loss": 0.3636,
      "step": 8260
    },
    {
      "epoch": 0.4632663921799289,
      "grad_norm": 4.620025634765625,
      "learning_rate": 8.848169740456304e-05,
      "loss": 0.2911,
      "step": 8270
    },
    {
      "epoch": 0.4638265691958659,
      "grad_norm": 5.070616722106934,
      "learning_rate": 8.846768316609676e-05,
      "loss": 0.4468,
      "step": 8280
    },
    {
      "epoch": 0.4643867462118029,
      "grad_norm": 2.981590747833252,
      "learning_rate": 8.845366892763048e-05,
      "loss": 0.2745,
      "step": 8290
    },
    {
      "epoch": 0.46494692322773995,
      "grad_norm": 5.585054397583008,
      "learning_rate": 8.84396546891642e-05,
      "loss": 0.4976,
      "step": 8300
    },
    {
      "epoch": 0.465507100243677,
      "grad_norm": 7.621122360229492,
      "learning_rate": 8.842564045069791e-05,
      "loss": 0.3157,
      "step": 8310
    },
    {
      "epoch": 0.46606727725961405,
      "grad_norm": 4.367258548736572,
      "learning_rate": 8.841162621223164e-05,
      "loss": 0.421,
      "step": 8320
    },
    {
      "epoch": 0.4666274542755511,
      "grad_norm": 5.205062389373779,
      "learning_rate": 8.839761197376535e-05,
      "loss": 0.3054,
      "step": 8330
    },
    {
      "epoch": 0.4671876312914881,
      "grad_norm": 6.1381916999816895,
      "learning_rate": 8.838359773529907e-05,
      "loss": 0.2331,
      "step": 8340
    },
    {
      "epoch": 0.46774780830742513,
      "grad_norm": 5.4982686042785645,
      "learning_rate": 8.836958349683278e-05,
      "loss": 0.4386,
      "step": 8350
    },
    {
      "epoch": 0.4683079853233622,
      "grad_norm": 5.7534260749816895,
      "learning_rate": 8.83555692583665e-05,
      "loss": 0.3876,
      "step": 8360
    },
    {
      "epoch": 0.46886816233929923,
      "grad_norm": 2.9470112323760986,
      "learning_rate": 8.834155501990022e-05,
      "loss": 0.4429,
      "step": 8370
    },
    {
      "epoch": 0.46942833935523626,
      "grad_norm": 3.027531862258911,
      "learning_rate": 8.832754078143394e-05,
      "loss": 0.2372,
      "step": 8380
    },
    {
      "epoch": 0.4699885163711733,
      "grad_norm": 5.97903299331665,
      "learning_rate": 8.831352654296767e-05,
      "loss": 0.2637,
      "step": 8390
    },
    {
      "epoch": 0.4705486933871103,
      "grad_norm": 5.401818752288818,
      "learning_rate": 8.829951230450137e-05,
      "loss": 0.4436,
      "step": 8400
    },
    {
      "epoch": 0.4711088704030474,
      "grad_norm": 3.318774700164795,
      "learning_rate": 8.82854980660351e-05,
      "loss": 0.3633,
      "step": 8410
    },
    {
      "epoch": 0.4716690474189844,
      "grad_norm": 4.417685508728027,
      "learning_rate": 8.827148382756881e-05,
      "loss": 0.4669,
      "step": 8420
    },
    {
      "epoch": 0.47222922443492144,
      "grad_norm": 7.236544132232666,
      "learning_rate": 8.825746958910254e-05,
      "loss": 0.4298,
      "step": 8430
    },
    {
      "epoch": 0.47278940145085846,
      "grad_norm": 4.625123023986816,
      "learning_rate": 8.824345535063625e-05,
      "loss": 0.3876,
      "step": 8440
    },
    {
      "epoch": 0.4733495784667955,
      "grad_norm": 7.455854415893555,
      "learning_rate": 8.822944111216997e-05,
      "loss": 0.4848,
      "step": 8450
    },
    {
      "epoch": 0.47390975548273256,
      "grad_norm": 5.2689900398254395,
      "learning_rate": 8.821542687370368e-05,
      "loss": 0.2961,
      "step": 8460
    },
    {
      "epoch": 0.4744699324986696,
      "grad_norm": 4.840888023376465,
      "learning_rate": 8.82014126352374e-05,
      "loss": 0.4117,
      "step": 8470
    },
    {
      "epoch": 0.4750301095146066,
      "grad_norm": 7.484151363372803,
      "learning_rate": 8.818739839677113e-05,
      "loss": 0.4592,
      "step": 8480
    },
    {
      "epoch": 0.47559028653054364,
      "grad_norm": 5.900075912475586,
      "learning_rate": 8.817338415830484e-05,
      "loss": 0.3005,
      "step": 8490
    },
    {
      "epoch": 0.47615046354648066,
      "grad_norm": 4.5618438720703125,
      "learning_rate": 8.815936991983857e-05,
      "loss": 0.3281,
      "step": 8500
    },
    {
      "epoch": 0.47671064056241774,
      "grad_norm": 7.8941755294799805,
      "learning_rate": 8.814535568137227e-05,
      "loss": 0.3326,
      "step": 8510
    },
    {
      "epoch": 0.47727081757835477,
      "grad_norm": 3.762357234954834,
      "learning_rate": 8.8131341442906e-05,
      "loss": 0.3248,
      "step": 8520
    },
    {
      "epoch": 0.4778309945942918,
      "grad_norm": 5.949591636657715,
      "learning_rate": 8.811732720443971e-05,
      "loss": 0.4421,
      "step": 8530
    },
    {
      "epoch": 0.4783911716102288,
      "grad_norm": 3.140700578689575,
      "learning_rate": 8.810331296597343e-05,
      "loss": 0.3883,
      "step": 8540
    },
    {
      "epoch": 0.4789513486261659,
      "grad_norm": 3.8478832244873047,
      "learning_rate": 8.808929872750716e-05,
      "loss": 0.3829,
      "step": 8550
    },
    {
      "epoch": 0.4795115256421029,
      "grad_norm": 7.846771717071533,
      "learning_rate": 8.807528448904087e-05,
      "loss": 0.3291,
      "step": 8560
    },
    {
      "epoch": 0.48007170265803994,
      "grad_norm": 5.596070289611816,
      "learning_rate": 8.806127025057459e-05,
      "loss": 0.3278,
      "step": 8570
    },
    {
      "epoch": 0.48063187967397697,
      "grad_norm": 3.607283592224121,
      "learning_rate": 8.80472560121083e-05,
      "loss": 0.3212,
      "step": 8580
    },
    {
      "epoch": 0.481192056689914,
      "grad_norm": 6.736266613006592,
      "learning_rate": 8.803324177364203e-05,
      "loss": 0.2984,
      "step": 8590
    },
    {
      "epoch": 0.4817522337058511,
      "grad_norm": 3.6672446727752686,
      "learning_rate": 8.801922753517574e-05,
      "loss": 0.4118,
      "step": 8600
    },
    {
      "epoch": 0.4823124107217881,
      "grad_norm": 5.545302391052246,
      "learning_rate": 8.800521329670946e-05,
      "loss": 0.5364,
      "step": 8610
    },
    {
      "epoch": 0.4828725877377251,
      "grad_norm": 3.0991833209991455,
      "learning_rate": 8.799119905824317e-05,
      "loss": 0.4043,
      "step": 8620
    },
    {
      "epoch": 0.48343276475366215,
      "grad_norm": 4.890252590179443,
      "learning_rate": 8.797718481977689e-05,
      "loss": 0.3677,
      "step": 8630
    },
    {
      "epoch": 0.48399294176959917,
      "grad_norm": 3.943690299987793,
      "learning_rate": 8.796317058131062e-05,
      "loss": 0.2845,
      "step": 8640
    },
    {
      "epoch": 0.48455311878553625,
      "grad_norm": 3.900871515274048,
      "learning_rate": 8.794915634284433e-05,
      "loss": 0.3846,
      "step": 8650
    },
    {
      "epoch": 0.4851132958014733,
      "grad_norm": 5.0627031326293945,
      "learning_rate": 8.793514210437806e-05,
      "loss": 0.4898,
      "step": 8660
    },
    {
      "epoch": 0.4856734728174103,
      "grad_norm": 4.759454727172852,
      "learning_rate": 8.792112786591176e-05,
      "loss": 0.384,
      "step": 8670
    },
    {
      "epoch": 0.4862336498333473,
      "grad_norm": 5.4329514503479,
      "learning_rate": 8.790711362744549e-05,
      "loss": 0.3892,
      "step": 8680
    },
    {
      "epoch": 0.48679382684928435,
      "grad_norm": 5.1066107749938965,
      "learning_rate": 8.78930993889792e-05,
      "loss": 0.2681,
      "step": 8690
    },
    {
      "epoch": 0.48735400386522143,
      "grad_norm": 4.864169120788574,
      "learning_rate": 8.787908515051293e-05,
      "loss": 0.7306,
      "step": 8700
    },
    {
      "epoch": 0.48791418088115845,
      "grad_norm": 5.260380268096924,
      "learning_rate": 8.786507091204665e-05,
      "loss": 0.478,
      "step": 8710
    },
    {
      "epoch": 0.4884743578970955,
      "grad_norm": 8.319209098815918,
      "learning_rate": 8.785105667358036e-05,
      "loss": 0.4347,
      "step": 8720
    },
    {
      "epoch": 0.4890345349130325,
      "grad_norm": 5.076253414154053,
      "learning_rate": 8.783704243511408e-05,
      "loss": 0.3849,
      "step": 8730
    },
    {
      "epoch": 0.48959471192896953,
      "grad_norm": 4.21259069442749,
      "learning_rate": 8.782302819664779e-05,
      "loss": 0.4278,
      "step": 8740
    },
    {
      "epoch": 0.4901548889449066,
      "grad_norm": 4.929974555969238,
      "learning_rate": 8.780901395818152e-05,
      "loss": 0.3838,
      "step": 8750
    },
    {
      "epoch": 0.49071506596084363,
      "grad_norm": 5.182560443878174,
      "learning_rate": 8.779499971971523e-05,
      "loss": 0.2941,
      "step": 8760
    },
    {
      "epoch": 0.49127524297678066,
      "grad_norm": 5.353848457336426,
      "learning_rate": 8.778098548124896e-05,
      "loss": 0.2745,
      "step": 8770
    },
    {
      "epoch": 0.4918354199927177,
      "grad_norm": 5.325397968292236,
      "learning_rate": 8.776697124278266e-05,
      "loss": 0.4221,
      "step": 8780
    },
    {
      "epoch": 0.49239559700865476,
      "grad_norm": 4.847470760345459,
      "learning_rate": 8.775295700431639e-05,
      "loss": 0.3855,
      "step": 8790
    },
    {
      "epoch": 0.4929557740245918,
      "grad_norm": 5.002839088439941,
      "learning_rate": 8.773894276585011e-05,
      "loss": 0.48,
      "step": 8800
    },
    {
      "epoch": 0.4935159510405288,
      "grad_norm": 8.126226425170898,
      "learning_rate": 8.772492852738382e-05,
      "loss": 0.4168,
      "step": 8810
    },
    {
      "epoch": 0.49407612805646584,
      "grad_norm": 5.9656782150268555,
      "learning_rate": 8.771091428891755e-05,
      "loss": 0.5278,
      "step": 8820
    },
    {
      "epoch": 0.49463630507240286,
      "grad_norm": 7.026637554168701,
      "learning_rate": 8.769690005045126e-05,
      "loss": 0.4497,
      "step": 8830
    },
    {
      "epoch": 0.49519648208833994,
      "grad_norm": 5.31386661529541,
      "learning_rate": 8.768288581198498e-05,
      "loss": 0.4585,
      "step": 8840
    },
    {
      "epoch": 0.49575665910427696,
      "grad_norm": 5.833161354064941,
      "learning_rate": 8.76688715735187e-05,
      "loss": 0.3245,
      "step": 8850
    },
    {
      "epoch": 0.496316836120214,
      "grad_norm": 3.7542078495025635,
      "learning_rate": 8.765485733505242e-05,
      "loss": 0.4142,
      "step": 8860
    },
    {
      "epoch": 0.496877013136151,
      "grad_norm": 6.861308574676514,
      "learning_rate": 8.764084309658614e-05,
      "loss": 0.3478,
      "step": 8870
    },
    {
      "epoch": 0.49743719015208804,
      "grad_norm": 6.220892429351807,
      "learning_rate": 8.762682885811985e-05,
      "loss": 0.4857,
      "step": 8880
    },
    {
      "epoch": 0.4979973671680251,
      "grad_norm": 6.643612861633301,
      "learning_rate": 8.761281461965357e-05,
      "loss": 0.4826,
      "step": 8890
    },
    {
      "epoch": 0.49855754418396214,
      "grad_norm": 5.425537109375,
      "learning_rate": 8.75988003811873e-05,
      "loss": 0.4855,
      "step": 8900
    },
    {
      "epoch": 0.49911772119989917,
      "grad_norm": 3.337277412414551,
      "learning_rate": 8.758478614272101e-05,
      "loss": 0.3938,
      "step": 8910
    },
    {
      "epoch": 0.4996778982158362,
      "grad_norm": 4.317862033843994,
      "learning_rate": 8.757077190425472e-05,
      "loss": 0.291,
      "step": 8920
    },
    {
      "epoch": 0.5002380752317732,
      "grad_norm": 5.703603744506836,
      "learning_rate": 8.755675766578845e-05,
      "loss": 0.3196,
      "step": 8930
    },
    {
      "epoch": 0.5007982522477102,
      "grad_norm": 4.930877208709717,
      "learning_rate": 8.754274342732215e-05,
      "loss": 0.3448,
      "step": 8940
    },
    {
      "epoch": 0.5013584292636473,
      "grad_norm": 5.41427755355835,
      "learning_rate": 8.752872918885588e-05,
      "loss": 0.3567,
      "step": 8950
    },
    {
      "epoch": 0.5019186062795844,
      "grad_norm": 5.3187150955200195,
      "learning_rate": 8.75147149503896e-05,
      "loss": 0.4411,
      "step": 8960
    },
    {
      "epoch": 0.5024787832955214,
      "grad_norm": 5.767317295074463,
      "learning_rate": 8.750070071192333e-05,
      "loss": 0.2141,
      "step": 8970
    },
    {
      "epoch": 0.5030389603114584,
      "grad_norm": 5.42854118347168,
      "learning_rate": 8.748668647345704e-05,
      "loss": 0.3437,
      "step": 8980
    },
    {
      "epoch": 0.5035991373273955,
      "grad_norm": 3.6964025497436523,
      "learning_rate": 8.747267223499076e-05,
      "loss": 0.3401,
      "step": 8990
    },
    {
      "epoch": 0.5041593143433325,
      "grad_norm": 4.184479236602783,
      "learning_rate": 8.745865799652447e-05,
      "loss": 0.4249,
      "step": 9000
    },
    {
      "epoch": 0.5047194913592695,
      "grad_norm": 6.673502445220947,
      "learning_rate": 8.744464375805818e-05,
      "loss": 0.3849,
      "step": 9010
    },
    {
      "epoch": 0.5052796683752065,
      "grad_norm": 6.409631729125977,
      "learning_rate": 8.743062951959191e-05,
      "loss": 0.4541,
      "step": 9020
    },
    {
      "epoch": 0.5058398453911436,
      "grad_norm": 4.496548652648926,
      "learning_rate": 8.741661528112563e-05,
      "loss": 0.3546,
      "step": 9030
    },
    {
      "epoch": 0.5064000224070806,
      "grad_norm": 3.640688419342041,
      "learning_rate": 8.740260104265934e-05,
      "loss": 0.3622,
      "step": 9040
    },
    {
      "epoch": 0.5069601994230176,
      "grad_norm": 4.168513774871826,
      "learning_rate": 8.738858680419306e-05,
      "loss": 0.278,
      "step": 9050
    },
    {
      "epoch": 0.5075203764389548,
      "grad_norm": 2.2553930282592773,
      "learning_rate": 8.737457256572679e-05,
      "loss": 0.322,
      "step": 9060
    },
    {
      "epoch": 0.5080805534548918,
      "grad_norm": 3.915613889694214,
      "learning_rate": 8.73605583272605e-05,
      "loss": 0.2171,
      "step": 9070
    },
    {
      "epoch": 0.5086407304708288,
      "grad_norm": 4.979234218597412,
      "learning_rate": 8.734654408879423e-05,
      "loss": 0.3006,
      "step": 9080
    },
    {
      "epoch": 0.5092009074867658,
      "grad_norm": 4.955223083496094,
      "learning_rate": 8.733252985032794e-05,
      "loss": 0.4531,
      "step": 9090
    },
    {
      "epoch": 0.5097610845027029,
      "grad_norm": 7.493312358856201,
      "learning_rate": 8.731851561186164e-05,
      "loss": 0.3374,
      "step": 9100
    },
    {
      "epoch": 0.5103212615186399,
      "grad_norm": 4.356698036193848,
      "learning_rate": 8.730450137339537e-05,
      "loss": 0.2592,
      "step": 9110
    },
    {
      "epoch": 0.5108814385345769,
      "grad_norm": 4.916286468505859,
      "learning_rate": 8.729048713492909e-05,
      "loss": 0.5836,
      "step": 9120
    },
    {
      "epoch": 0.5114416155505139,
      "grad_norm": 2.6632566452026367,
      "learning_rate": 8.727647289646282e-05,
      "loss": 0.2871,
      "step": 9130
    },
    {
      "epoch": 0.512001792566451,
      "grad_norm": 4.798089027404785,
      "learning_rate": 8.726245865799653e-05,
      "loss": 0.288,
      "step": 9140
    },
    {
      "epoch": 0.5125619695823881,
      "grad_norm": 4.482247352600098,
      "learning_rate": 8.724844441953025e-05,
      "loss": 0.3741,
      "step": 9150
    },
    {
      "epoch": 0.5131221465983251,
      "grad_norm": 5.326519012451172,
      "learning_rate": 8.723443018106396e-05,
      "loss": 0.3581,
      "step": 9160
    },
    {
      "epoch": 0.5136823236142621,
      "grad_norm": 5.830953121185303,
      "learning_rate": 8.722041594259769e-05,
      "loss": 0.3287,
      "step": 9170
    },
    {
      "epoch": 0.5142425006301992,
      "grad_norm": 6.502748489379883,
      "learning_rate": 8.72064017041314e-05,
      "loss": 0.4137,
      "step": 9180
    },
    {
      "epoch": 0.5148026776461362,
      "grad_norm": 6.746783256530762,
      "learning_rate": 8.719238746566512e-05,
      "loss": 0.4359,
      "step": 9190
    },
    {
      "epoch": 0.5153628546620732,
      "grad_norm": 5.471258163452148,
      "learning_rate": 8.717837322719885e-05,
      "loss": 0.4467,
      "step": 9200
    },
    {
      "epoch": 0.5159230316780102,
      "grad_norm": 5.467952728271484,
      "learning_rate": 8.716435898873255e-05,
      "loss": 0.3641,
      "step": 9210
    },
    {
      "epoch": 0.5164832086939473,
      "grad_norm": 3.9929423332214355,
      "learning_rate": 8.715034475026628e-05,
      "loss": 0.534,
      "step": 9220
    },
    {
      "epoch": 0.5170433857098843,
      "grad_norm": 5.320992946624756,
      "learning_rate": 8.713633051179999e-05,
      "loss": 0.2882,
      "step": 9230
    },
    {
      "epoch": 0.5176035627258213,
      "grad_norm": 4.870449542999268,
      "learning_rate": 8.712231627333372e-05,
      "loss": 0.3512,
      "step": 9240
    },
    {
      "epoch": 0.5181637397417584,
      "grad_norm": 3.9429850578308105,
      "learning_rate": 8.710830203486743e-05,
      "loss": 0.3741,
      "step": 9250
    },
    {
      "epoch": 0.5187239167576955,
      "grad_norm": 5.464535236358643,
      "learning_rate": 8.709428779640115e-05,
      "loss": 0.3628,
      "step": 9260
    },
    {
      "epoch": 0.5192840937736325,
      "grad_norm": 4.281028747558594,
      "learning_rate": 8.708027355793486e-05,
      "loss": 0.3986,
      "step": 9270
    },
    {
      "epoch": 0.5198442707895695,
      "grad_norm": 4.570788860321045,
      "learning_rate": 8.706625931946858e-05,
      "loss": 0.327,
      "step": 9280
    },
    {
      "epoch": 0.5204044478055065,
      "grad_norm": 5.670876502990723,
      "learning_rate": 8.70522450810023e-05,
      "loss": 0.353,
      "step": 9290
    },
    {
      "epoch": 0.5209646248214436,
      "grad_norm": 3.324824094772339,
      "learning_rate": 8.703823084253602e-05,
      "loss": 0.3989,
      "step": 9300
    },
    {
      "epoch": 0.5215248018373806,
      "grad_norm": 5.845600605010986,
      "learning_rate": 8.702421660406974e-05,
      "loss": 0.3972,
      "step": 9310
    },
    {
      "epoch": 0.5220849788533176,
      "grad_norm": 3.5695688724517822,
      "learning_rate": 8.701020236560345e-05,
      "loss": 0.419,
      "step": 9320
    },
    {
      "epoch": 0.5226451558692546,
      "grad_norm": 9.105734825134277,
      "learning_rate": 8.699618812713718e-05,
      "loss": 0.4917,
      "step": 9330
    },
    {
      "epoch": 0.5232053328851917,
      "grad_norm": 6.602591037750244,
      "learning_rate": 8.698217388867089e-05,
      "loss": 0.4219,
      "step": 9340
    },
    {
      "epoch": 0.5237655099011288,
      "grad_norm": 3.7830419540405273,
      "learning_rate": 8.696815965020462e-05,
      "loss": 0.2817,
      "step": 9350
    },
    {
      "epoch": 0.5243256869170658,
      "grad_norm": 6.054188251495361,
      "learning_rate": 8.695414541173834e-05,
      "loss": 0.3252,
      "step": 9360
    },
    {
      "epoch": 0.5248858639330028,
      "grad_norm": 3.259046792984009,
      "learning_rate": 8.694013117327204e-05,
      "loss": 0.223,
      "step": 9370
    },
    {
      "epoch": 0.5254460409489399,
      "grad_norm": 4.276895999908447,
      "learning_rate": 8.692611693480577e-05,
      "loss": 0.4393,
      "step": 9380
    },
    {
      "epoch": 0.5260062179648769,
      "grad_norm": 5.202093124389648,
      "learning_rate": 8.691210269633948e-05,
      "loss": 0.2321,
      "step": 9390
    },
    {
      "epoch": 0.5265663949808139,
      "grad_norm": 5.661857604980469,
      "learning_rate": 8.689808845787321e-05,
      "loss": 0.2651,
      "step": 9400
    },
    {
      "epoch": 0.527126571996751,
      "grad_norm": 9.929491996765137,
      "learning_rate": 8.688407421940692e-05,
      "loss": 0.5007,
      "step": 9410
    },
    {
      "epoch": 0.527686749012688,
      "grad_norm": 6.508125305175781,
      "learning_rate": 8.687005998094064e-05,
      "loss": 0.4226,
      "step": 9420
    },
    {
      "epoch": 0.528246926028625,
      "grad_norm": 5.076270580291748,
      "learning_rate": 8.685604574247435e-05,
      "loss": 0.3215,
      "step": 9430
    },
    {
      "epoch": 0.5288071030445621,
      "grad_norm": 4.871368408203125,
      "learning_rate": 8.684203150400808e-05,
      "loss": 0.2747,
      "step": 9440
    },
    {
      "epoch": 0.5293672800604992,
      "grad_norm": 4.0949602127075195,
      "learning_rate": 8.68280172655418e-05,
      "loss": 0.2279,
      "step": 9450
    },
    {
      "epoch": 0.5299274570764362,
      "grad_norm": 3.8840646743774414,
      "learning_rate": 8.681400302707551e-05,
      "loss": 0.3432,
      "step": 9460
    },
    {
      "epoch": 0.5304876340923732,
      "grad_norm": 3.231029987335205,
      "learning_rate": 8.679998878860924e-05,
      "loss": 0.3774,
      "step": 9470
    },
    {
      "epoch": 0.5310478111083102,
      "grad_norm": 6.597664833068848,
      "learning_rate": 8.678597455014294e-05,
      "loss": 0.2385,
      "step": 9480
    },
    {
      "epoch": 0.5316079881242473,
      "grad_norm": 6.447235107421875,
      "learning_rate": 8.677196031167667e-05,
      "loss": 0.5882,
      "step": 9490
    },
    {
      "epoch": 0.5321681651401843,
      "grad_norm": 5.280475616455078,
      "learning_rate": 8.675794607321038e-05,
      "loss": 0.5025,
      "step": 9500
    },
    {
      "epoch": 0.5327283421561213,
      "grad_norm": 3.9041268825531006,
      "learning_rate": 8.674393183474411e-05,
      "loss": 0.2609,
      "step": 9510
    },
    {
      "epoch": 0.5332885191720583,
      "grad_norm": 3.9073309898376465,
      "learning_rate": 8.672991759627783e-05,
      "loss": 0.2866,
      "step": 9520
    },
    {
      "epoch": 0.5338486961879954,
      "grad_norm": 3.3485286235809326,
      "learning_rate": 8.671590335781154e-05,
      "loss": 0.4194,
      "step": 9530
    },
    {
      "epoch": 0.5344088732039325,
      "grad_norm": 3.229339361190796,
      "learning_rate": 8.670188911934526e-05,
      "loss": 0.4739,
      "step": 9540
    },
    {
      "epoch": 0.5349690502198695,
      "grad_norm": 3.558464527130127,
      "learning_rate": 8.668787488087897e-05,
      "loss": 0.3282,
      "step": 9550
    },
    {
      "epoch": 0.5355292272358065,
      "grad_norm": 4.893809795379639,
      "learning_rate": 8.66738606424127e-05,
      "loss": 0.3305,
      "step": 9560
    },
    {
      "epoch": 0.5360894042517436,
      "grad_norm": 6.640714168548584,
      "learning_rate": 8.665984640394641e-05,
      "loss": 0.3611,
      "step": 9570
    },
    {
      "epoch": 0.5366495812676806,
      "grad_norm": 3.025712728500366,
      "learning_rate": 8.664583216548013e-05,
      "loss": 0.3451,
      "step": 9580
    },
    {
      "epoch": 0.5372097582836176,
      "grad_norm": 2.5533759593963623,
      "learning_rate": 8.663181792701384e-05,
      "loss": 0.4033,
      "step": 9590
    },
    {
      "epoch": 0.5377699352995546,
      "grad_norm": 5.39673376083374,
      "learning_rate": 8.661780368854757e-05,
      "loss": 0.2687,
      "step": 9600
    },
    {
      "epoch": 0.5383301123154917,
      "grad_norm": 4.88813591003418,
      "learning_rate": 8.660378945008129e-05,
      "loss": 0.2698,
      "step": 9610
    },
    {
      "epoch": 0.5388902893314287,
      "grad_norm": 3.860678195953369,
      "learning_rate": 8.658977521161501e-05,
      "loss": 0.3611,
      "step": 9620
    },
    {
      "epoch": 0.5394504663473658,
      "grad_norm": 4.803246974945068,
      "learning_rate": 8.657576097314873e-05,
      "loss": 0.4229,
      "step": 9630
    },
    {
      "epoch": 0.5400106433633028,
      "grad_norm": 3.4042484760284424,
      "learning_rate": 8.656174673468243e-05,
      "loss": 0.2842,
      "step": 9640
    },
    {
      "epoch": 0.5405708203792399,
      "grad_norm": 2.7824907302856445,
      "learning_rate": 8.654773249621616e-05,
      "loss": 0.3004,
      "step": 9650
    },
    {
      "epoch": 0.5411309973951769,
      "grad_norm": 3.7814221382141113,
      "learning_rate": 8.653371825774987e-05,
      "loss": 0.3481,
      "step": 9660
    },
    {
      "epoch": 0.5416911744111139,
      "grad_norm": 5.720696926116943,
      "learning_rate": 8.65197040192836e-05,
      "loss": 0.3457,
      "step": 9670
    },
    {
      "epoch": 0.5422513514270509,
      "grad_norm": 4.041826248168945,
      "learning_rate": 8.650568978081732e-05,
      "loss": 0.4815,
      "step": 9680
    },
    {
      "epoch": 0.542811528442988,
      "grad_norm": 4.113531589508057,
      "learning_rate": 8.649167554235103e-05,
      "loss": 0.2872,
      "step": 9690
    },
    {
      "epoch": 0.543371705458925,
      "grad_norm": 3.989795684814453,
      "learning_rate": 8.647766130388475e-05,
      "loss": 0.2939,
      "step": 9700
    },
    {
      "epoch": 0.543931882474862,
      "grad_norm": 3.6365749835968018,
      "learning_rate": 8.646364706541847e-05,
      "loss": 0.467,
      "step": 9710
    },
    {
      "epoch": 0.544492059490799,
      "grad_norm": 3.6536834239959717,
      "learning_rate": 8.644963282695219e-05,
      "loss": 0.4185,
      "step": 9720
    },
    {
      "epoch": 0.5450522365067362,
      "grad_norm": 6.049834728240967,
      "learning_rate": 8.64356185884859e-05,
      "loss": 0.2533,
      "step": 9730
    },
    {
      "epoch": 0.5456124135226732,
      "grad_norm": 6.9694504737854,
      "learning_rate": 8.642160435001962e-05,
      "loss": 0.3237,
      "step": 9740
    },
    {
      "epoch": 0.5461725905386102,
      "grad_norm": 6.694849967956543,
      "learning_rate": 8.640759011155333e-05,
      "loss": 0.4057,
      "step": 9750
    },
    {
      "epoch": 0.5467327675545472,
      "grad_norm": 5.02301025390625,
      "learning_rate": 8.639357587308706e-05,
      "loss": 0.2973,
      "step": 9760
    },
    {
      "epoch": 0.5472929445704843,
      "grad_norm": 3.60459303855896,
      "learning_rate": 8.637956163462078e-05,
      "loss": 0.3024,
      "step": 9770
    },
    {
      "epoch": 0.5478531215864213,
      "grad_norm": 4.993927955627441,
      "learning_rate": 8.63655473961545e-05,
      "loss": 0.3565,
      "step": 9780
    },
    {
      "epoch": 0.5484132986023583,
      "grad_norm": 3.603226900100708,
      "learning_rate": 8.635153315768822e-05,
      "loss": 0.2366,
      "step": 9790
    },
    {
      "epoch": 0.5489734756182953,
      "grad_norm": 2.5909979343414307,
      "learning_rate": 8.633751891922193e-05,
      "loss": 0.3507,
      "step": 9800
    },
    {
      "epoch": 0.5495336526342324,
      "grad_norm": 4.411965847015381,
      "learning_rate": 8.632350468075565e-05,
      "loss": 0.3849,
      "step": 9810
    },
    {
      "epoch": 0.5500938296501695,
      "grad_norm": 5.120526313781738,
      "learning_rate": 8.630949044228936e-05,
      "loss": 0.3918,
      "step": 9820
    },
    {
      "epoch": 0.5506540066661065,
      "grad_norm": 7.29962158203125,
      "learning_rate": 8.629547620382309e-05,
      "loss": 0.357,
      "step": 9830
    },
    {
      "epoch": 0.5512141836820436,
      "grad_norm": 5.027947902679443,
      "learning_rate": 8.62814619653568e-05,
      "loss": 0.4008,
      "step": 9840
    },
    {
      "epoch": 0.5517743606979806,
      "grad_norm": 4.896742343902588,
      "learning_rate": 8.626744772689052e-05,
      "loss": 0.3836,
      "step": 9850
    },
    {
      "epoch": 0.5523345377139176,
      "grad_norm": 6.031247615814209,
      "learning_rate": 8.625343348842424e-05,
      "loss": 0.1603,
      "step": 9860
    },
    {
      "epoch": 0.5528947147298546,
      "grad_norm": 4.0533223152160645,
      "learning_rate": 8.623941924995796e-05,
      "loss": 0.3033,
      "step": 9870
    },
    {
      "epoch": 0.5534548917457917,
      "grad_norm": 6.27040433883667,
      "learning_rate": 8.622540501149168e-05,
      "loss": 0.3263,
      "step": 9880
    },
    {
      "epoch": 0.5540150687617287,
      "grad_norm": 6.765181541442871,
      "learning_rate": 8.621139077302541e-05,
      "loss": 0.2934,
      "step": 9890
    },
    {
      "epoch": 0.5545752457776657,
      "grad_norm": 7.58268404006958,
      "learning_rate": 8.619737653455912e-05,
      "loss": 0.2798,
      "step": 9900
    },
    {
      "epoch": 0.5551354227936027,
      "grad_norm": 3.185999870300293,
      "learning_rate": 8.618336229609284e-05,
      "loss": 0.3891,
      "step": 9910
    },
    {
      "epoch": 0.5556955998095399,
      "grad_norm": 7.343570709228516,
      "learning_rate": 8.616934805762655e-05,
      "loss": 0.4026,
      "step": 9920
    },
    {
      "epoch": 0.5562557768254769,
      "grad_norm": 4.954355716705322,
      "learning_rate": 8.615533381916027e-05,
      "loss": 0.3518,
      "step": 9930
    },
    {
      "epoch": 0.5568159538414139,
      "grad_norm": 3.7187390327453613,
      "learning_rate": 8.6141319580694e-05,
      "loss": 0.2675,
      "step": 9940
    },
    {
      "epoch": 0.5573761308573509,
      "grad_norm": 4.457195281982422,
      "learning_rate": 8.612730534222771e-05,
      "loss": 0.2881,
      "step": 9950
    },
    {
      "epoch": 0.557936307873288,
      "grad_norm": 5.497809886932373,
      "learning_rate": 8.611329110376142e-05,
      "loss": 0.4843,
      "step": 9960
    },
    {
      "epoch": 0.558496484889225,
      "grad_norm": 7.762173175811768,
      "learning_rate": 8.609927686529514e-05,
      "loss": 0.3226,
      "step": 9970
    },
    {
      "epoch": 0.559056661905162,
      "grad_norm": 6.485975742340088,
      "learning_rate": 8.608526262682887e-05,
      "loss": 0.3646,
      "step": 9980
    },
    {
      "epoch": 0.559616838921099,
      "grad_norm": 6.483211517333984,
      "learning_rate": 8.607124838836258e-05,
      "loss": 0.2825,
      "step": 9990
    },
    {
      "epoch": 0.5601770159370361,
      "grad_norm": 2.970269203186035,
      "learning_rate": 8.605723414989631e-05,
      "loss": 0.2239,
      "step": 10000
    },
    {
      "epoch": 0.5607371929529731,
      "grad_norm": 3.8003594875335693,
      "learning_rate": 8.604321991143001e-05,
      "loss": 0.3134,
      "step": 10010
    },
    {
      "epoch": 0.5612973699689102,
      "grad_norm": 5.942800045013428,
      "learning_rate": 8.602920567296373e-05,
      "loss": 0.2631,
      "step": 10020
    },
    {
      "epoch": 0.5618575469848472,
      "grad_norm": 4.0695085525512695,
      "learning_rate": 8.601519143449745e-05,
      "loss": 0.3673,
      "step": 10030
    },
    {
      "epoch": 0.5624177240007843,
      "grad_norm": 2.6941559314727783,
      "learning_rate": 8.600117719603117e-05,
      "loss": 0.215,
      "step": 10040
    },
    {
      "epoch": 0.5629779010167213,
      "grad_norm": 3.910223960876465,
      "learning_rate": 8.59871629575649e-05,
      "loss": 0.2686,
      "step": 10050
    },
    {
      "epoch": 0.5635380780326583,
      "grad_norm": 3.4201366901397705,
      "learning_rate": 8.597314871909861e-05,
      "loss": 0.2704,
      "step": 10060
    },
    {
      "epoch": 0.5640982550485953,
      "grad_norm": 3.52314829826355,
      "learning_rate": 8.595913448063233e-05,
      "loss": 0.3525,
      "step": 10070
    },
    {
      "epoch": 0.5646584320645324,
      "grad_norm": 8.353510856628418,
      "learning_rate": 8.594512024216604e-05,
      "loss": 0.2579,
      "step": 10080
    },
    {
      "epoch": 0.5652186090804694,
      "grad_norm": 6.142429828643799,
      "learning_rate": 8.593110600369977e-05,
      "loss": 0.3395,
      "step": 10090
    },
    {
      "epoch": 0.5657787860964064,
      "grad_norm": 3.305224895477295,
      "learning_rate": 8.591709176523348e-05,
      "loss": 0.3581,
      "step": 10100
    },
    {
      "epoch": 0.5663389631123436,
      "grad_norm": 5.163854598999023,
      "learning_rate": 8.59030775267672e-05,
      "loss": 0.2438,
      "step": 10110
    },
    {
      "epoch": 0.5668991401282806,
      "grad_norm": 4.693624973297119,
      "learning_rate": 8.588906328830091e-05,
      "loss": 0.32,
      "step": 10120
    },
    {
      "epoch": 0.5674593171442176,
      "grad_norm": 6.405459403991699,
      "learning_rate": 8.587504904983463e-05,
      "loss": 0.2105,
      "step": 10130
    },
    {
      "epoch": 0.5680194941601546,
      "grad_norm": 4.04663610458374,
      "learning_rate": 8.586103481136836e-05,
      "loss": 0.4768,
      "step": 10140
    },
    {
      "epoch": 0.5685796711760916,
      "grad_norm": 5.926524639129639,
      "learning_rate": 8.584702057290207e-05,
      "loss": 0.241,
      "step": 10150
    },
    {
      "epoch": 0.5691398481920287,
      "grad_norm": 4.39694881439209,
      "learning_rate": 8.58330063344358e-05,
      "loss": 0.3135,
      "step": 10160
    },
    {
      "epoch": 0.5697000252079657,
      "grad_norm": 6.4754767417907715,
      "learning_rate": 8.581899209596951e-05,
      "loss": 0.481,
      "step": 10170
    },
    {
      "epoch": 0.5702602022239027,
      "grad_norm": 7.516071796417236,
      "learning_rate": 8.580497785750323e-05,
      "loss": 0.3768,
      "step": 10180
    },
    {
      "epoch": 0.5708203792398397,
      "grad_norm": 4.435842990875244,
      "learning_rate": 8.579096361903694e-05,
      "loss": 0.3737,
      "step": 10190
    },
    {
      "epoch": 0.5713805562557768,
      "grad_norm": 5.870908737182617,
      "learning_rate": 8.577694938057066e-05,
      "loss": 0.4188,
      "step": 10200
    },
    {
      "epoch": 0.5719407332717139,
      "grad_norm": 4.837855815887451,
      "learning_rate": 8.576293514210439e-05,
      "loss": 0.2869,
      "step": 10210
    },
    {
      "epoch": 0.5725009102876509,
      "grad_norm": 3.5417768955230713,
      "learning_rate": 8.57489209036381e-05,
      "loss": 0.2952,
      "step": 10220
    },
    {
      "epoch": 0.573061087303588,
      "grad_norm": 5.012426376342773,
      "learning_rate": 8.573490666517182e-05,
      "loss": 0.2795,
      "step": 10230
    },
    {
      "epoch": 0.573621264319525,
      "grad_norm": 2.9493284225463867,
      "learning_rate": 8.572089242670553e-05,
      "loss": 0.3326,
      "step": 10240
    },
    {
      "epoch": 0.574181441335462,
      "grad_norm": 9.152491569519043,
      "learning_rate": 8.570687818823926e-05,
      "loss": 0.3553,
      "step": 10250
    },
    {
      "epoch": 0.574741618351399,
      "grad_norm": 4.461032867431641,
      "learning_rate": 8.569286394977297e-05,
      "loss": 0.4446,
      "step": 10260
    },
    {
      "epoch": 0.575301795367336,
      "grad_norm": 4.425464153289795,
      "learning_rate": 8.56788497113067e-05,
      "loss": 0.4095,
      "step": 10270
    },
    {
      "epoch": 0.5758619723832731,
      "grad_norm": 6.062294006347656,
      "learning_rate": 8.56648354728404e-05,
      "loss": 0.2904,
      "step": 10280
    },
    {
      "epoch": 0.5764221493992101,
      "grad_norm": 3.8303468227386475,
      "learning_rate": 8.565082123437412e-05,
      "loss": 0.3575,
      "step": 10290
    },
    {
      "epoch": 0.5769823264151472,
      "grad_norm": 5.560162544250488,
      "learning_rate": 8.563680699590785e-05,
      "loss": 0.3872,
      "step": 10300
    },
    {
      "epoch": 0.5775425034310843,
      "grad_norm": 5.247459411621094,
      "learning_rate": 8.562279275744156e-05,
      "loss": 0.2258,
      "step": 10310
    },
    {
      "epoch": 0.5781026804470213,
      "grad_norm": 4.440158843994141,
      "learning_rate": 8.560877851897529e-05,
      "loss": 0.2721,
      "step": 10320
    },
    {
      "epoch": 0.5786628574629583,
      "grad_norm": 3.587514877319336,
      "learning_rate": 8.5594764280509e-05,
      "loss": 0.2956,
      "step": 10330
    },
    {
      "epoch": 0.5792230344788953,
      "grad_norm": 4.590729713439941,
      "learning_rate": 8.558075004204272e-05,
      "loss": 0.4199,
      "step": 10340
    },
    {
      "epoch": 0.5797832114948324,
      "grad_norm": 3.606316566467285,
      "learning_rate": 8.556673580357643e-05,
      "loss": 0.281,
      "step": 10350
    },
    {
      "epoch": 0.5803433885107694,
      "grad_norm": 6.264859199523926,
      "learning_rate": 8.555272156511016e-05,
      "loss": 0.3843,
      "step": 10360
    },
    {
      "epoch": 0.5809035655267064,
      "grad_norm": 5.008358478546143,
      "learning_rate": 8.553870732664388e-05,
      "loss": 0.2914,
      "step": 10370
    },
    {
      "epoch": 0.5814637425426434,
      "grad_norm": 5.44118595123291,
      "learning_rate": 8.552469308817759e-05,
      "loss": 0.2963,
      "step": 10380
    },
    {
      "epoch": 0.5820239195585805,
      "grad_norm": 2.611950635910034,
      "learning_rate": 8.551067884971131e-05,
      "loss": 0.486,
      "step": 10390
    },
    {
      "epoch": 0.5825840965745176,
      "grad_norm": 6.619309425354004,
      "learning_rate": 8.549666461124502e-05,
      "loss": 0.4409,
      "step": 10400
    },
    {
      "epoch": 0.5831442735904546,
      "grad_norm": 3.859492540359497,
      "learning_rate": 8.548265037277875e-05,
      "loss": 0.4828,
      "step": 10410
    },
    {
      "epoch": 0.5837044506063916,
      "grad_norm": 6.552728176116943,
      "learning_rate": 8.546863613431246e-05,
      "loss": 0.2791,
      "step": 10420
    },
    {
      "epoch": 0.5842646276223287,
      "grad_norm": 3.8089609146118164,
      "learning_rate": 8.545462189584619e-05,
      "loss": 0.4341,
      "step": 10430
    },
    {
      "epoch": 0.5848248046382657,
      "grad_norm": 5.541025638580322,
      "learning_rate": 8.54406076573799e-05,
      "loss": 0.2974,
      "step": 10440
    },
    {
      "epoch": 0.5853849816542027,
      "grad_norm": 7.320532321929932,
      "learning_rate": 8.542659341891362e-05,
      "loss": 0.4283,
      "step": 10450
    },
    {
      "epoch": 0.5859451586701397,
      "grad_norm": 5.281116008758545,
      "learning_rate": 8.541257918044734e-05,
      "loss": 0.274,
      "step": 10460
    },
    {
      "epoch": 0.5865053356860768,
      "grad_norm": 4.839946746826172,
      "learning_rate": 8.539856494198105e-05,
      "loss": 0.3921,
      "step": 10470
    },
    {
      "epoch": 0.5870655127020138,
      "grad_norm": 6.884232521057129,
      "learning_rate": 8.538455070351478e-05,
      "loss": 0.2077,
      "step": 10480
    },
    {
      "epoch": 0.5876256897179509,
      "grad_norm": 5.681644439697266,
      "learning_rate": 8.53705364650485e-05,
      "loss": 0.4134,
      "step": 10490
    },
    {
      "epoch": 0.588185866733888,
      "grad_norm": 5.4080939292907715,
      "learning_rate": 8.535652222658221e-05,
      "loss": 0.3623,
      "step": 10500
    },
    {
      "epoch": 0.588746043749825,
      "grad_norm": 5.54195499420166,
      "learning_rate": 8.534250798811592e-05,
      "loss": 0.3568,
      "step": 10510
    },
    {
      "epoch": 0.589306220765762,
      "grad_norm": 2.153193950653076,
      "learning_rate": 8.532849374964965e-05,
      "loss": 0.2773,
      "step": 10520
    },
    {
      "epoch": 0.589866397781699,
      "grad_norm": 4.58601713180542,
      "learning_rate": 8.531447951118337e-05,
      "loss": 0.3459,
      "step": 10530
    },
    {
      "epoch": 0.590426574797636,
      "grad_norm": 3.866428852081299,
      "learning_rate": 8.53004652727171e-05,
      "loss": 0.2109,
      "step": 10540
    },
    {
      "epoch": 0.5909867518135731,
      "grad_norm": 4.734591484069824,
      "learning_rate": 8.52864510342508e-05,
      "loss": 0.2856,
      "step": 10550
    },
    {
      "epoch": 0.5915469288295101,
      "grad_norm": 4.250787258148193,
      "learning_rate": 8.527243679578451e-05,
      "loss": 0.2734,
      "step": 10560
    },
    {
      "epoch": 0.5921071058454471,
      "grad_norm": 3.850132942199707,
      "learning_rate": 8.525842255731824e-05,
      "loss": 0.4823,
      "step": 10570
    },
    {
      "epoch": 0.5926672828613841,
      "grad_norm": 4.587120532989502,
      "learning_rate": 8.524440831885195e-05,
      "loss": 0.3124,
      "step": 10580
    },
    {
      "epoch": 0.5932274598773213,
      "grad_norm": 3.8336634635925293,
      "learning_rate": 8.523039408038568e-05,
      "loss": 0.3342,
      "step": 10590
    },
    {
      "epoch": 0.5937876368932583,
      "grad_norm": 4.495619297027588,
      "learning_rate": 8.52163798419194e-05,
      "loss": 0.4717,
      "step": 10600
    },
    {
      "epoch": 0.5943478139091953,
      "grad_norm": 4.614688873291016,
      "learning_rate": 8.520236560345311e-05,
      "loss": 0.3233,
      "step": 10610
    },
    {
      "epoch": 0.5949079909251324,
      "grad_norm": 4.789877891540527,
      "learning_rate": 8.518835136498683e-05,
      "loss": 0.2984,
      "step": 10620
    },
    {
      "epoch": 0.5954681679410694,
      "grad_norm": 5.321944713592529,
      "learning_rate": 8.517433712652056e-05,
      "loss": 0.2618,
      "step": 10630
    },
    {
      "epoch": 0.5960283449570064,
      "grad_norm": 4.205268859863281,
      "learning_rate": 8.516032288805427e-05,
      "loss": 0.5194,
      "step": 10640
    },
    {
      "epoch": 0.5965885219729434,
      "grad_norm": 4.160318374633789,
      "learning_rate": 8.514630864958799e-05,
      "loss": 0.3036,
      "step": 10650
    },
    {
      "epoch": 0.5971486989888805,
      "grad_norm": 4.6054182052612305,
      "learning_rate": 8.51322944111217e-05,
      "loss": 0.3476,
      "step": 10660
    },
    {
      "epoch": 0.5977088760048175,
      "grad_norm": 5.051565647125244,
      "learning_rate": 8.511828017265541e-05,
      "loss": 0.3357,
      "step": 10670
    },
    {
      "epoch": 0.5982690530207545,
      "grad_norm": 3.185602903366089,
      "learning_rate": 8.510426593418914e-05,
      "loss": 0.3713,
      "step": 10680
    },
    {
      "epoch": 0.5988292300366916,
      "grad_norm": 5.864507675170898,
      "learning_rate": 8.509025169572286e-05,
      "loss": 0.2498,
      "step": 10690
    },
    {
      "epoch": 0.5993894070526287,
      "grad_norm": 5.803577899932861,
      "learning_rate": 8.507623745725659e-05,
      "loss": 0.4069,
      "step": 10700
    },
    {
      "epoch": 0.5999495840685657,
      "grad_norm": 6.057193279266357,
      "learning_rate": 8.506222321879029e-05,
      "loss": 0.3602,
      "step": 10710
    },
    {
      "epoch": 0.6005097610845027,
      "grad_norm": 3.3657984733581543,
      "learning_rate": 8.504820898032402e-05,
      "loss": 0.3173,
      "step": 10720
    },
    {
      "epoch": 0.6010699381004397,
      "grad_norm": 3.8201727867126465,
      "learning_rate": 8.503419474185773e-05,
      "loss": 0.2744,
      "step": 10730
    },
    {
      "epoch": 0.6016301151163768,
      "grad_norm": 6.002891540527344,
      "learning_rate": 8.502018050339144e-05,
      "loss": 0.3385,
      "step": 10740
    },
    {
      "epoch": 0.6021902921323138,
      "grad_norm": 5.1392951011657715,
      "learning_rate": 8.500616626492517e-05,
      "loss": 0.3551,
      "step": 10750
    },
    {
      "epoch": 0.6027504691482508,
      "grad_norm": 5.3803582191467285,
      "learning_rate": 8.499215202645889e-05,
      "loss": 0.2183,
      "step": 10760
    },
    {
      "epoch": 0.6033106461641878,
      "grad_norm": 5.228137016296387,
      "learning_rate": 8.49781377879926e-05,
      "loss": 0.4161,
      "step": 10770
    },
    {
      "epoch": 0.603870823180125,
      "grad_norm": 3.7501652240753174,
      "learning_rate": 8.496412354952632e-05,
      "loss": 0.3348,
      "step": 10780
    },
    {
      "epoch": 0.604431000196062,
      "grad_norm": 4.135531425476074,
      "learning_rate": 8.495010931106005e-05,
      "loss": 0.2299,
      "step": 10790
    },
    {
      "epoch": 0.604991177211999,
      "grad_norm": 5.144546031951904,
      "learning_rate": 8.493609507259376e-05,
      "loss": 0.3113,
      "step": 10800
    },
    {
      "epoch": 0.605551354227936,
      "grad_norm": 6.325468063354492,
      "learning_rate": 8.492208083412749e-05,
      "loss": 0.46,
      "step": 10810
    },
    {
      "epoch": 0.6061115312438731,
      "grad_norm": 1.9776637554168701,
      "learning_rate": 8.490806659566119e-05,
      "loss": 0.4713,
      "step": 10820
    },
    {
      "epoch": 0.6066717082598101,
      "grad_norm": 6.719722747802734,
      "learning_rate": 8.48940523571949e-05,
      "loss": 0.3035,
      "step": 10830
    },
    {
      "epoch": 0.6072318852757471,
      "grad_norm": 3.7136359214782715,
      "learning_rate": 8.488003811872863e-05,
      "loss": 0.198,
      "step": 10840
    },
    {
      "epoch": 0.6077920622916841,
      "grad_norm": 2.0533642768859863,
      "learning_rate": 8.486602388026235e-05,
      "loss": 0.4085,
      "step": 10850
    },
    {
      "epoch": 0.6083522393076212,
      "grad_norm": 5.163476943969727,
      "learning_rate": 8.485200964179608e-05,
      "loss": 0.291,
      "step": 10860
    },
    {
      "epoch": 0.6089124163235582,
      "grad_norm": 2.7287988662719727,
      "learning_rate": 8.483799540332979e-05,
      "loss": 0.3079,
      "step": 10870
    },
    {
      "epoch": 0.6094725933394953,
      "grad_norm": 5.397292137145996,
      "learning_rate": 8.48239811648635e-05,
      "loss": 0.4123,
      "step": 10880
    },
    {
      "epoch": 0.6100327703554324,
      "grad_norm": 5.6294169425964355,
      "learning_rate": 8.480996692639722e-05,
      "loss": 0.2536,
      "step": 10890
    },
    {
      "epoch": 0.6105929473713694,
      "grad_norm": 5.732026100158691,
      "learning_rate": 8.479595268793095e-05,
      "loss": 0.4543,
      "step": 10900
    },
    {
      "epoch": 0.6111531243873064,
      "grad_norm": 3.6067163944244385,
      "learning_rate": 8.478193844946466e-05,
      "loss": 0.247,
      "step": 10910
    },
    {
      "epoch": 0.6117133014032434,
      "grad_norm": 2.674621820449829,
      "learning_rate": 8.476792421099838e-05,
      "loss": 0.2893,
      "step": 10920
    },
    {
      "epoch": 0.6122734784191805,
      "grad_norm": 4.344038963317871,
      "learning_rate": 8.475390997253209e-05,
      "loss": 0.3597,
      "step": 10930
    },
    {
      "epoch": 0.6128336554351175,
      "grad_norm": 5.751020431518555,
      "learning_rate": 8.473989573406581e-05,
      "loss": 0.3807,
      "step": 10940
    },
    {
      "epoch": 0.6133938324510545,
      "grad_norm": 5.688125133514404,
      "learning_rate": 8.472588149559954e-05,
      "loss": 0.3779,
      "step": 10950
    },
    {
      "epoch": 0.6139540094669915,
      "grad_norm": 7.112712383270264,
      "learning_rate": 8.471186725713325e-05,
      "loss": 0.4489,
      "step": 10960
    },
    {
      "epoch": 0.6145141864829287,
      "grad_norm": 7.2607011795043945,
      "learning_rate": 8.469785301866698e-05,
      "loss": 0.3638,
      "step": 10970
    },
    {
      "epoch": 0.6150743634988657,
      "grad_norm": 3.3399980068206787,
      "learning_rate": 8.468383878020068e-05,
      "loss": 0.2934,
      "step": 10980
    },
    {
      "epoch": 0.6156345405148027,
      "grad_norm": 5.56577730178833,
      "learning_rate": 8.466982454173441e-05,
      "loss": 0.2477,
      "step": 10990
    },
    {
      "epoch": 0.6161947175307397,
      "grad_norm": 2.5296075344085693,
      "learning_rate": 8.465581030326812e-05,
      "loss": 0.2718,
      "step": 11000
    },
    {
      "epoch": 0.6167548945466768,
      "grad_norm": 4.0435261726379395,
      "learning_rate": 8.464179606480185e-05,
      "loss": 0.4494,
      "step": 11010
    },
    {
      "epoch": 0.6173150715626138,
      "grad_norm": 5.0610504150390625,
      "learning_rate": 8.462778182633557e-05,
      "loss": 0.3401,
      "step": 11020
    },
    {
      "epoch": 0.6178752485785508,
      "grad_norm": 3.5715153217315674,
      "learning_rate": 8.461376758786928e-05,
      "loss": 0.2822,
      "step": 11030
    },
    {
      "epoch": 0.6184354255944878,
      "grad_norm": 5.554010391235352,
      "learning_rate": 8.4599753349403e-05,
      "loss": 0.2304,
      "step": 11040
    },
    {
      "epoch": 0.6189956026104249,
      "grad_norm": 5.200068473815918,
      "learning_rate": 8.458573911093671e-05,
      "loss": 0.3922,
      "step": 11050
    },
    {
      "epoch": 0.6195557796263619,
      "grad_norm": 5.961895942687988,
      "learning_rate": 8.457172487247044e-05,
      "loss": 0.2936,
      "step": 11060
    },
    {
      "epoch": 0.620115956642299,
      "grad_norm": 6.481044292449951,
      "learning_rate": 8.455771063400415e-05,
      "loss": 0.2913,
      "step": 11070
    },
    {
      "epoch": 0.620676133658236,
      "grad_norm": 3.832017421722412,
      "learning_rate": 8.454369639553787e-05,
      "loss": 0.3519,
      "step": 11080
    },
    {
      "epoch": 0.6212363106741731,
      "grad_norm": 3.1032845973968506,
      "learning_rate": 8.452968215707158e-05,
      "loss": 0.2921,
      "step": 11090
    },
    {
      "epoch": 0.6217964876901101,
      "grad_norm": 3.8231003284454346,
      "learning_rate": 8.451566791860531e-05,
      "loss": 0.2585,
      "step": 11100
    },
    {
      "epoch": 0.6223566647060471,
      "grad_norm": 4.780762195587158,
      "learning_rate": 8.450165368013903e-05,
      "loss": 0.2683,
      "step": 11110
    },
    {
      "epoch": 0.6229168417219841,
      "grad_norm": 5.535638809204102,
      "learning_rate": 8.448763944167274e-05,
      "loss": 0.3411,
      "step": 11120
    },
    {
      "epoch": 0.6234770187379212,
      "grad_norm": 3.078420877456665,
      "learning_rate": 8.447362520320647e-05,
      "loss": 0.186,
      "step": 11130
    },
    {
      "epoch": 0.6240371957538582,
      "grad_norm": 4.163439750671387,
      "learning_rate": 8.445961096474017e-05,
      "loss": 0.2715,
      "step": 11140
    },
    {
      "epoch": 0.6245973727697952,
      "grad_norm": 2.433995485305786,
      "learning_rate": 8.44455967262739e-05,
      "loss": 0.5455,
      "step": 11150
    },
    {
      "epoch": 0.6251575497857322,
      "grad_norm": 5.064175605773926,
      "learning_rate": 8.443158248780761e-05,
      "loss": 0.3318,
      "step": 11160
    },
    {
      "epoch": 0.6257177268016694,
      "grad_norm": 5.07220983505249,
      "learning_rate": 8.441756824934134e-05,
      "loss": 0.3893,
      "step": 11170
    },
    {
      "epoch": 0.6262779038176064,
      "grad_norm": 2.8174047470092773,
      "learning_rate": 8.440355401087506e-05,
      "loss": 0.4132,
      "step": 11180
    },
    {
      "epoch": 0.6268380808335434,
      "grad_norm": 5.249160289764404,
      "learning_rate": 8.438953977240877e-05,
      "loss": 0.3033,
      "step": 11190
    },
    {
      "epoch": 0.6273982578494804,
      "grad_norm": 5.618062973022461,
      "learning_rate": 8.437552553394249e-05,
      "loss": 0.2804,
      "step": 11200
    },
    {
      "epoch": 0.6279584348654175,
      "grad_norm": 6.577373027801514,
      "learning_rate": 8.43615112954762e-05,
      "loss": 0.2428,
      "step": 11210
    },
    {
      "epoch": 0.6285186118813545,
      "grad_norm": 4.915947914123535,
      "learning_rate": 8.434749705700993e-05,
      "loss": 0.295,
      "step": 11220
    },
    {
      "epoch": 0.6290787888972915,
      "grad_norm": 5.44497537612915,
      "learning_rate": 8.433348281854364e-05,
      "loss": 0.263,
      "step": 11230
    },
    {
      "epoch": 0.6296389659132285,
      "grad_norm": 2.306750535964966,
      "learning_rate": 8.431946858007737e-05,
      "loss": 0.2601,
      "step": 11240
    },
    {
      "epoch": 0.6301991429291656,
      "grad_norm": 5.1973185539245605,
      "learning_rate": 8.430545434161107e-05,
      "loss": 0.288,
      "step": 11250
    },
    {
      "epoch": 0.6307593199451027,
      "grad_norm": 5.645621299743652,
      "learning_rate": 8.42914401031448e-05,
      "loss": 0.3247,
      "step": 11260
    },
    {
      "epoch": 0.6313194969610397,
      "grad_norm": 4.019838809967041,
      "learning_rate": 8.427742586467852e-05,
      "loss": 0.3752,
      "step": 11270
    },
    {
      "epoch": 0.6318796739769768,
      "grad_norm": 4.783576488494873,
      "learning_rate": 8.426341162621224e-05,
      "loss": 0.2343,
      "step": 11280
    },
    {
      "epoch": 0.6324398509929138,
      "grad_norm": 2.859212875366211,
      "learning_rate": 8.424939738774596e-05,
      "loss": 0.3364,
      "step": 11290
    },
    {
      "epoch": 0.6330000280088508,
      "grad_norm": 1.9695988893508911,
      "learning_rate": 8.423538314927967e-05,
      "loss": 0.2891,
      "step": 11300
    },
    {
      "epoch": 0.6335602050247878,
      "grad_norm": 7.5959601402282715,
      "learning_rate": 8.422136891081339e-05,
      "loss": 0.3011,
      "step": 11310
    },
    {
      "epoch": 0.6341203820407249,
      "grad_norm": 2.9281649589538574,
      "learning_rate": 8.42073546723471e-05,
      "loss": 0.3428,
      "step": 11320
    },
    {
      "epoch": 0.6346805590566619,
      "grad_norm": 4.3597211837768555,
      "learning_rate": 8.419334043388083e-05,
      "loss": 0.2832,
      "step": 11330
    },
    {
      "epoch": 0.6352407360725989,
      "grad_norm": 7.434643745422363,
      "learning_rate": 8.417932619541455e-05,
      "loss": 0.3182,
      "step": 11340
    },
    {
      "epoch": 0.6358009130885359,
      "grad_norm": 3.8174502849578857,
      "learning_rate": 8.416531195694826e-05,
      "loss": 0.2272,
      "step": 11350
    },
    {
      "epoch": 0.6363610901044731,
      "grad_norm": 3.8329219818115234,
      "learning_rate": 8.415129771848198e-05,
      "loss": 0.2508,
      "step": 11360
    },
    {
      "epoch": 0.6369212671204101,
      "grad_norm": 2.331198215484619,
      "learning_rate": 8.41372834800157e-05,
      "loss": 0.344,
      "step": 11370
    },
    {
      "epoch": 0.6374814441363471,
      "grad_norm": 4.9567670822143555,
      "learning_rate": 8.412326924154942e-05,
      "loss": 0.2718,
      "step": 11380
    },
    {
      "epoch": 0.6380416211522841,
      "grad_norm": 5.609871864318848,
      "learning_rate": 8.410925500308313e-05,
      "loss": 0.2755,
      "step": 11390
    },
    {
      "epoch": 0.6386017981682212,
      "grad_norm": 6.222424030303955,
      "learning_rate": 8.409524076461686e-05,
      "loss": 0.3425,
      "step": 11400
    },
    {
      "epoch": 0.6391619751841582,
      "grad_norm": 4.595231056213379,
      "learning_rate": 8.408122652615056e-05,
      "loss": 0.4654,
      "step": 11410
    },
    {
      "epoch": 0.6397221522000952,
      "grad_norm": 4.54720401763916,
      "learning_rate": 8.406721228768429e-05,
      "loss": 0.3037,
      "step": 11420
    },
    {
      "epoch": 0.6402823292160322,
      "grad_norm": 5.117251396179199,
      "learning_rate": 8.4053198049218e-05,
      "loss": 0.2441,
      "step": 11430
    },
    {
      "epoch": 0.6408425062319693,
      "grad_norm": 3.7553958892822266,
      "learning_rate": 8.403918381075173e-05,
      "loss": 0.3961,
      "step": 11440
    },
    {
      "epoch": 0.6414026832479064,
      "grad_norm": 8.270973205566406,
      "learning_rate": 8.402516957228545e-05,
      "loss": 0.354,
      "step": 11450
    },
    {
      "epoch": 0.6419628602638434,
      "grad_norm": 2.853867769241333,
      "learning_rate": 8.401115533381916e-05,
      "loss": 0.3122,
      "step": 11460
    },
    {
      "epoch": 0.6425230372797804,
      "grad_norm": 8.58159351348877,
      "learning_rate": 8.399714109535288e-05,
      "loss": 0.3515,
      "step": 11470
    },
    {
      "epoch": 0.6430832142957175,
      "grad_norm": 5.356987953186035,
      "learning_rate": 8.39831268568866e-05,
      "loss": 0.3909,
      "step": 11480
    },
    {
      "epoch": 0.6436433913116545,
      "grad_norm": 2.0694870948791504,
      "learning_rate": 8.396911261842032e-05,
      "loss": 0.3625,
      "step": 11490
    },
    {
      "epoch": 0.6442035683275915,
      "grad_norm": 5.365023612976074,
      "learning_rate": 8.395509837995404e-05,
      "loss": 0.31,
      "step": 11500
    },
    {
      "epoch": 0.6447637453435285,
      "grad_norm": 4.845766544342041,
      "learning_rate": 8.394108414148776e-05,
      "loss": 0.3669,
      "step": 11510
    },
    {
      "epoch": 0.6453239223594656,
      "grad_norm": 7.614165306091309,
      "learning_rate": 8.392706990302147e-05,
      "loss": 0.2871,
      "step": 11520
    },
    {
      "epoch": 0.6458840993754026,
      "grad_norm": 3.3263912200927734,
      "learning_rate": 8.39130556645552e-05,
      "loss": 0.2135,
      "step": 11530
    },
    {
      "epoch": 0.6464442763913396,
      "grad_norm": 2.7803995609283447,
      "learning_rate": 8.389904142608891e-05,
      "loss": 0.2161,
      "step": 11540
    },
    {
      "epoch": 0.6470044534072767,
      "grad_norm": 5.809021472930908,
      "learning_rate": 8.388502718762264e-05,
      "loss": 0.3139,
      "step": 11550
    },
    {
      "epoch": 0.6475646304232138,
      "grad_norm": 5.171698570251465,
      "learning_rate": 8.387101294915635e-05,
      "loss": 0.1998,
      "step": 11560
    },
    {
      "epoch": 0.6481248074391508,
      "grad_norm": 6.935198783874512,
      "learning_rate": 8.385699871069007e-05,
      "loss": 0.2629,
      "step": 11570
    },
    {
      "epoch": 0.6486849844550878,
      "grad_norm": 6.140023708343506,
      "learning_rate": 8.384298447222378e-05,
      "loss": 0.397,
      "step": 11580
    },
    {
      "epoch": 0.6492451614710248,
      "grad_norm": 4.079397201538086,
      "learning_rate": 8.38289702337575e-05,
      "loss": 0.3709,
      "step": 11590
    },
    {
      "epoch": 0.6498053384869619,
      "grad_norm": 7.246945381164551,
      "learning_rate": 8.381495599529122e-05,
      "loss": 0.3127,
      "step": 11600
    },
    {
      "epoch": 0.6503655155028989,
      "grad_norm": 4.51418399810791,
      "learning_rate": 8.380094175682494e-05,
      "loss": 0.2017,
      "step": 11610
    },
    {
      "epoch": 0.6509256925188359,
      "grad_norm": 3.0218703746795654,
      "learning_rate": 8.378692751835865e-05,
      "loss": 0.229,
      "step": 11620
    },
    {
      "epoch": 0.6514858695347729,
      "grad_norm": 5.101027488708496,
      "learning_rate": 8.377291327989237e-05,
      "loss": 0.3875,
      "step": 11630
    },
    {
      "epoch": 0.6520460465507101,
      "grad_norm": 3.305185556411743,
      "learning_rate": 8.37588990414261e-05,
      "loss": 0.2863,
      "step": 11640
    },
    {
      "epoch": 0.6526062235666471,
      "grad_norm": 4.0749592781066895,
      "learning_rate": 8.374488480295981e-05,
      "loss": 0.2447,
      "step": 11650
    },
    {
      "epoch": 0.6531664005825841,
      "grad_norm": 5.454889297485352,
      "learning_rate": 8.373087056449353e-05,
      "loss": 0.3723,
      "step": 11660
    },
    {
      "epoch": 0.6537265775985212,
      "grad_norm": 3.756852865219116,
      "learning_rate": 8.371685632602725e-05,
      "loss": 0.2186,
      "step": 11670
    },
    {
      "epoch": 0.6542867546144582,
      "grad_norm": 4.422957897186279,
      "learning_rate": 8.370284208756096e-05,
      "loss": 0.28,
      "step": 11680
    },
    {
      "epoch": 0.6548469316303952,
      "grad_norm": 5.137227535247803,
      "learning_rate": 8.368882784909468e-05,
      "loss": 0.4291,
      "step": 11690
    },
    {
      "epoch": 0.6554071086463322,
      "grad_norm": 4.008235931396484,
      "learning_rate": 8.36748136106284e-05,
      "loss": 0.3023,
      "step": 11700
    },
    {
      "epoch": 0.6559672856622693,
      "grad_norm": 4.861349582672119,
      "learning_rate": 8.366079937216213e-05,
      "loss": 0.3176,
      "step": 11710
    },
    {
      "epoch": 0.6565274626782063,
      "grad_norm": 4.4701948165893555,
      "learning_rate": 8.364678513369584e-05,
      "loss": 0.2419,
      "step": 11720
    },
    {
      "epoch": 0.6570876396941433,
      "grad_norm": 2.934661626815796,
      "learning_rate": 8.363277089522956e-05,
      "loss": 0.3103,
      "step": 11730
    },
    {
      "epoch": 0.6576478167100804,
      "grad_norm": 5.4857001304626465,
      "learning_rate": 8.361875665676327e-05,
      "loss": 0.3277,
      "step": 11740
    },
    {
      "epoch": 0.6582079937260175,
      "grad_norm": 4.940674781799316,
      "learning_rate": 8.360474241829699e-05,
      "loss": 0.1681,
      "step": 11750
    },
    {
      "epoch": 0.6587681707419545,
      "grad_norm": 4.647923946380615,
      "learning_rate": 8.359072817983071e-05,
      "loss": 0.4237,
      "step": 11760
    },
    {
      "epoch": 0.6593283477578915,
      "grad_norm": 4.842194080352783,
      "learning_rate": 8.357671394136443e-05,
      "loss": 0.2722,
      "step": 11770
    },
    {
      "epoch": 0.6598885247738285,
      "grad_norm": 4.4735517501831055,
      "learning_rate": 8.356269970289814e-05,
      "loss": 0.3721,
      "step": 11780
    },
    {
      "epoch": 0.6604487017897656,
      "grad_norm": 5.059861660003662,
      "learning_rate": 8.354868546443186e-05,
      "loss": 0.1946,
      "step": 11790
    },
    {
      "epoch": 0.6610088788057026,
      "grad_norm": 6.239191055297852,
      "learning_rate": 8.353467122596559e-05,
      "loss": 0.2796,
      "step": 11800
    },
    {
      "epoch": 0.6615690558216396,
      "grad_norm": 6.473159313201904,
      "learning_rate": 8.35206569874993e-05,
      "loss": 0.5051,
      "step": 11810
    },
    {
      "epoch": 0.6621292328375766,
      "grad_norm": 3.105243444442749,
      "learning_rate": 8.350664274903303e-05,
      "loss": 0.3013,
      "step": 11820
    },
    {
      "epoch": 0.6626894098535137,
      "grad_norm": 5.556397438049316,
      "learning_rate": 8.349262851056674e-05,
      "loss": 0.1709,
      "step": 11830
    },
    {
      "epoch": 0.6632495868694508,
      "grad_norm": 5.249423027038574,
      "learning_rate": 8.347861427210045e-05,
      "loss": 0.3493,
      "step": 11840
    },
    {
      "epoch": 0.6638097638853878,
      "grad_norm": 4.423429012298584,
      "learning_rate": 8.346460003363417e-05,
      "loss": 0.2282,
      "step": 11850
    },
    {
      "epoch": 0.6643699409013248,
      "grad_norm": 1.923012137413025,
      "learning_rate": 8.345058579516789e-05,
      "loss": 0.2694,
      "step": 11860
    },
    {
      "epoch": 0.6649301179172619,
      "grad_norm": 3.0662763118743896,
      "learning_rate": 8.343657155670162e-05,
      "loss": 0.3903,
      "step": 11870
    },
    {
      "epoch": 0.6654902949331989,
      "grad_norm": 5.871947765350342,
      "learning_rate": 8.342255731823533e-05,
      "loss": 0.2917,
      "step": 11880
    },
    {
      "epoch": 0.6660504719491359,
      "grad_norm": 4.8481645584106445,
      "learning_rate": 8.340854307976905e-05,
      "loss": 0.2852,
      "step": 11890
    },
    {
      "epoch": 0.6666106489650729,
      "grad_norm": 2.799525499343872,
      "learning_rate": 8.339452884130276e-05,
      "loss": 0.3067,
      "step": 11900
    },
    {
      "epoch": 0.66717082598101,
      "grad_norm": 5.1028571128845215,
      "learning_rate": 8.338051460283649e-05,
      "loss": 0.3531,
      "step": 11910
    },
    {
      "epoch": 0.667731002996947,
      "grad_norm": 2.6545302867889404,
      "learning_rate": 8.33665003643702e-05,
      "loss": 0.3365,
      "step": 11920
    },
    {
      "epoch": 0.6682911800128841,
      "grad_norm": 3.4409806728363037,
      "learning_rate": 8.335248612590392e-05,
      "loss": 0.2925,
      "step": 11930
    },
    {
      "epoch": 0.6688513570288211,
      "grad_norm": 6.5750412940979,
      "learning_rate": 8.333847188743765e-05,
      "loss": 0.2862,
      "step": 11940
    },
    {
      "epoch": 0.6694115340447582,
      "grad_norm": 4.333851337432861,
      "learning_rate": 8.332445764897135e-05,
      "loss": 0.3086,
      "step": 11950
    },
    {
      "epoch": 0.6699717110606952,
      "grad_norm": 4.975213050842285,
      "learning_rate": 8.331044341050508e-05,
      "loss": 0.3969,
      "step": 11960
    },
    {
      "epoch": 0.6705318880766322,
      "grad_norm": 8.470430374145508,
      "learning_rate": 8.329642917203879e-05,
      "loss": 0.1911,
      "step": 11970
    },
    {
      "epoch": 0.6710920650925692,
      "grad_norm": 2.55344557762146,
      "learning_rate": 8.328241493357252e-05,
      "loss": 0.3112,
      "step": 11980
    },
    {
      "epoch": 0.6716522421085063,
      "grad_norm": 5.244578838348389,
      "learning_rate": 8.326840069510624e-05,
      "loss": 0.3367,
      "step": 11990
    },
    {
      "epoch": 0.6722124191244433,
      "grad_norm": 6.480720520019531,
      "learning_rate": 8.325438645663995e-05,
      "loss": 0.3512,
      "step": 12000
    },
    {
      "epoch": 0.6727725961403803,
      "grad_norm": 4.1847405433654785,
      "learning_rate": 8.324037221817366e-05,
      "loss": 0.2503,
      "step": 12010
    },
    {
      "epoch": 0.6733327731563173,
      "grad_norm": 3.110813617706299,
      "learning_rate": 8.322635797970739e-05,
      "loss": 0.3039,
      "step": 12020
    },
    {
      "epoch": 0.6738929501722545,
      "grad_norm": 4.318905353546143,
      "learning_rate": 8.321234374124111e-05,
      "loss": 0.2387,
      "step": 12030
    },
    {
      "epoch": 0.6744531271881915,
      "grad_norm": 4.578129768371582,
      "learning_rate": 8.319832950277482e-05,
      "loss": 0.227,
      "step": 12040
    },
    {
      "epoch": 0.6750133042041285,
      "grad_norm": 6.162400245666504,
      "learning_rate": 8.318431526430854e-05,
      "loss": 0.2954,
      "step": 12050
    },
    {
      "epoch": 0.6755734812200656,
      "grad_norm": 3.320863962173462,
      "learning_rate": 8.317030102584225e-05,
      "loss": 0.3159,
      "step": 12060
    },
    {
      "epoch": 0.6761336582360026,
      "grad_norm": 6.42199182510376,
      "learning_rate": 8.315628678737598e-05,
      "loss": 0.3219,
      "step": 12070
    },
    {
      "epoch": 0.6766938352519396,
      "grad_norm": 5.487936973571777,
      "learning_rate": 8.31422725489097e-05,
      "loss": 0.334,
      "step": 12080
    },
    {
      "epoch": 0.6772540122678766,
      "grad_norm": 4.544373512268066,
      "learning_rate": 8.312825831044342e-05,
      "loss": 0.2845,
      "step": 12090
    },
    {
      "epoch": 0.6778141892838137,
      "grad_norm": 5.771264553070068,
      "learning_rate": 8.311424407197714e-05,
      "loss": 0.3869,
      "step": 12100
    },
    {
      "epoch": 0.6783743662997507,
      "grad_norm": 4.595292091369629,
      "learning_rate": 8.310022983351085e-05,
      "loss": 0.2222,
      "step": 12110
    },
    {
      "epoch": 0.6789345433156878,
      "grad_norm": 3.077165365219116,
      "learning_rate": 8.308621559504457e-05,
      "loss": 0.1996,
      "step": 12120
    },
    {
      "epoch": 0.6794947203316248,
      "grad_norm": 7.155720233917236,
      "learning_rate": 8.307220135657828e-05,
      "loss": 0.3913,
      "step": 12130
    },
    {
      "epoch": 0.6800548973475619,
      "grad_norm": 2.839883804321289,
      "learning_rate": 8.305818711811201e-05,
      "loss": 0.2654,
      "step": 12140
    },
    {
      "epoch": 0.6806150743634989,
      "grad_norm": 7.709593772888184,
      "learning_rate": 8.304417287964573e-05,
      "loss": 0.1982,
      "step": 12150
    },
    {
      "epoch": 0.6811752513794359,
      "grad_norm": 4.057681560516357,
      "learning_rate": 8.303015864117944e-05,
      "loss": 0.5324,
      "step": 12160
    },
    {
      "epoch": 0.6817354283953729,
      "grad_norm": 5.60280179977417,
      "learning_rate": 8.301614440271315e-05,
      "loss": 0.2755,
      "step": 12170
    },
    {
      "epoch": 0.68229560541131,
      "grad_norm": 3.5109670162200928,
      "learning_rate": 8.300213016424688e-05,
      "loss": 0.2685,
      "step": 12180
    },
    {
      "epoch": 0.682855782427247,
      "grad_norm": 4.676346302032471,
      "learning_rate": 8.29881159257806e-05,
      "loss": 0.1793,
      "step": 12190
    },
    {
      "epoch": 0.683415959443184,
      "grad_norm": 6.8459601402282715,
      "learning_rate": 8.297410168731433e-05,
      "loss": 0.4468,
      "step": 12200
    },
    {
      "epoch": 0.683976136459121,
      "grad_norm": 6.706006050109863,
      "learning_rate": 8.296008744884804e-05,
      "loss": 0.2183,
      "step": 12210
    },
    {
      "epoch": 0.6845363134750582,
      "grad_norm": 3.688138484954834,
      "learning_rate": 8.294607321038174e-05,
      "loss": 0.2593,
      "step": 12220
    },
    {
      "epoch": 0.6850964904909952,
      "grad_norm": 3.946558713912964,
      "learning_rate": 8.293205897191547e-05,
      "loss": 0.2427,
      "step": 12230
    },
    {
      "epoch": 0.6856566675069322,
      "grad_norm": 4.0457963943481445,
      "learning_rate": 8.291804473344918e-05,
      "loss": 0.194,
      "step": 12240
    },
    {
      "epoch": 0.6862168445228692,
      "grad_norm": 6.7025556564331055,
      "learning_rate": 8.290403049498291e-05,
      "loss": 0.3671,
      "step": 12250
    },
    {
      "epoch": 0.6867770215388063,
      "grad_norm": 3.9266390800476074,
      "learning_rate": 8.289001625651663e-05,
      "loss": 0.4811,
      "step": 12260
    },
    {
      "epoch": 0.6873371985547433,
      "grad_norm": 5.6055402755737305,
      "learning_rate": 8.287600201805034e-05,
      "loss": 0.168,
      "step": 12270
    },
    {
      "epoch": 0.6878973755706803,
      "grad_norm": 3.472133159637451,
      "learning_rate": 8.286198777958406e-05,
      "loss": 0.3526,
      "step": 12280
    },
    {
      "epoch": 0.6884575525866173,
      "grad_norm": 4.0987443923950195,
      "learning_rate": 8.284797354111779e-05,
      "loss": 0.2855,
      "step": 12290
    },
    {
      "epoch": 0.6890177296025544,
      "grad_norm": 5.864036560058594,
      "learning_rate": 8.28339593026515e-05,
      "loss": 0.3141,
      "step": 12300
    },
    {
      "epoch": 0.6895779066184915,
      "grad_norm": 1.874807357788086,
      "learning_rate": 8.281994506418522e-05,
      "loss": 0.3116,
      "step": 12310
    },
    {
      "epoch": 0.6901380836344285,
      "grad_norm": 5.794522762298584,
      "learning_rate": 8.280593082571893e-05,
      "loss": 0.343,
      "step": 12320
    },
    {
      "epoch": 0.6906982606503655,
      "grad_norm": 3.80954909324646,
      "learning_rate": 8.279191658725264e-05,
      "loss": 0.2963,
      "step": 12330
    },
    {
      "epoch": 0.6912584376663026,
      "grad_norm": 3.353956937789917,
      "learning_rate": 8.277790234878637e-05,
      "loss": 0.2604,
      "step": 12340
    },
    {
      "epoch": 0.6918186146822396,
      "grad_norm": 6.688965320587158,
      "learning_rate": 8.276388811032009e-05,
      "loss": 0.2995,
      "step": 12350
    },
    {
      "epoch": 0.6923787916981766,
      "grad_norm": 5.982239723205566,
      "learning_rate": 8.274987387185382e-05,
      "loss": 0.2551,
      "step": 12360
    },
    {
      "epoch": 0.6929389687141136,
      "grad_norm": 3.1775364875793457,
      "learning_rate": 8.273585963338753e-05,
      "loss": 0.3998,
      "step": 12370
    },
    {
      "epoch": 0.6934991457300507,
      "grad_norm": 3.7412614822387695,
      "learning_rate": 8.272184539492125e-05,
      "loss": 0.2177,
      "step": 12380
    },
    {
      "epoch": 0.6940593227459877,
      "grad_norm": 5.288415908813477,
      "learning_rate": 8.270783115645496e-05,
      "loss": 0.1863,
      "step": 12390
    },
    {
      "epoch": 0.6946194997619247,
      "grad_norm": 4.799376010894775,
      "learning_rate": 8.269381691798868e-05,
      "loss": 0.3228,
      "step": 12400
    },
    {
      "epoch": 0.6951796767778619,
      "grad_norm": 4.931370258331299,
      "learning_rate": 8.26798026795224e-05,
      "loss": 0.3327,
      "step": 12410
    },
    {
      "epoch": 0.6957398537937989,
      "grad_norm": 2.4878792762756348,
      "learning_rate": 8.266578844105612e-05,
      "loss": 0.3528,
      "step": 12420
    },
    {
      "epoch": 0.6963000308097359,
      "grad_norm": 5.791815280914307,
      "learning_rate": 8.265177420258983e-05,
      "loss": 0.3387,
      "step": 12430
    },
    {
      "epoch": 0.6968602078256729,
      "grad_norm": 4.1399664878845215,
      "learning_rate": 8.263775996412355e-05,
      "loss": 0.3127,
      "step": 12440
    },
    {
      "epoch": 0.69742038484161,
      "grad_norm": 4.47131872177124,
      "learning_rate": 8.262374572565728e-05,
      "loss": 0.3318,
      "step": 12450
    },
    {
      "epoch": 0.697980561857547,
      "grad_norm": 5.0057268142700195,
      "learning_rate": 8.260973148719099e-05,
      "loss": 0.2986,
      "step": 12460
    },
    {
      "epoch": 0.698540738873484,
      "grad_norm": 5.807600498199463,
      "learning_rate": 8.259571724872472e-05,
      "loss": 0.3112,
      "step": 12470
    },
    {
      "epoch": 0.699100915889421,
      "grad_norm": 8.028942108154297,
      "learning_rate": 8.258170301025842e-05,
      "loss": 0.2785,
      "step": 12480
    },
    {
      "epoch": 0.699661092905358,
      "grad_norm": 6.208786964416504,
      "learning_rate": 8.256768877179213e-05,
      "loss": 0.1989,
      "step": 12490
    },
    {
      "epoch": 0.7002212699212951,
      "grad_norm": 4.325305938720703,
      "learning_rate": 8.255367453332586e-05,
      "loss": 0.2648,
      "step": 12500
    },
    {
      "epoch": 0.7007814469372322,
      "grad_norm": 4.907880783081055,
      "learning_rate": 8.253966029485958e-05,
      "loss": 0.1636,
      "step": 12510
    },
    {
      "epoch": 0.7013416239531692,
      "grad_norm": 2.235358476638794,
      "learning_rate": 8.25256460563933e-05,
      "loss": 0.182,
      "step": 12520
    },
    {
      "epoch": 0.7019018009691063,
      "grad_norm": 3.22541880607605,
      "learning_rate": 8.251163181792702e-05,
      "loss": 0.4173,
      "step": 12530
    },
    {
      "epoch": 0.7024619779850433,
      "grad_norm": 10.718125343322754,
      "learning_rate": 8.249761757946074e-05,
      "loss": 0.4084,
      "step": 12540
    },
    {
      "epoch": 0.7030221550009803,
      "grad_norm": 6.884542465209961,
      "learning_rate": 8.248360334099445e-05,
      "loss": 0.3851,
      "step": 12550
    },
    {
      "epoch": 0.7035823320169173,
      "grad_norm": 3.76689076423645,
      "learning_rate": 8.246958910252818e-05,
      "loss": 0.3124,
      "step": 12560
    },
    {
      "epoch": 0.7041425090328544,
      "grad_norm": 5.498812675476074,
      "learning_rate": 8.24555748640619e-05,
      "loss": 0.2506,
      "step": 12570
    },
    {
      "epoch": 0.7047026860487914,
      "grad_norm": 4.962085247039795,
      "learning_rate": 8.244156062559561e-05,
      "loss": 0.33,
      "step": 12580
    },
    {
      "epoch": 0.7052628630647284,
      "grad_norm": 4.987122535705566,
      "learning_rate": 8.242754638712932e-05,
      "loss": 0.3902,
      "step": 12590
    },
    {
      "epoch": 0.7058230400806655,
      "grad_norm": 5.552028656005859,
      "learning_rate": 8.241353214866304e-05,
      "loss": 0.3886,
      "step": 12600
    },
    {
      "epoch": 0.7063832170966026,
      "grad_norm": 4.937835693359375,
      "learning_rate": 8.239951791019677e-05,
      "loss": 0.238,
      "step": 12610
    },
    {
      "epoch": 0.7069433941125396,
      "grad_norm": 8.401891708374023,
      "learning_rate": 8.238550367173048e-05,
      "loss": 0.2389,
      "step": 12620
    },
    {
      "epoch": 0.7075035711284766,
      "grad_norm": 4.518152713775635,
      "learning_rate": 8.237148943326421e-05,
      "loss": 0.2286,
      "step": 12630
    },
    {
      "epoch": 0.7080637481444136,
      "grad_norm": 3.2579805850982666,
      "learning_rate": 8.235747519479792e-05,
      "loss": 0.2612,
      "step": 12640
    },
    {
      "epoch": 0.7086239251603507,
      "grad_norm": 5.046763896942139,
      "learning_rate": 8.234346095633164e-05,
      "loss": 0.2489,
      "step": 12650
    },
    {
      "epoch": 0.7091841021762877,
      "grad_norm": 8.079231262207031,
      "learning_rate": 8.232944671786535e-05,
      "loss": 0.2652,
      "step": 12660
    },
    {
      "epoch": 0.7097442791922247,
      "grad_norm": 3.656848907470703,
      "learning_rate": 8.231543247939907e-05,
      "loss": 0.3191,
      "step": 12670
    },
    {
      "epoch": 0.7103044562081617,
      "grad_norm": 4.380566596984863,
      "learning_rate": 8.23014182409328e-05,
      "loss": 0.3946,
      "step": 12680
    },
    {
      "epoch": 0.7108646332240988,
      "grad_norm": 2.691157817840576,
      "learning_rate": 8.228740400246651e-05,
      "loss": 0.2842,
      "step": 12690
    },
    {
      "epoch": 0.7114248102400359,
      "grad_norm": 3.502720355987549,
      "learning_rate": 8.227338976400023e-05,
      "loss": 0.2156,
      "step": 12700
    },
    {
      "epoch": 0.7119849872559729,
      "grad_norm": 7.00828742980957,
      "learning_rate": 8.225937552553394e-05,
      "loss": 0.3229,
      "step": 12710
    },
    {
      "epoch": 0.71254516427191,
      "grad_norm": 4.775701999664307,
      "learning_rate": 8.224536128706767e-05,
      "loss": 0.3605,
      "step": 12720
    },
    {
      "epoch": 0.713105341287847,
      "grad_norm": 7.549470901489258,
      "learning_rate": 8.223134704860138e-05,
      "loss": 0.347,
      "step": 12730
    },
    {
      "epoch": 0.713665518303784,
      "grad_norm": 3.985032558441162,
      "learning_rate": 8.221733281013511e-05,
      "loss": 0.3282,
      "step": 12740
    },
    {
      "epoch": 0.714225695319721,
      "grad_norm": 5.917499542236328,
      "learning_rate": 8.220331857166881e-05,
      "loss": 0.2823,
      "step": 12750
    },
    {
      "epoch": 0.714785872335658,
      "grad_norm": 4.956936836242676,
      "learning_rate": 8.218930433320253e-05,
      "loss": 0.336,
      "step": 12760
    },
    {
      "epoch": 0.7153460493515951,
      "grad_norm": 3.929079055786133,
      "learning_rate": 8.217529009473626e-05,
      "loss": 0.2655,
      "step": 12770
    },
    {
      "epoch": 0.7159062263675321,
      "grad_norm": 6.622883319854736,
      "learning_rate": 8.216127585626997e-05,
      "loss": 0.3106,
      "step": 12780
    },
    {
      "epoch": 0.7164664033834692,
      "grad_norm": 6.922969818115234,
      "learning_rate": 8.21472616178037e-05,
      "loss": 0.3834,
      "step": 12790
    },
    {
      "epoch": 0.7170265803994063,
      "grad_norm": 3.3250908851623535,
      "learning_rate": 8.213324737933741e-05,
      "loss": 0.2522,
      "step": 12800
    },
    {
      "epoch": 0.7175867574153433,
      "grad_norm": 5.041214942932129,
      "learning_rate": 8.211923314087113e-05,
      "loss": 0.2498,
      "step": 12810
    },
    {
      "epoch": 0.7181469344312803,
      "grad_norm": 5.664946556091309,
      "learning_rate": 8.210521890240484e-05,
      "loss": 0.2712,
      "step": 12820
    },
    {
      "epoch": 0.7187071114472173,
      "grad_norm": 4.493811130523682,
      "learning_rate": 8.209120466393857e-05,
      "loss": 0.3746,
      "step": 12830
    },
    {
      "epoch": 0.7192672884631544,
      "grad_norm": 5.270609378814697,
      "learning_rate": 8.207719042547229e-05,
      "loss": 0.312,
      "step": 12840
    },
    {
      "epoch": 0.7198274654790914,
      "grad_norm": 4.92240571975708,
      "learning_rate": 8.2063176187006e-05,
      "loss": 0.3943,
      "step": 12850
    },
    {
      "epoch": 0.7203876424950284,
      "grad_norm": 4.12033748626709,
      "learning_rate": 8.204916194853972e-05,
      "loss": 0.4299,
      "step": 12860
    },
    {
      "epoch": 0.7209478195109654,
      "grad_norm": 4.0947489738464355,
      "learning_rate": 8.203514771007343e-05,
      "loss": 0.2471,
      "step": 12870
    },
    {
      "epoch": 0.7215079965269025,
      "grad_norm": 3.326071262359619,
      "learning_rate": 8.202113347160716e-05,
      "loss": 0.3345,
      "step": 12880
    },
    {
      "epoch": 0.7220681735428396,
      "grad_norm": 3.2360711097717285,
      "learning_rate": 8.200711923314087e-05,
      "loss": 0.3027,
      "step": 12890
    },
    {
      "epoch": 0.7226283505587766,
      "grad_norm": 4.152468204498291,
      "learning_rate": 8.19931049946746e-05,
      "loss": 0.2439,
      "step": 12900
    },
    {
      "epoch": 0.7231885275747136,
      "grad_norm": 3.0217537879943848,
      "learning_rate": 8.197909075620832e-05,
      "loss": 0.1803,
      "step": 12910
    },
    {
      "epoch": 0.7237487045906507,
      "grad_norm": 3.945319175720215,
      "learning_rate": 8.196507651774203e-05,
      "loss": 0.3037,
      "step": 12920
    },
    {
      "epoch": 0.7243088816065877,
      "grad_norm": 5.387413024902344,
      "learning_rate": 8.195106227927575e-05,
      "loss": 0.2824,
      "step": 12930
    },
    {
      "epoch": 0.7248690586225247,
      "grad_norm": 2.144625663757324,
      "learning_rate": 8.193704804080946e-05,
      "loss": 0.1667,
      "step": 12940
    },
    {
      "epoch": 0.7254292356384617,
      "grad_norm": 5.437607288360596,
      "learning_rate": 8.192303380234319e-05,
      "loss": 0.2513,
      "step": 12950
    },
    {
      "epoch": 0.7259894126543988,
      "grad_norm": 5.530933856964111,
      "learning_rate": 8.19090195638769e-05,
      "loss": 0.3229,
      "step": 12960
    },
    {
      "epoch": 0.7265495896703358,
      "grad_norm": 7.432013034820557,
      "learning_rate": 8.189500532541062e-05,
      "loss": 0.3087,
      "step": 12970
    },
    {
      "epoch": 0.7271097666862728,
      "grad_norm": 5.979511737823486,
      "learning_rate": 8.188099108694433e-05,
      "loss": 0.2718,
      "step": 12980
    },
    {
      "epoch": 0.7276699437022099,
      "grad_norm": 4.72550630569458,
      "learning_rate": 8.186697684847806e-05,
      "loss": 0.2683,
      "step": 12990
    },
    {
      "epoch": 0.728230120718147,
      "grad_norm": 4.6583251953125,
      "learning_rate": 8.185296261001178e-05,
      "loss": 0.2471,
      "step": 13000
    },
    {
      "epoch": 0.728790297734084,
      "grad_norm": 2.565741539001465,
      "learning_rate": 8.18389483715455e-05,
      "loss": 0.3006,
      "step": 13010
    },
    {
      "epoch": 0.729350474750021,
      "grad_norm": 3.639388084411621,
      "learning_rate": 8.18249341330792e-05,
      "loss": 0.3257,
      "step": 13020
    },
    {
      "epoch": 0.729910651765958,
      "grad_norm": 6.396919250488281,
      "learning_rate": 8.181091989461292e-05,
      "loss": 0.2324,
      "step": 13030
    },
    {
      "epoch": 0.7304708287818951,
      "grad_norm": 5.383750915527344,
      "learning_rate": 8.179690565614665e-05,
      "loss": 0.371,
      "step": 13040
    },
    {
      "epoch": 0.7310310057978321,
      "grad_norm": 5.004764556884766,
      "learning_rate": 8.178289141768036e-05,
      "loss": 0.3831,
      "step": 13050
    },
    {
      "epoch": 0.7315911828137691,
      "grad_norm": 5.1421380043029785,
      "learning_rate": 8.176887717921409e-05,
      "loss": 0.3723,
      "step": 13060
    },
    {
      "epoch": 0.7321513598297061,
      "grad_norm": 2.730525493621826,
      "learning_rate": 8.175486294074781e-05,
      "loss": 0.4702,
      "step": 13070
    },
    {
      "epoch": 0.7327115368456433,
      "grad_norm": 5.50611686706543,
      "learning_rate": 8.174084870228152e-05,
      "loss": 0.2635,
      "step": 13080
    },
    {
      "epoch": 0.7332717138615803,
      "grad_norm": 2.6864447593688965,
      "learning_rate": 8.172683446381524e-05,
      "loss": 0.2246,
      "step": 13090
    },
    {
      "epoch": 0.7338318908775173,
      "grad_norm": 5.120033264160156,
      "learning_rate": 8.171282022534896e-05,
      "loss": 0.3474,
      "step": 13100
    },
    {
      "epoch": 0.7343920678934543,
      "grad_norm": 3.5233118534088135,
      "learning_rate": 8.169880598688268e-05,
      "loss": 0.3207,
      "step": 13110
    },
    {
      "epoch": 0.7349522449093914,
      "grad_norm": 3.230806350708008,
      "learning_rate": 8.16847917484164e-05,
      "loss": 0.2997,
      "step": 13120
    },
    {
      "epoch": 0.7355124219253284,
      "grad_norm": 5.578359127044678,
      "learning_rate": 8.167077750995011e-05,
      "loss": 0.2184,
      "step": 13130
    },
    {
      "epoch": 0.7360725989412654,
      "grad_norm": 3.393043279647827,
      "learning_rate": 8.165676327148382e-05,
      "loss": 0.2528,
      "step": 13140
    },
    {
      "epoch": 0.7366327759572024,
      "grad_norm": 3.529623031616211,
      "learning_rate": 8.164274903301755e-05,
      "loss": 0.4253,
      "step": 13150
    },
    {
      "epoch": 0.7371929529731395,
      "grad_norm": 5.18352746963501,
      "learning_rate": 8.162873479455127e-05,
      "loss": 0.3008,
      "step": 13160
    },
    {
      "epoch": 0.7377531299890765,
      "grad_norm": 3.861839532852173,
      "learning_rate": 8.1614720556085e-05,
      "loss": 0.3182,
      "step": 13170
    },
    {
      "epoch": 0.7383133070050136,
      "grad_norm": 4.418404579162598,
      "learning_rate": 8.16007063176187e-05,
      "loss": 0.3257,
      "step": 13180
    },
    {
      "epoch": 0.7388734840209507,
      "grad_norm": 5.126631259918213,
      "learning_rate": 8.158669207915242e-05,
      "loss": 0.3843,
      "step": 13190
    },
    {
      "epoch": 0.7394336610368877,
      "grad_norm": 4.588850975036621,
      "learning_rate": 8.157267784068614e-05,
      "loss": 0.2657,
      "step": 13200
    },
    {
      "epoch": 0.7399938380528247,
      "grad_norm": 5.516685485839844,
      "learning_rate": 8.155866360221987e-05,
      "loss": 0.2176,
      "step": 13210
    },
    {
      "epoch": 0.7405540150687617,
      "grad_norm": 6.529559135437012,
      "learning_rate": 8.154464936375358e-05,
      "loss": 0.3453,
      "step": 13220
    },
    {
      "epoch": 0.7411141920846988,
      "grad_norm": 6.624382495880127,
      "learning_rate": 8.15306351252873e-05,
      "loss": 0.2468,
      "step": 13230
    },
    {
      "epoch": 0.7416743691006358,
      "grad_norm": 4.694734573364258,
      "learning_rate": 8.151662088682101e-05,
      "loss": 0.3719,
      "step": 13240
    },
    {
      "epoch": 0.7422345461165728,
      "grad_norm": 2.1656904220581055,
      "learning_rate": 8.150260664835473e-05,
      "loss": 0.2149,
      "step": 13250
    },
    {
      "epoch": 0.7427947231325098,
      "grad_norm": 2.624241828918457,
      "learning_rate": 8.148859240988845e-05,
      "loss": 0.2491,
      "step": 13260
    },
    {
      "epoch": 0.743354900148447,
      "grad_norm": 2.6176860332489014,
      "learning_rate": 8.147457817142217e-05,
      "loss": 0.3581,
      "step": 13270
    },
    {
      "epoch": 0.743915077164384,
      "grad_norm": 4.073483943939209,
      "learning_rate": 8.14605639329559e-05,
      "loss": 0.3006,
      "step": 13280
    },
    {
      "epoch": 0.744475254180321,
      "grad_norm": 4.151487827301025,
      "learning_rate": 8.14465496944896e-05,
      "loss": 0.3293,
      "step": 13290
    },
    {
      "epoch": 0.745035431196258,
      "grad_norm": 4.802035808563232,
      "learning_rate": 8.143253545602333e-05,
      "loss": 0.2914,
      "step": 13300
    },
    {
      "epoch": 0.7455956082121951,
      "grad_norm": 2.3314366340637207,
      "learning_rate": 8.141852121755704e-05,
      "loss": 0.2603,
      "step": 13310
    },
    {
      "epoch": 0.7461557852281321,
      "grad_norm": 5.960531711578369,
      "learning_rate": 8.140450697909076e-05,
      "loss": 0.3158,
      "step": 13320
    },
    {
      "epoch": 0.7467159622440691,
      "grad_norm": 5.711512088775635,
      "learning_rate": 8.139049274062448e-05,
      "loss": 0.4183,
      "step": 13330
    },
    {
      "epoch": 0.7472761392600061,
      "grad_norm": 4.277263164520264,
      "learning_rate": 8.13764785021582e-05,
      "loss": 0.275,
      "step": 13340
    },
    {
      "epoch": 0.7478363162759432,
      "grad_norm": 6.702060222625732,
      "learning_rate": 8.136246426369191e-05,
      "loss": 0.3295,
      "step": 13350
    },
    {
      "epoch": 0.7483964932918802,
      "grad_norm": 4.660266399383545,
      "learning_rate": 8.134845002522563e-05,
      "loss": 0.2591,
      "step": 13360
    },
    {
      "epoch": 0.7489566703078173,
      "grad_norm": 4.769646167755127,
      "learning_rate": 8.133443578675936e-05,
      "loss": 0.3287,
      "step": 13370
    },
    {
      "epoch": 0.7495168473237543,
      "grad_norm": 5.3206658363342285,
      "learning_rate": 8.132042154829307e-05,
      "loss": 0.4004,
      "step": 13380
    },
    {
      "epoch": 0.7500770243396914,
      "grad_norm": 3.461491823196411,
      "learning_rate": 8.130640730982679e-05,
      "loss": 0.1943,
      "step": 13390
    },
    {
      "epoch": 0.7506372013556284,
      "grad_norm": 6.6758880615234375,
      "learning_rate": 8.12923930713605e-05,
      "loss": 0.4113,
      "step": 13400
    },
    {
      "epoch": 0.7511973783715654,
      "grad_norm": 1.4130487442016602,
      "learning_rate": 8.127837883289422e-05,
      "loss": 0.3113,
      "step": 13410
    },
    {
      "epoch": 0.7517575553875024,
      "grad_norm": 3.8899316787719727,
      "learning_rate": 8.126436459442794e-05,
      "loss": 0.2118,
      "step": 13420
    },
    {
      "epoch": 0.7523177324034395,
      "grad_norm": 4.518474102020264,
      "learning_rate": 8.125035035596166e-05,
      "loss": 0.4313,
      "step": 13430
    },
    {
      "epoch": 0.7528779094193765,
      "grad_norm": 4.434516906738281,
      "learning_rate": 8.123633611749539e-05,
      "loss": 0.2222,
      "step": 13440
    },
    {
      "epoch": 0.7534380864353135,
      "grad_norm": 4.4890055656433105,
      "learning_rate": 8.122232187902909e-05,
      "loss": 0.2415,
      "step": 13450
    },
    {
      "epoch": 0.7539982634512506,
      "grad_norm": 3.523167610168457,
      "learning_rate": 8.120830764056282e-05,
      "loss": 0.3084,
      "step": 13460
    },
    {
      "epoch": 0.7545584404671877,
      "grad_norm": 3.5645649433135986,
      "learning_rate": 8.119429340209653e-05,
      "loss": 0.2631,
      "step": 13470
    },
    {
      "epoch": 0.7551186174831247,
      "grad_norm": 3.3898706436157227,
      "learning_rate": 8.118027916363026e-05,
      "loss": 0.3103,
      "step": 13480
    },
    {
      "epoch": 0.7556787944990617,
      "grad_norm": 5.2092108726501465,
      "learning_rate": 8.116626492516398e-05,
      "loss": 0.2994,
      "step": 13490
    },
    {
      "epoch": 0.7562389715149987,
      "grad_norm": 4.399293422698975,
      "learning_rate": 8.115225068669769e-05,
      "loss": 0.3027,
      "step": 13500
    },
    {
      "epoch": 0.7567991485309358,
      "grad_norm": 5.28522253036499,
      "learning_rate": 8.11382364482314e-05,
      "loss": 0.2473,
      "step": 13510
    },
    {
      "epoch": 0.7573593255468728,
      "grad_norm": 5.417243003845215,
      "learning_rate": 8.112422220976512e-05,
      "loss": 0.2013,
      "step": 13520
    },
    {
      "epoch": 0.7579195025628098,
      "grad_norm": 2.100003957748413,
      "learning_rate": 8.111020797129885e-05,
      "loss": 0.1497,
      "step": 13530
    },
    {
      "epoch": 0.7584796795787468,
      "grad_norm": 7.506775856018066,
      "learning_rate": 8.109619373283256e-05,
      "loss": 0.1984,
      "step": 13540
    },
    {
      "epoch": 0.7590398565946839,
      "grad_norm": 3.8752427101135254,
      "learning_rate": 8.108217949436628e-05,
      "loss": 0.2218,
      "step": 13550
    },
    {
      "epoch": 0.759600033610621,
      "grad_norm": 6.321942329406738,
      "learning_rate": 8.106816525589999e-05,
      "loss": 0.3713,
      "step": 13560
    },
    {
      "epoch": 0.760160210626558,
      "grad_norm": 2.3194828033447266,
      "learning_rate": 8.105415101743372e-05,
      "loss": 0.2794,
      "step": 13570
    },
    {
      "epoch": 0.760720387642495,
      "grad_norm": 2.1459619998931885,
      "learning_rate": 8.104013677896743e-05,
      "loss": 0.2653,
      "step": 13580
    },
    {
      "epoch": 0.7612805646584321,
      "grad_norm": 4.07297420501709,
      "learning_rate": 8.102612254050115e-05,
      "loss": 0.2249,
      "step": 13590
    },
    {
      "epoch": 0.7618407416743691,
      "grad_norm": 7.506961345672607,
      "learning_rate": 8.101210830203488e-05,
      "loss": 0.2592,
      "step": 13600
    },
    {
      "epoch": 0.7624009186903061,
      "grad_norm": 4.340677738189697,
      "learning_rate": 8.099809406356859e-05,
      "loss": 0.2593,
      "step": 13610
    },
    {
      "epoch": 0.7629610957062432,
      "grad_norm": 4.768680095672607,
      "learning_rate": 8.098407982510231e-05,
      "loss": 0.3373,
      "step": 13620
    },
    {
      "epoch": 0.7635212727221802,
      "grad_norm": 2.8659541606903076,
      "learning_rate": 8.097006558663602e-05,
      "loss": 0.2941,
      "step": 13630
    },
    {
      "epoch": 0.7640814497381172,
      "grad_norm": 2.0433425903320312,
      "learning_rate": 8.095605134816975e-05,
      "loss": 0.289,
      "step": 13640
    },
    {
      "epoch": 0.7646416267540542,
      "grad_norm": 6.478954792022705,
      "learning_rate": 8.094203710970347e-05,
      "loss": 0.2298,
      "step": 13650
    },
    {
      "epoch": 0.7652018037699914,
      "grad_norm": 4.587181091308594,
      "learning_rate": 8.092802287123718e-05,
      "loss": 0.3101,
      "step": 13660
    },
    {
      "epoch": 0.7657619807859284,
      "grad_norm": 6.0301713943481445,
      "learning_rate": 8.09140086327709e-05,
      "loss": 0.2364,
      "step": 13670
    },
    {
      "epoch": 0.7663221578018654,
      "grad_norm": 2.927555561065674,
      "learning_rate": 8.089999439430461e-05,
      "loss": 0.1594,
      "step": 13680
    },
    {
      "epoch": 0.7668823348178024,
      "grad_norm": 5.0355024337768555,
      "learning_rate": 8.088598015583834e-05,
      "loss": 0.2719,
      "step": 13690
    },
    {
      "epoch": 0.7674425118337395,
      "grad_norm": 4.214252471923828,
      "learning_rate": 8.087196591737205e-05,
      "loss": 0.2903,
      "step": 13700
    },
    {
      "epoch": 0.7680026888496765,
      "grad_norm": 5.402551651000977,
      "learning_rate": 8.085795167890578e-05,
      "loss": 0.1743,
      "step": 13710
    },
    {
      "epoch": 0.7685628658656135,
      "grad_norm": 5.787720680236816,
      "learning_rate": 8.084393744043948e-05,
      "loss": 0.3474,
      "step": 13720
    },
    {
      "epoch": 0.7691230428815505,
      "grad_norm": 5.3060526847839355,
      "learning_rate": 8.082992320197321e-05,
      "loss": 0.3204,
      "step": 13730
    },
    {
      "epoch": 0.7696832198974876,
      "grad_norm": 6.4029717445373535,
      "learning_rate": 8.081590896350692e-05,
      "loss": 0.261,
      "step": 13740
    },
    {
      "epoch": 0.7702433969134247,
      "grad_norm": 5.9166483879089355,
      "learning_rate": 8.080189472504065e-05,
      "loss": 0.2366,
      "step": 13750
    },
    {
      "epoch": 0.7708035739293617,
      "grad_norm": 2.9569950103759766,
      "learning_rate": 8.078788048657437e-05,
      "loss": 0.3265,
      "step": 13760
    },
    {
      "epoch": 0.7713637509452987,
      "grad_norm": 1.9228384494781494,
      "learning_rate": 8.077386624810808e-05,
      "loss": 0.1981,
      "step": 13770
    },
    {
      "epoch": 0.7719239279612358,
      "grad_norm": 7.66079044342041,
      "learning_rate": 8.07598520096418e-05,
      "loss": 0.242,
      "step": 13780
    },
    {
      "epoch": 0.7724841049771728,
      "grad_norm": 3.1003572940826416,
      "learning_rate": 8.074583777117551e-05,
      "loss": 0.1546,
      "step": 13790
    },
    {
      "epoch": 0.7730442819931098,
      "grad_norm": 3.4496641159057617,
      "learning_rate": 8.073182353270924e-05,
      "loss": 0.2281,
      "step": 13800
    },
    {
      "epoch": 0.7736044590090468,
      "grad_norm": 5.132846832275391,
      "learning_rate": 8.071780929424296e-05,
      "loss": 0.2491,
      "step": 13810
    },
    {
      "epoch": 0.7741646360249839,
      "grad_norm": 5.248659610748291,
      "learning_rate": 8.070379505577667e-05,
      "loss": 0.299,
      "step": 13820
    },
    {
      "epoch": 0.7747248130409209,
      "grad_norm": 5.885998725891113,
      "learning_rate": 8.068978081731038e-05,
      "loss": 0.2791,
      "step": 13830
    },
    {
      "epoch": 0.7752849900568579,
      "grad_norm": 2.386396884918213,
      "learning_rate": 8.067576657884411e-05,
      "loss": 0.395,
      "step": 13840
    },
    {
      "epoch": 0.775845167072795,
      "grad_norm": 7.410590648651123,
      "learning_rate": 8.066175234037783e-05,
      "loss": 0.2527,
      "step": 13850
    },
    {
      "epoch": 0.7764053440887321,
      "grad_norm": 6.446203708648682,
      "learning_rate": 8.064773810191154e-05,
      "loss": 0.2375,
      "step": 13860
    },
    {
      "epoch": 0.7769655211046691,
      "grad_norm": 5.450140953063965,
      "learning_rate": 8.063372386344527e-05,
      "loss": 0.2889,
      "step": 13870
    },
    {
      "epoch": 0.7775256981206061,
      "grad_norm": 4.075782775878906,
      "learning_rate": 8.061970962497897e-05,
      "loss": 0.2434,
      "step": 13880
    },
    {
      "epoch": 0.7780858751365431,
      "grad_norm": 6.8355889320373535,
      "learning_rate": 8.06056953865127e-05,
      "loss": 0.4402,
      "step": 13890
    },
    {
      "epoch": 0.7786460521524802,
      "grad_norm": 3.9259047508239746,
      "learning_rate": 8.059168114804642e-05,
      "loss": 0.2594,
      "step": 13900
    },
    {
      "epoch": 0.7792062291684172,
      "grad_norm": 2.7717175483703613,
      "learning_rate": 8.057766690958014e-05,
      "loss": 0.3148,
      "step": 13910
    },
    {
      "epoch": 0.7797664061843542,
      "grad_norm": 4.94779109954834,
      "learning_rate": 8.056365267111386e-05,
      "loss": 0.2605,
      "step": 13920
    },
    {
      "epoch": 0.7803265832002912,
      "grad_norm": 4.904712677001953,
      "learning_rate": 8.054963843264757e-05,
      "loss": 0.2083,
      "step": 13930
    },
    {
      "epoch": 0.7808867602162284,
      "grad_norm": 4.164653301239014,
      "learning_rate": 8.053562419418129e-05,
      "loss": 0.3101,
      "step": 13940
    },
    {
      "epoch": 0.7814469372321654,
      "grad_norm": 4.694516658782959,
      "learning_rate": 8.0521609955715e-05,
      "loss": 0.2713,
      "step": 13950
    },
    {
      "epoch": 0.7820071142481024,
      "grad_norm": 1.202846646308899,
      "learning_rate": 8.050759571724873e-05,
      "loss": 0.2121,
      "step": 13960
    },
    {
      "epoch": 0.7825672912640395,
      "grad_norm": 4.050194263458252,
      "learning_rate": 8.049358147878245e-05,
      "loss": 0.3446,
      "step": 13970
    },
    {
      "epoch": 0.7831274682799765,
      "grad_norm": 4.092971324920654,
      "learning_rate": 8.047956724031617e-05,
      "loss": 0.286,
      "step": 13980
    },
    {
      "epoch": 0.7836876452959135,
      "grad_norm": 2.909724712371826,
      "learning_rate": 8.046555300184987e-05,
      "loss": 0.3178,
      "step": 13990
    },
    {
      "epoch": 0.7842478223118505,
      "grad_norm": 3.685887098312378,
      "learning_rate": 8.04515387633836e-05,
      "loss": 0.2835,
      "step": 14000
    },
    {
      "epoch": 0.7848079993277876,
      "grad_norm": 4.3973388671875,
      "learning_rate": 8.043752452491732e-05,
      "loss": 0.3302,
      "step": 14010
    },
    {
      "epoch": 0.7853681763437246,
      "grad_norm": 4.627108573913574,
      "learning_rate": 8.042351028645105e-05,
      "loss": 0.1799,
      "step": 14020
    },
    {
      "epoch": 0.7859283533596616,
      "grad_norm": 5.121342658996582,
      "learning_rate": 8.040949604798476e-05,
      "loss": 0.2002,
      "step": 14030
    },
    {
      "epoch": 0.7864885303755987,
      "grad_norm": 3.689821481704712,
      "learning_rate": 8.039548180951848e-05,
      "loss": 0.3111,
      "step": 14040
    },
    {
      "epoch": 0.7870487073915358,
      "grad_norm": 6.3772101402282715,
      "learning_rate": 8.038146757105219e-05,
      "loss": 0.4015,
      "step": 14050
    },
    {
      "epoch": 0.7876088844074728,
      "grad_norm": 5.1508870124816895,
      "learning_rate": 8.03674533325859e-05,
      "loss": 0.36,
      "step": 14060
    },
    {
      "epoch": 0.7881690614234098,
      "grad_norm": 4.704419136047363,
      "learning_rate": 8.035343909411963e-05,
      "loss": 0.3452,
      "step": 14070
    },
    {
      "epoch": 0.7887292384393468,
      "grad_norm": 4.478294372558594,
      "learning_rate": 8.033942485565335e-05,
      "loss": 0.373,
      "step": 14080
    },
    {
      "epoch": 0.7892894154552839,
      "grad_norm": 4.797060012817383,
      "learning_rate": 8.032541061718706e-05,
      "loss": 0.2169,
      "step": 14090
    },
    {
      "epoch": 0.7898495924712209,
      "grad_norm": 6.401747703552246,
      "learning_rate": 8.031139637872078e-05,
      "loss": 0.2458,
      "step": 14100
    },
    {
      "epoch": 0.7904097694871579,
      "grad_norm": 5.42693567276001,
      "learning_rate": 8.02973821402545e-05,
      "loss": 0.2908,
      "step": 14110
    },
    {
      "epoch": 0.7909699465030949,
      "grad_norm": 6.311504364013672,
      "learning_rate": 8.028336790178822e-05,
      "loss": 0.2623,
      "step": 14120
    },
    {
      "epoch": 0.7915301235190321,
      "grad_norm": 5.438673973083496,
      "learning_rate": 8.026935366332194e-05,
      "loss": 0.3355,
      "step": 14130
    },
    {
      "epoch": 0.7920903005349691,
      "grad_norm": 3.4148669242858887,
      "learning_rate": 8.025533942485566e-05,
      "loss": 0.2096,
      "step": 14140
    },
    {
      "epoch": 0.7926504775509061,
      "grad_norm": 1.1618738174438477,
      "learning_rate": 8.024132518638936e-05,
      "loss": 0.1668,
      "step": 14150
    },
    {
      "epoch": 0.7932106545668431,
      "grad_norm": 4.65405797958374,
      "learning_rate": 8.02273109479231e-05,
      "loss": 0.4238,
      "step": 14160
    },
    {
      "epoch": 0.7937708315827802,
      "grad_norm": 3.4240291118621826,
      "learning_rate": 8.021329670945681e-05,
      "loss": 0.2964,
      "step": 14170
    },
    {
      "epoch": 0.7943310085987172,
      "grad_norm": 5.300559043884277,
      "learning_rate": 8.019928247099054e-05,
      "loss": 0.2303,
      "step": 14180
    },
    {
      "epoch": 0.7948911856146542,
      "grad_norm": 5.5694193840026855,
      "learning_rate": 8.018526823252425e-05,
      "loss": 0.2787,
      "step": 14190
    },
    {
      "epoch": 0.7954513626305912,
      "grad_norm": 5.817259311676025,
      "learning_rate": 8.017125399405797e-05,
      "loss": 0.3755,
      "step": 14200
    },
    {
      "epoch": 0.7960115396465283,
      "grad_norm": 5.144740581512451,
      "learning_rate": 8.015723975559168e-05,
      "loss": 0.2261,
      "step": 14210
    },
    {
      "epoch": 0.7965717166624653,
      "grad_norm": 6.770496845245361,
      "learning_rate": 8.014322551712541e-05,
      "loss": 0.2411,
      "step": 14220
    },
    {
      "epoch": 0.7971318936784024,
      "grad_norm": 5.430883407592773,
      "learning_rate": 8.012921127865912e-05,
      "loss": 0.3378,
      "step": 14230
    },
    {
      "epoch": 0.7976920706943395,
      "grad_norm": 4.235560894012451,
      "learning_rate": 8.011519704019284e-05,
      "loss": 0.2597,
      "step": 14240
    },
    {
      "epoch": 0.7982522477102765,
      "grad_norm": 2.346471071243286,
      "learning_rate": 8.010118280172655e-05,
      "loss": 0.1977,
      "step": 14250
    },
    {
      "epoch": 0.7988124247262135,
      "grad_norm": 3.9931204319000244,
      "learning_rate": 8.008716856326027e-05,
      "loss": 0.2946,
      "step": 14260
    },
    {
      "epoch": 0.7993726017421505,
      "grad_norm": 5.306519031524658,
      "learning_rate": 8.0073154324794e-05,
      "loss": 0.3981,
      "step": 14270
    },
    {
      "epoch": 0.7999327787580875,
      "grad_norm": 3.309356689453125,
      "learning_rate": 8.005914008632771e-05,
      "loss": 0.2168,
      "step": 14280
    },
    {
      "epoch": 0.8004929557740246,
      "grad_norm": 3.988903045654297,
      "learning_rate": 8.004512584786144e-05,
      "loss": 0.3708,
      "step": 14290
    },
    {
      "epoch": 0.8010531327899616,
      "grad_norm": 7.92747688293457,
      "learning_rate": 8.003111160939515e-05,
      "loss": 0.2179,
      "step": 14300
    },
    {
      "epoch": 0.8016133098058986,
      "grad_norm": 2.82177996635437,
      "learning_rate": 8.001709737092887e-05,
      "loss": 0.4117,
      "step": 14310
    },
    {
      "epoch": 0.8021734868218356,
      "grad_norm": 4.628977298736572,
      "learning_rate": 8.000308313246258e-05,
      "loss": 0.3361,
      "step": 14320
    },
    {
      "epoch": 0.8027336638377728,
      "grad_norm": 4.804776191711426,
      "learning_rate": 7.99890688939963e-05,
      "loss": 0.2919,
      "step": 14330
    },
    {
      "epoch": 0.8032938408537098,
      "grad_norm": 2.4249393939971924,
      "learning_rate": 7.997505465553003e-05,
      "loss": 0.2045,
      "step": 14340
    },
    {
      "epoch": 0.8038540178696468,
      "grad_norm": 7.087888717651367,
      "learning_rate": 7.996104041706374e-05,
      "loss": 0.2612,
      "step": 14350
    },
    {
      "epoch": 0.8044141948855839,
      "grad_norm": 5.8592071533203125,
      "learning_rate": 7.994702617859746e-05,
      "loss": 0.2868,
      "step": 14360
    },
    {
      "epoch": 0.8049743719015209,
      "grad_norm": 5.72475528717041,
      "learning_rate": 7.993301194013117e-05,
      "loss": 0.2131,
      "step": 14370
    },
    {
      "epoch": 0.8055345489174579,
      "grad_norm": 2.9978272914886475,
      "learning_rate": 7.99189977016649e-05,
      "loss": 0.2992,
      "step": 14380
    },
    {
      "epoch": 0.8060947259333949,
      "grad_norm": 6.049496650695801,
      "learning_rate": 7.990498346319861e-05,
      "loss": 0.3837,
      "step": 14390
    },
    {
      "epoch": 0.806654902949332,
      "grad_norm": 2.0597262382507324,
      "learning_rate": 7.989096922473234e-05,
      "loss": 0.2729,
      "step": 14400
    },
    {
      "epoch": 0.807215079965269,
      "grad_norm": 3.7151851654052734,
      "learning_rate": 7.987695498626606e-05,
      "loss": 0.2054,
      "step": 14410
    },
    {
      "epoch": 0.8077752569812061,
      "grad_norm": 5.735159397125244,
      "learning_rate": 7.986294074779976e-05,
      "loss": 0.2914,
      "step": 14420
    },
    {
      "epoch": 0.8083354339971431,
      "grad_norm": 5.287568092346191,
      "learning_rate": 7.984892650933349e-05,
      "loss": 0.2509,
      "step": 14430
    },
    {
      "epoch": 0.8088956110130802,
      "grad_norm": 4.447030544281006,
      "learning_rate": 7.98349122708672e-05,
      "loss": 0.327,
      "step": 14440
    },
    {
      "epoch": 0.8094557880290172,
      "grad_norm": 5.263370990753174,
      "learning_rate": 7.982089803240093e-05,
      "loss": 0.5003,
      "step": 14450
    },
    {
      "epoch": 0.8100159650449542,
      "grad_norm": 3.8450329303741455,
      "learning_rate": 7.980688379393464e-05,
      "loss": 0.2803,
      "step": 14460
    },
    {
      "epoch": 0.8105761420608912,
      "grad_norm": 4.789317607879639,
      "learning_rate": 7.979286955546836e-05,
      "loss": 0.2622,
      "step": 14470
    },
    {
      "epoch": 0.8111363190768283,
      "grad_norm": 4.143743991851807,
      "learning_rate": 7.977885531700207e-05,
      "loss": 0.1915,
      "step": 14480
    },
    {
      "epoch": 0.8116964960927653,
      "grad_norm": 4.34218692779541,
      "learning_rate": 7.97648410785358e-05,
      "loss": 0.208,
      "step": 14490
    },
    {
      "epoch": 0.8122566731087023,
      "grad_norm": 5.163724422454834,
      "learning_rate": 7.975082684006952e-05,
      "loss": 0.4244,
      "step": 14500
    },
    {
      "epoch": 0.8128168501246393,
      "grad_norm": 7.021280288696289,
      "learning_rate": 7.973681260160323e-05,
      "loss": 0.2752,
      "step": 14510
    },
    {
      "epoch": 0.8133770271405765,
      "grad_norm": 3.3391613960266113,
      "learning_rate": 7.972279836313695e-05,
      "loss": 0.2902,
      "step": 14520
    },
    {
      "epoch": 0.8139372041565135,
      "grad_norm": 6.967248916625977,
      "learning_rate": 7.970878412467066e-05,
      "loss": 0.3604,
      "step": 14530
    },
    {
      "epoch": 0.8144973811724505,
      "grad_norm": 3.553351640701294,
      "learning_rate": 7.969476988620439e-05,
      "loss": 0.2208,
      "step": 14540
    },
    {
      "epoch": 0.8150575581883875,
      "grad_norm": 5.7597880363464355,
      "learning_rate": 7.96807556477381e-05,
      "loss": 0.3,
      "step": 14550
    },
    {
      "epoch": 0.8156177352043246,
      "grad_norm": 1.9585720300674438,
      "learning_rate": 7.966674140927183e-05,
      "loss": 0.3047,
      "step": 14560
    },
    {
      "epoch": 0.8161779122202616,
      "grad_norm": 5.865050792694092,
      "learning_rate": 7.965272717080555e-05,
      "loss": 0.2675,
      "step": 14570
    },
    {
      "epoch": 0.8167380892361986,
      "grad_norm": 3.120321750640869,
      "learning_rate": 7.963871293233926e-05,
      "loss": 0.2087,
      "step": 14580
    },
    {
      "epoch": 0.8172982662521356,
      "grad_norm": 2.6078543663024902,
      "learning_rate": 7.962469869387298e-05,
      "loss": 0.31,
      "step": 14590
    },
    {
      "epoch": 0.8178584432680727,
      "grad_norm": 7.463593006134033,
      "learning_rate": 7.961068445540669e-05,
      "loss": 0.3482,
      "step": 14600
    },
    {
      "epoch": 0.8184186202840098,
      "grad_norm": 2.2452595233917236,
      "learning_rate": 7.959667021694042e-05,
      "loss": 0.207,
      "step": 14610
    },
    {
      "epoch": 0.8189787972999468,
      "grad_norm": 4.445211887359619,
      "learning_rate": 7.958265597847413e-05,
      "loss": 0.472,
      "step": 14620
    },
    {
      "epoch": 0.8195389743158839,
      "grad_norm": 4.865604877471924,
      "learning_rate": 7.956864174000785e-05,
      "loss": 0.2115,
      "step": 14630
    },
    {
      "epoch": 0.8200991513318209,
      "grad_norm": 4.173550128936768,
      "learning_rate": 7.955462750154156e-05,
      "loss": 0.3313,
      "step": 14640
    },
    {
      "epoch": 0.8206593283477579,
      "grad_norm": 3.3600735664367676,
      "learning_rate": 7.954061326307529e-05,
      "loss": 0.3686,
      "step": 14650
    },
    {
      "epoch": 0.8212195053636949,
      "grad_norm": 4.016227722167969,
      "learning_rate": 7.9526599024609e-05,
      "loss": 0.2496,
      "step": 14660
    },
    {
      "epoch": 0.821779682379632,
      "grad_norm": 8.8795804977417,
      "learning_rate": 7.951258478614273e-05,
      "loss": 0.1902,
      "step": 14670
    },
    {
      "epoch": 0.822339859395569,
      "grad_norm": 2.3033342361450195,
      "learning_rate": 7.949857054767645e-05,
      "loss": 0.2034,
      "step": 14680
    },
    {
      "epoch": 0.822900036411506,
      "grad_norm": 7.987071514129639,
      "learning_rate": 7.948455630921015e-05,
      "loss": 0.2833,
      "step": 14690
    },
    {
      "epoch": 0.823460213427443,
      "grad_norm": 5.526429653167725,
      "learning_rate": 7.947054207074388e-05,
      "loss": 0.3024,
      "step": 14700
    },
    {
      "epoch": 0.8240203904433802,
      "grad_norm": 4.544691562652588,
      "learning_rate": 7.94565278322776e-05,
      "loss": 0.2416,
      "step": 14710
    },
    {
      "epoch": 0.8245805674593172,
      "grad_norm": 4.981795787811279,
      "learning_rate": 7.944251359381132e-05,
      "loss": 0.4,
      "step": 14720
    },
    {
      "epoch": 0.8251407444752542,
      "grad_norm": 4.530941486358643,
      "learning_rate": 7.942849935534504e-05,
      "loss": 0.337,
      "step": 14730
    },
    {
      "epoch": 0.8257009214911912,
      "grad_norm": 3.2785065174102783,
      "learning_rate": 7.941448511687875e-05,
      "loss": 0.2085,
      "step": 14740
    },
    {
      "epoch": 0.8262610985071283,
      "grad_norm": 5.119744300842285,
      "learning_rate": 7.940047087841247e-05,
      "loss": 0.2198,
      "step": 14750
    },
    {
      "epoch": 0.8268212755230653,
      "grad_norm": 6.217959403991699,
      "learning_rate": 7.93864566399462e-05,
      "loss": 0.4394,
      "step": 14760
    },
    {
      "epoch": 0.8273814525390023,
      "grad_norm": 1.5492768287658691,
      "learning_rate": 7.937244240147991e-05,
      "loss": 0.2709,
      "step": 14770
    },
    {
      "epoch": 0.8279416295549393,
      "grad_norm": 3.370643138885498,
      "learning_rate": 7.935842816301362e-05,
      "loss": 0.2265,
      "step": 14780
    },
    {
      "epoch": 0.8285018065708764,
      "grad_norm": 4.611509323120117,
      "learning_rate": 7.934441392454734e-05,
      "loss": 0.1898,
      "step": 14790
    },
    {
      "epoch": 0.8290619835868134,
      "grad_norm": 5.826704025268555,
      "learning_rate": 7.933039968608105e-05,
      "loss": 0.2731,
      "step": 14800
    },
    {
      "epoch": 0.8296221606027505,
      "grad_norm": 3.380122184753418,
      "learning_rate": 7.931638544761478e-05,
      "loss": 0.1585,
      "step": 14810
    },
    {
      "epoch": 0.8301823376186875,
      "grad_norm": 3.026280641555786,
      "learning_rate": 7.93023712091485e-05,
      "loss": 0.364,
      "step": 14820
    },
    {
      "epoch": 0.8307425146346246,
      "grad_norm": 3.147113800048828,
      "learning_rate": 7.928835697068222e-05,
      "loss": 0.2687,
      "step": 14830
    },
    {
      "epoch": 0.8313026916505616,
      "grad_norm": 4.715763568878174,
      "learning_rate": 7.927434273221594e-05,
      "loss": 0.2025,
      "step": 14840
    },
    {
      "epoch": 0.8318628686664986,
      "grad_norm": 1.914340615272522,
      "learning_rate": 7.926032849374965e-05,
      "loss": 0.2224,
      "step": 14850
    },
    {
      "epoch": 0.8324230456824356,
      "grad_norm": 3.194423198699951,
      "learning_rate": 7.924631425528337e-05,
      "loss": 0.3292,
      "step": 14860
    },
    {
      "epoch": 0.8329832226983727,
      "grad_norm": 3.8419225215911865,
      "learning_rate": 7.923230001681708e-05,
      "loss": 0.1876,
      "step": 14870
    },
    {
      "epoch": 0.8335433997143097,
      "grad_norm": 2.48224139213562,
      "learning_rate": 7.921828577835081e-05,
      "loss": 0.2515,
      "step": 14880
    },
    {
      "epoch": 0.8341035767302467,
      "grad_norm": 4.734988212585449,
      "learning_rate": 7.920427153988453e-05,
      "loss": 0.2061,
      "step": 14890
    },
    {
      "epoch": 0.8346637537461838,
      "grad_norm": 3.564464807510376,
      "learning_rate": 7.919025730141824e-05,
      "loss": 0.2251,
      "step": 14900
    },
    {
      "epoch": 0.8352239307621209,
      "grad_norm": 2.9928641319274902,
      "learning_rate": 7.917624306295196e-05,
      "loss": 0.2774,
      "step": 14910
    },
    {
      "epoch": 0.8357841077780579,
      "grad_norm": 4.892086505889893,
      "learning_rate": 7.916222882448568e-05,
      "loss": 0.2382,
      "step": 14920
    },
    {
      "epoch": 0.8363442847939949,
      "grad_norm": 2.785383701324463,
      "learning_rate": 7.91482145860194e-05,
      "loss": 0.2085,
      "step": 14930
    },
    {
      "epoch": 0.8369044618099319,
      "grad_norm": 4.132854461669922,
      "learning_rate": 7.913420034755313e-05,
      "loss": 0.2302,
      "step": 14940
    },
    {
      "epoch": 0.837464638825869,
      "grad_norm": 3.4734647274017334,
      "learning_rate": 7.912018610908683e-05,
      "loss": 0.1672,
      "step": 14950
    },
    {
      "epoch": 0.838024815841806,
      "grad_norm": 4.073808193206787,
      "learning_rate": 7.910617187062054e-05,
      "loss": 0.397,
      "step": 14960
    },
    {
      "epoch": 0.838584992857743,
      "grad_norm": 3.4636716842651367,
      "learning_rate": 7.909215763215427e-05,
      "loss": 0.1897,
      "step": 14970
    },
    {
      "epoch": 0.83914516987368,
      "grad_norm": 3.0464870929718018,
      "learning_rate": 7.907814339368799e-05,
      "loss": 0.2577,
      "step": 14980
    },
    {
      "epoch": 0.8397053468896171,
      "grad_norm": 4.573392868041992,
      "learning_rate": 7.906412915522172e-05,
      "loss": 0.1935,
      "step": 14990
    },
    {
      "epoch": 0.8402655239055542,
      "grad_norm": 3.452465772628784,
      "learning_rate": 7.905011491675543e-05,
      "loss": 0.1864,
      "step": 15000
    },
    {
      "epoch": 0.8408257009214912,
      "grad_norm": 4.936844825744629,
      "learning_rate": 7.903610067828914e-05,
      "loss": 0.3026,
      "step": 15010
    },
    {
      "epoch": 0.8413858779374283,
      "grad_norm": 5.5706939697265625,
      "learning_rate": 7.902208643982286e-05,
      "loss": 0.2052,
      "step": 15020
    },
    {
      "epoch": 0.8419460549533653,
      "grad_norm": 7.853938102722168,
      "learning_rate": 7.900807220135659e-05,
      "loss": 0.4275,
      "step": 15030
    },
    {
      "epoch": 0.8425062319693023,
      "grad_norm": 5.216657638549805,
      "learning_rate": 7.89940579628903e-05,
      "loss": 0.2852,
      "step": 15040
    },
    {
      "epoch": 0.8430664089852393,
      "grad_norm": 5.625676155090332,
      "learning_rate": 7.898004372442402e-05,
      "loss": 0.227,
      "step": 15050
    },
    {
      "epoch": 0.8436265860011763,
      "grad_norm": 2.497474431991577,
      "learning_rate": 7.896602948595773e-05,
      "loss": 0.2911,
      "step": 15060
    },
    {
      "epoch": 0.8441867630171134,
      "grad_norm": 2.92781400680542,
      "learning_rate": 7.895201524749145e-05,
      "loss": 0.2471,
      "step": 15070
    },
    {
      "epoch": 0.8447469400330504,
      "grad_norm": 5.761830806732178,
      "learning_rate": 7.893800100902517e-05,
      "loss": 0.2018,
      "step": 15080
    },
    {
      "epoch": 0.8453071170489875,
      "grad_norm": 3.760354995727539,
      "learning_rate": 7.892398677055889e-05,
      "loss": 0.2691,
      "step": 15090
    },
    {
      "epoch": 0.8458672940649246,
      "grad_norm": 4.589401721954346,
      "learning_rate": 7.890997253209262e-05,
      "loss": 0.2909,
      "step": 15100
    },
    {
      "epoch": 0.8464274710808616,
      "grad_norm": 4.8351030349731445,
      "learning_rate": 7.889595829362633e-05,
      "loss": 0.2241,
      "step": 15110
    },
    {
      "epoch": 0.8469876480967986,
      "grad_norm": 3.6208553314208984,
      "learning_rate": 7.888194405516005e-05,
      "loss": 0.2253,
      "step": 15120
    },
    {
      "epoch": 0.8475478251127356,
      "grad_norm": 2.661991596221924,
      "learning_rate": 7.886792981669376e-05,
      "loss": 0.5152,
      "step": 15130
    },
    {
      "epoch": 0.8481080021286727,
      "grad_norm": 9.544848442077637,
      "learning_rate": 7.885391557822748e-05,
      "loss": 0.33,
      "step": 15140
    },
    {
      "epoch": 0.8486681791446097,
      "grad_norm": 6.27915620803833,
      "learning_rate": 7.88399013397612e-05,
      "loss": 0.1783,
      "step": 15150
    },
    {
      "epoch": 0.8492283561605467,
      "grad_norm": 5.2400689125061035,
      "learning_rate": 7.882588710129492e-05,
      "loss": 0.2332,
      "step": 15160
    },
    {
      "epoch": 0.8497885331764837,
      "grad_norm": 1.976444959640503,
      "learning_rate": 7.881187286282863e-05,
      "loss": 0.2615,
      "step": 15170
    },
    {
      "epoch": 0.8503487101924208,
      "grad_norm": 1.5762276649475098,
      "learning_rate": 7.879785862436235e-05,
      "loss": 0.25,
      "step": 15180
    },
    {
      "epoch": 0.8509088872083579,
      "grad_norm": 4.333518981933594,
      "learning_rate": 7.878384438589608e-05,
      "loss": 0.3097,
      "step": 15190
    },
    {
      "epoch": 0.8514690642242949,
      "grad_norm": 6.495981216430664,
      "learning_rate": 7.876983014742979e-05,
      "loss": 0.2389,
      "step": 15200
    },
    {
      "epoch": 0.8520292412402319,
      "grad_norm": 3.449997663497925,
      "learning_rate": 7.875581590896352e-05,
      "loss": 0.2058,
      "step": 15210
    },
    {
      "epoch": 0.852589418256169,
      "grad_norm": 4.170287609100342,
      "learning_rate": 7.874180167049722e-05,
      "loss": 0.3286,
      "step": 15220
    },
    {
      "epoch": 0.853149595272106,
      "grad_norm": 5.115141868591309,
      "learning_rate": 7.872778743203095e-05,
      "loss": 0.163,
      "step": 15230
    },
    {
      "epoch": 0.853709772288043,
      "grad_norm": 2.5167267322540283,
      "learning_rate": 7.871377319356466e-05,
      "loss": 0.2938,
      "step": 15240
    },
    {
      "epoch": 0.85426994930398,
      "grad_norm": 2.009343147277832,
      "learning_rate": 7.869975895509838e-05,
      "loss": 0.1885,
      "step": 15250
    },
    {
      "epoch": 0.8548301263199171,
      "grad_norm": 6.926055431365967,
      "learning_rate": 7.868574471663211e-05,
      "loss": 0.2663,
      "step": 15260
    },
    {
      "epoch": 0.8553903033358541,
      "grad_norm": 4.219727516174316,
      "learning_rate": 7.867173047816582e-05,
      "loss": 0.2363,
      "step": 15270
    },
    {
      "epoch": 0.8559504803517912,
      "grad_norm": 1.4993369579315186,
      "learning_rate": 7.865771623969954e-05,
      "loss": 0.3073,
      "step": 15280
    },
    {
      "epoch": 0.8565106573677282,
      "grad_norm": 4.2591352462768555,
      "learning_rate": 7.864370200123325e-05,
      "loss": 0.1561,
      "step": 15290
    },
    {
      "epoch": 0.8570708343836653,
      "grad_norm": 5.552445411682129,
      "learning_rate": 7.862968776276698e-05,
      "loss": 0.2665,
      "step": 15300
    },
    {
      "epoch": 0.8576310113996023,
      "grad_norm": 7.762876510620117,
      "learning_rate": 7.86156735243007e-05,
      "loss": 0.2567,
      "step": 15310
    },
    {
      "epoch": 0.8581911884155393,
      "grad_norm": 2.2467949390411377,
      "learning_rate": 7.860165928583442e-05,
      "loss": 0.2752,
      "step": 15320
    },
    {
      "epoch": 0.8587513654314763,
      "grad_norm": 1.437464952468872,
      "learning_rate": 7.858764504736812e-05,
      "loss": 0.2241,
      "step": 15330
    },
    {
      "epoch": 0.8593115424474134,
      "grad_norm": 5.658867359161377,
      "learning_rate": 7.857363080890184e-05,
      "loss": 0.3849,
      "step": 15340
    },
    {
      "epoch": 0.8598717194633504,
      "grad_norm": 2.863595724105835,
      "learning_rate": 7.855961657043557e-05,
      "loss": 0.1563,
      "step": 15350
    },
    {
      "epoch": 0.8604318964792874,
      "grad_norm": 4.604240417480469,
      "learning_rate": 7.854560233196928e-05,
      "loss": 0.2403,
      "step": 15360
    },
    {
      "epoch": 0.8609920734952244,
      "grad_norm": 3.2334249019622803,
      "learning_rate": 7.853158809350301e-05,
      "loss": 0.1982,
      "step": 15370
    },
    {
      "epoch": 0.8615522505111616,
      "grad_norm": 4.729203701019287,
      "learning_rate": 7.851757385503673e-05,
      "loss": 0.2405,
      "step": 15380
    },
    {
      "epoch": 0.8621124275270986,
      "grad_norm": 3.72737193107605,
      "learning_rate": 7.850355961657044e-05,
      "loss": 0.3282,
      "step": 15390
    },
    {
      "epoch": 0.8626726045430356,
      "grad_norm": 4.210101127624512,
      "learning_rate": 7.848954537810416e-05,
      "loss": 0.2967,
      "step": 15400
    },
    {
      "epoch": 0.8632327815589727,
      "grad_norm": 7.566063404083252,
      "learning_rate": 7.847553113963788e-05,
      "loss": 0.2095,
      "step": 15410
    },
    {
      "epoch": 0.8637929585749097,
      "grad_norm": 3.850693941116333,
      "learning_rate": 7.84615169011716e-05,
      "loss": 0.1845,
      "step": 15420
    },
    {
      "epoch": 0.8643531355908467,
      "grad_norm": 4.4870991706848145,
      "learning_rate": 7.844750266270531e-05,
      "loss": 0.331,
      "step": 15430
    },
    {
      "epoch": 0.8649133126067837,
      "grad_norm": 3.6397299766540527,
      "learning_rate": 7.843348842423903e-05,
      "loss": 0.287,
      "step": 15440
    },
    {
      "epoch": 0.8654734896227207,
      "grad_norm": 5.269100666046143,
      "learning_rate": 7.841947418577274e-05,
      "loss": 0.2221,
      "step": 15450
    },
    {
      "epoch": 0.8660336666386578,
      "grad_norm": 4.208644866943359,
      "learning_rate": 7.840545994730647e-05,
      "loss": 0.3533,
      "step": 15460
    },
    {
      "epoch": 0.8665938436545948,
      "grad_norm": 3.6180570125579834,
      "learning_rate": 7.839144570884019e-05,
      "loss": 0.2897,
      "step": 15470
    },
    {
      "epoch": 0.8671540206705319,
      "grad_norm": 3.0352096557617188,
      "learning_rate": 7.837743147037391e-05,
      "loss": 0.2635,
      "step": 15480
    },
    {
      "epoch": 0.867714197686469,
      "grad_norm": 2.4988656044006348,
      "learning_rate": 7.836341723190761e-05,
      "loss": 0.2862,
      "step": 15490
    },
    {
      "epoch": 0.868274374702406,
      "grad_norm": 7.212678909301758,
      "learning_rate": 7.834940299344134e-05,
      "loss": 0.3064,
      "step": 15500
    },
    {
      "epoch": 0.868834551718343,
      "grad_norm": 4.495339393615723,
      "learning_rate": 7.833538875497506e-05,
      "loss": 0.2846,
      "step": 15510
    },
    {
      "epoch": 0.86939472873428,
      "grad_norm": 4.135845184326172,
      "learning_rate": 7.832137451650877e-05,
      "loss": 0.1973,
      "step": 15520
    },
    {
      "epoch": 0.8699549057502171,
      "grad_norm": 4.057037353515625,
      "learning_rate": 7.83073602780425e-05,
      "loss": 0.3302,
      "step": 15530
    },
    {
      "epoch": 0.8705150827661541,
      "grad_norm": 4.7608442306518555,
      "learning_rate": 7.829334603957622e-05,
      "loss": 0.2392,
      "step": 15540
    },
    {
      "epoch": 0.8710752597820911,
      "grad_norm": 6.178560256958008,
      "learning_rate": 7.827933180110993e-05,
      "loss": 0.1847,
      "step": 15550
    },
    {
      "epoch": 0.8716354367980281,
      "grad_norm": 3.215099573135376,
      "learning_rate": 7.826531756264365e-05,
      "loss": 0.2553,
      "step": 15560
    },
    {
      "epoch": 0.8721956138139653,
      "grad_norm": 3.9879605770111084,
      "learning_rate": 7.825130332417737e-05,
      "loss": 0.4007,
      "step": 15570
    },
    {
      "epoch": 0.8727557908299023,
      "grad_norm": 3.8689565658569336,
      "learning_rate": 7.823728908571109e-05,
      "loss": 0.2567,
      "step": 15580
    },
    {
      "epoch": 0.8733159678458393,
      "grad_norm": 4.610128879547119,
      "learning_rate": 7.82232748472448e-05,
      "loss": 0.2749,
      "step": 15590
    },
    {
      "epoch": 0.8738761448617763,
      "grad_norm": 3.0041990280151367,
      "learning_rate": 7.820926060877852e-05,
      "loss": 0.2666,
      "step": 15600
    },
    {
      "epoch": 0.8744363218777134,
      "grad_norm": 4.238605976104736,
      "learning_rate": 7.819524637031223e-05,
      "loss": 0.2244,
      "step": 15610
    },
    {
      "epoch": 0.8749964988936504,
      "grad_norm": 3.696720838546753,
      "learning_rate": 7.818123213184596e-05,
      "loss": 0.2985,
      "step": 15620
    },
    {
      "epoch": 0.8755566759095874,
      "grad_norm": 5.695276737213135,
      "learning_rate": 7.816721789337968e-05,
      "loss": 0.2563,
      "step": 15630
    },
    {
      "epoch": 0.8761168529255244,
      "grad_norm": 4.088397026062012,
      "learning_rate": 7.81532036549134e-05,
      "loss": 0.3092,
      "step": 15640
    },
    {
      "epoch": 0.8766770299414615,
      "grad_norm": 2.3349788188934326,
      "learning_rate": 7.81391894164471e-05,
      "loss": 0.2589,
      "step": 15650
    },
    {
      "epoch": 0.8772372069573985,
      "grad_norm": 4.08914041519165,
      "learning_rate": 7.812517517798083e-05,
      "loss": 0.2984,
      "step": 15660
    },
    {
      "epoch": 0.8777973839733356,
      "grad_norm": 3.961026191711426,
      "learning_rate": 7.811116093951455e-05,
      "loss": 0.258,
      "step": 15670
    },
    {
      "epoch": 0.8783575609892726,
      "grad_norm": 1.6477848291397095,
      "learning_rate": 7.809714670104828e-05,
      "loss": 0.1871,
      "step": 15680
    },
    {
      "epoch": 0.8789177380052097,
      "grad_norm": 3.5786478519439697,
      "learning_rate": 7.808313246258199e-05,
      "loss": 0.2987,
      "step": 15690
    },
    {
      "epoch": 0.8794779150211467,
      "grad_norm": 3.7511610984802246,
      "learning_rate": 7.80691182241157e-05,
      "loss": 0.3884,
      "step": 15700
    },
    {
      "epoch": 0.8800380920370837,
      "grad_norm": 3.6332640647888184,
      "learning_rate": 7.805510398564942e-05,
      "loss": 0.26,
      "step": 15710
    },
    {
      "epoch": 0.8805982690530207,
      "grad_norm": 4.792588233947754,
      "learning_rate": 7.804108974718314e-05,
      "loss": 0.2954,
      "step": 15720
    },
    {
      "epoch": 0.8811584460689578,
      "grad_norm": 3.0755136013031006,
      "learning_rate": 7.802707550871686e-05,
      "loss": 0.2118,
      "step": 15730
    },
    {
      "epoch": 0.8817186230848948,
      "grad_norm": 5.1258368492126465,
      "learning_rate": 7.801306127025058e-05,
      "loss": 0.2272,
      "step": 15740
    },
    {
      "epoch": 0.8822788001008318,
      "grad_norm": 7.724515438079834,
      "learning_rate": 7.79990470317843e-05,
      "loss": 0.2438,
      "step": 15750
    },
    {
      "epoch": 0.882838977116769,
      "grad_norm": 2.134678602218628,
      "learning_rate": 7.798503279331801e-05,
      "loss": 0.1838,
      "step": 15760
    },
    {
      "epoch": 0.883399154132706,
      "grad_norm": 7.031232833862305,
      "learning_rate": 7.797101855485174e-05,
      "loss": 0.2129,
      "step": 15770
    },
    {
      "epoch": 0.883959331148643,
      "grad_norm": 4.628110408782959,
      "learning_rate": 7.795700431638545e-05,
      "loss": 0.1737,
      "step": 15780
    },
    {
      "epoch": 0.88451950816458,
      "grad_norm": 3.2266721725463867,
      "learning_rate": 7.794299007791917e-05,
      "loss": 0.2476,
      "step": 15790
    },
    {
      "epoch": 0.885079685180517,
      "grad_norm": 3.248577117919922,
      "learning_rate": 7.79289758394529e-05,
      "loss": 0.2135,
      "step": 15800
    },
    {
      "epoch": 0.8856398621964541,
      "grad_norm": 2.172706127166748,
      "learning_rate": 7.791496160098661e-05,
      "loss": 0.2536,
      "step": 15810
    },
    {
      "epoch": 0.8862000392123911,
      "grad_norm": 2.6044540405273438,
      "learning_rate": 7.790094736252032e-05,
      "loss": 0.2083,
      "step": 15820
    },
    {
      "epoch": 0.8867602162283281,
      "grad_norm": 5.123244285583496,
      "learning_rate": 7.788693312405404e-05,
      "loss": 0.193,
      "step": 15830
    },
    {
      "epoch": 0.8873203932442651,
      "grad_norm": 4.886030673980713,
      "learning_rate": 7.787291888558777e-05,
      "loss": 0.2469,
      "step": 15840
    },
    {
      "epoch": 0.8878805702602022,
      "grad_norm": 3.2193572521209717,
      "learning_rate": 7.785890464712148e-05,
      "loss": 0.224,
      "step": 15850
    },
    {
      "epoch": 0.8884407472761393,
      "grad_norm": 2.013127088546753,
      "learning_rate": 7.78448904086552e-05,
      "loss": 0.1794,
      "step": 15860
    },
    {
      "epoch": 0.8890009242920763,
      "grad_norm": 5.696493625640869,
      "learning_rate": 7.783087617018891e-05,
      "loss": 0.2196,
      "step": 15870
    },
    {
      "epoch": 0.8895611013080134,
      "grad_norm": 7.336048126220703,
      "learning_rate": 7.781686193172263e-05,
      "loss": 0.192,
      "step": 15880
    },
    {
      "epoch": 0.8901212783239504,
      "grad_norm": 5.951319694519043,
      "learning_rate": 7.780284769325635e-05,
      "loss": 0.2584,
      "step": 15890
    },
    {
      "epoch": 0.8906814553398874,
      "grad_norm": 2.4924185276031494,
      "learning_rate": 7.778883345479007e-05,
      "loss": 0.259,
      "step": 15900
    },
    {
      "epoch": 0.8912416323558244,
      "grad_norm": 3.939446210861206,
      "learning_rate": 7.77748192163238e-05,
      "loss": 0.177,
      "step": 15910
    },
    {
      "epoch": 0.8918018093717615,
      "grad_norm": 6.086137771606445,
      "learning_rate": 7.77608049778575e-05,
      "loss": 0.2931,
      "step": 15920
    },
    {
      "epoch": 0.8923619863876985,
      "grad_norm": 4.392418384552002,
      "learning_rate": 7.774679073939123e-05,
      "loss": 0.2144,
      "step": 15930
    },
    {
      "epoch": 0.8929221634036355,
      "grad_norm": 4.979692459106445,
      "learning_rate": 7.773277650092494e-05,
      "loss": 0.2581,
      "step": 15940
    },
    {
      "epoch": 0.8934823404195726,
      "grad_norm": 1.7442835569381714,
      "learning_rate": 7.771876226245867e-05,
      "loss": 0.2822,
      "step": 15950
    },
    {
      "epoch": 0.8940425174355097,
      "grad_norm": 3.7342629432678223,
      "learning_rate": 7.770474802399238e-05,
      "loss": 0.2667,
      "step": 15960
    },
    {
      "epoch": 0.8946026944514467,
      "grad_norm": 6.335451126098633,
      "learning_rate": 7.76907337855261e-05,
      "loss": 0.178,
      "step": 15970
    },
    {
      "epoch": 0.8951628714673837,
      "grad_norm": 4.5286407470703125,
      "learning_rate": 7.767671954705981e-05,
      "loss": 0.2456,
      "step": 15980
    },
    {
      "epoch": 0.8957230484833207,
      "grad_norm": 8.053945541381836,
      "learning_rate": 7.766270530859353e-05,
      "loss": 0.2915,
      "step": 15990
    },
    {
      "epoch": 0.8962832254992578,
      "grad_norm": 6.475142955780029,
      "learning_rate": 7.764869107012726e-05,
      "loss": 0.3192,
      "step": 16000
    },
    {
      "epoch": 0.8968434025151948,
      "grad_norm": 7.068024158477783,
      "learning_rate": 7.763467683166097e-05,
      "loss": 0.2656,
      "step": 16010
    },
    {
      "epoch": 0.8974035795311318,
      "grad_norm": 6.792867660522461,
      "learning_rate": 7.76206625931947e-05,
      "loss": 0.3088,
      "step": 16020
    },
    {
      "epoch": 0.8979637565470688,
      "grad_norm": 2.1588480472564697,
      "learning_rate": 7.76066483547284e-05,
      "loss": 0.2276,
      "step": 16030
    },
    {
      "epoch": 0.8985239335630059,
      "grad_norm": 7.787173271179199,
      "learning_rate": 7.759263411626213e-05,
      "loss": 0.2801,
      "step": 16040
    },
    {
      "epoch": 0.899084110578943,
      "grad_norm": 4.849830627441406,
      "learning_rate": 7.757861987779584e-05,
      "loss": 0.231,
      "step": 16050
    },
    {
      "epoch": 0.89964428759488,
      "grad_norm": 5.4201531410217285,
      "learning_rate": 7.756460563932956e-05,
      "loss": 0.2639,
      "step": 16060
    },
    {
      "epoch": 0.900204464610817,
      "grad_norm": 3.8329732418060303,
      "learning_rate": 7.755059140086329e-05,
      "loss": 0.2161,
      "step": 16070
    },
    {
      "epoch": 0.9007646416267541,
      "grad_norm": 4.647788047790527,
      "learning_rate": 7.7536577162397e-05,
      "loss": 0.2903,
      "step": 16080
    },
    {
      "epoch": 0.9013248186426911,
      "grad_norm": 2.504974365234375,
      "learning_rate": 7.752256292393072e-05,
      "loss": 0.2635,
      "step": 16090
    },
    {
      "epoch": 0.9018849956586281,
      "grad_norm": 3.0655815601348877,
      "learning_rate": 7.750854868546443e-05,
      "loss": 0.3216,
      "step": 16100
    },
    {
      "epoch": 0.9024451726745651,
      "grad_norm": 4.971844673156738,
      "learning_rate": 7.749453444699816e-05,
      "loss": 0.3605,
      "step": 16110
    },
    {
      "epoch": 0.9030053496905022,
      "grad_norm": 7.231813907623291,
      "learning_rate": 7.748052020853187e-05,
      "loss": 0.2468,
      "step": 16120
    },
    {
      "epoch": 0.9035655267064392,
      "grad_norm": 3.5965373516082764,
      "learning_rate": 7.746650597006559e-05,
      "loss": 0.2506,
      "step": 16130
    },
    {
      "epoch": 0.9041257037223762,
      "grad_norm": 6.7392168045043945,
      "learning_rate": 7.74524917315993e-05,
      "loss": 0.4496,
      "step": 16140
    },
    {
      "epoch": 0.9046858807383134,
      "grad_norm": 7.905601978302002,
      "learning_rate": 7.743847749313302e-05,
      "loss": 0.2391,
      "step": 16150
    },
    {
      "epoch": 0.9052460577542504,
      "grad_norm": 2.7039434909820557,
      "learning_rate": 7.742446325466675e-05,
      "loss": 0.2719,
      "step": 16160
    },
    {
      "epoch": 0.9058062347701874,
      "grad_norm": 2.9100635051727295,
      "learning_rate": 7.741044901620046e-05,
      "loss": 0.3371,
      "step": 16170
    },
    {
      "epoch": 0.9063664117861244,
      "grad_norm": 5.460380554199219,
      "learning_rate": 7.739643477773419e-05,
      "loss": 0.2738,
      "step": 16180
    },
    {
      "epoch": 0.9069265888020615,
      "grad_norm": 4.298295497894287,
      "learning_rate": 7.738242053926789e-05,
      "loss": 0.284,
      "step": 16190
    },
    {
      "epoch": 0.9074867658179985,
      "grad_norm": 4.715746879577637,
      "learning_rate": 7.736840630080162e-05,
      "loss": 0.2164,
      "step": 16200
    },
    {
      "epoch": 0.9080469428339355,
      "grad_norm": 4.14360237121582,
      "learning_rate": 7.735439206233533e-05,
      "loss": 0.2312,
      "step": 16210
    },
    {
      "epoch": 0.9086071198498725,
      "grad_norm": 4.1647114753723145,
      "learning_rate": 7.734037782386906e-05,
      "loss": 0.3364,
      "step": 16220
    },
    {
      "epoch": 0.9091672968658095,
      "grad_norm": 4.185786247253418,
      "learning_rate": 7.732636358540278e-05,
      "loss": 0.2408,
      "step": 16230
    },
    {
      "epoch": 0.9097274738817467,
      "grad_norm": 4.326193809509277,
      "learning_rate": 7.731234934693649e-05,
      "loss": 0.315,
      "step": 16240
    },
    {
      "epoch": 0.9102876508976837,
      "grad_norm": 4.424912929534912,
      "learning_rate": 7.72983351084702e-05,
      "loss": 0.2351,
      "step": 16250
    },
    {
      "epoch": 0.9108478279136207,
      "grad_norm": 3.94826340675354,
      "learning_rate": 7.728432087000392e-05,
      "loss": 0.2182,
      "step": 16260
    },
    {
      "epoch": 0.9114080049295578,
      "grad_norm": 3.4701616764068604,
      "learning_rate": 7.727030663153765e-05,
      "loss": 0.1385,
      "step": 16270
    },
    {
      "epoch": 0.9119681819454948,
      "grad_norm": 5.293144226074219,
      "learning_rate": 7.725629239307136e-05,
      "loss": 0.2466,
      "step": 16280
    },
    {
      "epoch": 0.9125283589614318,
      "grad_norm": 3.238762855529785,
      "learning_rate": 7.724227815460508e-05,
      "loss": 0.2188,
      "step": 16290
    },
    {
      "epoch": 0.9130885359773688,
      "grad_norm": 4.253602027893066,
      "learning_rate": 7.72282639161388e-05,
      "loss": 0.3138,
      "step": 16300
    },
    {
      "epoch": 0.9136487129933059,
      "grad_norm": 5.658969402313232,
      "learning_rate": 7.721424967767252e-05,
      "loss": 0.1428,
      "step": 16310
    },
    {
      "epoch": 0.9142088900092429,
      "grad_norm": 6.380308151245117,
      "learning_rate": 7.720023543920624e-05,
      "loss": 0.2006,
      "step": 16320
    },
    {
      "epoch": 0.9147690670251799,
      "grad_norm": 2.488231897354126,
      "learning_rate": 7.718622120073996e-05,
      "loss": 0.272,
      "step": 16330
    },
    {
      "epoch": 0.915329244041117,
      "grad_norm": 3.8860013484954834,
      "learning_rate": 7.717220696227368e-05,
      "loss": 0.4281,
      "step": 16340
    },
    {
      "epoch": 0.9158894210570541,
      "grad_norm": 5.583334445953369,
      "learning_rate": 7.715819272380738e-05,
      "loss": 0.1768,
      "step": 16350
    },
    {
      "epoch": 0.9164495980729911,
      "grad_norm": 3.29109263420105,
      "learning_rate": 7.714417848534111e-05,
      "loss": 0.323,
      "step": 16360
    },
    {
      "epoch": 0.9170097750889281,
      "grad_norm": 1.4277421236038208,
      "learning_rate": 7.713016424687482e-05,
      "loss": 0.2108,
      "step": 16370
    },
    {
      "epoch": 0.9175699521048651,
      "grad_norm": 6.560962200164795,
      "learning_rate": 7.711615000840855e-05,
      "loss": 0.2737,
      "step": 16380
    },
    {
      "epoch": 0.9181301291208022,
      "grad_norm": 2.6108975410461426,
      "learning_rate": 7.710213576994227e-05,
      "loss": 0.157,
      "step": 16390
    },
    {
      "epoch": 0.9186903061367392,
      "grad_norm": 4.101748943328857,
      "learning_rate": 7.708812153147598e-05,
      "loss": 0.2446,
      "step": 16400
    },
    {
      "epoch": 0.9192504831526762,
      "grad_norm": 4.382645606994629,
      "learning_rate": 7.70741072930097e-05,
      "loss": 0.3292,
      "step": 16410
    },
    {
      "epoch": 0.9198106601686132,
      "grad_norm": 3.781468391418457,
      "learning_rate": 7.706009305454342e-05,
      "loss": 0.2991,
      "step": 16420
    },
    {
      "epoch": 0.9203708371845504,
      "grad_norm": 4.188180923461914,
      "learning_rate": 7.704607881607714e-05,
      "loss": 0.2776,
      "step": 16430
    },
    {
      "epoch": 0.9209310142004874,
      "grad_norm": 5.219563961029053,
      "learning_rate": 7.703206457761085e-05,
      "loss": 0.1989,
      "step": 16440
    },
    {
      "epoch": 0.9214911912164244,
      "grad_norm": 3.956437110900879,
      "learning_rate": 7.701805033914458e-05,
      "loss": 0.2017,
      "step": 16450
    },
    {
      "epoch": 0.9220513682323614,
      "grad_norm": 5.627065658569336,
      "learning_rate": 7.700403610067828e-05,
      "loss": 0.128,
      "step": 16460
    },
    {
      "epoch": 0.9226115452482985,
      "grad_norm": 5.062681674957275,
      "learning_rate": 7.699002186221201e-05,
      "loss": 0.36,
      "step": 16470
    },
    {
      "epoch": 0.9231717222642355,
      "grad_norm": 7.29652214050293,
      "learning_rate": 7.697600762374573e-05,
      "loss": 0.2966,
      "step": 16480
    },
    {
      "epoch": 0.9237318992801725,
      "grad_norm": 5.51915979385376,
      "learning_rate": 7.696199338527946e-05,
      "loss": 0.2806,
      "step": 16490
    },
    {
      "epoch": 0.9242920762961095,
      "grad_norm": 6.0818586349487305,
      "learning_rate": 7.694797914681317e-05,
      "loss": 0.2172,
      "step": 16500
    },
    {
      "epoch": 0.9248522533120466,
      "grad_norm": 3.770993709564209,
      "learning_rate": 7.693396490834688e-05,
      "loss": 0.244,
      "step": 16510
    },
    {
      "epoch": 0.9254124303279836,
      "grad_norm": 9.482804298400879,
      "learning_rate": 7.69199506698806e-05,
      "loss": 0.1958,
      "step": 16520
    },
    {
      "epoch": 0.9259726073439207,
      "grad_norm": 5.223092079162598,
      "learning_rate": 7.690593643141431e-05,
      "loss": 0.2935,
      "step": 16530
    },
    {
      "epoch": 0.9265327843598578,
      "grad_norm": 2.367295503616333,
      "learning_rate": 7.689192219294804e-05,
      "loss": 0.2111,
      "step": 16540
    },
    {
      "epoch": 0.9270929613757948,
      "grad_norm": 1.8457374572753906,
      "learning_rate": 7.687790795448176e-05,
      "loss": 0.371,
      "step": 16550
    },
    {
      "epoch": 0.9276531383917318,
      "grad_norm": 4.070508003234863,
      "learning_rate": 7.686389371601547e-05,
      "loss": 0.1958,
      "step": 16560
    },
    {
      "epoch": 0.9282133154076688,
      "grad_norm": 5.984312057495117,
      "learning_rate": 7.684987947754919e-05,
      "loss": 0.2947,
      "step": 16570
    },
    {
      "epoch": 0.9287734924236059,
      "grad_norm": 8.920982360839844,
      "learning_rate": 7.683586523908291e-05,
      "loss": 0.2367,
      "step": 16580
    },
    {
      "epoch": 0.9293336694395429,
      "grad_norm": 2.2716116905212402,
      "learning_rate": 7.682185100061663e-05,
      "loss": 0.2629,
      "step": 16590
    },
    {
      "epoch": 0.9298938464554799,
      "grad_norm": 4.723851203918457,
      "learning_rate": 7.680783676215036e-05,
      "loss": 0.281,
      "step": 16600
    },
    {
      "epoch": 0.9304540234714169,
      "grad_norm": 3.8408212661743164,
      "learning_rate": 7.679382252368407e-05,
      "loss": 0.2616,
      "step": 16610
    },
    {
      "epoch": 0.931014200487354,
      "grad_norm": 2.0913803577423096,
      "learning_rate": 7.677980828521777e-05,
      "loss": 0.2779,
      "step": 16620
    },
    {
      "epoch": 0.9315743775032911,
      "grad_norm": 4.322445869445801,
      "learning_rate": 7.67657940467515e-05,
      "loss": 0.2691,
      "step": 16630
    },
    {
      "epoch": 0.9321345545192281,
      "grad_norm": 3.475205421447754,
      "learning_rate": 7.675177980828522e-05,
      "loss": 0.3461,
      "step": 16640
    },
    {
      "epoch": 0.9326947315351651,
      "grad_norm": 2.7750213146209717,
      "learning_rate": 7.673776556981895e-05,
      "loss": 0.2321,
      "step": 16650
    },
    {
      "epoch": 0.9332549085511022,
      "grad_norm": 5.095193386077881,
      "learning_rate": 7.672375133135266e-05,
      "loss": 0.3089,
      "step": 16660
    },
    {
      "epoch": 0.9338150855670392,
      "grad_norm": 5.682036876678467,
      "learning_rate": 7.670973709288637e-05,
      "loss": 0.1859,
      "step": 16670
    },
    {
      "epoch": 0.9343752625829762,
      "grad_norm": 4.32558536529541,
      "learning_rate": 7.669572285442009e-05,
      "loss": 0.3309,
      "step": 16680
    },
    {
      "epoch": 0.9349354395989132,
      "grad_norm": 4.280486583709717,
      "learning_rate": 7.668170861595382e-05,
      "loss": 0.2358,
      "step": 16690
    },
    {
      "epoch": 0.9354956166148503,
      "grad_norm": 8.282525062561035,
      "learning_rate": 7.666769437748753e-05,
      "loss": 0.3077,
      "step": 16700
    },
    {
      "epoch": 0.9360557936307873,
      "grad_norm": 3.8103725910186768,
      "learning_rate": 7.665368013902125e-05,
      "loss": 0.2188,
      "step": 16710
    },
    {
      "epoch": 0.9366159706467244,
      "grad_norm": 5.288068771362305,
      "learning_rate": 7.663966590055498e-05,
      "loss": 0.2732,
      "step": 16720
    },
    {
      "epoch": 0.9371761476626614,
      "grad_norm": 4.056981086730957,
      "learning_rate": 7.662565166208868e-05,
      "loss": 0.1554,
      "step": 16730
    },
    {
      "epoch": 0.9377363246785985,
      "grad_norm": 7.760406017303467,
      "learning_rate": 7.66116374236224e-05,
      "loss": 0.3948,
      "step": 16740
    },
    {
      "epoch": 0.9382965016945355,
      "grad_norm": 4.13053560256958,
      "learning_rate": 7.659762318515612e-05,
      "loss": 0.1648,
      "step": 16750
    },
    {
      "epoch": 0.9388566787104725,
      "grad_norm": 4.025157451629639,
      "learning_rate": 7.658360894668985e-05,
      "loss": 0.1804,
      "step": 16760
    },
    {
      "epoch": 0.9394168557264095,
      "grad_norm": 4.613972187042236,
      "learning_rate": 7.656959470822356e-05,
      "loss": 0.2147,
      "step": 16770
    },
    {
      "epoch": 0.9399770327423466,
      "grad_norm": 3.0387156009674072,
      "learning_rate": 7.655558046975728e-05,
      "loss": 0.2177,
      "step": 16780
    },
    {
      "epoch": 0.9405372097582836,
      "grad_norm": 6.553962707519531,
      "learning_rate": 7.654156623129099e-05,
      "loss": 0.3806,
      "step": 16790
    },
    {
      "epoch": 0.9410973867742206,
      "grad_norm": 3.2145488262176514,
      "learning_rate": 7.652755199282471e-05,
      "loss": 0.1991,
      "step": 16800
    },
    {
      "epoch": 0.9416575637901576,
      "grad_norm": 3.291344404220581,
      "learning_rate": 7.651353775435844e-05,
      "loss": 0.3481,
      "step": 16810
    },
    {
      "epoch": 0.9422177408060948,
      "grad_norm": 3.8427326679229736,
      "learning_rate": 7.649952351589215e-05,
      "loss": 0.2791,
      "step": 16820
    },
    {
      "epoch": 0.9427779178220318,
      "grad_norm": 5.155929088592529,
      "learning_rate": 7.648550927742586e-05,
      "loss": 0.1444,
      "step": 16830
    },
    {
      "epoch": 0.9433380948379688,
      "grad_norm": 3.6607847213745117,
      "learning_rate": 7.647149503895958e-05,
      "loss": 0.1952,
      "step": 16840
    },
    {
      "epoch": 0.9438982718539058,
      "grad_norm": 1.809995412826538,
      "learning_rate": 7.645748080049331e-05,
      "loss": 0.2511,
      "step": 16850
    },
    {
      "epoch": 0.9444584488698429,
      "grad_norm": 4.189427852630615,
      "learning_rate": 7.644346656202702e-05,
      "loss": 0.2327,
      "step": 16860
    },
    {
      "epoch": 0.9450186258857799,
      "grad_norm": 6.895671367645264,
      "learning_rate": 7.642945232356075e-05,
      "loss": 0.2532,
      "step": 16870
    },
    {
      "epoch": 0.9455788029017169,
      "grad_norm": 4.55097770690918,
      "learning_rate": 7.641543808509447e-05,
      "loss": 0.3721,
      "step": 16880
    },
    {
      "epoch": 0.9461389799176539,
      "grad_norm": 5.863316059112549,
      "learning_rate": 7.640142384662817e-05,
      "loss": 0.2247,
      "step": 16890
    },
    {
      "epoch": 0.946699156933591,
      "grad_norm": 3.7150967121124268,
      "learning_rate": 7.63874096081619e-05,
      "loss": 0.2928,
      "step": 16900
    },
    {
      "epoch": 0.9472593339495281,
      "grad_norm": 2.5897257328033447,
      "learning_rate": 7.637339536969561e-05,
      "loss": 0.2749,
      "step": 16910
    },
    {
      "epoch": 0.9478195109654651,
      "grad_norm": 3.7928667068481445,
      "learning_rate": 7.635938113122934e-05,
      "loss": 0.2349,
      "step": 16920
    },
    {
      "epoch": 0.9483796879814022,
      "grad_norm": 6.231876373291016,
      "learning_rate": 7.634536689276305e-05,
      "loss": 0.3957,
      "step": 16930
    },
    {
      "epoch": 0.9489398649973392,
      "grad_norm": 2.337836265563965,
      "learning_rate": 7.633135265429677e-05,
      "loss": 0.2146,
      "step": 16940
    },
    {
      "epoch": 0.9495000420132762,
      "grad_norm": 4.535388469696045,
      "learning_rate": 7.631733841583048e-05,
      "loss": 0.1676,
      "step": 16950
    },
    {
      "epoch": 0.9500602190292132,
      "grad_norm": 5.401134014129639,
      "learning_rate": 7.630332417736421e-05,
      "loss": 0.1828,
      "step": 16960
    },
    {
      "epoch": 0.9506203960451503,
      "grad_norm": 3.77472186088562,
      "learning_rate": 7.628930993889793e-05,
      "loss": 0.2294,
      "step": 16970
    },
    {
      "epoch": 0.9511805730610873,
      "grad_norm": 5.433481693267822,
      "learning_rate": 7.627529570043164e-05,
      "loss": 0.2572,
      "step": 16980
    },
    {
      "epoch": 0.9517407500770243,
      "grad_norm": 4.033188343048096,
      "learning_rate": 7.626128146196535e-05,
      "loss": 0.2903,
      "step": 16990
    },
    {
      "epoch": 0.9523009270929613,
      "grad_norm": 4.900999546051025,
      "learning_rate": 7.624726722349907e-05,
      "loss": 0.1972,
      "step": 17000
    },
    {
      "epoch": 0.9528611041088985,
      "grad_norm": 4.417952537536621,
      "learning_rate": 7.62332529850328e-05,
      "loss": 0.237,
      "step": 17010
    },
    {
      "epoch": 0.9534212811248355,
      "grad_norm": 4.284285068511963,
      "learning_rate": 7.621923874656651e-05,
      "loss": 0.2564,
      "step": 17020
    },
    {
      "epoch": 0.9539814581407725,
      "grad_norm": 3.1926357746124268,
      "learning_rate": 7.620522450810024e-05,
      "loss": 0.2615,
      "step": 17030
    },
    {
      "epoch": 0.9545416351567095,
      "grad_norm": 2.990659236907959,
      "learning_rate": 7.619121026963396e-05,
      "loss": 0.1937,
      "step": 17040
    },
    {
      "epoch": 0.9551018121726466,
      "grad_norm": 1.8932256698608398,
      "learning_rate": 7.617719603116767e-05,
      "loss": 0.1248,
      "step": 17050
    },
    {
      "epoch": 0.9556619891885836,
      "grad_norm": 4.469521522521973,
      "learning_rate": 7.616318179270139e-05,
      "loss": 0.1821,
      "step": 17060
    },
    {
      "epoch": 0.9562221662045206,
      "grad_norm": 4.404479503631592,
      "learning_rate": 7.61491675542351e-05,
      "loss": 0.1797,
      "step": 17070
    },
    {
      "epoch": 0.9567823432204576,
      "grad_norm": 4.492583751678467,
      "learning_rate": 7.613515331576883e-05,
      "loss": 0.4409,
      "step": 17080
    },
    {
      "epoch": 0.9573425202363947,
      "grad_norm": 5.967878341674805,
      "learning_rate": 7.612113907730254e-05,
      "loss": 0.3408,
      "step": 17090
    },
    {
      "epoch": 0.9579026972523318,
      "grad_norm": 4.632932662963867,
      "learning_rate": 7.610712483883626e-05,
      "loss": 0.2823,
      "step": 17100
    },
    {
      "epoch": 0.9584628742682688,
      "grad_norm": 6.192112445831299,
      "learning_rate": 7.609311060036997e-05,
      "loss": 0.1688,
      "step": 17110
    },
    {
      "epoch": 0.9590230512842058,
      "grad_norm": 4.520325660705566,
      "learning_rate": 7.60790963619037e-05,
      "loss": 0.2625,
      "step": 17120
    },
    {
      "epoch": 0.9595832283001429,
      "grad_norm": 4.363159656524658,
      "learning_rate": 7.606508212343742e-05,
      "loss": 0.2512,
      "step": 17130
    },
    {
      "epoch": 0.9601434053160799,
      "grad_norm": 3.5374209880828857,
      "learning_rate": 7.605106788497114e-05,
      "loss": 0.2908,
      "step": 17140
    },
    {
      "epoch": 0.9607035823320169,
      "grad_norm": 5.807353973388672,
      "learning_rate": 7.603705364650486e-05,
      "loss": 0.2584,
      "step": 17150
    },
    {
      "epoch": 0.9612637593479539,
      "grad_norm": 3.646987199783325,
      "learning_rate": 7.602303940803856e-05,
      "loss": 0.2584,
      "step": 17160
    },
    {
      "epoch": 0.961823936363891,
      "grad_norm": 4.753291130065918,
      "learning_rate": 7.600902516957229e-05,
      "loss": 0.3351,
      "step": 17170
    },
    {
      "epoch": 0.962384113379828,
      "grad_norm": 3.113868474960327,
      "learning_rate": 7.5995010931106e-05,
      "loss": 0.2531,
      "step": 17180
    },
    {
      "epoch": 0.962944290395765,
      "grad_norm": 2.167193651199341,
      "learning_rate": 7.598099669263973e-05,
      "loss": 0.1934,
      "step": 17190
    },
    {
      "epoch": 0.9635044674117021,
      "grad_norm": 4.351755142211914,
      "learning_rate": 7.596698245417345e-05,
      "loss": 0.3151,
      "step": 17200
    },
    {
      "epoch": 0.9640646444276392,
      "grad_norm": 5.730225563049316,
      "learning_rate": 7.595296821570716e-05,
      "loss": 0.3382,
      "step": 17210
    },
    {
      "epoch": 0.9646248214435762,
      "grad_norm": 6.173962116241455,
      "learning_rate": 7.593895397724088e-05,
      "loss": 0.3209,
      "step": 17220
    },
    {
      "epoch": 0.9651849984595132,
      "grad_norm": 6.766454219818115,
      "learning_rate": 7.59249397387746e-05,
      "loss": 0.249,
      "step": 17230
    },
    {
      "epoch": 0.9657451754754502,
      "grad_norm": 4.312241554260254,
      "learning_rate": 7.591092550030832e-05,
      "loss": 0.236,
      "step": 17240
    },
    {
      "epoch": 0.9663053524913873,
      "grad_norm": 4.963443756103516,
      "learning_rate": 7.589691126184203e-05,
      "loss": 0.1958,
      "step": 17250
    },
    {
      "epoch": 0.9668655295073243,
      "grad_norm": 5.297885894775391,
      "learning_rate": 7.588289702337575e-05,
      "loss": 0.2676,
      "step": 17260
    },
    {
      "epoch": 0.9674257065232613,
      "grad_norm": 4.3269734382629395,
      "learning_rate": 7.586888278490946e-05,
      "loss": 0.2125,
      "step": 17270
    },
    {
      "epoch": 0.9679858835391983,
      "grad_norm": 3.7843921184539795,
      "learning_rate": 7.585486854644319e-05,
      "loss": 0.1759,
      "step": 17280
    },
    {
      "epoch": 0.9685460605551354,
      "grad_norm": 5.25250244140625,
      "learning_rate": 7.58408543079769e-05,
      "loss": 0.241,
      "step": 17290
    },
    {
      "epoch": 0.9691062375710725,
      "grad_norm": 3.9370133876800537,
      "learning_rate": 7.582684006951063e-05,
      "loss": 0.1707,
      "step": 17300
    },
    {
      "epoch": 0.9696664145870095,
      "grad_norm": 3.177546977996826,
      "learning_rate": 7.581282583104435e-05,
      "loss": 0.3917,
      "step": 17310
    },
    {
      "epoch": 0.9702265916029466,
      "grad_norm": 3.4049453735351562,
      "learning_rate": 7.579881159257806e-05,
      "loss": 0.3898,
      "step": 17320
    },
    {
      "epoch": 0.9707867686188836,
      "grad_norm": 2.7482738494873047,
      "learning_rate": 7.578479735411178e-05,
      "loss": 0.3322,
      "step": 17330
    },
    {
      "epoch": 0.9713469456348206,
      "grad_norm": 3.8103654384613037,
      "learning_rate": 7.577078311564549e-05,
      "loss": 0.2868,
      "step": 17340
    },
    {
      "epoch": 0.9719071226507576,
      "grad_norm": 3.314052104949951,
      "learning_rate": 7.575676887717922e-05,
      "loss": 0.1829,
      "step": 17350
    },
    {
      "epoch": 0.9724672996666947,
      "grad_norm": 4.907362937927246,
      "learning_rate": 7.574275463871294e-05,
      "loss": 0.2423,
      "step": 17360
    },
    {
      "epoch": 0.9730274766826317,
      "grad_norm": 6.621172904968262,
      "learning_rate": 7.572874040024665e-05,
      "loss": 0.2099,
      "step": 17370
    },
    {
      "epoch": 0.9735876536985687,
      "grad_norm": 4.6526055335998535,
      "learning_rate": 7.571472616178037e-05,
      "loss": 0.219,
      "step": 17380
    },
    {
      "epoch": 0.9741478307145058,
      "grad_norm": 6.156003475189209,
      "learning_rate": 7.57007119233141e-05,
      "loss": 0.1839,
      "step": 17390
    },
    {
      "epoch": 0.9747080077304429,
      "grad_norm": 2.5026321411132812,
      "learning_rate": 7.568669768484781e-05,
      "loss": 0.3191,
      "step": 17400
    },
    {
      "epoch": 0.9752681847463799,
      "grad_norm": 2.1846532821655273,
      "learning_rate": 7.567268344638154e-05,
      "loss": 0.3817,
      "step": 17410
    },
    {
      "epoch": 0.9758283617623169,
      "grad_norm": 2.4732882976531982,
      "learning_rate": 7.565866920791525e-05,
      "loss": 0.2483,
      "step": 17420
    },
    {
      "epoch": 0.9763885387782539,
      "grad_norm": 6.240377902984619,
      "learning_rate": 7.564465496944897e-05,
      "loss": 0.2695,
      "step": 17430
    },
    {
      "epoch": 0.976948715794191,
      "grad_norm": 4.541306018829346,
      "learning_rate": 7.563064073098268e-05,
      "loss": 0.3377,
      "step": 17440
    },
    {
      "epoch": 0.977508892810128,
      "grad_norm": 3.735069513320923,
      "learning_rate": 7.56166264925164e-05,
      "loss": 0.1687,
      "step": 17450
    },
    {
      "epoch": 0.978069069826065,
      "grad_norm": 4.865911960601807,
      "learning_rate": 7.560261225405012e-05,
      "loss": 0.1787,
      "step": 17460
    },
    {
      "epoch": 0.978629246842002,
      "grad_norm": 2.029277801513672,
      "learning_rate": 7.558859801558384e-05,
      "loss": 0.1878,
      "step": 17470
    },
    {
      "epoch": 0.9791894238579391,
      "grad_norm": 4.395437717437744,
      "learning_rate": 7.557458377711755e-05,
      "loss": 0.1995,
      "step": 17480
    },
    {
      "epoch": 0.9797496008738762,
      "grad_norm": 5.071166515350342,
      "learning_rate": 7.556056953865127e-05,
      "loss": 0.349,
      "step": 17490
    },
    {
      "epoch": 0.9803097778898132,
      "grad_norm": 2.7558305263519287,
      "learning_rate": 7.5546555300185e-05,
      "loss": 0.3111,
      "step": 17500
    },
    {
      "epoch": 0.9808699549057502,
      "grad_norm": 5.746564865112305,
      "learning_rate": 7.553254106171871e-05,
      "loss": 0.2109,
      "step": 17510
    },
    {
      "epoch": 0.9814301319216873,
      "grad_norm": 2.446883201599121,
      "learning_rate": 7.551852682325244e-05,
      "loss": 0.1319,
      "step": 17520
    },
    {
      "epoch": 0.9819903089376243,
      "grad_norm": 3.7851788997650146,
      "learning_rate": 7.550451258478614e-05,
      "loss": 0.2378,
      "step": 17530
    },
    {
      "epoch": 0.9825504859535613,
      "grad_norm": 5.002935886383057,
      "learning_rate": 7.549049834631986e-05,
      "loss": 0.2186,
      "step": 17540
    },
    {
      "epoch": 0.9831106629694983,
      "grad_norm": 5.723207473754883,
      "learning_rate": 7.547648410785358e-05,
      "loss": 0.2646,
      "step": 17550
    },
    {
      "epoch": 0.9836708399854354,
      "grad_norm": 4.440149307250977,
      "learning_rate": 7.54624698693873e-05,
      "loss": 0.248,
      "step": 17560
    },
    {
      "epoch": 0.9842310170013724,
      "grad_norm": 4.12375020980835,
      "learning_rate": 7.544845563092103e-05,
      "loss": 0.1892,
      "step": 17570
    },
    {
      "epoch": 0.9847911940173095,
      "grad_norm": 3.9302213191986084,
      "learning_rate": 7.543444139245474e-05,
      "loss": 0.4103,
      "step": 17580
    },
    {
      "epoch": 0.9853513710332465,
      "grad_norm": 5.53455924987793,
      "learning_rate": 7.542042715398846e-05,
      "loss": 0.2064,
      "step": 17590
    },
    {
      "epoch": 0.9859115480491836,
      "grad_norm": 3.695389986038208,
      "learning_rate": 7.540641291552217e-05,
      "loss": 0.2905,
      "step": 17600
    },
    {
      "epoch": 0.9864717250651206,
      "grad_norm": 1.9030494689941406,
      "learning_rate": 7.53923986770559e-05,
      "loss": 0.3202,
      "step": 17610
    },
    {
      "epoch": 0.9870319020810576,
      "grad_norm": 5.65755033493042,
      "learning_rate": 7.537838443858961e-05,
      "loss": 0.2992,
      "step": 17620
    },
    {
      "epoch": 0.9875920790969946,
      "grad_norm": 4.190275192260742,
      "learning_rate": 7.536437020012333e-05,
      "loss": 0.3142,
      "step": 17630
    },
    {
      "epoch": 0.9881522561129317,
      "grad_norm": 3.5259432792663574,
      "learning_rate": 7.535035596165704e-05,
      "loss": 0.254,
      "step": 17640
    },
    {
      "epoch": 0.9887124331288687,
      "grad_norm": 3.940202236175537,
      "learning_rate": 7.533634172319076e-05,
      "loss": 0.3699,
      "step": 17650
    },
    {
      "epoch": 0.9892726101448057,
      "grad_norm": 5.06796932220459,
      "learning_rate": 7.532232748472449e-05,
      "loss": 0.2323,
      "step": 17660
    },
    {
      "epoch": 0.9898327871607427,
      "grad_norm": 3.6413493156433105,
      "learning_rate": 7.53083132462582e-05,
      "loss": 0.2665,
      "step": 17670
    },
    {
      "epoch": 0.9903929641766799,
      "grad_norm": 4.734989643096924,
      "learning_rate": 7.529429900779193e-05,
      "loss": 0.23,
      "step": 17680
    },
    {
      "epoch": 0.9909531411926169,
      "grad_norm": 2.813775062561035,
      "learning_rate": 7.528028476932563e-05,
      "loss": 0.3019,
      "step": 17690
    },
    {
      "epoch": 0.9915133182085539,
      "grad_norm": 6.815264701843262,
      "learning_rate": 7.526627053085936e-05,
      "loss": 0.2568,
      "step": 17700
    },
    {
      "epoch": 0.992073495224491,
      "grad_norm": 2.752391815185547,
      "learning_rate": 7.525225629239307e-05,
      "loss": 0.2412,
      "step": 17710
    },
    {
      "epoch": 0.992633672240428,
      "grad_norm": 7.579617023468018,
      "learning_rate": 7.523824205392679e-05,
      "loss": 0.2895,
      "step": 17720
    },
    {
      "epoch": 0.993193849256365,
      "grad_norm": 2.725369930267334,
      "learning_rate": 7.522422781546052e-05,
      "loss": 0.2818,
      "step": 17730
    },
    {
      "epoch": 0.993754026272302,
      "grad_norm": 3.263319730758667,
      "learning_rate": 7.521021357699423e-05,
      "loss": 0.2291,
      "step": 17740
    },
    {
      "epoch": 0.994314203288239,
      "grad_norm": 6.675732612609863,
      "learning_rate": 7.519619933852795e-05,
      "loss": 0.1993,
      "step": 17750
    },
    {
      "epoch": 0.9948743803041761,
      "grad_norm": 3.544835329055786,
      "learning_rate": 7.518218510006166e-05,
      "loss": 0.3383,
      "step": 17760
    },
    {
      "epoch": 0.9954345573201131,
      "grad_norm": 5.226060390472412,
      "learning_rate": 7.516817086159539e-05,
      "loss": 0.2721,
      "step": 17770
    },
    {
      "epoch": 0.9959947343360502,
      "grad_norm": 4.925366401672363,
      "learning_rate": 7.51541566231291e-05,
      "loss": 0.4175,
      "step": 17780
    },
    {
      "epoch": 0.9965549113519873,
      "grad_norm": 4.471324443817139,
      "learning_rate": 7.514014238466283e-05,
      "loss": 0.2853,
      "step": 17790
    },
    {
      "epoch": 0.9971150883679243,
      "grad_norm": 3.3580667972564697,
      "learning_rate": 7.512612814619653e-05,
      "loss": 0.2759,
      "step": 17800
    },
    {
      "epoch": 0.9976752653838613,
      "grad_norm": 1.1299556493759155,
      "learning_rate": 7.511211390773025e-05,
      "loss": 0.1984,
      "step": 17810
    },
    {
      "epoch": 0.9982354423997983,
      "grad_norm": 3.628126859664917,
      "learning_rate": 7.509809966926398e-05,
      "loss": 0.2553,
      "step": 17820
    },
    {
      "epoch": 0.9987956194157354,
      "grad_norm": 2.746399402618408,
      "learning_rate": 7.508408543079769e-05,
      "loss": 0.197,
      "step": 17830
    },
    {
      "epoch": 0.9993557964316724,
      "grad_norm": 2.3465685844421387,
      "learning_rate": 7.507007119233142e-05,
      "loss": 0.1779,
      "step": 17840
    },
    {
      "epoch": 0.9999159734476094,
      "grad_norm": 5.952977657318115,
      "learning_rate": 7.505605695386513e-05,
      "loss": 0.3817,
      "step": 17850
    },
    {
      "epoch": 1.0004481416127495,
      "grad_norm": 6.015754222869873,
      "learning_rate": 7.504204271539885e-05,
      "loss": 0.2661,
      "step": 17860
    },
    {
      "epoch": 1.0010083186286867,
      "grad_norm": 5.859701156616211,
      "learning_rate": 7.502802847693256e-05,
      "loss": 0.184,
      "step": 17870
    },
    {
      "epoch": 1.0015684956446238,
      "grad_norm": 4.969489097595215,
      "learning_rate": 7.501401423846629e-05,
      "loss": 0.2426,
      "step": 17880
    },
    {
      "epoch": 1.0021286726605607,
      "grad_norm": 5.229700088500977,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.2476,
      "step": 17890
    },
    {
      "epoch": 1.0026888496764979,
      "grad_norm": 1.3888968229293823,
      "learning_rate": 7.498598576153372e-05,
      "loss": 0.1582,
      "step": 17900
    },
    {
      "epoch": 1.0032490266924348,
      "grad_norm": 4.483640670776367,
      "learning_rate": 7.497197152306744e-05,
      "loss": 0.1833,
      "step": 17910
    },
    {
      "epoch": 1.003809203708372,
      "grad_norm": 4.795689582824707,
      "learning_rate": 7.495795728460115e-05,
      "loss": 0.2202,
      "step": 17920
    },
    {
      "epoch": 1.0043693807243088,
      "grad_norm": 3.9932949542999268,
      "learning_rate": 7.494394304613488e-05,
      "loss": 0.2794,
      "step": 17930
    },
    {
      "epoch": 1.004929557740246,
      "grad_norm": 2.9770588874816895,
      "learning_rate": 7.49299288076686e-05,
      "loss": 0.2903,
      "step": 17940
    },
    {
      "epoch": 1.0054897347561829,
      "grad_norm": 5.313627243041992,
      "learning_rate": 7.491591456920232e-05,
      "loss": 0.2526,
      "step": 17950
    },
    {
      "epoch": 1.00604991177212,
      "grad_norm": 7.015229225158691,
      "learning_rate": 7.490190033073602e-05,
      "loss": 0.3014,
      "step": 17960
    },
    {
      "epoch": 1.0066100887880571,
      "grad_norm": 4.399384498596191,
      "learning_rate": 7.488788609226975e-05,
      "loss": 0.2914,
      "step": 17970
    },
    {
      "epoch": 1.007170265803994,
      "grad_norm": 5.756534099578857,
      "learning_rate": 7.487387185380347e-05,
      "loss": 0.1379,
      "step": 17980
    },
    {
      "epoch": 1.0077304428199312,
      "grad_norm": 1.3950623273849487,
      "learning_rate": 7.485985761533718e-05,
      "loss": 0.2072,
      "step": 17990
    },
    {
      "epoch": 1.008290619835868,
      "grad_norm": 3.658780097961426,
      "learning_rate": 7.484584337687091e-05,
      "loss": 0.2722,
      "step": 18000
    },
    {
      "epoch": 1.0088507968518052,
      "grad_norm": 3.632126808166504,
      "learning_rate": 7.483182913840462e-05,
      "loss": 0.2314,
      "step": 18010
    },
    {
      "epoch": 1.0094109738677421,
      "grad_norm": 4.8052287101745605,
      "learning_rate": 7.481781489993834e-05,
      "loss": 0.1709,
      "step": 18020
    },
    {
      "epoch": 1.0099711508836793,
      "grad_norm": 3.363039970397949,
      "learning_rate": 7.480380066147205e-05,
      "loss": 0.2458,
      "step": 18030
    },
    {
      "epoch": 1.0105313278996162,
      "grad_norm": 5.441819190979004,
      "learning_rate": 7.478978642300578e-05,
      "loss": 0.2544,
      "step": 18040
    },
    {
      "epoch": 1.0110915049155533,
      "grad_norm": 4.703139781951904,
      "learning_rate": 7.47757721845395e-05,
      "loss": 0.1904,
      "step": 18050
    },
    {
      "epoch": 1.0116516819314902,
      "grad_norm": 6.5794148445129395,
      "learning_rate": 7.476175794607323e-05,
      "loss": 0.2956,
      "step": 18060
    },
    {
      "epoch": 1.0122118589474274,
      "grad_norm": 5.060230255126953,
      "learning_rate": 7.474774370760693e-05,
      "loss": 0.2575,
      "step": 18070
    },
    {
      "epoch": 1.0127720359633645,
      "grad_norm": 3.2322094440460205,
      "learning_rate": 7.473372946914064e-05,
      "loss": 0.2107,
      "step": 18080
    },
    {
      "epoch": 1.0133322129793014,
      "grad_norm": 5.70134162902832,
      "learning_rate": 7.471971523067437e-05,
      "loss": 0.2696,
      "step": 18090
    },
    {
      "epoch": 1.0138923899952386,
      "grad_norm": 2.4366703033447266,
      "learning_rate": 7.470570099220808e-05,
      "loss": 0.1607,
      "step": 18100
    },
    {
      "epoch": 1.0144525670111755,
      "grad_norm": 6.087276935577393,
      "learning_rate": 7.469168675374181e-05,
      "loss": 0.2483,
      "step": 18110
    },
    {
      "epoch": 1.0150127440271126,
      "grad_norm": 5.703116416931152,
      "learning_rate": 7.467767251527553e-05,
      "loss": 0.2632,
      "step": 18120
    },
    {
      "epoch": 1.0155729210430495,
      "grad_norm": 4.53622579574585,
      "learning_rate": 7.466365827680924e-05,
      "loss": 0.2387,
      "step": 18130
    },
    {
      "epoch": 1.0161330980589867,
      "grad_norm": 6.382032871246338,
      "learning_rate": 7.464964403834296e-05,
      "loss": 0.1849,
      "step": 18140
    },
    {
      "epoch": 1.0166932750749236,
      "grad_norm": 4.387608528137207,
      "learning_rate": 7.463562979987669e-05,
      "loss": 0.261,
      "step": 18150
    },
    {
      "epoch": 1.0172534520908607,
      "grad_norm": 3.027494430541992,
      "learning_rate": 7.46216155614104e-05,
      "loss": 0.1827,
      "step": 18160
    },
    {
      "epoch": 1.0178136291067978,
      "grad_norm": 5.230003356933594,
      "learning_rate": 7.460760132294411e-05,
      "loss": 0.2358,
      "step": 18170
    },
    {
      "epoch": 1.0183738061227348,
      "grad_norm": 4.073848247528076,
      "learning_rate": 7.459358708447783e-05,
      "loss": 0.2114,
      "step": 18180
    },
    {
      "epoch": 1.018933983138672,
      "grad_norm": 4.604107856750488,
      "learning_rate": 7.457957284601154e-05,
      "loss": 0.2841,
      "step": 18190
    },
    {
      "epoch": 1.0194941601546088,
      "grad_norm": 3.6405715942382812,
      "learning_rate": 7.456555860754527e-05,
      "loss": 0.2394,
      "step": 18200
    },
    {
      "epoch": 1.020054337170546,
      "grad_norm": 6.771060943603516,
      "learning_rate": 7.455154436907899e-05,
      "loss": 0.1545,
      "step": 18210
    },
    {
      "epoch": 1.0206145141864829,
      "grad_norm": 5.146042346954346,
      "learning_rate": 7.453753013061272e-05,
      "loss": 0.314,
      "step": 18220
    },
    {
      "epoch": 1.02117469120242,
      "grad_norm": 0.9499387741088867,
      "learning_rate": 7.452351589214642e-05,
      "loss": 0.3123,
      "step": 18230
    },
    {
      "epoch": 1.021734868218357,
      "grad_norm": 3.057403326034546,
      "learning_rate": 7.450950165368014e-05,
      "loss": 0.2242,
      "step": 18240
    },
    {
      "epoch": 1.022295045234294,
      "grad_norm": 5.30567741394043,
      "learning_rate": 7.449548741521386e-05,
      "loss": 0.2374,
      "step": 18250
    },
    {
      "epoch": 1.0228552222502312,
      "grad_norm": 3.378584861755371,
      "learning_rate": 7.448147317674757e-05,
      "loss": 0.2415,
      "step": 18260
    },
    {
      "epoch": 1.023415399266168,
      "grad_norm": 4.9144463539123535,
      "learning_rate": 7.44674589382813e-05,
      "loss": 0.17,
      "step": 18270
    },
    {
      "epoch": 1.0239755762821052,
      "grad_norm": 5.4441914558410645,
      "learning_rate": 7.445344469981502e-05,
      "loss": 0.2283,
      "step": 18280
    },
    {
      "epoch": 1.0245357532980421,
      "grad_norm": 3.6020374298095703,
      "learning_rate": 7.443943046134873e-05,
      "loss": 0.3162,
      "step": 18290
    },
    {
      "epoch": 1.0250959303139793,
      "grad_norm": 3.3271355628967285,
      "learning_rate": 7.442541622288245e-05,
      "loss": 0.2399,
      "step": 18300
    },
    {
      "epoch": 1.0256561073299162,
      "grad_norm": 4.571032524108887,
      "learning_rate": 7.441140198441618e-05,
      "loss": 0.2753,
      "step": 18310
    },
    {
      "epoch": 1.0262162843458533,
      "grad_norm": 5.163634300231934,
      "learning_rate": 7.439738774594989e-05,
      "loss": 0.1622,
      "step": 18320
    },
    {
      "epoch": 1.0267764613617902,
      "grad_norm": 4.040511131286621,
      "learning_rate": 7.43833735074836e-05,
      "loss": 0.1588,
      "step": 18330
    },
    {
      "epoch": 1.0273366383777274,
      "grad_norm": 3.84616756439209,
      "learning_rate": 7.436935926901732e-05,
      "loss": 0.2243,
      "step": 18340
    },
    {
      "epoch": 1.0278968153936643,
      "grad_norm": 3.4509077072143555,
      "learning_rate": 7.435534503055103e-05,
      "loss": 0.1732,
      "step": 18350
    },
    {
      "epoch": 1.0284569924096014,
      "grad_norm": 3.6055984497070312,
      "learning_rate": 7.434133079208476e-05,
      "loss": 0.2132,
      "step": 18360
    },
    {
      "epoch": 1.0290171694255386,
      "grad_norm": 5.514278888702393,
      "learning_rate": 7.432731655361848e-05,
      "loss": 0.1952,
      "step": 18370
    },
    {
      "epoch": 1.0295773464414755,
      "grad_norm": 2.6730620861053467,
      "learning_rate": 7.43133023151522e-05,
      "loss": 0.1166,
      "step": 18380
    },
    {
      "epoch": 1.0301375234574126,
      "grad_norm": 3.237640142440796,
      "learning_rate": 7.42992880766859e-05,
      "loss": 0.2062,
      "step": 18390
    },
    {
      "epoch": 1.0306977004733495,
      "grad_norm": 2.0413601398468018,
      "learning_rate": 7.428527383821964e-05,
      "loss": 0.157,
      "step": 18400
    },
    {
      "epoch": 1.0312578774892867,
      "grad_norm": 3.9845690727233887,
      "learning_rate": 7.427125959975335e-05,
      "loss": 0.1659,
      "step": 18410
    },
    {
      "epoch": 1.0318180545052236,
      "grad_norm": 2.8095741271972656,
      "learning_rate": 7.425724536128708e-05,
      "loss": 0.2377,
      "step": 18420
    },
    {
      "epoch": 1.0323782315211607,
      "grad_norm": 6.269524574279785,
      "learning_rate": 7.424323112282079e-05,
      "loss": 0.1908,
      "step": 18430
    },
    {
      "epoch": 1.0329384085370976,
      "grad_norm": 4.924324989318848,
      "learning_rate": 7.422921688435451e-05,
      "loss": 0.1884,
      "step": 18440
    },
    {
      "epoch": 1.0334985855530348,
      "grad_norm": 4.0226969718933105,
      "learning_rate": 7.421520264588822e-05,
      "loss": 0.369,
      "step": 18450
    },
    {
      "epoch": 1.034058762568972,
      "grad_norm": 4.040205478668213,
      "learning_rate": 7.420118840742194e-05,
      "loss": 0.1958,
      "step": 18460
    },
    {
      "epoch": 1.0346189395849088,
      "grad_norm": 2.061067581176758,
      "learning_rate": 7.418717416895567e-05,
      "loss": 0.2045,
      "step": 18470
    },
    {
      "epoch": 1.035179116600846,
      "grad_norm": 3.223867893218994,
      "learning_rate": 7.417315993048938e-05,
      "loss": 0.1549,
      "step": 18480
    },
    {
      "epoch": 1.0357392936167829,
      "grad_norm": 5.790672302246094,
      "learning_rate": 7.415914569202311e-05,
      "loss": 0.2985,
      "step": 18490
    },
    {
      "epoch": 1.03629947063272,
      "grad_norm": 2.635284662246704,
      "learning_rate": 7.414513145355681e-05,
      "loss": 0.2632,
      "step": 18500
    },
    {
      "epoch": 1.036859647648657,
      "grad_norm": 3.2427263259887695,
      "learning_rate": 7.413111721509054e-05,
      "loss": 0.1816,
      "step": 18510
    },
    {
      "epoch": 1.037419824664594,
      "grad_norm": 5.530759811401367,
      "learning_rate": 7.411710297662425e-05,
      "loss": 0.2086,
      "step": 18520
    },
    {
      "epoch": 1.037980001680531,
      "grad_norm": 5.43455696105957,
      "learning_rate": 7.410308873815798e-05,
      "loss": 0.2548,
      "step": 18530
    },
    {
      "epoch": 1.038540178696468,
      "grad_norm": 4.4693193435668945,
      "learning_rate": 7.40890744996917e-05,
      "loss": 0.2271,
      "step": 18540
    },
    {
      "epoch": 1.0391003557124052,
      "grad_norm": 3.507584571838379,
      "learning_rate": 7.407506026122541e-05,
      "loss": 0.3106,
      "step": 18550
    },
    {
      "epoch": 1.0396605327283421,
      "grad_norm": 3.0893146991729736,
      "learning_rate": 7.406104602275913e-05,
      "loss": 0.2449,
      "step": 18560
    },
    {
      "epoch": 1.0402207097442793,
      "grad_norm": 2.9857022762298584,
      "learning_rate": 7.404703178429284e-05,
      "loss": 0.2065,
      "step": 18570
    },
    {
      "epoch": 1.0407808867602162,
      "grad_norm": 1.3517227172851562,
      "learning_rate": 7.403301754582657e-05,
      "loss": 0.1967,
      "step": 18580
    },
    {
      "epoch": 1.0413410637761533,
      "grad_norm": 5.520045757293701,
      "learning_rate": 7.401900330736028e-05,
      "loss": 0.2374,
      "step": 18590
    },
    {
      "epoch": 1.0419012407920902,
      "grad_norm": 2.8957464694976807,
      "learning_rate": 7.4004989068894e-05,
      "loss": 0.1731,
      "step": 18600
    },
    {
      "epoch": 1.0424614178080274,
      "grad_norm": 4.092864513397217,
      "learning_rate": 7.399097483042771e-05,
      "loss": 0.2824,
      "step": 18610
    },
    {
      "epoch": 1.0430215948239643,
      "grad_norm": 5.321719646453857,
      "learning_rate": 7.397696059196144e-05,
      "loss": 0.1946,
      "step": 18620
    },
    {
      "epoch": 1.0435817718399014,
      "grad_norm": 6.745258331298828,
      "learning_rate": 7.396294635349516e-05,
      "loss": 0.1414,
      "step": 18630
    },
    {
      "epoch": 1.0441419488558386,
      "grad_norm": 5.079189777374268,
      "learning_rate": 7.394893211502887e-05,
      "loss": 0.2712,
      "step": 18640
    },
    {
      "epoch": 1.0447021258717755,
      "grad_norm": 1.5979503393173218,
      "learning_rate": 7.39349178765626e-05,
      "loss": 0.1621,
      "step": 18650
    },
    {
      "epoch": 1.0452623028877126,
      "grad_norm": 3.5002973079681396,
      "learning_rate": 7.39209036380963e-05,
      "loss": 0.1244,
      "step": 18660
    },
    {
      "epoch": 1.0458224799036495,
      "grad_norm": 3.8332724571228027,
      "learning_rate": 7.390688939963003e-05,
      "loss": 0.2516,
      "step": 18670
    },
    {
      "epoch": 1.0463826569195867,
      "grad_norm": 4.484160900115967,
      "learning_rate": 7.389287516116374e-05,
      "loss": 0.2917,
      "step": 18680
    },
    {
      "epoch": 1.0469428339355236,
      "grad_norm": 5.087944507598877,
      "learning_rate": 7.387886092269747e-05,
      "loss": 0.2877,
      "step": 18690
    },
    {
      "epoch": 1.0475030109514607,
      "grad_norm": 6.460926055908203,
      "learning_rate": 7.386484668423119e-05,
      "loss": 0.192,
      "step": 18700
    },
    {
      "epoch": 1.0480631879673976,
      "grad_norm": 5.397305488586426,
      "learning_rate": 7.38508324457649e-05,
      "loss": 0.3897,
      "step": 18710
    },
    {
      "epoch": 1.0486233649833347,
      "grad_norm": 2.396456480026245,
      "learning_rate": 7.383681820729862e-05,
      "loss": 0.2282,
      "step": 18720
    },
    {
      "epoch": 1.0491835419992717,
      "grad_norm": 5.048255920410156,
      "learning_rate": 7.382280396883233e-05,
      "loss": 0.1953,
      "step": 18730
    },
    {
      "epoch": 1.0497437190152088,
      "grad_norm": 1.6682116985321045,
      "learning_rate": 7.380878973036606e-05,
      "loss": 0.1819,
      "step": 18740
    },
    {
      "epoch": 1.050303896031146,
      "grad_norm": 4.3440632820129395,
      "learning_rate": 7.379477549189977e-05,
      "loss": 0.1826,
      "step": 18750
    },
    {
      "epoch": 1.0508640730470828,
      "grad_norm": 3.959965229034424,
      "learning_rate": 7.37807612534335e-05,
      "loss": 0.2657,
      "step": 18760
    },
    {
      "epoch": 1.05142425006302,
      "grad_norm": 5.700157642364502,
      "learning_rate": 7.37667470149672e-05,
      "loss": 0.3725,
      "step": 18770
    },
    {
      "epoch": 1.051984427078957,
      "grad_norm": 2.864083766937256,
      "learning_rate": 7.375273277650093e-05,
      "loss": 0.2014,
      "step": 18780
    },
    {
      "epoch": 1.052544604094894,
      "grad_norm": 4.698436260223389,
      "learning_rate": 7.373871853803465e-05,
      "loss": 0.1707,
      "step": 18790
    },
    {
      "epoch": 1.053104781110831,
      "grad_norm": 4.8122453689575195,
      "learning_rate": 7.372470429956837e-05,
      "loss": 0.2133,
      "step": 18800
    },
    {
      "epoch": 1.053664958126768,
      "grad_norm": 3.096585988998413,
      "learning_rate": 7.371069006110209e-05,
      "loss": 0.229,
      "step": 18810
    },
    {
      "epoch": 1.054225135142705,
      "grad_norm": 1.3229790925979614,
      "learning_rate": 7.36966758226358e-05,
      "loss": 0.2417,
      "step": 18820
    },
    {
      "epoch": 1.0547853121586421,
      "grad_norm": 3.22208571434021,
      "learning_rate": 7.368266158416952e-05,
      "loss": 0.2236,
      "step": 18830
    },
    {
      "epoch": 1.0553454891745793,
      "grad_norm": 3.490152597427368,
      "learning_rate": 7.366864734570323e-05,
      "loss": 0.2299,
      "step": 18840
    },
    {
      "epoch": 1.0559056661905162,
      "grad_norm": 4.8525800704956055,
      "learning_rate": 7.365463310723696e-05,
      "loss": 0.2525,
      "step": 18850
    },
    {
      "epoch": 1.0564658432064533,
      "grad_norm": 3.4876697063446045,
      "learning_rate": 7.364061886877068e-05,
      "loss": 0.1787,
      "step": 18860
    },
    {
      "epoch": 1.0570260202223902,
      "grad_norm": 5.4536356925964355,
      "learning_rate": 7.362660463030439e-05,
      "loss": 0.2595,
      "step": 18870
    },
    {
      "epoch": 1.0575861972383274,
      "grad_norm": 6.07955265045166,
      "learning_rate": 7.36125903918381e-05,
      "loss": 0.1927,
      "step": 18880
    },
    {
      "epoch": 1.0581463742542643,
      "grad_norm": 4.151477813720703,
      "learning_rate": 7.359857615337183e-05,
      "loss": 0.1929,
      "step": 18890
    },
    {
      "epoch": 1.0587065512702014,
      "grad_norm": 4.9325337409973145,
      "learning_rate": 7.358456191490555e-05,
      "loss": 0.2011,
      "step": 18900
    },
    {
      "epoch": 1.0592667282861383,
      "grad_norm": 2.983412981033325,
      "learning_rate": 7.357054767643926e-05,
      "loss": 0.2072,
      "step": 18910
    },
    {
      "epoch": 1.0598269053020755,
      "grad_norm": 4.811306476593018,
      "learning_rate": 7.355653343797299e-05,
      "loss": 0.1846,
      "step": 18920
    },
    {
      "epoch": 1.0603870823180126,
      "grad_norm": 3.7220757007598877,
      "learning_rate": 7.354251919950669e-05,
      "loss": 0.1767,
      "step": 18930
    },
    {
      "epoch": 1.0609472593339495,
      "grad_norm": 4.562938690185547,
      "learning_rate": 7.352850496104042e-05,
      "loss": 0.1898,
      "step": 18940
    },
    {
      "epoch": 1.0615074363498866,
      "grad_norm": 5.870044231414795,
      "learning_rate": 7.351449072257414e-05,
      "loss": 0.267,
      "step": 18950
    },
    {
      "epoch": 1.0620676133658236,
      "grad_norm": 4.479991912841797,
      "learning_rate": 7.350047648410786e-05,
      "loss": 0.3231,
      "step": 18960
    },
    {
      "epoch": 1.0626277903817607,
      "grad_norm": 3.2907044887542725,
      "learning_rate": 7.348646224564158e-05,
      "loss": 0.2424,
      "step": 18970
    },
    {
      "epoch": 1.0631879673976976,
      "grad_norm": 3.537869691848755,
      "learning_rate": 7.34724480071753e-05,
      "loss": 0.1521,
      "step": 18980
    },
    {
      "epoch": 1.0637481444136347,
      "grad_norm": 2.3664257526397705,
      "learning_rate": 7.345843376870901e-05,
      "loss": 0.1743,
      "step": 18990
    },
    {
      "epoch": 1.0643083214295717,
      "grad_norm": 5.44535493850708,
      "learning_rate": 7.344441953024272e-05,
      "loss": 0.1765,
      "step": 19000
    },
    {
      "epoch": 1.0648684984455088,
      "grad_norm": 8.10246753692627,
      "learning_rate": 7.343040529177645e-05,
      "loss": 0.295,
      "step": 19010
    },
    {
      "epoch": 1.0654286754614457,
      "grad_norm": 5.740837574005127,
      "learning_rate": 7.341639105331017e-05,
      "loss": 0.2112,
      "step": 19020
    },
    {
      "epoch": 1.0659888524773828,
      "grad_norm": 4.122358798980713,
      "learning_rate": 7.340237681484388e-05,
      "loss": 0.1983,
      "step": 19030
    },
    {
      "epoch": 1.06654902949332,
      "grad_norm": 6.784163951873779,
      "learning_rate": 7.33883625763776e-05,
      "loss": 0.2931,
      "step": 19040
    },
    {
      "epoch": 1.067109206509257,
      "grad_norm": 3.2069637775421143,
      "learning_rate": 7.337434833791132e-05,
      "loss": 0.2066,
      "step": 19050
    },
    {
      "epoch": 1.067669383525194,
      "grad_norm": 4.7977118492126465,
      "learning_rate": 7.336033409944504e-05,
      "loss": 0.2234,
      "step": 19060
    },
    {
      "epoch": 1.068229560541131,
      "grad_norm": 7.179179668426514,
      "learning_rate": 7.334631986097877e-05,
      "loss": 0.1727,
      "step": 19070
    },
    {
      "epoch": 1.068789737557068,
      "grad_norm": 3.3872509002685547,
      "learning_rate": 7.333230562251248e-05,
      "loss": 0.2194,
      "step": 19080
    },
    {
      "epoch": 1.069349914573005,
      "grad_norm": 3.1442060470581055,
      "learning_rate": 7.331829138404618e-05,
      "loss": 0.1628,
      "step": 19090
    },
    {
      "epoch": 1.0699100915889421,
      "grad_norm": 1.6863770484924316,
      "learning_rate": 7.330427714557991e-05,
      "loss": 0.1402,
      "step": 19100
    },
    {
      "epoch": 1.070470268604879,
      "grad_norm": 3.376415491104126,
      "learning_rate": 7.329026290711363e-05,
      "loss": 0.2013,
      "step": 19110
    },
    {
      "epoch": 1.0710304456208162,
      "grad_norm": 1.79305899143219,
      "learning_rate": 7.327624866864735e-05,
      "loss": 0.1915,
      "step": 19120
    },
    {
      "epoch": 1.0715906226367533,
      "grad_norm": 4.773913860321045,
      "learning_rate": 7.326223443018107e-05,
      "loss": 0.296,
      "step": 19130
    },
    {
      "epoch": 1.0721507996526902,
      "grad_norm": 1.9891841411590576,
      "learning_rate": 7.324822019171478e-05,
      "loss": 0.1749,
      "step": 19140
    },
    {
      "epoch": 1.0727109766686274,
      "grad_norm": 5.976236820220947,
      "learning_rate": 7.32342059532485e-05,
      "loss": 0.1717,
      "step": 19150
    },
    {
      "epoch": 1.0732711536845643,
      "grad_norm": 2.4423117637634277,
      "learning_rate": 7.322019171478223e-05,
      "loss": 0.2545,
      "step": 19160
    },
    {
      "epoch": 1.0738313307005014,
      "grad_norm": 5.025755405426025,
      "learning_rate": 7.320617747631594e-05,
      "loss": 0.1956,
      "step": 19170
    },
    {
      "epoch": 1.0743915077164383,
      "grad_norm": 7.894402503967285,
      "learning_rate": 7.319216323784966e-05,
      "loss": 0.2195,
      "step": 19180
    },
    {
      "epoch": 1.0749516847323755,
      "grad_norm": 2.3775994777679443,
      "learning_rate": 7.317814899938338e-05,
      "loss": 0.1464,
      "step": 19190
    },
    {
      "epoch": 1.0755118617483124,
      "grad_norm": 6.826841831207275,
      "learning_rate": 7.316413476091709e-05,
      "loss": 0.2515,
      "step": 19200
    },
    {
      "epoch": 1.0760720387642495,
      "grad_norm": 3.488433361053467,
      "learning_rate": 7.315012052245081e-05,
      "loss": 0.2635,
      "step": 19210
    },
    {
      "epoch": 1.0766322157801866,
      "grad_norm": 3.73046612739563,
      "learning_rate": 7.313610628398453e-05,
      "loss": 0.2233,
      "step": 19220
    },
    {
      "epoch": 1.0771923927961236,
      "grad_norm": 4.315127372741699,
      "learning_rate": 7.312209204551826e-05,
      "loss": 0.2051,
      "step": 19230
    },
    {
      "epoch": 1.0777525698120607,
      "grad_norm": 4.544227123260498,
      "learning_rate": 7.310807780705197e-05,
      "loss": 0.228,
      "step": 19240
    },
    {
      "epoch": 1.0783127468279976,
      "grad_norm": 3.634007215499878,
      "learning_rate": 7.309406356858569e-05,
      "loss": 0.1508,
      "step": 19250
    },
    {
      "epoch": 1.0788729238439347,
      "grad_norm": 1.7691607475280762,
      "learning_rate": 7.30800493301194e-05,
      "loss": 0.189,
      "step": 19260
    },
    {
      "epoch": 1.0794331008598717,
      "grad_norm": 5.407435417175293,
      "learning_rate": 7.306603509165312e-05,
      "loss": 0.2239,
      "step": 19270
    },
    {
      "epoch": 1.0799932778758088,
      "grad_norm": 2.8419535160064697,
      "learning_rate": 7.305202085318684e-05,
      "loss": 0.1813,
      "step": 19280
    },
    {
      "epoch": 1.0805534548917457,
      "grad_norm": 2.537022113800049,
      "learning_rate": 7.303800661472056e-05,
      "loss": 0.1572,
      "step": 19290
    },
    {
      "epoch": 1.0811136319076828,
      "grad_norm": 4.865969657897949,
      "learning_rate": 7.302399237625427e-05,
      "loss": 0.2085,
      "step": 19300
    },
    {
      "epoch": 1.08167380892362,
      "grad_norm": 5.433277130126953,
      "learning_rate": 7.300997813778799e-05,
      "loss": 0.3211,
      "step": 19310
    },
    {
      "epoch": 1.0822339859395569,
      "grad_norm": 1.0228281021118164,
      "learning_rate": 7.299596389932172e-05,
      "loss": 0.2116,
      "step": 19320
    },
    {
      "epoch": 1.082794162955494,
      "grad_norm": 3.859266996383667,
      "learning_rate": 7.298194966085543e-05,
      "loss": 0.2626,
      "step": 19330
    },
    {
      "epoch": 1.083354339971431,
      "grad_norm": 4.85157585144043,
      "learning_rate": 7.296793542238916e-05,
      "loss": 0.1876,
      "step": 19340
    },
    {
      "epoch": 1.083914516987368,
      "grad_norm": 3.9893198013305664,
      "learning_rate": 7.295392118392287e-05,
      "loss": 0.1585,
      "step": 19350
    },
    {
      "epoch": 1.084474694003305,
      "grad_norm": 3.976696252822876,
      "learning_rate": 7.293990694545658e-05,
      "loss": 0.2897,
      "step": 19360
    },
    {
      "epoch": 1.0850348710192421,
      "grad_norm": 5.353207111358643,
      "learning_rate": 7.29258927069903e-05,
      "loss": 0.2784,
      "step": 19370
    },
    {
      "epoch": 1.085595048035179,
      "grad_norm": 2.192551612854004,
      "learning_rate": 7.291187846852402e-05,
      "loss": 0.177,
      "step": 19380
    },
    {
      "epoch": 1.0861552250511162,
      "grad_norm": 2.5932600498199463,
      "learning_rate": 7.289786423005775e-05,
      "loss": 0.2201,
      "step": 19390
    },
    {
      "epoch": 1.0867154020670533,
      "grad_norm": 2.8726985454559326,
      "learning_rate": 7.288384999159146e-05,
      "loss": 0.2849,
      "step": 19400
    },
    {
      "epoch": 1.0872755790829902,
      "grad_norm": 3.833922863006592,
      "learning_rate": 7.286983575312518e-05,
      "loss": 0.2329,
      "step": 19410
    },
    {
      "epoch": 1.0878357560989274,
      "grad_norm": 9.452842712402344,
      "learning_rate": 7.285582151465889e-05,
      "loss": 0.2153,
      "step": 19420
    },
    {
      "epoch": 1.0883959331148643,
      "grad_norm": 3.416433811187744,
      "learning_rate": 7.284180727619262e-05,
      "loss": 0.2554,
      "step": 19430
    },
    {
      "epoch": 1.0889561101308014,
      "grad_norm": 3.8962457180023193,
      "learning_rate": 7.282779303772633e-05,
      "loss": 0.186,
      "step": 19440
    },
    {
      "epoch": 1.0895162871467383,
      "grad_norm": 4.641165256500244,
      "learning_rate": 7.281377879926005e-05,
      "loss": 0.2495,
      "step": 19450
    },
    {
      "epoch": 1.0900764641626755,
      "grad_norm": 1.8004788160324097,
      "learning_rate": 7.279976456079378e-05,
      "loss": 0.1851,
      "step": 19460
    },
    {
      "epoch": 1.0906366411786124,
      "grad_norm": 5.6759796142578125,
      "learning_rate": 7.278575032232748e-05,
      "loss": 0.2903,
      "step": 19470
    },
    {
      "epoch": 1.0911968181945495,
      "grad_norm": 2.0931615829467773,
      "learning_rate": 7.277173608386121e-05,
      "loss": 0.3157,
      "step": 19480
    },
    {
      "epoch": 1.0917569952104864,
      "grad_norm": 4.2427191734313965,
      "learning_rate": 7.275772184539492e-05,
      "loss": 0.3219,
      "step": 19490
    },
    {
      "epoch": 1.0923171722264235,
      "grad_norm": 5.936444282531738,
      "learning_rate": 7.274370760692865e-05,
      "loss": 0.2367,
      "step": 19500
    },
    {
      "epoch": 1.0928773492423607,
      "grad_norm": 4.993212699890137,
      "learning_rate": 7.272969336846236e-05,
      "loss": 0.1285,
      "step": 19510
    },
    {
      "epoch": 1.0934375262582976,
      "grad_norm": 2.3321237564086914,
      "learning_rate": 7.271567912999608e-05,
      "loss": 0.1629,
      "step": 19520
    },
    {
      "epoch": 1.0939977032742347,
      "grad_norm": 3.778789758682251,
      "learning_rate": 7.27016648915298e-05,
      "loss": 0.2206,
      "step": 19530
    },
    {
      "epoch": 1.0945578802901716,
      "grad_norm": 4.545456886291504,
      "learning_rate": 7.268765065306352e-05,
      "loss": 0.2362,
      "step": 19540
    },
    {
      "epoch": 1.0951180573061088,
      "grad_norm": 2.515211820602417,
      "learning_rate": 7.267363641459724e-05,
      "loss": 0.1587,
      "step": 19550
    },
    {
      "epoch": 1.0956782343220457,
      "grad_norm": 3.1543540954589844,
      "learning_rate": 7.265962217613095e-05,
      "loss": 0.2826,
      "step": 19560
    },
    {
      "epoch": 1.0962384113379828,
      "grad_norm": 6.729045391082764,
      "learning_rate": 7.264560793766467e-05,
      "loss": 0.3106,
      "step": 19570
    },
    {
      "epoch": 1.0967985883539197,
      "grad_norm": 5.0502190589904785,
      "learning_rate": 7.263159369919838e-05,
      "loss": 0.235,
      "step": 19580
    },
    {
      "epoch": 1.0973587653698569,
      "grad_norm": 1.8678556680679321,
      "learning_rate": 7.261757946073211e-05,
      "loss": 0.1543,
      "step": 19590
    },
    {
      "epoch": 1.0979189423857938,
      "grad_norm": 7.457864761352539,
      "learning_rate": 7.260356522226582e-05,
      "loss": 0.2793,
      "step": 19600
    },
    {
      "epoch": 1.098479119401731,
      "grad_norm": 4.205929756164551,
      "learning_rate": 7.258955098379955e-05,
      "loss": 0.235,
      "step": 19610
    },
    {
      "epoch": 1.099039296417668,
      "grad_norm": 2.572000026702881,
      "learning_rate": 7.257553674533327e-05,
      "loss": 0.2359,
      "step": 19620
    },
    {
      "epoch": 1.099599473433605,
      "grad_norm": 3.151170253753662,
      "learning_rate": 7.256152250686698e-05,
      "loss": 0.2587,
      "step": 19630
    },
    {
      "epoch": 1.1001596504495421,
      "grad_norm": 5.434924602508545,
      "learning_rate": 7.25475082684007e-05,
      "loss": 0.2456,
      "step": 19640
    },
    {
      "epoch": 1.100719827465479,
      "grad_norm": 4.964720249176025,
      "learning_rate": 7.253349402993441e-05,
      "loss": 0.2223,
      "step": 19650
    },
    {
      "epoch": 1.1012800044814162,
      "grad_norm": 4.616926193237305,
      "learning_rate": 7.251947979146814e-05,
      "loss": 0.3003,
      "step": 19660
    },
    {
      "epoch": 1.101840181497353,
      "grad_norm": 3.1509358882904053,
      "learning_rate": 7.250546555300185e-05,
      "loss": 0.2453,
      "step": 19670
    },
    {
      "epoch": 1.1024003585132902,
      "grad_norm": 2.7540204524993896,
      "learning_rate": 7.249145131453557e-05,
      "loss": 0.2151,
      "step": 19680
    },
    {
      "epoch": 1.1029605355292271,
      "grad_norm": 9.0289306640625,
      "learning_rate": 7.247743707606928e-05,
      "loss": 0.1856,
      "step": 19690
    },
    {
      "epoch": 1.1035207125451643,
      "grad_norm": 1.834731101989746,
      "learning_rate": 7.246342283760301e-05,
      "loss": 0.1854,
      "step": 19700
    },
    {
      "epoch": 1.1040808895611014,
      "grad_norm": 3.873234748840332,
      "learning_rate": 7.244940859913673e-05,
      "loss": 0.301,
      "step": 19710
    },
    {
      "epoch": 1.1046410665770383,
      "grad_norm": 5.176951885223389,
      "learning_rate": 7.243539436067046e-05,
      "loss": 0.1891,
      "step": 19720
    },
    {
      "epoch": 1.1052012435929754,
      "grad_norm": 5.7888712882995605,
      "learning_rate": 7.242138012220416e-05,
      "loss": 0.2603,
      "step": 19730
    },
    {
      "epoch": 1.1057614206089124,
      "grad_norm": 4.424188137054443,
      "learning_rate": 7.240736588373787e-05,
      "loss": 0.2117,
      "step": 19740
    },
    {
      "epoch": 1.1063215976248495,
      "grad_norm": 1.465327501296997,
      "learning_rate": 7.23933516452716e-05,
      "loss": 0.1494,
      "step": 19750
    },
    {
      "epoch": 1.1068817746407864,
      "grad_norm": 4.909609317779541,
      "learning_rate": 7.237933740680531e-05,
      "loss": 0.2566,
      "step": 19760
    },
    {
      "epoch": 1.1074419516567235,
      "grad_norm": 2.9040443897247314,
      "learning_rate": 7.236532316833904e-05,
      "loss": 0.1519,
      "step": 19770
    },
    {
      "epoch": 1.1080021286726605,
      "grad_norm": 4.648003101348877,
      "learning_rate": 7.235130892987276e-05,
      "loss": 0.2177,
      "step": 19780
    },
    {
      "epoch": 1.1085623056885976,
      "grad_norm": 3.720768928527832,
      "learning_rate": 7.233729469140647e-05,
      "loss": 0.2226,
      "step": 19790
    },
    {
      "epoch": 1.1091224827045347,
      "grad_norm": 2.2594120502471924,
      "learning_rate": 7.232328045294019e-05,
      "loss": 0.1727,
      "step": 19800
    },
    {
      "epoch": 1.1096826597204716,
      "grad_norm": 6.205025672912598,
      "learning_rate": 7.230926621447392e-05,
      "loss": 0.2197,
      "step": 19810
    },
    {
      "epoch": 1.1102428367364088,
      "grad_norm": 1.9627536535263062,
      "learning_rate": 7.229525197600763e-05,
      "loss": 0.2262,
      "step": 19820
    },
    {
      "epoch": 1.1108030137523457,
      "grad_norm": 4.770446300506592,
      "learning_rate": 7.228123773754134e-05,
      "loss": 0.1388,
      "step": 19830
    },
    {
      "epoch": 1.1113631907682828,
      "grad_norm": 6.125466823577881,
      "learning_rate": 7.226722349907506e-05,
      "loss": 0.1968,
      "step": 19840
    },
    {
      "epoch": 1.1119233677842197,
      "grad_norm": 3.2605669498443604,
      "learning_rate": 7.225320926060877e-05,
      "loss": 0.1963,
      "step": 19850
    },
    {
      "epoch": 1.1124835448001569,
      "grad_norm": 2.872424840927124,
      "learning_rate": 7.22391950221425e-05,
      "loss": 0.3474,
      "step": 19860
    },
    {
      "epoch": 1.1130437218160938,
      "grad_norm": 4.803404331207275,
      "learning_rate": 7.222518078367622e-05,
      "loss": 0.2535,
      "step": 19870
    },
    {
      "epoch": 1.113603898832031,
      "grad_norm": 3.412626028060913,
      "learning_rate": 7.221116654520995e-05,
      "loss": 0.1433,
      "step": 19880
    },
    {
      "epoch": 1.114164075847968,
      "grad_norm": 5.815260887145996,
      "learning_rate": 7.219715230674366e-05,
      "loss": 0.1983,
      "step": 19890
    },
    {
      "epoch": 1.114724252863905,
      "grad_norm": 4.785655498504639,
      "learning_rate": 7.218313806827738e-05,
      "loss": 0.1618,
      "step": 19900
    },
    {
      "epoch": 1.115284429879842,
      "grad_norm": 4.935753345489502,
      "learning_rate": 7.216912382981109e-05,
      "loss": 0.2706,
      "step": 19910
    },
    {
      "epoch": 1.115844606895779,
      "grad_norm": 1.734522819519043,
      "learning_rate": 7.21551095913448e-05,
      "loss": 0.2305,
      "step": 19920
    },
    {
      "epoch": 1.1164047839117162,
      "grad_norm": 3.845341444015503,
      "learning_rate": 7.214109535287853e-05,
      "loss": 0.1678,
      "step": 19930
    },
    {
      "epoch": 1.116964960927653,
      "grad_norm": 3.7960078716278076,
      "learning_rate": 7.212708111441225e-05,
      "loss": 0.1714,
      "step": 19940
    },
    {
      "epoch": 1.1175251379435902,
      "grad_norm": 3.556084632873535,
      "learning_rate": 7.211306687594596e-05,
      "loss": 0.3082,
      "step": 19950
    },
    {
      "epoch": 1.1180853149595271,
      "grad_norm": 4.256795883178711,
      "learning_rate": 7.209905263747968e-05,
      "loss": 0.2374,
      "step": 19960
    },
    {
      "epoch": 1.1186454919754643,
      "grad_norm": 5.518171787261963,
      "learning_rate": 7.20850383990134e-05,
      "loss": 0.1905,
      "step": 19970
    },
    {
      "epoch": 1.1192056689914014,
      "grad_norm": 3.0472469329833984,
      "learning_rate": 7.207102416054712e-05,
      "loss": 0.1819,
      "step": 19980
    },
    {
      "epoch": 1.1197658460073383,
      "grad_norm": 1.8112846612930298,
      "learning_rate": 7.205700992208085e-05,
      "loss": 0.32,
      "step": 19990
    },
    {
      "epoch": 1.1203260230232754,
      "grad_norm": 3.452500343322754,
      "learning_rate": 7.204299568361455e-05,
      "loss": 0.2301,
      "step": 20000
    },
    {
      "epoch": 1.1208862000392124,
      "grad_norm": 4.88242769241333,
      "learning_rate": 7.202898144514826e-05,
      "loss": 0.239,
      "step": 20010
    },
    {
      "epoch": 1.1214463770551495,
      "grad_norm": 3.7964093685150146,
      "learning_rate": 7.201496720668199e-05,
      "loss": 0.2054,
      "step": 20020
    },
    {
      "epoch": 1.1220065540710864,
      "grad_norm": 4.977604389190674,
      "learning_rate": 7.200095296821571e-05,
      "loss": 0.3021,
      "step": 20030
    },
    {
      "epoch": 1.1225667310870235,
      "grad_norm": 3.8129591941833496,
      "learning_rate": 7.198693872974944e-05,
      "loss": 0.2384,
      "step": 20040
    },
    {
      "epoch": 1.1231269081029605,
      "grad_norm": 3.8675756454467773,
      "learning_rate": 7.197292449128315e-05,
      "loss": 0.164,
      "step": 20050
    },
    {
      "epoch": 1.1236870851188976,
      "grad_norm": 4.334582805633545,
      "learning_rate": 7.195891025281687e-05,
      "loss": 0.2044,
      "step": 20060
    },
    {
      "epoch": 1.1242472621348347,
      "grad_norm": 7.093461990356445,
      "learning_rate": 7.194489601435058e-05,
      "loss": 0.2473,
      "step": 20070
    },
    {
      "epoch": 1.1248074391507716,
      "grad_norm": 1.710817813873291,
      "learning_rate": 7.193088177588431e-05,
      "loss": 0.1121,
      "step": 20080
    },
    {
      "epoch": 1.1253676161667088,
      "grad_norm": 2.307574510574341,
      "learning_rate": 7.191686753741802e-05,
      "loss": 0.231,
      "step": 20090
    },
    {
      "epoch": 1.1259277931826457,
      "grad_norm": 3.7401678562164307,
      "learning_rate": 7.190285329895174e-05,
      "loss": 0.2368,
      "step": 20100
    },
    {
      "epoch": 1.1264879701985828,
      "grad_norm": 2.5305709838867188,
      "learning_rate": 7.188883906048545e-05,
      "loss": 0.2612,
      "step": 20110
    },
    {
      "epoch": 1.1270481472145197,
      "grad_norm": 3.3377692699432373,
      "learning_rate": 7.187482482201917e-05,
      "loss": 0.2256,
      "step": 20120
    },
    {
      "epoch": 1.1276083242304569,
      "grad_norm": 4.609630107879639,
      "learning_rate": 7.18608105835529e-05,
      "loss": 0.1399,
      "step": 20130
    },
    {
      "epoch": 1.1281685012463938,
      "grad_norm": 3.951890468597412,
      "learning_rate": 7.184679634508661e-05,
      "loss": 0.309,
      "step": 20140
    },
    {
      "epoch": 1.128728678262331,
      "grad_norm": 4.961673736572266,
      "learning_rate": 7.183278210662034e-05,
      "loss": 0.3265,
      "step": 20150
    },
    {
      "epoch": 1.129288855278268,
      "grad_norm": 5.948001861572266,
      "learning_rate": 7.181876786815405e-05,
      "loss": 0.1845,
      "step": 20160
    },
    {
      "epoch": 1.129849032294205,
      "grad_norm": 3.525581121444702,
      "learning_rate": 7.180475362968777e-05,
      "loss": 0.2251,
      "step": 20170
    },
    {
      "epoch": 1.1304092093101419,
      "grad_norm": 2.746633529663086,
      "learning_rate": 7.179073939122148e-05,
      "loss": 0.2114,
      "step": 20180
    },
    {
      "epoch": 1.130969386326079,
      "grad_norm": 5.286538124084473,
      "learning_rate": 7.17767251527552e-05,
      "loss": 0.2262,
      "step": 20190
    },
    {
      "epoch": 1.1315295633420162,
      "grad_norm": 4.981271743774414,
      "learning_rate": 7.176271091428893e-05,
      "loss": 0.1267,
      "step": 20200
    },
    {
      "epoch": 1.132089740357953,
      "grad_norm": 6.539365768432617,
      "learning_rate": 7.174869667582264e-05,
      "loss": 0.2329,
      "step": 20210
    },
    {
      "epoch": 1.1326499173738902,
      "grad_norm": 5.4134907722473145,
      "learning_rate": 7.173468243735636e-05,
      "loss": 0.2386,
      "step": 20220
    },
    {
      "epoch": 1.1332100943898271,
      "grad_norm": 4.783242225646973,
      "learning_rate": 7.172066819889007e-05,
      "loss": 0.2469,
      "step": 20230
    },
    {
      "epoch": 1.1337702714057643,
      "grad_norm": 2.431028366088867,
      "learning_rate": 7.17066539604238e-05,
      "loss": 0.1593,
      "step": 20240
    },
    {
      "epoch": 1.1343304484217012,
      "grad_norm": 3.6848807334899902,
      "learning_rate": 7.169263972195751e-05,
      "loss": 0.1775,
      "step": 20250
    },
    {
      "epoch": 1.1348906254376383,
      "grad_norm": 3.2193288803100586,
      "learning_rate": 7.167862548349124e-05,
      "loss": 0.2093,
      "step": 20260
    },
    {
      "epoch": 1.1354508024535752,
      "grad_norm": 6.434858798980713,
      "learning_rate": 7.166461124502494e-05,
      "loss": 0.2547,
      "step": 20270
    },
    {
      "epoch": 1.1360109794695123,
      "grad_norm": 4.4528326988220215,
      "learning_rate": 7.165059700655866e-05,
      "loss": 0.2343,
      "step": 20280
    },
    {
      "epoch": 1.1365711564854495,
      "grad_norm": 3.3925118446350098,
      "learning_rate": 7.163658276809239e-05,
      "loss": 0.1221,
      "step": 20290
    },
    {
      "epoch": 1.1371313335013864,
      "grad_norm": 3.3847551345825195,
      "learning_rate": 7.16225685296261e-05,
      "loss": 0.2733,
      "step": 20300
    },
    {
      "epoch": 1.1376915105173235,
      "grad_norm": 1.912374496459961,
      "learning_rate": 7.160855429115983e-05,
      "loss": 0.1554,
      "step": 20310
    },
    {
      "epoch": 1.1382516875332604,
      "grad_norm": 4.658609390258789,
      "learning_rate": 7.159454005269354e-05,
      "loss": 0.2373,
      "step": 20320
    },
    {
      "epoch": 1.1388118645491976,
      "grad_norm": 3.7321105003356934,
      "learning_rate": 7.158052581422726e-05,
      "loss": 0.1363,
      "step": 20330
    },
    {
      "epoch": 1.1393720415651345,
      "grad_norm": 5.362423419952393,
      "learning_rate": 7.156651157576097e-05,
      "loss": 0.1748,
      "step": 20340
    },
    {
      "epoch": 1.1399322185810716,
      "grad_norm": 3.764120578765869,
      "learning_rate": 7.15524973372947e-05,
      "loss": 0.1904,
      "step": 20350
    },
    {
      "epoch": 1.1404923955970085,
      "grad_norm": 5.07243013381958,
      "learning_rate": 7.153848309882842e-05,
      "loss": 0.3108,
      "step": 20360
    },
    {
      "epoch": 1.1410525726129457,
      "grad_norm": 4.200117588043213,
      "learning_rate": 7.152446886036213e-05,
      "loss": 0.1824,
      "step": 20370
    },
    {
      "epoch": 1.1416127496288828,
      "grad_norm": 7.166640281677246,
      "learning_rate": 7.151045462189585e-05,
      "loss": 0.2135,
      "step": 20380
    },
    {
      "epoch": 1.1421729266448197,
      "grad_norm": 6.0978264808654785,
      "learning_rate": 7.149644038342956e-05,
      "loss": 0.2696,
      "step": 20390
    },
    {
      "epoch": 1.1427331036607569,
      "grad_norm": 4.452443599700928,
      "learning_rate": 7.148242614496329e-05,
      "loss": 0.1693,
      "step": 20400
    },
    {
      "epoch": 1.1432932806766938,
      "grad_norm": 4.494635581970215,
      "learning_rate": 7.1468411906497e-05,
      "loss": 0.1721,
      "step": 20410
    },
    {
      "epoch": 1.143853457692631,
      "grad_norm": 3.839834690093994,
      "learning_rate": 7.145439766803073e-05,
      "loss": 0.1271,
      "step": 20420
    },
    {
      "epoch": 1.1444136347085678,
      "grad_norm": 5.80844259262085,
      "learning_rate": 7.144038342956443e-05,
      "loss": 0.2221,
      "step": 20430
    },
    {
      "epoch": 1.144973811724505,
      "grad_norm": 4.443397521972656,
      "learning_rate": 7.142636919109816e-05,
      "loss": 0.2754,
      "step": 20440
    },
    {
      "epoch": 1.1455339887404419,
      "grad_norm": 4.377804756164551,
      "learning_rate": 7.141235495263188e-05,
      "loss": 0.2457,
      "step": 20450
    },
    {
      "epoch": 1.146094165756379,
      "grad_norm": 2.9055869579315186,
      "learning_rate": 7.139834071416559e-05,
      "loss": 0.1691,
      "step": 20460
    },
    {
      "epoch": 1.1466543427723161,
      "grad_norm": 4.737386703491211,
      "learning_rate": 7.138432647569932e-05,
      "loss": 0.119,
      "step": 20470
    },
    {
      "epoch": 1.147214519788253,
      "grad_norm": 3.652491569519043,
      "learning_rate": 7.137031223723303e-05,
      "loss": 0.2517,
      "step": 20480
    },
    {
      "epoch": 1.1477746968041902,
      "grad_norm": 5.023604869842529,
      "learning_rate": 7.135629799876675e-05,
      "loss": 0.2752,
      "step": 20490
    },
    {
      "epoch": 1.1483348738201271,
      "grad_norm": 5.571501731872559,
      "learning_rate": 7.134228376030046e-05,
      "loss": 0.2296,
      "step": 20500
    },
    {
      "epoch": 1.1488950508360642,
      "grad_norm": 6.222975730895996,
      "learning_rate": 7.132826952183419e-05,
      "loss": 0.3444,
      "step": 20510
    },
    {
      "epoch": 1.1494552278520012,
      "grad_norm": 2.1425986289978027,
      "learning_rate": 7.13142552833679e-05,
      "loss": 0.243,
      "step": 20520
    },
    {
      "epoch": 1.1500154048679383,
      "grad_norm": 3.2925381660461426,
      "learning_rate": 7.130024104490163e-05,
      "loss": 0.188,
      "step": 20530
    },
    {
      "epoch": 1.1505755818838752,
      "grad_norm": 5.713124752044678,
      "learning_rate": 7.128622680643534e-05,
      "loss": 0.1547,
      "step": 20540
    },
    {
      "epoch": 1.1511357588998123,
      "grad_norm": 3.8677351474761963,
      "learning_rate": 7.127221256796905e-05,
      "loss": 0.2506,
      "step": 20550
    },
    {
      "epoch": 1.1516959359157495,
      "grad_norm": 5.798137664794922,
      "learning_rate": 7.125819832950278e-05,
      "loss": 0.1358,
      "step": 20560
    },
    {
      "epoch": 1.1522561129316864,
      "grad_norm": 3.739785671234131,
      "learning_rate": 7.12441840910365e-05,
      "loss": 0.1798,
      "step": 20570
    },
    {
      "epoch": 1.1528162899476235,
      "grad_norm": 3.5180654525756836,
      "learning_rate": 7.123016985257022e-05,
      "loss": 0.2379,
      "step": 20580
    },
    {
      "epoch": 1.1533764669635604,
      "grad_norm": 1.7844158411026,
      "learning_rate": 7.121615561410394e-05,
      "loss": 0.1387,
      "step": 20590
    },
    {
      "epoch": 1.1539366439794976,
      "grad_norm": 5.930273532867432,
      "learning_rate": 7.120214137563765e-05,
      "loss": 0.2706,
      "step": 20600
    },
    {
      "epoch": 1.1544968209954345,
      "grad_norm": 4.561623573303223,
      "learning_rate": 7.118812713717137e-05,
      "loss": 0.1441,
      "step": 20610
    },
    {
      "epoch": 1.1550569980113716,
      "grad_norm": 2.3614633083343506,
      "learning_rate": 7.11741128987051e-05,
      "loss": 0.215,
      "step": 20620
    },
    {
      "epoch": 1.1556171750273085,
      "grad_norm": 1.9529764652252197,
      "learning_rate": 7.116009866023881e-05,
      "loss": 0.2456,
      "step": 20630
    },
    {
      "epoch": 1.1561773520432457,
      "grad_norm": 4.471974849700928,
      "learning_rate": 7.114608442177252e-05,
      "loss": 0.1334,
      "step": 20640
    },
    {
      "epoch": 1.1567375290591828,
      "grad_norm": 1.9009584188461304,
      "learning_rate": 7.113207018330624e-05,
      "loss": 0.1308,
      "step": 20650
    },
    {
      "epoch": 1.1572977060751197,
      "grad_norm": 2.494447708129883,
      "learning_rate": 7.111805594483995e-05,
      "loss": 0.1839,
      "step": 20660
    },
    {
      "epoch": 1.1578578830910569,
      "grad_norm": 3.633870840072632,
      "learning_rate": 7.110404170637368e-05,
      "loss": 0.168,
      "step": 20670
    },
    {
      "epoch": 1.1584180601069938,
      "grad_norm": 4.554808616638184,
      "learning_rate": 7.10900274679074e-05,
      "loss": 0.2659,
      "step": 20680
    },
    {
      "epoch": 1.158978237122931,
      "grad_norm": 4.381264686584473,
      "learning_rate": 7.107601322944112e-05,
      "loss": 0.173,
      "step": 20690
    },
    {
      "epoch": 1.1595384141388678,
      "grad_norm": 2.0711350440979004,
      "learning_rate": 7.106199899097483e-05,
      "loss": 0.2962,
      "step": 20700
    },
    {
      "epoch": 1.160098591154805,
      "grad_norm": 2.6976420879364014,
      "learning_rate": 7.104798475250855e-05,
      "loss": 0.1735,
      "step": 20710
    },
    {
      "epoch": 1.1606587681707419,
      "grad_norm": 3.286766767501831,
      "learning_rate": 7.103397051404227e-05,
      "loss": 0.2908,
      "step": 20720
    },
    {
      "epoch": 1.161218945186679,
      "grad_norm": 5.25011682510376,
      "learning_rate": 7.1019956275576e-05,
      "loss": 0.2505,
      "step": 20730
    },
    {
      "epoch": 1.1617791222026161,
      "grad_norm": 4.550980091094971,
      "learning_rate": 7.100594203710971e-05,
      "loss": 0.2167,
      "step": 20740
    },
    {
      "epoch": 1.162339299218553,
      "grad_norm": 4.163647174835205,
      "learning_rate": 7.099192779864343e-05,
      "loss": 0.2179,
      "step": 20750
    },
    {
      "epoch": 1.1628994762344902,
      "grad_norm": 2.7271060943603516,
      "learning_rate": 7.097791356017714e-05,
      "loss": 0.1777,
      "step": 20760
    },
    {
      "epoch": 1.163459653250427,
      "grad_norm": 2.633218288421631,
      "learning_rate": 7.096389932171086e-05,
      "loss": 0.1875,
      "step": 20770
    },
    {
      "epoch": 1.1640198302663642,
      "grad_norm": 3.891862630844116,
      "learning_rate": 7.094988508324458e-05,
      "loss": 0.3108,
      "step": 20780
    },
    {
      "epoch": 1.1645800072823012,
      "grad_norm": 3.7902021408081055,
      "learning_rate": 7.09358708447783e-05,
      "loss": 0.1623,
      "step": 20790
    },
    {
      "epoch": 1.1651401842982383,
      "grad_norm": 4.149496555328369,
      "learning_rate": 7.092185660631203e-05,
      "loss": 0.2295,
      "step": 20800
    },
    {
      "epoch": 1.1657003613141752,
      "grad_norm": 4.432379245758057,
      "learning_rate": 7.090784236784573e-05,
      "loss": 0.1783,
      "step": 20810
    },
    {
      "epoch": 1.1662605383301123,
      "grad_norm": 4.829603672027588,
      "learning_rate": 7.089382812937946e-05,
      "loss": 0.2016,
      "step": 20820
    },
    {
      "epoch": 1.1668207153460495,
      "grad_norm": 4.787430763244629,
      "learning_rate": 7.087981389091317e-05,
      "loss": 0.1888,
      "step": 20830
    },
    {
      "epoch": 1.1673808923619864,
      "grad_norm": 2.4643869400024414,
      "learning_rate": 7.086579965244689e-05,
      "loss": 0.1662,
      "step": 20840
    },
    {
      "epoch": 1.1679410693779233,
      "grad_norm": 2.90989351272583,
      "learning_rate": 7.085178541398061e-05,
      "loss": 0.2442,
      "step": 20850
    },
    {
      "epoch": 1.1685012463938604,
      "grad_norm": 1.9116530418395996,
      "learning_rate": 7.083777117551433e-05,
      "loss": 0.2394,
      "step": 20860
    },
    {
      "epoch": 1.1690614234097976,
      "grad_norm": 2.8408355712890625,
      "learning_rate": 7.082375693704804e-05,
      "loss": 0.2546,
      "step": 20870
    },
    {
      "epoch": 1.1696216004257345,
      "grad_norm": 6.846198558807373,
      "learning_rate": 7.080974269858176e-05,
      "loss": 0.136,
      "step": 20880
    },
    {
      "epoch": 1.1701817774416716,
      "grad_norm": 2.9534475803375244,
      "learning_rate": 7.079572846011549e-05,
      "loss": 0.2559,
      "step": 20890
    },
    {
      "epoch": 1.1707419544576085,
      "grad_norm": 1.4062210321426392,
      "learning_rate": 7.07817142216492e-05,
      "loss": 0.2657,
      "step": 20900
    },
    {
      "epoch": 1.1713021314735457,
      "grad_norm": 2.90372633934021,
      "learning_rate": 7.076769998318292e-05,
      "loss": 0.2167,
      "step": 20910
    },
    {
      "epoch": 1.1718623084894826,
      "grad_norm": 5.088181972503662,
      "learning_rate": 7.075368574471663e-05,
      "loss": 0.2022,
      "step": 20920
    },
    {
      "epoch": 1.1724224855054197,
      "grad_norm": 8.762760162353516,
      "learning_rate": 7.073967150625035e-05,
      "loss": 0.244,
      "step": 20930
    },
    {
      "epoch": 1.1729826625213566,
      "grad_norm": 4.750052452087402,
      "learning_rate": 7.072565726778407e-05,
      "loss": 0.2828,
      "step": 20940
    },
    {
      "epoch": 1.1735428395372938,
      "grad_norm": 3.5462067127227783,
      "learning_rate": 7.071164302931779e-05,
      "loss": 0.1648,
      "step": 20950
    },
    {
      "epoch": 1.174103016553231,
      "grad_norm": 2.501918315887451,
      "learning_rate": 7.069762879085152e-05,
      "loss": 0.1883,
      "step": 20960
    },
    {
      "epoch": 1.1746631935691678,
      "grad_norm": 1.377437949180603,
      "learning_rate": 7.068361455238522e-05,
      "loss": 0.2403,
      "step": 20970
    },
    {
      "epoch": 1.175223370585105,
      "grad_norm": 4.713749885559082,
      "learning_rate": 7.066960031391895e-05,
      "loss": 0.2469,
      "step": 20980
    },
    {
      "epoch": 1.1757835476010419,
      "grad_norm": 3.095172643661499,
      "learning_rate": 7.065558607545266e-05,
      "loss": 0.1839,
      "step": 20990
    },
    {
      "epoch": 1.176343724616979,
      "grad_norm": 3.8082919120788574,
      "learning_rate": 7.064157183698639e-05,
      "loss": 0.2305,
      "step": 21000
    },
    {
      "epoch": 1.176903901632916,
      "grad_norm": 4.321255683898926,
      "learning_rate": 7.06275575985201e-05,
      "loss": 0.2975,
      "step": 21010
    },
    {
      "epoch": 1.177464078648853,
      "grad_norm": 3.049931526184082,
      "learning_rate": 7.061354336005382e-05,
      "loss": 0.152,
      "step": 21020
    },
    {
      "epoch": 1.17802425566479,
      "grad_norm": 4.612367153167725,
      "learning_rate": 7.059952912158753e-05,
      "loss": 0.1662,
      "step": 21030
    },
    {
      "epoch": 1.178584432680727,
      "grad_norm": 4.55357551574707,
      "learning_rate": 7.058551488312125e-05,
      "loss": 0.2543,
      "step": 21040
    },
    {
      "epoch": 1.1791446096966642,
      "grad_norm": 4.657543182373047,
      "learning_rate": 7.057150064465498e-05,
      "loss": 0.1725,
      "step": 21050
    },
    {
      "epoch": 1.1797047867126012,
      "grad_norm": 1.7755672931671143,
      "learning_rate": 7.055748640618869e-05,
      "loss": 0.2053,
      "step": 21060
    },
    {
      "epoch": 1.1802649637285383,
      "grad_norm": 5.808933258056641,
      "learning_rate": 7.05434721677224e-05,
      "loss": 0.3225,
      "step": 21070
    },
    {
      "epoch": 1.1808251407444752,
      "grad_norm": 2.5635874271392822,
      "learning_rate": 7.052945792925612e-05,
      "loss": 0.2902,
      "step": 21080
    },
    {
      "epoch": 1.1813853177604123,
      "grad_norm": 3.0330049991607666,
      "learning_rate": 7.051544369078985e-05,
      "loss": 0.1723,
      "step": 21090
    },
    {
      "epoch": 1.1819454947763492,
      "grad_norm": 3.849574327468872,
      "learning_rate": 7.050142945232356e-05,
      "loss": 0.1503,
      "step": 21100
    },
    {
      "epoch": 1.1825056717922864,
      "grad_norm": 3.5532619953155518,
      "learning_rate": 7.048741521385728e-05,
      "loss": 0.1567,
      "step": 21110
    },
    {
      "epoch": 1.1830658488082233,
      "grad_norm": 3.343555450439453,
      "learning_rate": 7.047340097539101e-05,
      "loss": 0.2156,
      "step": 21120
    },
    {
      "epoch": 1.1836260258241604,
      "grad_norm": 3.0171873569488525,
      "learning_rate": 7.045938673692471e-05,
      "loss": 0.259,
      "step": 21130
    },
    {
      "epoch": 1.1841862028400976,
      "grad_norm": 6.445980548858643,
      "learning_rate": 7.044537249845844e-05,
      "loss": 0.2318,
      "step": 21140
    },
    {
      "epoch": 1.1847463798560345,
      "grad_norm": 3.6024060249328613,
      "learning_rate": 7.043135825999215e-05,
      "loss": 0.1401,
      "step": 21150
    },
    {
      "epoch": 1.1853065568719716,
      "grad_norm": 4.876775741577148,
      "learning_rate": 7.041734402152588e-05,
      "loss": 0.3499,
      "step": 21160
    },
    {
      "epoch": 1.1858667338879085,
      "grad_norm": 2.897376298904419,
      "learning_rate": 7.04033297830596e-05,
      "loss": 0.2658,
      "step": 21170
    },
    {
      "epoch": 1.1864269109038457,
      "grad_norm": 3.514768123626709,
      "learning_rate": 7.038931554459331e-05,
      "loss": 0.2306,
      "step": 21180
    },
    {
      "epoch": 1.1869870879197826,
      "grad_norm": 5.660740375518799,
      "learning_rate": 7.037530130612702e-05,
      "loss": 0.2087,
      "step": 21190
    },
    {
      "epoch": 1.1875472649357197,
      "grad_norm": 4.536446571350098,
      "learning_rate": 7.036128706766074e-05,
      "loss": 0.2845,
      "step": 21200
    },
    {
      "epoch": 1.1881074419516566,
      "grad_norm": 5.4112114906311035,
      "learning_rate": 7.034727282919447e-05,
      "loss": 0.3075,
      "step": 21210
    },
    {
      "epoch": 1.1886676189675938,
      "grad_norm": 8.435050964355469,
      "learning_rate": 7.033325859072818e-05,
      "loss": 0.2156,
      "step": 21220
    },
    {
      "epoch": 1.189227795983531,
      "grad_norm": 2.589367628097534,
      "learning_rate": 7.031924435226191e-05,
      "loss": 0.2039,
      "step": 21230
    },
    {
      "epoch": 1.1897879729994678,
      "grad_norm": 1.9688938856124878,
      "learning_rate": 7.030523011379561e-05,
      "loss": 0.2523,
      "step": 21240
    },
    {
      "epoch": 1.190348150015405,
      "grad_norm": 2.2355570793151855,
      "learning_rate": 7.029121587532934e-05,
      "loss": 0.1825,
      "step": 21250
    },
    {
      "epoch": 1.1909083270313419,
      "grad_norm": 3.8360159397125244,
      "learning_rate": 7.027720163686305e-05,
      "loss": 0.237,
      "step": 21260
    },
    {
      "epoch": 1.191468504047279,
      "grad_norm": 6.14301872253418,
      "learning_rate": 7.026318739839678e-05,
      "loss": 0.2553,
      "step": 21270
    },
    {
      "epoch": 1.192028681063216,
      "grad_norm": 3.745514392852783,
      "learning_rate": 7.02491731599305e-05,
      "loss": 0.2453,
      "step": 21280
    },
    {
      "epoch": 1.192588858079153,
      "grad_norm": 4.154840469360352,
      "learning_rate": 7.023515892146421e-05,
      "loss": 0.2385,
      "step": 21290
    },
    {
      "epoch": 1.19314903509509,
      "grad_norm": 2.381925106048584,
      "learning_rate": 7.022114468299793e-05,
      "loss": 0.3169,
      "step": 21300
    },
    {
      "epoch": 1.193709212111027,
      "grad_norm": 3.3332250118255615,
      "learning_rate": 7.020713044453164e-05,
      "loss": 0.2486,
      "step": 21310
    },
    {
      "epoch": 1.1942693891269642,
      "grad_norm": 3.5377724170684814,
      "learning_rate": 7.019311620606537e-05,
      "loss": 0.2766,
      "step": 21320
    },
    {
      "epoch": 1.1948295661429011,
      "grad_norm": 1.8524079322814941,
      "learning_rate": 7.017910196759908e-05,
      "loss": 0.2072,
      "step": 21330
    },
    {
      "epoch": 1.1953897431588383,
      "grad_norm": 3.874762535095215,
      "learning_rate": 7.01650877291328e-05,
      "loss": 0.287,
      "step": 21340
    },
    {
      "epoch": 1.1959499201747752,
      "grad_norm": 2.48801326751709,
      "learning_rate": 7.015107349066651e-05,
      "loss": 0.2427,
      "step": 21350
    },
    {
      "epoch": 1.1965100971907123,
      "grad_norm": 3.915954113006592,
      "learning_rate": 7.013705925220024e-05,
      "loss": 0.2716,
      "step": 21360
    },
    {
      "epoch": 1.1970702742066492,
      "grad_norm": 4.464347839355469,
      "learning_rate": 7.012304501373396e-05,
      "loss": 0.301,
      "step": 21370
    },
    {
      "epoch": 1.1976304512225864,
      "grad_norm": 5.461700916290283,
      "learning_rate": 7.010903077526767e-05,
      "loss": 0.1283,
      "step": 21380
    },
    {
      "epoch": 1.1981906282385233,
      "grad_norm": 3.1689164638519287,
      "learning_rate": 7.00950165368014e-05,
      "loss": 0.1626,
      "step": 21390
    },
    {
      "epoch": 1.1987508052544604,
      "grad_norm": 3.2733047008514404,
      "learning_rate": 7.00810022983351e-05,
      "loss": 0.195,
      "step": 21400
    },
    {
      "epoch": 1.1993109822703976,
      "grad_norm": 4.044807434082031,
      "learning_rate": 7.006698805986883e-05,
      "loss": 0.2089,
      "step": 21410
    },
    {
      "epoch": 1.1998711592863345,
      "grad_norm": 3.1153669357299805,
      "learning_rate": 7.005297382140254e-05,
      "loss": 0.1759,
      "step": 21420
    },
    {
      "epoch": 1.2004313363022716,
      "grad_norm": 2.8229997158050537,
      "learning_rate": 7.003895958293627e-05,
      "loss": 0.2233,
      "step": 21430
    },
    {
      "epoch": 1.2009915133182085,
      "grad_norm": 6.175103187561035,
      "learning_rate": 7.002494534446999e-05,
      "loss": 0.1954,
      "step": 21440
    },
    {
      "epoch": 1.2015516903341457,
      "grad_norm": 4.867275714874268,
      "learning_rate": 7.00109311060037e-05,
      "loss": 0.178,
      "step": 21450
    },
    {
      "epoch": 1.2021118673500826,
      "grad_norm": 1.6439396142959595,
      "learning_rate": 6.999691686753742e-05,
      "loss": 0.3284,
      "step": 21460
    },
    {
      "epoch": 1.2026720443660197,
      "grad_norm": 5.0537800788879395,
      "learning_rate": 6.998290262907113e-05,
      "loss": 0.1713,
      "step": 21470
    },
    {
      "epoch": 1.2032322213819566,
      "grad_norm": 3.0889370441436768,
      "learning_rate": 6.996888839060486e-05,
      "loss": 0.2235,
      "step": 21480
    },
    {
      "epoch": 1.2037923983978938,
      "grad_norm": 3.7861177921295166,
      "learning_rate": 6.995487415213857e-05,
      "loss": 0.2771,
      "step": 21490
    },
    {
      "epoch": 1.204352575413831,
      "grad_norm": 3.1470320224761963,
      "learning_rate": 6.99408599136723e-05,
      "loss": 0.2195,
      "step": 21500
    },
    {
      "epoch": 1.2049127524297678,
      "grad_norm": 3.339367151260376,
      "learning_rate": 6.9926845675206e-05,
      "loss": 0.2814,
      "step": 21510
    },
    {
      "epoch": 1.2054729294457047,
      "grad_norm": 4.0287580490112305,
      "learning_rate": 6.991283143673973e-05,
      "loss": 0.2305,
      "step": 21520
    },
    {
      "epoch": 1.2060331064616419,
      "grad_norm": 6.639214515686035,
      "learning_rate": 6.989881719827345e-05,
      "loss": 0.3039,
      "step": 21530
    },
    {
      "epoch": 1.206593283477579,
      "grad_norm": 5.1183271408081055,
      "learning_rate": 6.988480295980718e-05,
      "loss": 0.3121,
      "step": 21540
    },
    {
      "epoch": 1.207153460493516,
      "grad_norm": 1.6079318523406982,
      "learning_rate": 6.987078872134089e-05,
      "loss": 0.3039,
      "step": 21550
    },
    {
      "epoch": 1.207713637509453,
      "grad_norm": 3.893209218978882,
      "learning_rate": 6.98567744828746e-05,
      "loss": 0.1614,
      "step": 21560
    },
    {
      "epoch": 1.20827381452539,
      "grad_norm": 3.131760835647583,
      "learning_rate": 6.984276024440832e-05,
      "loss": 0.2502,
      "step": 21570
    },
    {
      "epoch": 1.208833991541327,
      "grad_norm": 5.390048503875732,
      "learning_rate": 6.982874600594203e-05,
      "loss": 0.3012,
      "step": 21580
    },
    {
      "epoch": 1.209394168557264,
      "grad_norm": 6.96218204498291,
      "learning_rate": 6.981473176747576e-05,
      "loss": 0.2931,
      "step": 21590
    },
    {
      "epoch": 1.2099543455732011,
      "grad_norm": 3.6699225902557373,
      "learning_rate": 6.980071752900948e-05,
      "loss": 0.2169,
      "step": 21600
    },
    {
      "epoch": 1.210514522589138,
      "grad_norm": 3.4870572090148926,
      "learning_rate": 6.978670329054319e-05,
      "loss": 0.1224,
      "step": 21610
    },
    {
      "epoch": 1.2110746996050752,
      "grad_norm": 5.588247776031494,
      "learning_rate": 6.977268905207691e-05,
      "loss": 0.1424,
      "step": 21620
    },
    {
      "epoch": 1.2116348766210123,
      "grad_norm": 4.340110778808594,
      "learning_rate": 6.975867481361064e-05,
      "loss": 0.2677,
      "step": 21630
    },
    {
      "epoch": 1.2121950536369492,
      "grad_norm": 4.862105369567871,
      "learning_rate": 6.974466057514435e-05,
      "loss": 0.2853,
      "step": 21640
    },
    {
      "epoch": 1.2127552306528864,
      "grad_norm": 3.052945613861084,
      "learning_rate": 6.973064633667808e-05,
      "loss": 0.1774,
      "step": 21650
    },
    {
      "epoch": 1.2133154076688233,
      "grad_norm": 4.800687313079834,
      "learning_rate": 6.97166320982118e-05,
      "loss": 0.135,
      "step": 21660
    },
    {
      "epoch": 1.2138755846847604,
      "grad_norm": 6.3432512283325195,
      "learning_rate": 6.97026178597455e-05,
      "loss": 0.1793,
      "step": 21670
    },
    {
      "epoch": 1.2144357617006973,
      "grad_norm": 3.6117255687713623,
      "learning_rate": 6.968860362127922e-05,
      "loss": 0.1237,
      "step": 21680
    },
    {
      "epoch": 1.2149959387166345,
      "grad_norm": 5.03350830078125,
      "learning_rate": 6.967458938281294e-05,
      "loss": 0.3379,
      "step": 21690
    },
    {
      "epoch": 1.2155561157325714,
      "grad_norm": 3.3615119457244873,
      "learning_rate": 6.966057514434667e-05,
      "loss": 0.1761,
      "step": 21700
    },
    {
      "epoch": 1.2161162927485085,
      "grad_norm": 2.920862913131714,
      "learning_rate": 6.964656090588038e-05,
      "loss": 0.3206,
      "step": 21710
    },
    {
      "epoch": 1.2166764697644457,
      "grad_norm": 2.7542788982391357,
      "learning_rate": 6.96325466674141e-05,
      "loss": 0.2262,
      "step": 21720
    },
    {
      "epoch": 1.2172366467803826,
      "grad_norm": 2.6472792625427246,
      "learning_rate": 6.961853242894781e-05,
      "loss": 0.2454,
      "step": 21730
    },
    {
      "epoch": 1.2177968237963197,
      "grad_norm": 8.203963279724121,
      "learning_rate": 6.960451819048154e-05,
      "loss": 0.326,
      "step": 21740
    },
    {
      "epoch": 1.2183570008122566,
      "grad_norm": 3.4270758628845215,
      "learning_rate": 6.959050395201525e-05,
      "loss": 0.2181,
      "step": 21750
    },
    {
      "epoch": 1.2189171778281938,
      "grad_norm": 4.075053691864014,
      "learning_rate": 6.957648971354897e-05,
      "loss": 0.3219,
      "step": 21760
    },
    {
      "epoch": 1.2194773548441307,
      "grad_norm": 2.112788677215576,
      "learning_rate": 6.956247547508268e-05,
      "loss": 0.1621,
      "step": 21770
    },
    {
      "epoch": 1.2200375318600678,
      "grad_norm": 4.413717746734619,
      "learning_rate": 6.95484612366164e-05,
      "loss": 0.1554,
      "step": 21780
    },
    {
      "epoch": 1.2205977088760047,
      "grad_norm": 3.844970464706421,
      "learning_rate": 6.953444699815013e-05,
      "loss": 0.1652,
      "step": 21790
    },
    {
      "epoch": 1.2211578858919419,
      "grad_norm": 3.881148338317871,
      "learning_rate": 6.952043275968384e-05,
      "loss": 0.2322,
      "step": 21800
    },
    {
      "epoch": 1.221718062907879,
      "grad_norm": 2.5177416801452637,
      "learning_rate": 6.950641852121757e-05,
      "loss": 0.18,
      "step": 21810
    },
    {
      "epoch": 1.222278239923816,
      "grad_norm": 3.0480010509490967,
      "learning_rate": 6.949240428275128e-05,
      "loss": 0.1801,
      "step": 21820
    },
    {
      "epoch": 1.222838416939753,
      "grad_norm": 4.855071067810059,
      "learning_rate": 6.9478390044285e-05,
      "loss": 0.1455,
      "step": 21830
    },
    {
      "epoch": 1.22339859395569,
      "grad_norm": 5.820586204528809,
      "learning_rate": 6.946437580581871e-05,
      "loss": 0.2296,
      "step": 21840
    },
    {
      "epoch": 1.223958770971627,
      "grad_norm": 5.963835716247559,
      "learning_rate": 6.945036156735243e-05,
      "loss": 0.1823,
      "step": 21850
    },
    {
      "epoch": 1.224518947987564,
      "grad_norm": 2.9897046089172363,
      "learning_rate": 6.943634732888616e-05,
      "loss": 0.2201,
      "step": 21860
    },
    {
      "epoch": 1.2250791250035011,
      "grad_norm": 6.801009178161621,
      "learning_rate": 6.942233309041987e-05,
      "loss": 0.2423,
      "step": 21870
    },
    {
      "epoch": 1.225639302019438,
      "grad_norm": 1.9853343963623047,
      "learning_rate": 6.940831885195359e-05,
      "loss": 0.2233,
      "step": 21880
    },
    {
      "epoch": 1.2261994790353752,
      "grad_norm": 3.8857643604278564,
      "learning_rate": 6.93943046134873e-05,
      "loss": 0.1562,
      "step": 21890
    },
    {
      "epoch": 1.2267596560513123,
      "grad_norm": 4.558109283447266,
      "learning_rate": 6.938029037502103e-05,
      "loss": 0.2637,
      "step": 21900
    },
    {
      "epoch": 1.2273198330672492,
      "grad_norm": 3.8717567920684814,
      "learning_rate": 6.936627613655474e-05,
      "loss": 0.2619,
      "step": 21910
    },
    {
      "epoch": 1.2278800100831864,
      "grad_norm": 4.546285629272461,
      "learning_rate": 6.935226189808847e-05,
      "loss": 0.158,
      "step": 21920
    },
    {
      "epoch": 1.2284401870991233,
      "grad_norm": 6.676210403442383,
      "learning_rate": 6.933824765962219e-05,
      "loss": 0.2687,
      "step": 21930
    },
    {
      "epoch": 1.2290003641150604,
      "grad_norm": 2.586165428161621,
      "learning_rate": 6.932423342115589e-05,
      "loss": 0.1757,
      "step": 21940
    },
    {
      "epoch": 1.2295605411309973,
      "grad_norm": 5.395492076873779,
      "learning_rate": 6.931021918268962e-05,
      "loss": 0.178,
      "step": 21950
    },
    {
      "epoch": 1.2301207181469345,
      "grad_norm": 4.015616416931152,
      "learning_rate": 6.929620494422333e-05,
      "loss": 0.2264,
      "step": 21960
    },
    {
      "epoch": 1.2306808951628714,
      "grad_norm": 2.3854448795318604,
      "learning_rate": 6.928219070575706e-05,
      "loss": 0.1948,
      "step": 21970
    },
    {
      "epoch": 1.2312410721788085,
      "grad_norm": 4.191545486450195,
      "learning_rate": 6.926817646729077e-05,
      "loss": 0.2975,
      "step": 21980
    },
    {
      "epoch": 1.2318012491947457,
      "grad_norm": 3.6389987468719482,
      "learning_rate": 6.925416222882449e-05,
      "loss": 0.1602,
      "step": 21990
    },
    {
      "epoch": 1.2323614262106826,
      "grad_norm": 4.2515668869018555,
      "learning_rate": 6.92401479903582e-05,
      "loss": 0.2195,
      "step": 22000
    },
    {
      "epoch": 1.2329216032266197,
      "grad_norm": 6.51767635345459,
      "learning_rate": 6.922613375189193e-05,
      "loss": 0.1818,
      "step": 22010
    },
    {
      "epoch": 1.2334817802425566,
      "grad_norm": 3.7934792041778564,
      "learning_rate": 6.921211951342565e-05,
      "loss": 0.1954,
      "step": 22020
    },
    {
      "epoch": 1.2340419572584937,
      "grad_norm": 3.0610430240631104,
      "learning_rate": 6.919810527495936e-05,
      "loss": 0.2884,
      "step": 22030
    },
    {
      "epoch": 1.2346021342744307,
      "grad_norm": 3.4725887775421143,
      "learning_rate": 6.918409103649308e-05,
      "loss": 0.1643,
      "step": 22040
    },
    {
      "epoch": 1.2351623112903678,
      "grad_norm": 5.884650707244873,
      "learning_rate": 6.917007679802679e-05,
      "loss": 0.2027,
      "step": 22050
    },
    {
      "epoch": 1.2357224883063047,
      "grad_norm": 3.7146620750427246,
      "learning_rate": 6.915606255956052e-05,
      "loss": 0.1693,
      "step": 22060
    },
    {
      "epoch": 1.2362826653222418,
      "grad_norm": 3.7986528873443604,
      "learning_rate": 6.914204832109423e-05,
      "loss": 0.2147,
      "step": 22070
    },
    {
      "epoch": 1.236842842338179,
      "grad_norm": 3.3127665519714355,
      "learning_rate": 6.912803408262796e-05,
      "loss": 0.1973,
      "step": 22080
    },
    {
      "epoch": 1.237403019354116,
      "grad_norm": 5.4897308349609375,
      "learning_rate": 6.911401984416168e-05,
      "loss": 0.2577,
      "step": 22090
    },
    {
      "epoch": 1.237963196370053,
      "grad_norm": 3.774627923965454,
      "learning_rate": 6.910000560569539e-05,
      "loss": 0.1793,
      "step": 22100
    },
    {
      "epoch": 1.23852337338599,
      "grad_norm": 6.960055351257324,
      "learning_rate": 6.90859913672291e-05,
      "loss": 0.3975,
      "step": 22110
    },
    {
      "epoch": 1.239083550401927,
      "grad_norm": 2.9364256858825684,
      "learning_rate": 6.907197712876282e-05,
      "loss": 0.1767,
      "step": 22120
    },
    {
      "epoch": 1.239643727417864,
      "grad_norm": 4.382056713104248,
      "learning_rate": 6.905796289029655e-05,
      "loss": 0.2347,
      "step": 22130
    },
    {
      "epoch": 1.2402039044338011,
      "grad_norm": 3.798062324523926,
      "learning_rate": 6.904394865183026e-05,
      "loss": 0.2071,
      "step": 22140
    },
    {
      "epoch": 1.240764081449738,
      "grad_norm": 2.0249688625335693,
      "learning_rate": 6.902993441336398e-05,
      "loss": 0.1918,
      "step": 22150
    },
    {
      "epoch": 1.2413242584656752,
      "grad_norm": 3.869614839553833,
      "learning_rate": 6.901592017489769e-05,
      "loss": 0.137,
      "step": 22160
    },
    {
      "epoch": 1.2418844354816123,
      "grad_norm": 2.9785690307617188,
      "learning_rate": 6.900190593643142e-05,
      "loss": 0.2038,
      "step": 22170
    },
    {
      "epoch": 1.2424446124975492,
      "grad_norm": 1.2598876953125,
      "learning_rate": 6.898789169796514e-05,
      "loss": 0.2364,
      "step": 22180
    },
    {
      "epoch": 1.2430047895134861,
      "grad_norm": 4.390822887420654,
      "learning_rate": 6.897387745949886e-05,
      "loss": 0.2362,
      "step": 22190
    },
    {
      "epoch": 1.2435649665294233,
      "grad_norm": 5.100509166717529,
      "learning_rate": 6.895986322103258e-05,
      "loss": 0.2242,
      "step": 22200
    },
    {
      "epoch": 1.2441251435453604,
      "grad_norm": 4.521462917327881,
      "learning_rate": 6.894584898256628e-05,
      "loss": 0.2943,
      "step": 22210
    },
    {
      "epoch": 1.2446853205612973,
      "grad_norm": 3.8044700622558594,
      "learning_rate": 6.893183474410001e-05,
      "loss": 0.1391,
      "step": 22220
    },
    {
      "epoch": 1.2452454975772345,
      "grad_norm": 5.301822662353516,
      "learning_rate": 6.891782050563372e-05,
      "loss": 0.3319,
      "step": 22230
    },
    {
      "epoch": 1.2458056745931714,
      "grad_norm": 2.4334685802459717,
      "learning_rate": 6.890380626716745e-05,
      "loss": 0.2065,
      "step": 22240
    },
    {
      "epoch": 1.2463658516091085,
      "grad_norm": 6.334212779998779,
      "learning_rate": 6.888979202870117e-05,
      "loss": 0.2212,
      "step": 22250
    },
    {
      "epoch": 1.2469260286250454,
      "grad_norm": 5.070579528808594,
      "learning_rate": 6.887577779023488e-05,
      "loss": 0.1741,
      "step": 22260
    },
    {
      "epoch": 1.2474862056409826,
      "grad_norm": 4.690525531768799,
      "learning_rate": 6.88617635517686e-05,
      "loss": 0.3177,
      "step": 22270
    },
    {
      "epoch": 1.2480463826569195,
      "grad_norm": 1.3356574773788452,
      "learning_rate": 6.884774931330232e-05,
      "loss": 0.1432,
      "step": 22280
    },
    {
      "epoch": 1.2486065596728566,
      "grad_norm": 4.321639537811279,
      "learning_rate": 6.883373507483604e-05,
      "loss": 0.1359,
      "step": 22290
    },
    {
      "epoch": 1.2491667366887937,
      "grad_norm": 4.178925037384033,
      "learning_rate": 6.881972083636975e-05,
      "loss": 0.168,
      "step": 22300
    },
    {
      "epoch": 1.2497269137047307,
      "grad_norm": 5.088742256164551,
      "learning_rate": 6.880570659790347e-05,
      "loss": 0.3509,
      "step": 22310
    },
    {
      "epoch": 1.2502870907206678,
      "grad_norm": 3.3765995502471924,
      "learning_rate": 6.879169235943718e-05,
      "loss": 0.3063,
      "step": 22320
    },
    {
      "epoch": 1.2508472677366047,
      "grad_norm": 3.851243495941162,
      "learning_rate": 6.877767812097091e-05,
      "loss": 0.2329,
      "step": 22330
    },
    {
      "epoch": 1.2514074447525418,
      "grad_norm": 2.2583062648773193,
      "learning_rate": 6.876366388250463e-05,
      "loss": 0.2582,
      "step": 22340
    },
    {
      "epoch": 1.251967621768479,
      "grad_norm": 3.869365930557251,
      "learning_rate": 6.874964964403835e-05,
      "loss": 0.1583,
      "step": 22350
    },
    {
      "epoch": 1.252527798784416,
      "grad_norm": 3.1351728439331055,
      "learning_rate": 6.873563540557207e-05,
      "loss": 0.2092,
      "step": 22360
    },
    {
      "epoch": 1.2530879758003528,
      "grad_norm": 3.4866063594818115,
      "learning_rate": 6.872162116710578e-05,
      "loss": 0.2555,
      "step": 22370
    },
    {
      "epoch": 1.25364815281629,
      "grad_norm": 5.519598007202148,
      "learning_rate": 6.87076069286395e-05,
      "loss": 0.2763,
      "step": 22380
    },
    {
      "epoch": 1.254208329832227,
      "grad_norm": 1.7493486404418945,
      "learning_rate": 6.869359269017321e-05,
      "loss": 0.2374,
      "step": 22390
    },
    {
      "epoch": 1.254768506848164,
      "grad_norm": 5.790786266326904,
      "learning_rate": 6.867957845170694e-05,
      "loss": 0.2753,
      "step": 22400
    },
    {
      "epoch": 1.2553286838641011,
      "grad_norm": 2.111268997192383,
      "learning_rate": 6.866556421324066e-05,
      "loss": 0.3167,
      "step": 22410
    },
    {
      "epoch": 1.255888860880038,
      "grad_norm": 3.0359416007995605,
      "learning_rate": 6.865154997477437e-05,
      "loss": 0.2365,
      "step": 22420
    },
    {
      "epoch": 1.2564490378959752,
      "grad_norm": 2.685506820678711,
      "learning_rate": 6.863753573630809e-05,
      "loss": 0.1517,
      "step": 22430
    },
    {
      "epoch": 1.257009214911912,
      "grad_norm": 7.564730167388916,
      "learning_rate": 6.862352149784181e-05,
      "loss": 0.1785,
      "step": 22440
    },
    {
      "epoch": 1.2575693919278492,
      "grad_norm": 1.9145418405532837,
      "learning_rate": 6.860950725937553e-05,
      "loss": 0.1481,
      "step": 22450
    },
    {
      "epoch": 1.2581295689437861,
      "grad_norm": 5.495984077453613,
      "learning_rate": 6.859549302090926e-05,
      "loss": 0.1963,
      "step": 22460
    },
    {
      "epoch": 1.2586897459597233,
      "grad_norm": 3.1792993545532227,
      "learning_rate": 6.858147878244296e-05,
      "loss": 0.1831,
      "step": 22470
    },
    {
      "epoch": 1.2592499229756604,
      "grad_norm": 1.4895461797714233,
      "learning_rate": 6.856746454397667e-05,
      "loss": 0.2149,
      "step": 22480
    },
    {
      "epoch": 1.2598100999915973,
      "grad_norm": 3.932427167892456,
      "learning_rate": 6.85534503055104e-05,
      "loss": 0.2985,
      "step": 22490
    },
    {
      "epoch": 1.2603702770075345,
      "grad_norm": 4.007020950317383,
      "learning_rate": 6.853943606704412e-05,
      "loss": 0.1499,
      "step": 22500
    },
    {
      "epoch": 1.2609304540234714,
      "grad_norm": 2.0073013305664062,
      "learning_rate": 6.852542182857784e-05,
      "loss": 0.1671,
      "step": 22510
    },
    {
      "epoch": 1.2614906310394085,
      "grad_norm": 1.4229964017868042,
      "learning_rate": 6.851140759011156e-05,
      "loss": 0.2227,
      "step": 22520
    },
    {
      "epoch": 1.2620508080553454,
      "grad_norm": 3.732264995574951,
      "learning_rate": 6.849739335164527e-05,
      "loss": 0.143,
      "step": 22530
    },
    {
      "epoch": 1.2626109850712826,
      "grad_norm": 3.6555471420288086,
      "learning_rate": 6.848337911317899e-05,
      "loss": 0.1802,
      "step": 22540
    },
    {
      "epoch": 1.2631711620872195,
      "grad_norm": 5.842589378356934,
      "learning_rate": 6.846936487471272e-05,
      "loss": 0.3153,
      "step": 22550
    },
    {
      "epoch": 1.2637313391031566,
      "grad_norm": 1.95120108127594,
      "learning_rate": 6.845535063624643e-05,
      "loss": 0.1384,
      "step": 22560
    },
    {
      "epoch": 1.2642915161190937,
      "grad_norm": 3.357832670211792,
      "learning_rate": 6.844133639778015e-05,
      "loss": 0.2311,
      "step": 22570
    },
    {
      "epoch": 1.2648516931350307,
      "grad_norm": 3.667375087738037,
      "learning_rate": 6.842732215931386e-05,
      "loss": 0.1972,
      "step": 22580
    },
    {
      "epoch": 1.2654118701509678,
      "grad_norm": 3.7373273372650146,
      "learning_rate": 6.841330792084758e-05,
      "loss": 0.1559,
      "step": 22590
    },
    {
      "epoch": 1.2659720471669047,
      "grad_norm": 2.693505048751831,
      "learning_rate": 6.83992936823813e-05,
      "loss": 0.182,
      "step": 22600
    },
    {
      "epoch": 1.2665322241828418,
      "grad_norm": 2.181201457977295,
      "learning_rate": 6.838527944391502e-05,
      "loss": 0.1258,
      "step": 22610
    },
    {
      "epoch": 1.2670924011987788,
      "grad_norm": 1.4799599647521973,
      "learning_rate": 6.837126520544875e-05,
      "loss": 0.1547,
      "step": 22620
    },
    {
      "epoch": 1.2676525782147159,
      "grad_norm": 2.1984455585479736,
      "learning_rate": 6.835725096698246e-05,
      "loss": 0.1776,
      "step": 22630
    },
    {
      "epoch": 1.2682127552306528,
      "grad_norm": 3.201493263244629,
      "learning_rate": 6.834323672851618e-05,
      "loss": 0.2163,
      "step": 22640
    },
    {
      "epoch": 1.26877293224659,
      "grad_norm": 3.5100698471069336,
      "learning_rate": 6.832922249004989e-05,
      "loss": 0.3404,
      "step": 22650
    },
    {
      "epoch": 1.269333109262527,
      "grad_norm": 2.77052903175354,
      "learning_rate": 6.83152082515836e-05,
      "loss": 0.1424,
      "step": 22660
    },
    {
      "epoch": 1.269893286278464,
      "grad_norm": 3.199280261993408,
      "learning_rate": 6.830119401311733e-05,
      "loss": 0.189,
      "step": 22670
    },
    {
      "epoch": 1.270453463294401,
      "grad_norm": 2.0647547245025635,
      "learning_rate": 6.828717977465105e-05,
      "loss": 0.1895,
      "step": 22680
    },
    {
      "epoch": 1.271013640310338,
      "grad_norm": 4.147311687469482,
      "learning_rate": 6.827316553618476e-05,
      "loss": 0.1638,
      "step": 22690
    },
    {
      "epoch": 1.2715738173262752,
      "grad_norm": 5.939637184143066,
      "learning_rate": 6.825915129771848e-05,
      "loss": 0.1446,
      "step": 22700
    },
    {
      "epoch": 1.272133994342212,
      "grad_norm": 6.314230918884277,
      "learning_rate": 6.824513705925221e-05,
      "loss": 0.1858,
      "step": 22710
    },
    {
      "epoch": 1.2726941713581492,
      "grad_norm": 5.896679878234863,
      "learning_rate": 6.823112282078592e-05,
      "loss": 0.1636,
      "step": 22720
    },
    {
      "epoch": 1.2732543483740861,
      "grad_norm": 7.043984889984131,
      "learning_rate": 6.821710858231965e-05,
      "loss": 0.2517,
      "step": 22730
    },
    {
      "epoch": 1.2738145253900233,
      "grad_norm": 2.680584192276001,
      "learning_rate": 6.820309434385335e-05,
      "loss": 0.1952,
      "step": 22740
    },
    {
      "epoch": 1.2743747024059604,
      "grad_norm": 2.624854564666748,
      "learning_rate": 6.818908010538708e-05,
      "loss": 0.2047,
      "step": 22750
    },
    {
      "epoch": 1.2749348794218973,
      "grad_norm": 3.6011343002319336,
      "learning_rate": 6.81750658669208e-05,
      "loss": 0.1766,
      "step": 22760
    },
    {
      "epoch": 1.2754950564378342,
      "grad_norm": 5.226641654968262,
      "learning_rate": 6.816105162845451e-05,
      "loss": 0.2513,
      "step": 22770
    },
    {
      "epoch": 1.2760552334537714,
      "grad_norm": 2.3683598041534424,
      "learning_rate": 6.814703738998824e-05,
      "loss": 0.2689,
      "step": 22780
    },
    {
      "epoch": 1.2766154104697085,
      "grad_norm": 2.9502806663513184,
      "learning_rate": 6.813302315152195e-05,
      "loss": 0.1537,
      "step": 22790
    },
    {
      "epoch": 1.2771755874856454,
      "grad_norm": 4.0643391609191895,
      "learning_rate": 6.811900891305567e-05,
      "loss": 0.1849,
      "step": 22800
    },
    {
      "epoch": 1.2777357645015825,
      "grad_norm": 7.4536356925964355,
      "learning_rate": 6.810499467458938e-05,
      "loss": 0.2725,
      "step": 22810
    },
    {
      "epoch": 1.2782959415175195,
      "grad_norm": 4.851587772369385,
      "learning_rate": 6.809098043612311e-05,
      "loss": 0.163,
      "step": 22820
    },
    {
      "epoch": 1.2788561185334566,
      "grad_norm": 2.024590253829956,
      "learning_rate": 6.807696619765682e-05,
      "loss": 0.1963,
      "step": 22830
    },
    {
      "epoch": 1.2794162955493937,
      "grad_norm": 3.0623810291290283,
      "learning_rate": 6.806295195919054e-05,
      "loss": 0.2445,
      "step": 22840
    },
    {
      "epoch": 1.2799764725653306,
      "grad_norm": 2.4103212356567383,
      "learning_rate": 6.804893772072425e-05,
      "loss": 0.1154,
      "step": 22850
    },
    {
      "epoch": 1.2805366495812676,
      "grad_norm": 2.8986129760742188,
      "learning_rate": 6.803492348225797e-05,
      "loss": 0.2187,
      "step": 22860
    },
    {
      "epoch": 1.2810968265972047,
      "grad_norm": 5.90444278717041,
      "learning_rate": 6.80209092437917e-05,
      "loss": 0.1832,
      "step": 22870
    },
    {
      "epoch": 1.2816570036131418,
      "grad_norm": 2.527878522872925,
      "learning_rate": 6.800689500532541e-05,
      "loss": 0.2212,
      "step": 22880
    },
    {
      "epoch": 1.2822171806290787,
      "grad_norm": 4.435274124145508,
      "learning_rate": 6.799288076685914e-05,
      "loss": 0.2092,
      "step": 22890
    },
    {
      "epoch": 1.2827773576450159,
      "grad_norm": 4.198739528656006,
      "learning_rate": 6.797886652839286e-05,
      "loss": 0.2118,
      "step": 22900
    },
    {
      "epoch": 1.2833375346609528,
      "grad_norm": 7.50334358215332,
      "learning_rate": 6.796485228992657e-05,
      "loss": 0.2316,
      "step": 22910
    },
    {
      "epoch": 1.28389771167689,
      "grad_norm": 2.732692241668701,
      "learning_rate": 6.795083805146028e-05,
      "loss": 0.1487,
      "step": 22920
    },
    {
      "epoch": 1.284457888692827,
      "grad_norm": 3.930790662765503,
      "learning_rate": 6.793682381299401e-05,
      "loss": 0.198,
      "step": 22930
    },
    {
      "epoch": 1.285018065708764,
      "grad_norm": 6.338946342468262,
      "learning_rate": 6.792280957452773e-05,
      "loss": 0.2241,
      "step": 22940
    },
    {
      "epoch": 1.285578242724701,
      "grad_norm": 3.8137896060943604,
      "learning_rate": 6.790879533606144e-05,
      "loss": 0.2153,
      "step": 22950
    },
    {
      "epoch": 1.286138419740638,
      "grad_norm": 4.771847248077393,
      "learning_rate": 6.789478109759516e-05,
      "loss": 0.147,
      "step": 22960
    },
    {
      "epoch": 1.2866985967565752,
      "grad_norm": 3.823989152908325,
      "learning_rate": 6.788076685912887e-05,
      "loss": 0.2324,
      "step": 22970
    },
    {
      "epoch": 1.287258773772512,
      "grad_norm": 7.177920341491699,
      "learning_rate": 6.78667526206626e-05,
      "loss": 0.292,
      "step": 22980
    },
    {
      "epoch": 1.2878189507884492,
      "grad_norm": 3.608759641647339,
      "learning_rate": 6.785273838219631e-05,
      "loss": 0.2933,
      "step": 22990
    },
    {
      "epoch": 1.2883791278043861,
      "grad_norm": 3.484344720840454,
      "learning_rate": 6.783872414373004e-05,
      "loss": 0.1213,
      "step": 23000
    },
    {
      "epoch": 1.2889393048203233,
      "grad_norm": 3.9996933937072754,
      "learning_rate": 6.782470990526374e-05,
      "loss": 0.2211,
      "step": 23010
    },
    {
      "epoch": 1.2894994818362602,
      "grad_norm": 4.419142723083496,
      "learning_rate": 6.781069566679747e-05,
      "loss": 0.1636,
      "step": 23020
    },
    {
      "epoch": 1.2900596588521973,
      "grad_norm": 5.129065036773682,
      "learning_rate": 6.779668142833119e-05,
      "loss": 0.4056,
      "step": 23030
    },
    {
      "epoch": 1.2906198358681342,
      "grad_norm": 3.053872585296631,
      "learning_rate": 6.77826671898649e-05,
      "loss": 0.2633,
      "step": 23040
    },
    {
      "epoch": 1.2911800128840714,
      "grad_norm": 4.3660969734191895,
      "learning_rate": 6.776865295139863e-05,
      "loss": 0.1654,
      "step": 23050
    },
    {
      "epoch": 1.2917401899000085,
      "grad_norm": 2.6774468421936035,
      "learning_rate": 6.775463871293235e-05,
      "loss": 0.1332,
      "step": 23060
    },
    {
      "epoch": 1.2923003669159454,
      "grad_norm": 3.5768942832946777,
      "learning_rate": 6.774062447446606e-05,
      "loss": 0.1449,
      "step": 23070
    },
    {
      "epoch": 1.2928605439318825,
      "grad_norm": 3.1014623641967773,
      "learning_rate": 6.772661023599977e-05,
      "loss": 0.1767,
      "step": 23080
    },
    {
      "epoch": 1.2934207209478195,
      "grad_norm": 3.4492878913879395,
      "learning_rate": 6.77125959975335e-05,
      "loss": 0.1357,
      "step": 23090
    },
    {
      "epoch": 1.2939808979637566,
      "grad_norm": 4.8441267013549805,
      "learning_rate": 6.769858175906722e-05,
      "loss": 0.1896,
      "step": 23100
    },
    {
      "epoch": 1.2945410749796935,
      "grad_norm": 3.9596822261810303,
      "learning_rate": 6.768456752060093e-05,
      "loss": 0.232,
      "step": 23110
    },
    {
      "epoch": 1.2951012519956306,
      "grad_norm": 7.034506797790527,
      "learning_rate": 6.767055328213465e-05,
      "loss": 0.2875,
      "step": 23120
    },
    {
      "epoch": 1.2956614290115676,
      "grad_norm": 4.652458667755127,
      "learning_rate": 6.765653904366836e-05,
      "loss": 0.2723,
      "step": 23130
    },
    {
      "epoch": 1.2962216060275047,
      "grad_norm": 5.631280422210693,
      "learning_rate": 6.764252480520209e-05,
      "loss": 0.1934,
      "step": 23140
    },
    {
      "epoch": 1.2967817830434418,
      "grad_norm": 4.8769941329956055,
      "learning_rate": 6.76285105667358e-05,
      "loss": 0.1941,
      "step": 23150
    },
    {
      "epoch": 1.2973419600593787,
      "grad_norm": 4.897346496582031,
      "learning_rate": 6.761449632826953e-05,
      "loss": 0.1978,
      "step": 23160
    },
    {
      "epoch": 1.2979021370753159,
      "grad_norm": 4.65420389175415,
      "learning_rate": 6.760048208980323e-05,
      "loss": 0.258,
      "step": 23170
    },
    {
      "epoch": 1.2984623140912528,
      "grad_norm": 2.3839194774627686,
      "learning_rate": 6.758646785133696e-05,
      "loss": 0.2366,
      "step": 23180
    },
    {
      "epoch": 1.29902249110719,
      "grad_norm": 2.5732741355895996,
      "learning_rate": 6.757245361287068e-05,
      "loss": 0.2353,
      "step": 23190
    },
    {
      "epoch": 1.2995826681231268,
      "grad_norm": 2.42740797996521,
      "learning_rate": 6.75584393744044e-05,
      "loss": 0.2139,
      "step": 23200
    },
    {
      "epoch": 1.300142845139064,
      "grad_norm": 3.3507649898529053,
      "learning_rate": 6.754442513593812e-05,
      "loss": 0.2701,
      "step": 23210
    },
    {
      "epoch": 1.3007030221550009,
      "grad_norm": 6.873369216918945,
      "learning_rate": 6.753041089747184e-05,
      "loss": 0.2331,
      "step": 23220
    },
    {
      "epoch": 1.301263199170938,
      "grad_norm": 4.460168361663818,
      "learning_rate": 6.751639665900555e-05,
      "loss": 0.2008,
      "step": 23230
    },
    {
      "epoch": 1.3018233761868752,
      "grad_norm": 6.0990142822265625,
      "learning_rate": 6.750238242053926e-05,
      "loss": 0.2612,
      "step": 23240
    },
    {
      "epoch": 1.302383553202812,
      "grad_norm": 5.068634033203125,
      "learning_rate": 6.748836818207299e-05,
      "loss": 0.1695,
      "step": 23250
    },
    {
      "epoch": 1.3029437302187492,
      "grad_norm": 4.143449306488037,
      "learning_rate": 6.747435394360671e-05,
      "loss": 0.1624,
      "step": 23260
    },
    {
      "epoch": 1.3035039072346861,
      "grad_norm": 2.7284085750579834,
      "learning_rate": 6.746033970514044e-05,
      "loss": 0.2835,
      "step": 23270
    },
    {
      "epoch": 1.3040640842506233,
      "grad_norm": 4.036292552947998,
      "learning_rate": 6.744632546667414e-05,
      "loss": 0.1606,
      "step": 23280
    },
    {
      "epoch": 1.3046242612665602,
      "grad_norm": 2.9125962257385254,
      "learning_rate": 6.743231122820787e-05,
      "loss": 0.1839,
      "step": 23290
    },
    {
      "epoch": 1.3051844382824973,
      "grad_norm": 3.0304646492004395,
      "learning_rate": 6.741829698974158e-05,
      "loss": 0.2927,
      "step": 23300
    },
    {
      "epoch": 1.3057446152984342,
      "grad_norm": 3.449674606323242,
      "learning_rate": 6.74042827512753e-05,
      "loss": 0.2741,
      "step": 23310
    },
    {
      "epoch": 1.3063047923143714,
      "grad_norm": 5.182722568511963,
      "learning_rate": 6.739026851280902e-05,
      "loss": 0.2909,
      "step": 23320
    },
    {
      "epoch": 1.3068649693303085,
      "grad_norm": 3.7569658756256104,
      "learning_rate": 6.737625427434274e-05,
      "loss": 0.1462,
      "step": 23330
    },
    {
      "epoch": 1.3074251463462454,
      "grad_norm": 2.7241411209106445,
      "learning_rate": 6.736224003587645e-05,
      "loss": 0.1568,
      "step": 23340
    },
    {
      "epoch": 1.3079853233621823,
      "grad_norm": 6.412095546722412,
      "learning_rate": 6.734822579741017e-05,
      "loss": 0.2046,
      "step": 23350
    },
    {
      "epoch": 1.3085455003781195,
      "grad_norm": 3.2098069190979004,
      "learning_rate": 6.73342115589439e-05,
      "loss": 0.2883,
      "step": 23360
    },
    {
      "epoch": 1.3091056773940566,
      "grad_norm": 3.051368236541748,
      "learning_rate": 6.732019732047761e-05,
      "loss": 0.1785,
      "step": 23370
    },
    {
      "epoch": 1.3096658544099935,
      "grad_norm": 1.1673458814620972,
      "learning_rate": 6.730618308201133e-05,
      "loss": 0.1903,
      "step": 23380
    },
    {
      "epoch": 1.3102260314259306,
      "grad_norm": 4.034472465515137,
      "learning_rate": 6.729216884354504e-05,
      "loss": 0.1966,
      "step": 23390
    },
    {
      "epoch": 1.3107862084418676,
      "grad_norm": 3.5898361206054688,
      "learning_rate": 6.727815460507875e-05,
      "loss": 0.327,
      "step": 23400
    },
    {
      "epoch": 1.3113463854578047,
      "grad_norm": 4.2889299392700195,
      "learning_rate": 6.726414036661248e-05,
      "loss": 0.3093,
      "step": 23410
    },
    {
      "epoch": 1.3119065624737418,
      "grad_norm": 7.226346969604492,
      "learning_rate": 6.72501261281462e-05,
      "loss": 0.2663,
      "step": 23420
    },
    {
      "epoch": 1.3124667394896787,
      "grad_norm": 5.20883846282959,
      "learning_rate": 6.723611188967993e-05,
      "loss": 0.2062,
      "step": 23430
    },
    {
      "epoch": 1.3130269165056156,
      "grad_norm": 3.657505750656128,
      "learning_rate": 6.722209765121363e-05,
      "loss": 0.1871,
      "step": 23440
    },
    {
      "epoch": 1.3135870935215528,
      "grad_norm": 3.658268451690674,
      "learning_rate": 6.720808341274736e-05,
      "loss": 0.1727,
      "step": 23450
    },
    {
      "epoch": 1.31414727053749,
      "grad_norm": 1.1577200889587402,
      "learning_rate": 6.719406917428107e-05,
      "loss": 0.2483,
      "step": 23460
    },
    {
      "epoch": 1.3147074475534268,
      "grad_norm": 4.070917129516602,
      "learning_rate": 6.71800549358148e-05,
      "loss": 0.217,
      "step": 23470
    },
    {
      "epoch": 1.315267624569364,
      "grad_norm": 1.3002054691314697,
      "learning_rate": 6.716604069734851e-05,
      "loss": 0.1283,
      "step": 23480
    },
    {
      "epoch": 1.3158278015853009,
      "grad_norm": 1.9977972507476807,
      "learning_rate": 6.715202645888223e-05,
      "loss": 0.2196,
      "step": 23490
    },
    {
      "epoch": 1.316387978601238,
      "grad_norm": 5.167367458343506,
      "learning_rate": 6.713801222041594e-05,
      "loss": 0.2266,
      "step": 23500
    },
    {
      "epoch": 1.3169481556171752,
      "grad_norm": 4.119267463684082,
      "learning_rate": 6.712399798194966e-05,
      "loss": 0.2389,
      "step": 23510
    },
    {
      "epoch": 1.317508332633112,
      "grad_norm": 5.1419196128845215,
      "learning_rate": 6.710998374348339e-05,
      "loss": 0.2216,
      "step": 23520
    },
    {
      "epoch": 1.318068509649049,
      "grad_norm": 3.801041841506958,
      "learning_rate": 6.70959695050171e-05,
      "loss": 0.1224,
      "step": 23530
    },
    {
      "epoch": 1.3186286866649861,
      "grad_norm": 2.9690146446228027,
      "learning_rate": 6.708195526655082e-05,
      "loss": 0.2965,
      "step": 23540
    },
    {
      "epoch": 1.3191888636809233,
      "grad_norm": 4.531205177307129,
      "learning_rate": 6.706794102808453e-05,
      "loss": 0.1165,
      "step": 23550
    },
    {
      "epoch": 1.3197490406968602,
      "grad_norm": 3.0014054775238037,
      "learning_rate": 6.705392678961826e-05,
      "loss": 0.1813,
      "step": 23560
    },
    {
      "epoch": 1.3203092177127973,
      "grad_norm": 5.3327555656433105,
      "learning_rate": 6.703991255115197e-05,
      "loss": 0.216,
      "step": 23570
    },
    {
      "epoch": 1.3208693947287342,
      "grad_norm": 4.045382022857666,
      "learning_rate": 6.702589831268569e-05,
      "loss": 0.1927,
      "step": 23580
    },
    {
      "epoch": 1.3214295717446713,
      "grad_norm": 5.067276477813721,
      "learning_rate": 6.701188407421942e-05,
      "loss": 0.2479,
      "step": 23590
    },
    {
      "epoch": 1.3219897487606085,
      "grad_norm": 7.803884029388428,
      "learning_rate": 6.699786983575313e-05,
      "loss": 0.291,
      "step": 23600
    },
    {
      "epoch": 1.3225499257765454,
      "grad_norm": 3.8403162956237793,
      "learning_rate": 6.698385559728685e-05,
      "loss": 0.31,
      "step": 23610
    },
    {
      "epoch": 1.3231101027924823,
      "grad_norm": 5.26108980178833,
      "learning_rate": 6.696984135882056e-05,
      "loss": 0.2627,
      "step": 23620
    },
    {
      "epoch": 1.3236702798084194,
      "grad_norm": 5.204928398132324,
      "learning_rate": 6.695582712035429e-05,
      "loss": 0.1921,
      "step": 23630
    },
    {
      "epoch": 1.3242304568243566,
      "grad_norm": 2.783510208129883,
      "learning_rate": 6.6941812881888e-05,
      "loss": 0.2232,
      "step": 23640
    },
    {
      "epoch": 1.3247906338402935,
      "grad_norm": 2.890307664871216,
      "learning_rate": 6.692779864342172e-05,
      "loss": 0.188,
      "step": 23650
    },
    {
      "epoch": 1.3253508108562306,
      "grad_norm": 2.2295618057250977,
      "learning_rate": 6.691378440495543e-05,
      "loss": 0.1863,
      "step": 23660
    },
    {
      "epoch": 1.3259109878721675,
      "grad_norm": 6.438549041748047,
      "learning_rate": 6.689977016648915e-05,
      "loss": 0.2616,
      "step": 23670
    },
    {
      "epoch": 1.3264711648881047,
      "grad_norm": 1.973098874092102,
      "learning_rate": 6.688575592802288e-05,
      "loss": 0.191,
      "step": 23680
    },
    {
      "epoch": 1.3270313419040416,
      "grad_norm": 2.6277706623077393,
      "learning_rate": 6.687174168955659e-05,
      "loss": 0.1951,
      "step": 23690
    },
    {
      "epoch": 1.3275915189199787,
      "grad_norm": 7.341480255126953,
      "learning_rate": 6.685772745109032e-05,
      "loss": 0.2044,
      "step": 23700
    },
    {
      "epoch": 1.3281516959359156,
      "grad_norm": 3.3404202461242676,
      "learning_rate": 6.684371321262402e-05,
      "loss": 0.1825,
      "step": 23710
    },
    {
      "epoch": 1.3287118729518528,
      "grad_norm": 1.334202766418457,
      "learning_rate": 6.682969897415775e-05,
      "loss": 0.2008,
      "step": 23720
    },
    {
      "epoch": 1.32927204996779,
      "grad_norm": 3.5559933185577393,
      "learning_rate": 6.681568473569146e-05,
      "loss": 0.2891,
      "step": 23730
    },
    {
      "epoch": 1.3298322269837268,
      "grad_norm": 5.588245868682861,
      "learning_rate": 6.680167049722519e-05,
      "loss": 0.2658,
      "step": 23740
    },
    {
      "epoch": 1.330392403999664,
      "grad_norm": 4.923928737640381,
      "learning_rate": 6.67876562587589e-05,
      "loss": 0.2352,
      "step": 23750
    },
    {
      "epoch": 1.3309525810156009,
      "grad_norm": 4.638368606567383,
      "learning_rate": 6.677364202029262e-05,
      "loss": 0.2678,
      "step": 23760
    },
    {
      "epoch": 1.331512758031538,
      "grad_norm": 1.8836432695388794,
      "learning_rate": 6.675962778182634e-05,
      "loss": 0.1859,
      "step": 23770
    },
    {
      "epoch": 1.332072935047475,
      "grad_norm": 2.983452558517456,
      "learning_rate": 6.674561354336005e-05,
      "loss": 0.2851,
      "step": 23780
    },
    {
      "epoch": 1.332633112063412,
      "grad_norm": 3.1898117065429688,
      "learning_rate": 6.673159930489378e-05,
      "loss": 0.1777,
      "step": 23790
    },
    {
      "epoch": 1.333193289079349,
      "grad_norm": 5.781672477722168,
      "learning_rate": 6.67175850664275e-05,
      "loss": 0.2736,
      "step": 23800
    },
    {
      "epoch": 1.333753466095286,
      "grad_norm": 5.492773532867432,
      "learning_rate": 6.670357082796121e-05,
      "loss": 0.2266,
      "step": 23810
    },
    {
      "epoch": 1.3343136431112232,
      "grad_norm": 5.920627593994141,
      "learning_rate": 6.668955658949492e-05,
      "loss": 0.1872,
      "step": 23820
    },
    {
      "epoch": 1.3348738201271602,
      "grad_norm": 2.5755975246429443,
      "learning_rate": 6.667554235102865e-05,
      "loss": 0.2946,
      "step": 23830
    },
    {
      "epoch": 1.3354339971430973,
      "grad_norm": 2.175462484359741,
      "learning_rate": 6.666152811256237e-05,
      "loss": 0.183,
      "step": 23840
    },
    {
      "epoch": 1.3359941741590342,
      "grad_norm": 6.423839569091797,
      "learning_rate": 6.66475138740961e-05,
      "loss": 0.183,
      "step": 23850
    },
    {
      "epoch": 1.3365543511749713,
      "grad_norm": 3.1956276893615723,
      "learning_rate": 6.663349963562981e-05,
      "loss": 0.0842,
      "step": 23860
    },
    {
      "epoch": 1.3371145281909083,
      "grad_norm": 2.4567272663116455,
      "learning_rate": 6.661948539716351e-05,
      "loss": 0.1621,
      "step": 23870
    },
    {
      "epoch": 1.3376747052068454,
      "grad_norm": 5.103384971618652,
      "learning_rate": 6.660547115869724e-05,
      "loss": 0.1661,
      "step": 23880
    },
    {
      "epoch": 1.3382348822227823,
      "grad_norm": 2.689073324203491,
      "learning_rate": 6.659145692023095e-05,
      "loss": 0.2881,
      "step": 23890
    },
    {
      "epoch": 1.3387950592387194,
      "grad_norm": 5.080840587615967,
      "learning_rate": 6.657744268176468e-05,
      "loss": 0.1807,
      "step": 23900
    },
    {
      "epoch": 1.3393552362546566,
      "grad_norm": 3.0626392364501953,
      "learning_rate": 6.65634284432984e-05,
      "loss": 0.1326,
      "step": 23910
    },
    {
      "epoch": 1.3399154132705935,
      "grad_norm": 2.956425428390503,
      "learning_rate": 6.654941420483211e-05,
      "loss": 0.1494,
      "step": 23920
    },
    {
      "epoch": 1.3404755902865304,
      "grad_norm": 3.9458048343658447,
      "learning_rate": 6.653539996636583e-05,
      "loss": 0.2049,
      "step": 23930
    },
    {
      "epoch": 1.3410357673024675,
      "grad_norm": 4.047397136688232,
      "learning_rate": 6.652138572789955e-05,
      "loss": 0.1904,
      "step": 23940
    },
    {
      "epoch": 1.3415959443184047,
      "grad_norm": 3.655669927597046,
      "learning_rate": 6.650737148943327e-05,
      "loss": 0.2208,
      "step": 23950
    },
    {
      "epoch": 1.3421561213343416,
      "grad_norm": 5.529308795928955,
      "learning_rate": 6.649335725096698e-05,
      "loss": 0.2636,
      "step": 23960
    },
    {
      "epoch": 1.3427162983502787,
      "grad_norm": 5.200319290161133,
      "learning_rate": 6.647934301250071e-05,
      "loss": 0.1478,
      "step": 23970
    },
    {
      "epoch": 1.3432764753662156,
      "grad_norm": 3.9927432537078857,
      "learning_rate": 6.646532877403441e-05,
      "loss": 0.1747,
      "step": 23980
    },
    {
      "epoch": 1.3438366523821528,
      "grad_norm": 2.5128464698791504,
      "learning_rate": 6.645131453556814e-05,
      "loss": 0.3295,
      "step": 23990
    },
    {
      "epoch": 1.34439682939809,
      "grad_norm": 6.792896270751953,
      "learning_rate": 6.643730029710186e-05,
      "loss": 0.2134,
      "step": 24000
    },
    {
      "epoch": 1.3449570064140268,
      "grad_norm": 2.931731939315796,
      "learning_rate": 6.642328605863558e-05,
      "loss": 0.1472,
      "step": 24010
    },
    {
      "epoch": 1.3455171834299637,
      "grad_norm": 2.810992956161499,
      "learning_rate": 6.64092718201693e-05,
      "loss": 0.203,
      "step": 24020
    },
    {
      "epoch": 1.3460773604459009,
      "grad_norm": 4.428670883178711,
      "learning_rate": 6.639525758170301e-05,
      "loss": 0.2861,
      "step": 24030
    },
    {
      "epoch": 1.346637537461838,
      "grad_norm": 3.076838970184326,
      "learning_rate": 6.638124334323673e-05,
      "loss": 0.1526,
      "step": 24040
    },
    {
      "epoch": 1.347197714477775,
      "grad_norm": 6.078512191772461,
      "learning_rate": 6.636722910477044e-05,
      "loss": 0.5206,
      "step": 24050
    },
    {
      "epoch": 1.347757891493712,
      "grad_norm": 4.288636207580566,
      "learning_rate": 6.635321486630417e-05,
      "loss": 0.1802,
      "step": 24060
    },
    {
      "epoch": 1.348318068509649,
      "grad_norm": 4.604637145996094,
      "learning_rate": 6.633920062783789e-05,
      "loss": 0.1963,
      "step": 24070
    },
    {
      "epoch": 1.348878245525586,
      "grad_norm": 3.669123649597168,
      "learning_rate": 6.63251863893716e-05,
      "loss": 0.2271,
      "step": 24080
    },
    {
      "epoch": 1.3494384225415232,
      "grad_norm": 3.705249071121216,
      "learning_rate": 6.631117215090532e-05,
      "loss": 0.1839,
      "step": 24090
    },
    {
      "epoch": 1.3499985995574602,
      "grad_norm": 3.4868264198303223,
      "learning_rate": 6.629715791243904e-05,
      "loss": 0.131,
      "step": 24100
    },
    {
      "epoch": 1.350558776573397,
      "grad_norm": 5.045363426208496,
      "learning_rate": 6.628314367397276e-05,
      "loss": 0.1572,
      "step": 24110
    },
    {
      "epoch": 1.3511189535893342,
      "grad_norm": 5.47105073928833,
      "learning_rate": 6.626912943550649e-05,
      "loss": 0.1817,
      "step": 24120
    },
    {
      "epoch": 1.3516791306052713,
      "grad_norm": 2.9347164630889893,
      "learning_rate": 6.62551151970402e-05,
      "loss": 0.1719,
      "step": 24130
    },
    {
      "epoch": 1.3522393076212083,
      "grad_norm": 6.2016401290893555,
      "learning_rate": 6.62411009585739e-05,
      "loss": 0.2282,
      "step": 24140
    },
    {
      "epoch": 1.3527994846371454,
      "grad_norm": 4.100637912750244,
      "learning_rate": 6.622708672010763e-05,
      "loss": 0.2101,
      "step": 24150
    },
    {
      "epoch": 1.3533596616530823,
      "grad_norm": 4.19749641418457,
      "learning_rate": 6.621307248164135e-05,
      "loss": 0.1564,
      "step": 24160
    },
    {
      "epoch": 1.3539198386690194,
      "grad_norm": 2.4945805072784424,
      "learning_rate": 6.619905824317507e-05,
      "loss": 0.1345,
      "step": 24170
    },
    {
      "epoch": 1.3544800156849566,
      "grad_norm": 1.0444040298461914,
      "learning_rate": 6.618504400470879e-05,
      "loss": 0.2442,
      "step": 24180
    },
    {
      "epoch": 1.3550401927008935,
      "grad_norm": 2.593881845474243,
      "learning_rate": 6.61710297662425e-05,
      "loss": 0.1489,
      "step": 24190
    },
    {
      "epoch": 1.3556003697168304,
      "grad_norm": 4.4111104011535645,
      "learning_rate": 6.615701552777622e-05,
      "loss": 0.2165,
      "step": 24200
    },
    {
      "epoch": 1.3561605467327675,
      "grad_norm": 3.404017448425293,
      "learning_rate": 6.614300128930995e-05,
      "loss": 0.1989,
      "step": 24210
    },
    {
      "epoch": 1.3567207237487047,
      "grad_norm": 3.6715195178985596,
      "learning_rate": 6.612898705084366e-05,
      "loss": 0.1809,
      "step": 24220
    },
    {
      "epoch": 1.3572809007646416,
      "grad_norm": 3.3884873390197754,
      "learning_rate": 6.611497281237738e-05,
      "loss": 0.1892,
      "step": 24230
    },
    {
      "epoch": 1.3578410777805787,
      "grad_norm": 4.113253116607666,
      "learning_rate": 6.610095857391109e-05,
      "loss": 0.2157,
      "step": 24240
    },
    {
      "epoch": 1.3584012547965156,
      "grad_norm": 1.61849045753479,
      "learning_rate": 6.60869443354448e-05,
      "loss": 0.2018,
      "step": 24250
    },
    {
      "epoch": 1.3589614318124528,
      "grad_norm": 1.7834502458572388,
      "learning_rate": 6.607293009697853e-05,
      "loss": 0.2496,
      "step": 24260
    },
    {
      "epoch": 1.35952160882839,
      "grad_norm": 6.2114152908325195,
      "learning_rate": 6.605891585851225e-05,
      "loss": 0.2049,
      "step": 24270
    },
    {
      "epoch": 1.3600817858443268,
      "grad_norm": 2.8858535289764404,
      "learning_rate": 6.604490162004598e-05,
      "loss": 0.1878,
      "step": 24280
    },
    {
      "epoch": 1.3606419628602637,
      "grad_norm": 2.533649444580078,
      "learning_rate": 6.603088738157969e-05,
      "loss": 0.1484,
      "step": 24290
    },
    {
      "epoch": 1.3612021398762009,
      "grad_norm": 3.711543321609497,
      "learning_rate": 6.601687314311341e-05,
      "loss": 0.2323,
      "step": 24300
    },
    {
      "epoch": 1.361762316892138,
      "grad_norm": 2.8560738563537598,
      "learning_rate": 6.600285890464712e-05,
      "loss": 0.2053,
      "step": 24310
    },
    {
      "epoch": 1.362322493908075,
      "grad_norm": 2.6397244930267334,
      "learning_rate": 6.598884466618084e-05,
      "loss": 0.2625,
      "step": 24320
    },
    {
      "epoch": 1.362882670924012,
      "grad_norm": 3.535823345184326,
      "learning_rate": 6.597483042771456e-05,
      "loss": 0.1296,
      "step": 24330
    },
    {
      "epoch": 1.363442847939949,
      "grad_norm": 1.974064826965332,
      "learning_rate": 6.596081618924828e-05,
      "loss": 0.1708,
      "step": 24340
    },
    {
      "epoch": 1.364003024955886,
      "grad_norm": 4.419323444366455,
      "learning_rate": 6.5946801950782e-05,
      "loss": 0.1927,
      "step": 24350
    },
    {
      "epoch": 1.364563201971823,
      "grad_norm": 1.2962056398391724,
      "learning_rate": 6.593278771231571e-05,
      "loss": 0.218,
      "step": 24360
    },
    {
      "epoch": 1.3651233789877601,
      "grad_norm": 4.862527370452881,
      "learning_rate": 6.591877347384944e-05,
      "loss": 0.2178,
      "step": 24370
    },
    {
      "epoch": 1.365683556003697,
      "grad_norm": 5.126040458679199,
      "learning_rate": 6.590475923538315e-05,
      "loss": 0.2048,
      "step": 24380
    },
    {
      "epoch": 1.3662437330196342,
      "grad_norm": 3.1580772399902344,
      "learning_rate": 6.589074499691688e-05,
      "loss": 0.2603,
      "step": 24390
    },
    {
      "epoch": 1.3668039100355713,
      "grad_norm": 4.7892889976501465,
      "learning_rate": 6.58767307584506e-05,
      "loss": 0.1818,
      "step": 24400
    },
    {
      "epoch": 1.3673640870515082,
      "grad_norm": 2.309072971343994,
      "learning_rate": 6.58627165199843e-05,
      "loss": 0.1904,
      "step": 24410
    },
    {
      "epoch": 1.3679242640674454,
      "grad_norm": 3.567532539367676,
      "learning_rate": 6.584870228151802e-05,
      "loss": 0.1542,
      "step": 24420
    },
    {
      "epoch": 1.3684844410833823,
      "grad_norm": 2.4248266220092773,
      "learning_rate": 6.583468804305174e-05,
      "loss": 0.1746,
      "step": 24430
    },
    {
      "epoch": 1.3690446180993194,
      "grad_norm": 4.58017635345459,
      "learning_rate": 6.582067380458547e-05,
      "loss": 0.1831,
      "step": 24440
    },
    {
      "epoch": 1.3696047951152563,
      "grad_norm": 3.9189772605895996,
      "learning_rate": 6.580665956611918e-05,
      "loss": 0.1889,
      "step": 24450
    },
    {
      "epoch": 1.3701649721311935,
      "grad_norm": 5.715414524078369,
      "learning_rate": 6.57926453276529e-05,
      "loss": 0.1372,
      "step": 24460
    },
    {
      "epoch": 1.3707251491471304,
      "grad_norm": 2.8936426639556885,
      "learning_rate": 6.577863108918661e-05,
      "loss": 0.1024,
      "step": 24470
    },
    {
      "epoch": 1.3712853261630675,
      "grad_norm": 2.2641286849975586,
      "learning_rate": 6.576461685072034e-05,
      "loss": 0.1848,
      "step": 24480
    },
    {
      "epoch": 1.3718455031790047,
      "grad_norm": 4.679457664489746,
      "learning_rate": 6.575060261225405e-05,
      "loss": 0.132,
      "step": 24490
    },
    {
      "epoch": 1.3724056801949416,
      "grad_norm": 4.963174819946289,
      "learning_rate": 6.573658837378777e-05,
      "loss": 0.1718,
      "step": 24500
    },
    {
      "epoch": 1.3729658572108787,
      "grad_norm": 5.640881061553955,
      "learning_rate": 6.572257413532148e-05,
      "loss": 0.2247,
      "step": 24510
    },
    {
      "epoch": 1.3735260342268156,
      "grad_norm": 4.276551723480225,
      "learning_rate": 6.57085598968552e-05,
      "loss": 0.2067,
      "step": 24520
    },
    {
      "epoch": 1.3740862112427528,
      "grad_norm": 3.9437859058380127,
      "learning_rate": 6.569454565838893e-05,
      "loss": 0.2239,
      "step": 24530
    },
    {
      "epoch": 1.3746463882586897,
      "grad_norm": 3.1652045249938965,
      "learning_rate": 6.568053141992264e-05,
      "loss": 0.2208,
      "step": 24540
    },
    {
      "epoch": 1.3752065652746268,
      "grad_norm": 5.178674221038818,
      "learning_rate": 6.566651718145637e-05,
      "loss": 0.2663,
      "step": 24550
    },
    {
      "epoch": 1.3757667422905637,
      "grad_norm": 3.9674081802368164,
      "learning_rate": 6.565250294299009e-05,
      "loss": 0.1939,
      "step": 24560
    },
    {
      "epoch": 1.3763269193065009,
      "grad_norm": 3.0074379444122314,
      "learning_rate": 6.56384887045238e-05,
      "loss": 0.1682,
      "step": 24570
    },
    {
      "epoch": 1.376887096322438,
      "grad_norm": 5.664230823516846,
      "learning_rate": 6.562447446605751e-05,
      "loss": 0.238,
      "step": 24580
    },
    {
      "epoch": 1.377447273338375,
      "grad_norm": 4.800724983215332,
      "learning_rate": 6.561046022759123e-05,
      "loss": 0.2818,
      "step": 24590
    },
    {
      "epoch": 1.3780074503543118,
      "grad_norm": 1.3639026880264282,
      "learning_rate": 6.559644598912496e-05,
      "loss": 0.2373,
      "step": 24600
    },
    {
      "epoch": 1.378567627370249,
      "grad_norm": 3.087005615234375,
      "learning_rate": 6.558243175065867e-05,
      "loss": 0.1619,
      "step": 24610
    },
    {
      "epoch": 1.379127804386186,
      "grad_norm": 5.352210521697998,
      "learning_rate": 6.556841751219239e-05,
      "loss": 0.2243,
      "step": 24620
    },
    {
      "epoch": 1.379687981402123,
      "grad_norm": 3.83980131149292,
      "learning_rate": 6.55544032737261e-05,
      "loss": 0.187,
      "step": 24630
    },
    {
      "epoch": 1.3802481584180601,
      "grad_norm": 2.9784154891967773,
      "learning_rate": 6.554038903525983e-05,
      "loss": 0.2122,
      "step": 24640
    },
    {
      "epoch": 1.380808335433997,
      "grad_norm": 5.247861385345459,
      "learning_rate": 6.552637479679354e-05,
      "loss": 0.1987,
      "step": 24650
    },
    {
      "epoch": 1.3813685124499342,
      "grad_norm": 3.3549227714538574,
      "learning_rate": 6.551236055832727e-05,
      "loss": 0.3333,
      "step": 24660
    },
    {
      "epoch": 1.3819286894658713,
      "grad_norm": 4.3716020584106445,
      "learning_rate": 6.549834631986099e-05,
      "loss": 0.2293,
      "step": 24670
    },
    {
      "epoch": 1.3824888664818082,
      "grad_norm": 1.9016294479370117,
      "learning_rate": 6.548433208139469e-05,
      "loss": 0.1476,
      "step": 24680
    },
    {
      "epoch": 1.3830490434977452,
      "grad_norm": 3.7014293670654297,
      "learning_rate": 6.547031784292842e-05,
      "loss": 0.2545,
      "step": 24690
    },
    {
      "epoch": 1.3836092205136823,
      "grad_norm": 6.119163990020752,
      "learning_rate": 6.545630360446213e-05,
      "loss": 0.1909,
      "step": 24700
    },
    {
      "epoch": 1.3841693975296194,
      "grad_norm": 4.601601600646973,
      "learning_rate": 6.544228936599586e-05,
      "loss": 0.1627,
      "step": 24710
    },
    {
      "epoch": 1.3847295745455563,
      "grad_norm": 4.073034286499023,
      "learning_rate": 6.542827512752958e-05,
      "loss": 0.2422,
      "step": 24720
    },
    {
      "epoch": 1.3852897515614935,
      "grad_norm": 4.612695693969727,
      "learning_rate": 6.541426088906329e-05,
      "loss": 0.2084,
      "step": 24730
    },
    {
      "epoch": 1.3858499285774304,
      "grad_norm": 2.302903652191162,
      "learning_rate": 6.5400246650597e-05,
      "loss": 0.2239,
      "step": 24740
    },
    {
      "epoch": 1.3864101055933675,
      "grad_norm": 4.4321088790893555,
      "learning_rate": 6.538623241213073e-05,
      "loss": 0.2272,
      "step": 24750
    },
    {
      "epoch": 1.3869702826093047,
      "grad_norm": 4.016089916229248,
      "learning_rate": 6.537221817366445e-05,
      "loss": 0.1794,
      "step": 24760
    },
    {
      "epoch": 1.3875304596252416,
      "grad_norm": 2.2440834045410156,
      "learning_rate": 6.535820393519816e-05,
      "loss": 0.1554,
      "step": 24770
    },
    {
      "epoch": 1.3880906366411785,
      "grad_norm": 5.266111373901367,
      "learning_rate": 6.534418969673188e-05,
      "loss": 0.1295,
      "step": 24780
    },
    {
      "epoch": 1.3886508136571156,
      "grad_norm": 1.7277884483337402,
      "learning_rate": 6.533017545826559e-05,
      "loss": 0.1623,
      "step": 24790
    },
    {
      "epoch": 1.3892109906730528,
      "grad_norm": 1.5114600658416748,
      "learning_rate": 6.531616121979932e-05,
      "loss": 0.1719,
      "step": 24800
    },
    {
      "epoch": 1.3897711676889897,
      "grad_norm": 1.7100634574890137,
      "learning_rate": 6.530214698133304e-05,
      "loss": 0.2795,
      "step": 24810
    },
    {
      "epoch": 1.3903313447049268,
      "grad_norm": 2.6460444927215576,
      "learning_rate": 6.528813274286676e-05,
      "loss": 0.207,
      "step": 24820
    },
    {
      "epoch": 1.3908915217208637,
      "grad_norm": 3.7991695404052734,
      "learning_rate": 6.527411850440048e-05,
      "loss": 0.221,
      "step": 24830
    },
    {
      "epoch": 1.3914516987368009,
      "grad_norm": 4.616146564483643,
      "learning_rate": 6.526010426593419e-05,
      "loss": 0.1789,
      "step": 24840
    },
    {
      "epoch": 1.392011875752738,
      "grad_norm": 3.087538003921509,
      "learning_rate": 6.524609002746791e-05,
      "loss": 0.1665,
      "step": 24850
    },
    {
      "epoch": 1.392572052768675,
      "grad_norm": 4.788047790527344,
      "learning_rate": 6.523207578900164e-05,
      "loss": 0.1762,
      "step": 24860
    },
    {
      "epoch": 1.3931322297846118,
      "grad_norm": 5.8860883712768555,
      "learning_rate": 6.521806155053535e-05,
      "loss": 0.2521,
      "step": 24870
    },
    {
      "epoch": 1.393692406800549,
      "grad_norm": 3.3063323497772217,
      "learning_rate": 6.520404731206907e-05,
      "loss": 0.1605,
      "step": 24880
    },
    {
      "epoch": 1.394252583816486,
      "grad_norm": 2.2064194679260254,
      "learning_rate": 6.519003307360278e-05,
      "loss": 0.1607,
      "step": 24890
    },
    {
      "epoch": 1.394812760832423,
      "grad_norm": 3.9396822452545166,
      "learning_rate": 6.51760188351365e-05,
      "loss": 0.2234,
      "step": 24900
    },
    {
      "epoch": 1.3953729378483601,
      "grad_norm": 4.811328887939453,
      "learning_rate": 6.516200459667022e-05,
      "loss": 0.1607,
      "step": 24910
    },
    {
      "epoch": 1.395933114864297,
      "grad_norm": 5.866013050079346,
      "learning_rate": 6.514799035820394e-05,
      "loss": 0.1727,
      "step": 24920
    },
    {
      "epoch": 1.3964932918802342,
      "grad_norm": 2.377234935760498,
      "learning_rate": 6.513397611973767e-05,
      "loss": 0.1485,
      "step": 24930
    },
    {
      "epoch": 1.3970534688961713,
      "grad_norm": 3.52736759185791,
      "learning_rate": 6.511996188127137e-05,
      "loss": 0.1654,
      "step": 24940
    },
    {
      "epoch": 1.3976136459121082,
      "grad_norm": 2.387305736541748,
      "learning_rate": 6.51059476428051e-05,
      "loss": 0.1824,
      "step": 24950
    },
    {
      "epoch": 1.3981738229280452,
      "grad_norm": 5.21172571182251,
      "learning_rate": 6.509193340433881e-05,
      "loss": 0.3213,
      "step": 24960
    },
    {
      "epoch": 1.3987339999439823,
      "grad_norm": 3.4594104290008545,
      "learning_rate": 6.507791916587253e-05,
      "loss": 0.1412,
      "step": 24970
    },
    {
      "epoch": 1.3992941769599194,
      "grad_norm": 3.6863913536071777,
      "learning_rate": 6.506390492740625e-05,
      "loss": 0.1508,
      "step": 24980
    },
    {
      "epoch": 1.3998543539758563,
      "grad_norm": 2.576768159866333,
      "learning_rate": 6.504989068893997e-05,
      "loss": 0.1684,
      "step": 24990
    },
    {
      "epoch": 1.4004145309917935,
      "grad_norm": 4.7759623527526855,
      "learning_rate": 6.503587645047368e-05,
      "loss": 0.2059,
      "step": 25000
    },
    {
      "epoch": 1.4009747080077304,
      "grad_norm": 6.290666580200195,
      "learning_rate": 6.50218622120074e-05,
      "loss": 0.2026,
      "step": 25010
    },
    {
      "epoch": 1.4015348850236675,
      "grad_norm": 2.4772000312805176,
      "learning_rate": 6.500784797354113e-05,
      "loss": 0.1468,
      "step": 25020
    },
    {
      "epoch": 1.4020950620396044,
      "grad_norm": 5.642445087432861,
      "learning_rate": 6.499383373507484e-05,
      "loss": 0.3018,
      "step": 25030
    },
    {
      "epoch": 1.4026552390555416,
      "grad_norm": 3.058788299560547,
      "learning_rate": 6.497981949660857e-05,
      "loss": 0.1347,
      "step": 25040
    },
    {
      "epoch": 1.4032154160714785,
      "grad_norm": 3.880892515182495,
      "learning_rate": 6.496580525814227e-05,
      "loss": 0.1583,
      "step": 25050
    },
    {
      "epoch": 1.4037755930874156,
      "grad_norm": 3.695255994796753,
      "learning_rate": 6.495179101967598e-05,
      "loss": 0.2182,
      "step": 25060
    },
    {
      "epoch": 1.4043357701033528,
      "grad_norm": 9.6637544631958,
      "learning_rate": 6.493777678120971e-05,
      "loss": 0.2907,
      "step": 25070
    },
    {
      "epoch": 1.4048959471192897,
      "grad_norm": 6.813857078552246,
      "learning_rate": 6.492376254274343e-05,
      "loss": 0.2095,
      "step": 25080
    },
    {
      "epoch": 1.4054561241352268,
      "grad_norm": 2.031374216079712,
      "learning_rate": 6.490974830427716e-05,
      "loss": 0.2553,
      "step": 25090
    },
    {
      "epoch": 1.4060163011511637,
      "grad_norm": 3.7511253356933594,
      "learning_rate": 6.489573406581087e-05,
      "loss": 0.2008,
      "step": 25100
    },
    {
      "epoch": 1.4065764781671009,
      "grad_norm": 2.937849998474121,
      "learning_rate": 6.488171982734459e-05,
      "loss": 0.1843,
      "step": 25110
    },
    {
      "epoch": 1.4071366551830378,
      "grad_norm": 4.709676742553711,
      "learning_rate": 6.48677055888783e-05,
      "loss": 0.2472,
      "step": 25120
    },
    {
      "epoch": 1.407696832198975,
      "grad_norm": 3.148399829864502,
      "learning_rate": 6.485369135041203e-05,
      "loss": 0.2227,
      "step": 25130
    },
    {
      "epoch": 1.4082570092149118,
      "grad_norm": 2.6122171878814697,
      "learning_rate": 6.483967711194574e-05,
      "loss": 0.2111,
      "step": 25140
    },
    {
      "epoch": 1.408817186230849,
      "grad_norm": 5.897751331329346,
      "learning_rate": 6.482566287347946e-05,
      "loss": 0.1909,
      "step": 25150
    },
    {
      "epoch": 1.409377363246786,
      "grad_norm": 3.673886775970459,
      "learning_rate": 6.481164863501317e-05,
      "loss": 0.1816,
      "step": 25160
    },
    {
      "epoch": 1.409937540262723,
      "grad_norm": 2.202122926712036,
      "learning_rate": 6.479763439654689e-05,
      "loss": 0.2349,
      "step": 25170
    },
    {
      "epoch": 1.4104977172786601,
      "grad_norm": 3.246140480041504,
      "learning_rate": 6.478362015808062e-05,
      "loss": 0.2032,
      "step": 25180
    },
    {
      "epoch": 1.411057894294597,
      "grad_norm": 6.215374946594238,
      "learning_rate": 6.476960591961433e-05,
      "loss": 0.1724,
      "step": 25190
    },
    {
      "epoch": 1.4116180713105342,
      "grad_norm": 3.835937738418579,
      "learning_rate": 6.475559168114806e-05,
      "loss": 0.1663,
      "step": 25200
    },
    {
      "epoch": 1.412178248326471,
      "grad_norm": 4.536922454833984,
      "learning_rate": 6.474157744268176e-05,
      "loss": 0.1247,
      "step": 25210
    },
    {
      "epoch": 1.4127384253424082,
      "grad_norm": 1.362452507019043,
      "learning_rate": 6.472756320421549e-05,
      "loss": 0.2283,
      "step": 25220
    },
    {
      "epoch": 1.4132986023583451,
      "grad_norm": 0.9550991058349609,
      "learning_rate": 6.47135489657492e-05,
      "loss": 0.2601,
      "step": 25230
    },
    {
      "epoch": 1.4138587793742823,
      "grad_norm": 6.2704548835754395,
      "learning_rate": 6.469953472728292e-05,
      "loss": 0.3612,
      "step": 25240
    },
    {
      "epoch": 1.4144189563902194,
      "grad_norm": 5.248956203460693,
      "learning_rate": 6.468552048881665e-05,
      "loss": 0.2712,
      "step": 25250
    },
    {
      "epoch": 1.4149791334061563,
      "grad_norm": 4.910370349884033,
      "learning_rate": 6.467150625035036e-05,
      "loss": 0.1635,
      "step": 25260
    },
    {
      "epoch": 1.4155393104220932,
      "grad_norm": 0.9785031676292419,
      "learning_rate": 6.465749201188408e-05,
      "loss": 0.1112,
      "step": 25270
    },
    {
      "epoch": 1.4160994874380304,
      "grad_norm": 7.1194539070129395,
      "learning_rate": 6.464347777341779e-05,
      "loss": 0.2077,
      "step": 25280
    },
    {
      "epoch": 1.4166596644539675,
      "grad_norm": 0.9181464314460754,
      "learning_rate": 6.462946353495152e-05,
      "loss": 0.2627,
      "step": 25290
    },
    {
      "epoch": 1.4172198414699044,
      "grad_norm": 3.6203877925872803,
      "learning_rate": 6.461544929648523e-05,
      "loss": 0.1703,
      "step": 25300
    },
    {
      "epoch": 1.4177800184858416,
      "grad_norm": 5.4466471672058105,
      "learning_rate": 6.460143505801896e-05,
      "loss": 0.2754,
      "step": 25310
    },
    {
      "epoch": 1.4183401955017785,
      "grad_norm": 2.8744752407073975,
      "learning_rate": 6.458742081955266e-05,
      "loss": 0.1357,
      "step": 25320
    },
    {
      "epoch": 1.4189003725177156,
      "grad_norm": 4.66883659362793,
      "learning_rate": 6.457340658108638e-05,
      "loss": 0.2687,
      "step": 25330
    },
    {
      "epoch": 1.4194605495336527,
      "grad_norm": 2.624554395675659,
      "learning_rate": 6.45593923426201e-05,
      "loss": 0.1893,
      "step": 25340
    },
    {
      "epoch": 1.4200207265495897,
      "grad_norm": 4.210475444793701,
      "learning_rate": 6.454537810415382e-05,
      "loss": 0.1607,
      "step": 25350
    },
    {
      "epoch": 1.4205809035655266,
      "grad_norm": 3.8755979537963867,
      "learning_rate": 6.453136386568755e-05,
      "loss": 0.1449,
      "step": 25360
    },
    {
      "epoch": 1.4211410805814637,
      "grad_norm": 4.830386161804199,
      "learning_rate": 6.451734962722126e-05,
      "loss": 0.2575,
      "step": 25370
    },
    {
      "epoch": 1.4217012575974008,
      "grad_norm": 4.0888190269470215,
      "learning_rate": 6.450333538875498e-05,
      "loss": 0.2497,
      "step": 25380
    },
    {
      "epoch": 1.4222614346133378,
      "grad_norm": 3.0026345252990723,
      "learning_rate": 6.44893211502887e-05,
      "loss": 0.166,
      "step": 25390
    },
    {
      "epoch": 1.422821611629275,
      "grad_norm": 4.953674793243408,
      "learning_rate": 6.447530691182242e-05,
      "loss": 0.1725,
      "step": 25400
    },
    {
      "epoch": 1.4233817886452118,
      "grad_norm": 3.445260763168335,
      "learning_rate": 6.446129267335614e-05,
      "loss": 0.1774,
      "step": 25410
    },
    {
      "epoch": 1.423941965661149,
      "grad_norm": 9.182705879211426,
      "learning_rate": 6.444727843488985e-05,
      "loss": 0.3023,
      "step": 25420
    },
    {
      "epoch": 1.424502142677086,
      "grad_norm": 1.7220836877822876,
      "learning_rate": 6.443326419642357e-05,
      "loss": 0.2207,
      "step": 25430
    },
    {
      "epoch": 1.425062319693023,
      "grad_norm": 2.168696403503418,
      "learning_rate": 6.441924995795728e-05,
      "loss": 0.2147,
      "step": 25440
    },
    {
      "epoch": 1.42562249670896,
      "grad_norm": 4.005819797515869,
      "learning_rate": 6.440523571949101e-05,
      "loss": 0.2195,
      "step": 25450
    },
    {
      "epoch": 1.426182673724897,
      "grad_norm": 2.3399271965026855,
      "learning_rate": 6.439122148102472e-05,
      "loss": 0.1886,
      "step": 25460
    },
    {
      "epoch": 1.4267428507408342,
      "grad_norm": 5.3789801597595215,
      "learning_rate": 6.437720724255845e-05,
      "loss": 0.1548,
      "step": 25470
    },
    {
      "epoch": 1.427303027756771,
      "grad_norm": 3.6302318572998047,
      "learning_rate": 6.436319300409215e-05,
      "loss": 0.169,
      "step": 25480
    },
    {
      "epoch": 1.4278632047727082,
      "grad_norm": 4.168633937835693,
      "learning_rate": 6.434917876562588e-05,
      "loss": 0.1806,
      "step": 25490
    },
    {
      "epoch": 1.4284233817886451,
      "grad_norm": 3.0765225887298584,
      "learning_rate": 6.43351645271596e-05,
      "loss": 0.2308,
      "step": 25500
    },
    {
      "epoch": 1.4289835588045823,
      "grad_norm": 3.9853909015655518,
      "learning_rate": 6.432115028869331e-05,
      "loss": 0.1254,
      "step": 25510
    },
    {
      "epoch": 1.4295437358205194,
      "grad_norm": 3.1115634441375732,
      "learning_rate": 6.430713605022704e-05,
      "loss": 0.2379,
      "step": 25520
    },
    {
      "epoch": 1.4301039128364563,
      "grad_norm": 3.1563453674316406,
      "learning_rate": 6.429312181176075e-05,
      "loss": 0.0978,
      "step": 25530
    },
    {
      "epoch": 1.4306640898523932,
      "grad_norm": 6.461420059204102,
      "learning_rate": 6.427910757329447e-05,
      "loss": 0.1388,
      "step": 25540
    },
    {
      "epoch": 1.4312242668683304,
      "grad_norm": 2.8227224349975586,
      "learning_rate": 6.426509333482818e-05,
      "loss": 0.1494,
      "step": 25550
    },
    {
      "epoch": 1.4317844438842675,
      "grad_norm": 3.6662509441375732,
      "learning_rate": 6.425107909636191e-05,
      "loss": 0.2584,
      "step": 25560
    },
    {
      "epoch": 1.4323446209002044,
      "grad_norm": 2.629213809967041,
      "learning_rate": 6.423706485789563e-05,
      "loss": 0.1961,
      "step": 25570
    },
    {
      "epoch": 1.4329047979161416,
      "grad_norm": 3.461777925491333,
      "learning_rate": 6.422305061942934e-05,
      "loss": 0.123,
      "step": 25580
    },
    {
      "epoch": 1.4334649749320785,
      "grad_norm": 1.6922574043273926,
      "learning_rate": 6.420903638096306e-05,
      "loss": 0.1806,
      "step": 25590
    },
    {
      "epoch": 1.4340251519480156,
      "grad_norm": 4.0232625007629395,
      "learning_rate": 6.419502214249677e-05,
      "loss": 0.2636,
      "step": 25600
    },
    {
      "epoch": 1.4345853289639527,
      "grad_norm": 4.044038772583008,
      "learning_rate": 6.41810079040305e-05,
      "loss": 0.1199,
      "step": 25610
    },
    {
      "epoch": 1.4351455059798897,
      "grad_norm": 3.818218469619751,
      "learning_rate": 6.416699366556421e-05,
      "loss": 0.1744,
      "step": 25620
    },
    {
      "epoch": 1.4357056829958266,
      "grad_norm": 1.7389533519744873,
      "learning_rate": 6.415297942709794e-05,
      "loss": 0.2081,
      "step": 25630
    },
    {
      "epoch": 1.4362658600117637,
      "grad_norm": 5.699114799499512,
      "learning_rate": 6.413896518863164e-05,
      "loss": 0.2046,
      "step": 25640
    },
    {
      "epoch": 1.4368260370277008,
      "grad_norm": 2.2927393913269043,
      "learning_rate": 6.412495095016537e-05,
      "loss": 0.1627,
      "step": 25650
    },
    {
      "epoch": 1.4373862140436378,
      "grad_norm": 6.776498317718506,
      "learning_rate": 6.411093671169909e-05,
      "loss": 0.3039,
      "step": 25660
    },
    {
      "epoch": 1.437946391059575,
      "grad_norm": 4.486667156219482,
      "learning_rate": 6.409692247323281e-05,
      "loss": 0.1795,
      "step": 25670
    },
    {
      "epoch": 1.4385065680755118,
      "grad_norm": 2.3825459480285645,
      "learning_rate": 6.408290823476653e-05,
      "loss": 0.2045,
      "step": 25680
    },
    {
      "epoch": 1.439066745091449,
      "grad_norm": 3.057478427886963,
      "learning_rate": 6.406889399630024e-05,
      "loss": 0.1355,
      "step": 25690
    },
    {
      "epoch": 1.4396269221073859,
      "grad_norm": 2.0383007526397705,
      "learning_rate": 6.405487975783396e-05,
      "loss": 0.254,
      "step": 25700
    },
    {
      "epoch": 1.440187099123323,
      "grad_norm": 2.465487480163574,
      "learning_rate": 6.404086551936767e-05,
      "loss": 0.1266,
      "step": 25710
    },
    {
      "epoch": 1.44074727613926,
      "grad_norm": 5.8946123123168945,
      "learning_rate": 6.40268512809014e-05,
      "loss": 0.3165,
      "step": 25720
    },
    {
      "epoch": 1.441307453155197,
      "grad_norm": 2.6528029441833496,
      "learning_rate": 6.401283704243512e-05,
      "loss": 0.1368,
      "step": 25730
    },
    {
      "epoch": 1.4418676301711342,
      "grad_norm": 3.7247023582458496,
      "learning_rate": 6.399882280396884e-05,
      "loss": 0.1499,
      "step": 25740
    },
    {
      "epoch": 1.442427807187071,
      "grad_norm": 3.6129355430603027,
      "learning_rate": 6.398480856550255e-05,
      "loss": 0.2133,
      "step": 25750
    },
    {
      "epoch": 1.4429879842030082,
      "grad_norm": 3.618896722793579,
      "learning_rate": 6.397079432703627e-05,
      "loss": 0.1897,
      "step": 25760
    },
    {
      "epoch": 1.4435481612189451,
      "grad_norm": 2.930645227432251,
      "learning_rate": 6.395678008856999e-05,
      "loss": 0.1814,
      "step": 25770
    },
    {
      "epoch": 1.4441083382348823,
      "grad_norm": 4.016590595245361,
      "learning_rate": 6.39427658501037e-05,
      "loss": 0.2402,
      "step": 25780
    },
    {
      "epoch": 1.4446685152508192,
      "grad_norm": 2.876896619796753,
      "learning_rate": 6.392875161163743e-05,
      "loss": 0.2008,
      "step": 25790
    },
    {
      "epoch": 1.4452286922667563,
      "grad_norm": 3.78690767288208,
      "learning_rate": 6.391473737317115e-05,
      "loss": 0.2742,
      "step": 25800
    },
    {
      "epoch": 1.4457888692826932,
      "grad_norm": 2.7322449684143066,
      "learning_rate": 6.390072313470486e-05,
      "loss": 0.1197,
      "step": 25810
    },
    {
      "epoch": 1.4463490462986304,
      "grad_norm": 7.637822151184082,
      "learning_rate": 6.388670889623858e-05,
      "loss": 0.2171,
      "step": 25820
    },
    {
      "epoch": 1.4469092233145675,
      "grad_norm": 2.0960278511047363,
      "learning_rate": 6.38726946577723e-05,
      "loss": 0.1848,
      "step": 25830
    },
    {
      "epoch": 1.4474694003305044,
      "grad_norm": 3.681527614593506,
      "learning_rate": 6.385868041930602e-05,
      "loss": 0.2843,
      "step": 25840
    },
    {
      "epoch": 1.4480295773464416,
      "grad_norm": 1.5092475414276123,
      "learning_rate": 6.384466618083973e-05,
      "loss": 0.1546,
      "step": 25850
    },
    {
      "epoch": 1.4485897543623785,
      "grad_norm": 2.591538906097412,
      "learning_rate": 6.383065194237345e-05,
      "loss": 0.1291,
      "step": 25860
    },
    {
      "epoch": 1.4491499313783156,
      "grad_norm": 5.125640392303467,
      "learning_rate": 6.381663770390716e-05,
      "loss": 0.193,
      "step": 25870
    },
    {
      "epoch": 1.4497101083942525,
      "grad_norm": 3.910738706588745,
      "learning_rate": 6.380262346544089e-05,
      "loss": 0.1556,
      "step": 25880
    },
    {
      "epoch": 1.4502702854101897,
      "grad_norm": 2.2957074642181396,
      "learning_rate": 6.37886092269746e-05,
      "loss": 0.1103,
      "step": 25890
    },
    {
      "epoch": 1.4508304624261266,
      "grad_norm": 5.172615051269531,
      "learning_rate": 6.377459498850834e-05,
      "loss": 0.1909,
      "step": 25900
    },
    {
      "epoch": 1.4513906394420637,
      "grad_norm": 4.559089183807373,
      "learning_rate": 6.376058075004204e-05,
      "loss": 0.1886,
      "step": 25910
    },
    {
      "epoch": 1.4519508164580008,
      "grad_norm": 1.4780254364013672,
      "learning_rate": 6.374656651157576e-05,
      "loss": 0.1237,
      "step": 25920
    },
    {
      "epoch": 1.4525109934739378,
      "grad_norm": 4.119236469268799,
      "learning_rate": 6.373255227310948e-05,
      "loss": 0.1789,
      "step": 25930
    },
    {
      "epoch": 1.4530711704898747,
      "grad_norm": 4.746026992797852,
      "learning_rate": 6.371853803464321e-05,
      "loss": 0.2365,
      "step": 25940
    },
    {
      "epoch": 1.4536313475058118,
      "grad_norm": 2.526177167892456,
      "learning_rate": 6.370452379617692e-05,
      "loss": 0.1356,
      "step": 25950
    },
    {
      "epoch": 1.454191524521749,
      "grad_norm": 1.9018113613128662,
      "learning_rate": 6.369050955771064e-05,
      "loss": 0.1448,
      "step": 25960
    },
    {
      "epoch": 1.4547517015376858,
      "grad_norm": 4.909099578857422,
      "learning_rate": 6.367649531924435e-05,
      "loss": 0.2128,
      "step": 25970
    },
    {
      "epoch": 1.455311878553623,
      "grad_norm": 1.5723810195922852,
      "learning_rate": 6.366248108077807e-05,
      "loss": 0.207,
      "step": 25980
    },
    {
      "epoch": 1.45587205556956,
      "grad_norm": 2.1817421913146973,
      "learning_rate": 6.36484668423118e-05,
      "loss": 0.1754,
      "step": 25990
    },
    {
      "epoch": 1.456432232585497,
      "grad_norm": 4.708670139312744,
      "learning_rate": 6.363445260384551e-05,
      "loss": 0.1694,
      "step": 26000
    },
    {
      "epoch": 1.4569924096014342,
      "grad_norm": 3.3958818912506104,
      "learning_rate": 6.362043836537924e-05,
      "loss": 0.1567,
      "step": 26010
    },
    {
      "epoch": 1.457552586617371,
      "grad_norm": 4.215518474578857,
      "learning_rate": 6.360642412691294e-05,
      "loss": 0.2091,
      "step": 26020
    },
    {
      "epoch": 1.458112763633308,
      "grad_norm": 6.687575340270996,
      "learning_rate": 6.359240988844667e-05,
      "loss": 0.222,
      "step": 26030
    },
    {
      "epoch": 1.4586729406492451,
      "grad_norm": 7.160773754119873,
      "learning_rate": 6.357839564998038e-05,
      "loss": 0.1579,
      "step": 26040
    },
    {
      "epoch": 1.4592331176651823,
      "grad_norm": 6.613987445831299,
      "learning_rate": 6.356438141151411e-05,
      "loss": 0.2743,
      "step": 26050
    },
    {
      "epoch": 1.4597932946811192,
      "grad_norm": 2.964395523071289,
      "learning_rate": 6.355036717304783e-05,
      "loss": 0.1555,
      "step": 26060
    },
    {
      "epoch": 1.4603534716970563,
      "grad_norm": 5.135263442993164,
      "learning_rate": 6.353635293458154e-05,
      "loss": 0.19,
      "step": 26070
    },
    {
      "epoch": 1.4609136487129932,
      "grad_norm": 2.316683292388916,
      "learning_rate": 6.352233869611525e-05,
      "loss": 0.1294,
      "step": 26080
    },
    {
      "epoch": 1.4614738257289304,
      "grad_norm": 1.870354175567627,
      "learning_rate": 6.350832445764897e-05,
      "loss": 0.1263,
      "step": 26090
    },
    {
      "epoch": 1.4620340027448675,
      "grad_norm": 1.6541746854782104,
      "learning_rate": 6.34943102191827e-05,
      "loss": 0.132,
      "step": 26100
    },
    {
      "epoch": 1.4625941797608044,
      "grad_norm": 5.397609233856201,
      "learning_rate": 6.348029598071641e-05,
      "loss": 0.1658,
      "step": 26110
    },
    {
      "epoch": 1.4631543567767413,
      "grad_norm": 1.8713299036026,
      "learning_rate": 6.346628174225013e-05,
      "loss": 0.1416,
      "step": 26120
    },
    {
      "epoch": 1.4637145337926785,
      "grad_norm": 1.282163381576538,
      "learning_rate": 6.345226750378384e-05,
      "loss": 0.1408,
      "step": 26130
    },
    {
      "epoch": 1.4642747108086156,
      "grad_norm": 4.0226945877075195,
      "learning_rate": 6.343825326531757e-05,
      "loss": 0.182,
      "step": 26140
    },
    {
      "epoch": 1.4648348878245525,
      "grad_norm": 3.5361011028289795,
      "learning_rate": 6.342423902685128e-05,
      "loss": 0.258,
      "step": 26150
    },
    {
      "epoch": 1.4653950648404896,
      "grad_norm": 2.8601973056793213,
      "learning_rate": 6.3410224788385e-05,
      "loss": 0.2129,
      "step": 26160
    },
    {
      "epoch": 1.4659552418564266,
      "grad_norm": 4.07810640335083,
      "learning_rate": 6.339621054991873e-05,
      "loss": 0.2827,
      "step": 26170
    },
    {
      "epoch": 1.4665154188723637,
      "grad_norm": 5.936563968658447,
      "learning_rate": 6.338219631145243e-05,
      "loss": 0.1851,
      "step": 26180
    },
    {
      "epoch": 1.4670755958883008,
      "grad_norm": 3.207028865814209,
      "learning_rate": 6.336818207298616e-05,
      "loss": 0.338,
      "step": 26190
    },
    {
      "epoch": 1.4676357729042377,
      "grad_norm": 3.4417223930358887,
      "learning_rate": 6.335416783451987e-05,
      "loss": 0.1554,
      "step": 26200
    },
    {
      "epoch": 1.4681959499201747,
      "grad_norm": 3.3769731521606445,
      "learning_rate": 6.33401535960536e-05,
      "loss": 0.1584,
      "step": 26210
    },
    {
      "epoch": 1.4687561269361118,
      "grad_norm": 4.063285827636719,
      "learning_rate": 6.332613935758732e-05,
      "loss": 0.1822,
      "step": 26220
    },
    {
      "epoch": 1.469316303952049,
      "grad_norm": 3.186086654663086,
      "learning_rate": 6.331212511912103e-05,
      "loss": 0.2078,
      "step": 26230
    },
    {
      "epoch": 1.4698764809679858,
      "grad_norm": 3.3945586681365967,
      "learning_rate": 6.329811088065474e-05,
      "loss": 0.168,
      "step": 26240
    },
    {
      "epoch": 1.470436657983923,
      "grad_norm": 3.6795480251312256,
      "learning_rate": 6.328409664218846e-05,
      "loss": 0.1484,
      "step": 26250
    },
    {
      "epoch": 1.47099683499986,
      "grad_norm": 5.763440132141113,
      "learning_rate": 6.327008240372219e-05,
      "loss": 0.2298,
      "step": 26260
    },
    {
      "epoch": 1.471557012015797,
      "grad_norm": 6.448683261871338,
      "learning_rate": 6.32560681652559e-05,
      "loss": 0.188,
      "step": 26270
    },
    {
      "epoch": 1.4721171890317342,
      "grad_norm": 3.588676691055298,
      "learning_rate": 6.324205392678962e-05,
      "loss": 0.3224,
      "step": 26280
    },
    {
      "epoch": 1.472677366047671,
      "grad_norm": 4.653669834136963,
      "learning_rate": 6.322803968832333e-05,
      "loss": 0.1839,
      "step": 26290
    },
    {
      "epoch": 1.473237543063608,
      "grad_norm": 9.04920768737793,
      "learning_rate": 6.321402544985706e-05,
      "loss": 0.1664,
      "step": 26300
    },
    {
      "epoch": 1.4737977200795451,
      "grad_norm": 1.9878649711608887,
      "learning_rate": 6.320001121139078e-05,
      "loss": 0.177,
      "step": 26310
    },
    {
      "epoch": 1.4743578970954823,
      "grad_norm": 2.2079551219940186,
      "learning_rate": 6.31859969729245e-05,
      "loss": 0.4382,
      "step": 26320
    },
    {
      "epoch": 1.4749180741114192,
      "grad_norm": 3.4608118534088135,
      "learning_rate": 6.317198273445822e-05,
      "loss": 0.1878,
      "step": 26330
    },
    {
      "epoch": 1.4754782511273563,
      "grad_norm": 3.941768169403076,
      "learning_rate": 6.315796849599192e-05,
      "loss": 0.2566,
      "step": 26340
    },
    {
      "epoch": 1.4760384281432932,
      "grad_norm": 3.6780495643615723,
      "learning_rate": 6.314395425752565e-05,
      "loss": 0.2866,
      "step": 26350
    },
    {
      "epoch": 1.4765986051592304,
      "grad_norm": 4.393308639526367,
      "learning_rate": 6.312994001905936e-05,
      "loss": 0.193,
      "step": 26360
    },
    {
      "epoch": 1.4771587821751673,
      "grad_norm": 7.602653980255127,
      "learning_rate": 6.311592578059309e-05,
      "loss": 0.195,
      "step": 26370
    },
    {
      "epoch": 1.4777189591911044,
      "grad_norm": 3.2970376014709473,
      "learning_rate": 6.31019115421268e-05,
      "loss": 0.1466,
      "step": 26380
    },
    {
      "epoch": 1.4782791362070413,
      "grad_norm": 2.6099257469177246,
      "learning_rate": 6.308789730366052e-05,
      "loss": 0.2288,
      "step": 26390
    },
    {
      "epoch": 1.4788393132229785,
      "grad_norm": 5.493298530578613,
      "learning_rate": 6.307388306519423e-05,
      "loss": 0.1982,
      "step": 26400
    },
    {
      "epoch": 1.4793994902389156,
      "grad_norm": 3.394120931625366,
      "learning_rate": 6.305986882672796e-05,
      "loss": 0.1341,
      "step": 26410
    },
    {
      "epoch": 1.4799596672548525,
      "grad_norm": 5.42844295501709,
      "learning_rate": 6.304585458826168e-05,
      "loss": 0.2179,
      "step": 26420
    },
    {
      "epoch": 1.4805198442707896,
      "grad_norm": 3.2965445518493652,
      "learning_rate": 6.303184034979539e-05,
      "loss": 0.225,
      "step": 26430
    },
    {
      "epoch": 1.4810800212867266,
      "grad_norm": 4.565081596374512,
      "learning_rate": 6.301782611132912e-05,
      "loss": 0.1298,
      "step": 26440
    },
    {
      "epoch": 1.4816401983026637,
      "grad_norm": 4.5966997146606445,
      "learning_rate": 6.300381187286282e-05,
      "loss": 0.2537,
      "step": 26450
    },
    {
      "epoch": 1.4822003753186006,
      "grad_norm": 4.929366588592529,
      "learning_rate": 6.298979763439655e-05,
      "loss": 0.1753,
      "step": 26460
    },
    {
      "epoch": 1.4827605523345377,
      "grad_norm": 1.552518606185913,
      "learning_rate": 6.297578339593027e-05,
      "loss": 0.1192,
      "step": 26470
    },
    {
      "epoch": 1.4833207293504747,
      "grad_norm": 1.4193835258483887,
      "learning_rate": 6.2961769157464e-05,
      "loss": 0.2222,
      "step": 26480
    },
    {
      "epoch": 1.4838809063664118,
      "grad_norm": 3.2954165935516357,
      "learning_rate": 6.294775491899771e-05,
      "loss": 0.1482,
      "step": 26490
    },
    {
      "epoch": 1.484441083382349,
      "grad_norm": 3.1050193309783936,
      "learning_rate": 6.293374068053142e-05,
      "loss": 0.1257,
      "step": 26500
    },
    {
      "epoch": 1.4850012603982858,
      "grad_norm": 3.343507766723633,
      "learning_rate": 6.291972644206514e-05,
      "loss": 0.1269,
      "step": 26510
    },
    {
      "epoch": 1.485561437414223,
      "grad_norm": 5.68377161026001,
      "learning_rate": 6.290571220359885e-05,
      "loss": 0.1639,
      "step": 26520
    },
    {
      "epoch": 1.4861216144301599,
      "grad_norm": 4.982240200042725,
      "learning_rate": 6.289169796513258e-05,
      "loss": 0.2205,
      "step": 26530
    },
    {
      "epoch": 1.486681791446097,
      "grad_norm": 1.7979609966278076,
      "learning_rate": 6.28776837266663e-05,
      "loss": 0.1903,
      "step": 26540
    },
    {
      "epoch": 1.487241968462034,
      "grad_norm": 4.167343616485596,
      "learning_rate": 6.286366948820001e-05,
      "loss": 0.2062,
      "step": 26550
    },
    {
      "epoch": 1.487802145477971,
      "grad_norm": 2.1068692207336426,
      "learning_rate": 6.284965524973372e-05,
      "loss": 0.2222,
      "step": 26560
    },
    {
      "epoch": 1.488362322493908,
      "grad_norm": 2.384570837020874,
      "learning_rate": 6.283564101126745e-05,
      "loss": 0.1776,
      "step": 26570
    },
    {
      "epoch": 1.4889224995098451,
      "grad_norm": 4.091814041137695,
      "learning_rate": 6.282162677280117e-05,
      "loss": 0.2725,
      "step": 26580
    },
    {
      "epoch": 1.4894826765257823,
      "grad_norm": 2.1003215312957764,
      "learning_rate": 6.28076125343349e-05,
      "loss": 0.111,
      "step": 26590
    },
    {
      "epoch": 1.4900428535417192,
      "grad_norm": 4.268505096435547,
      "learning_rate": 6.279359829586861e-05,
      "loss": 0.2088,
      "step": 26600
    },
    {
      "epoch": 1.490603030557656,
      "grad_norm": 3.594060182571411,
      "learning_rate": 6.277958405740231e-05,
      "loss": 0.1855,
      "step": 26610
    },
    {
      "epoch": 1.4911632075735932,
      "grad_norm": 4.899384021759033,
      "learning_rate": 6.276556981893604e-05,
      "loss": 0.2384,
      "step": 26620
    },
    {
      "epoch": 1.4917233845895304,
      "grad_norm": 2.1332499980926514,
      "learning_rate": 6.275155558046976e-05,
      "loss": 0.1827,
      "step": 26630
    },
    {
      "epoch": 1.4922835616054673,
      "grad_norm": 5.711189270019531,
      "learning_rate": 6.273754134200348e-05,
      "loss": 0.1578,
      "step": 26640
    },
    {
      "epoch": 1.4928437386214044,
      "grad_norm": 3.118311882019043,
      "learning_rate": 6.27235271035372e-05,
      "loss": 0.1729,
      "step": 26650
    },
    {
      "epoch": 1.4934039156373413,
      "grad_norm": 3.6147100925445557,
      "learning_rate": 6.270951286507091e-05,
      "loss": 0.2295,
      "step": 26660
    },
    {
      "epoch": 1.4939640926532785,
      "grad_norm": 3.17164945602417,
      "learning_rate": 6.269549862660463e-05,
      "loss": 0.1338,
      "step": 26670
    },
    {
      "epoch": 1.4945242696692156,
      "grad_norm": 3.8237223625183105,
      "learning_rate": 6.268148438813836e-05,
      "loss": 0.2499,
      "step": 26680
    },
    {
      "epoch": 1.4950844466851525,
      "grad_norm": 4.640375137329102,
      "learning_rate": 6.266747014967207e-05,
      "loss": 0.1648,
      "step": 26690
    },
    {
      "epoch": 1.4956446237010894,
      "grad_norm": 4.673856258392334,
      "learning_rate": 6.265345591120579e-05,
      "loss": 0.1909,
      "step": 26700
    },
    {
      "epoch": 1.4962048007170266,
      "grad_norm": 3.349637269973755,
      "learning_rate": 6.263944167273951e-05,
      "loss": 0.2502,
      "step": 26710
    },
    {
      "epoch": 1.4967649777329637,
      "grad_norm": 4.00558614730835,
      "learning_rate": 6.262542743427322e-05,
      "loss": 0.2257,
      "step": 26720
    },
    {
      "epoch": 1.4973251547489006,
      "grad_norm": 7.298214435577393,
      "learning_rate": 6.261141319580694e-05,
      "loss": 0.1808,
      "step": 26730
    },
    {
      "epoch": 1.4978853317648377,
      "grad_norm": 5.913401126861572,
      "learning_rate": 6.259739895734066e-05,
      "loss": 0.3212,
      "step": 26740
    },
    {
      "epoch": 1.4984455087807746,
      "grad_norm": 2.821599245071411,
      "learning_rate": 6.258338471887439e-05,
      "loss": 0.1886,
      "step": 26750
    },
    {
      "epoch": 1.4990056857967118,
      "grad_norm": 6.288467884063721,
      "learning_rate": 6.25693704804081e-05,
      "loss": 0.2426,
      "step": 26760
    },
    {
      "epoch": 1.499565862812649,
      "grad_norm": 5.067842483520508,
      "learning_rate": 6.255535624194182e-05,
      "loss": 0.2041,
      "step": 26770
    },
    {
      "epoch": 1.5001260398285858,
      "grad_norm": 5.655511856079102,
      "learning_rate": 6.254134200347553e-05,
      "loss": 0.1628,
      "step": 26780
    },
    {
      "epoch": 1.5006862168445227,
      "grad_norm": 7.29325008392334,
      "learning_rate": 6.252732776500925e-05,
      "loss": 0.2204,
      "step": 26790
    },
    {
      "epoch": 1.5012463938604599,
      "grad_norm": 2.892746686935425,
      "learning_rate": 6.251331352654297e-05,
      "loss": 0.1281,
      "step": 26800
    },
    {
      "epoch": 1.501806570876397,
      "grad_norm": 1.467557430267334,
      "learning_rate": 6.249929928807669e-05,
      "loss": 0.1657,
      "step": 26810
    },
    {
      "epoch": 1.502366747892334,
      "grad_norm": 2.9897162914276123,
      "learning_rate": 6.24852850496104e-05,
      "loss": 0.1243,
      "step": 26820
    },
    {
      "epoch": 1.502926924908271,
      "grad_norm": 3.444854259490967,
      "learning_rate": 6.247127081114412e-05,
      "loss": 0.1236,
      "step": 26830
    },
    {
      "epoch": 1.503487101924208,
      "grad_norm": 4.248371124267578,
      "learning_rate": 6.245725657267785e-05,
      "loss": 0.288,
      "step": 26840
    },
    {
      "epoch": 1.5040472789401451,
      "grad_norm": 1.5996460914611816,
      "learning_rate": 6.244324233421156e-05,
      "loss": 0.3155,
      "step": 26850
    },
    {
      "epoch": 1.5046074559560823,
      "grad_norm": 5.063094615936279,
      "learning_rate": 6.242922809574529e-05,
      "loss": 0.2024,
      "step": 26860
    },
    {
      "epoch": 1.5051676329720192,
      "grad_norm": 5.131322383880615,
      "learning_rate": 6.2415213857279e-05,
      "loss": 0.1832,
      "step": 26870
    },
    {
      "epoch": 1.505727809987956,
      "grad_norm": 2.9673550128936768,
      "learning_rate": 6.24011996188127e-05,
      "loss": 0.1313,
      "step": 26880
    },
    {
      "epoch": 1.5062879870038932,
      "grad_norm": 2.825364351272583,
      "learning_rate": 6.238718538034643e-05,
      "loss": 0.182,
      "step": 26890
    },
    {
      "epoch": 1.5068481640198303,
      "grad_norm": 2.300541877746582,
      "learning_rate": 6.237317114188015e-05,
      "loss": 0.167,
      "step": 26900
    },
    {
      "epoch": 1.5074083410357673,
      "grad_norm": 1.7481567859649658,
      "learning_rate": 6.235915690341388e-05,
      "loss": 0.2181,
      "step": 26910
    },
    {
      "epoch": 1.5079685180517042,
      "grad_norm": 4.517910480499268,
      "learning_rate": 6.234514266494759e-05,
      "loss": 0.292,
      "step": 26920
    },
    {
      "epoch": 1.5085286950676413,
      "grad_norm": 3.176016092300415,
      "learning_rate": 6.23311284264813e-05,
      "loss": 0.1371,
      "step": 26930
    },
    {
      "epoch": 1.5090888720835784,
      "grad_norm": 0.7156632542610168,
      "learning_rate": 6.231711418801502e-05,
      "loss": 0.1166,
      "step": 26940
    },
    {
      "epoch": 1.5096490490995156,
      "grad_norm": 8.58166790008545,
      "learning_rate": 6.230309994954875e-05,
      "loss": 0.2317,
      "step": 26950
    },
    {
      "epoch": 1.5102092261154525,
      "grad_norm": 1.0712480545043945,
      "learning_rate": 6.228908571108246e-05,
      "loss": 0.2182,
      "step": 26960
    },
    {
      "epoch": 1.5107694031313894,
      "grad_norm": 4.216548919677734,
      "learning_rate": 6.227507147261619e-05,
      "loss": 0.1602,
      "step": 26970
    },
    {
      "epoch": 1.5113295801473265,
      "grad_norm": 2.716665506362915,
      "learning_rate": 6.22610572341499e-05,
      "loss": 0.2717,
      "step": 26980
    },
    {
      "epoch": 1.5118897571632637,
      "grad_norm": 6.576091289520264,
      "learning_rate": 6.224704299568361e-05,
      "loss": 0.2372,
      "step": 26990
    },
    {
      "epoch": 1.5124499341792006,
      "grad_norm": 3.984807252883911,
      "learning_rate": 6.223302875721734e-05,
      "loss": 0.1369,
      "step": 27000
    },
    {
      "epoch": 1.5130101111951375,
      "grad_norm": 1.1004904508590698,
      "learning_rate": 6.221901451875105e-05,
      "loss": 0.1341,
      "step": 27010
    },
    {
      "epoch": 1.5135702882110746,
      "grad_norm": 4.985215187072754,
      "learning_rate": 6.220500028028478e-05,
      "loss": 0.1361,
      "step": 27020
    },
    {
      "epoch": 1.5141304652270118,
      "grad_norm": 2.6023333072662354,
      "learning_rate": 6.21909860418185e-05,
      "loss": 0.1395,
      "step": 27030
    },
    {
      "epoch": 1.514690642242949,
      "grad_norm": 3.6074888706207275,
      "learning_rate": 6.217697180335221e-05,
      "loss": 0.1272,
      "step": 27040
    },
    {
      "epoch": 1.5152508192588858,
      "grad_norm": 5.420705795288086,
      "learning_rate": 6.216295756488592e-05,
      "loss": 0.2841,
      "step": 27050
    },
    {
      "epoch": 1.5158109962748227,
      "grad_norm": 6.910854339599609,
      "learning_rate": 6.214894332641965e-05,
      "loss": 0.2339,
      "step": 27060
    },
    {
      "epoch": 1.5163711732907599,
      "grad_norm": 5.435317516326904,
      "learning_rate": 6.213492908795337e-05,
      "loss": 0.2865,
      "step": 27070
    },
    {
      "epoch": 1.516931350306697,
      "grad_norm": 5.056004524230957,
      "learning_rate": 6.212091484948708e-05,
      "loss": 0.2331,
      "step": 27080
    },
    {
      "epoch": 1.517491527322634,
      "grad_norm": 4.841380596160889,
      "learning_rate": 6.21069006110208e-05,
      "loss": 0.223,
      "step": 27090
    },
    {
      "epoch": 1.5180517043385708,
      "grad_norm": 4.754685401916504,
      "learning_rate": 6.209288637255451e-05,
      "loss": 0.2032,
      "step": 27100
    },
    {
      "epoch": 1.518611881354508,
      "grad_norm": 2.9967877864837646,
      "learning_rate": 6.207887213408824e-05,
      "loss": 0.1892,
      "step": 27110
    },
    {
      "epoch": 1.519172058370445,
      "grad_norm": 5.099271297454834,
      "learning_rate": 6.206485789562195e-05,
      "loss": 0.1599,
      "step": 27120
    },
    {
      "epoch": 1.5197322353863822,
      "grad_norm": 5.080990314483643,
      "learning_rate": 6.205084365715568e-05,
      "loss": 0.2618,
      "step": 27130
    },
    {
      "epoch": 1.5202924124023192,
      "grad_norm": 1.6989351511001587,
      "learning_rate": 6.20368294186894e-05,
      "loss": 0.198,
      "step": 27140
    },
    {
      "epoch": 1.520852589418256,
      "grad_norm": 2.310664415359497,
      "learning_rate": 6.202281518022311e-05,
      "loss": 0.2224,
      "step": 27150
    },
    {
      "epoch": 1.5214127664341932,
      "grad_norm": 4.79174280166626,
      "learning_rate": 6.200880094175683e-05,
      "loss": 0.2016,
      "step": 27160
    },
    {
      "epoch": 1.5219729434501303,
      "grad_norm": 4.655147075653076,
      "learning_rate": 6.199478670329054e-05,
      "loss": 0.2218,
      "step": 27170
    },
    {
      "epoch": 1.5225331204660673,
      "grad_norm": 2.7935428619384766,
      "learning_rate": 6.198077246482427e-05,
      "loss": 0.1641,
      "step": 27180
    },
    {
      "epoch": 1.5230932974820042,
      "grad_norm": 2.2030439376831055,
      "learning_rate": 6.196675822635798e-05,
      "loss": 0.1864,
      "step": 27190
    },
    {
      "epoch": 1.5236534744979413,
      "grad_norm": 2.49318265914917,
      "learning_rate": 6.19527439878917e-05,
      "loss": 0.1249,
      "step": 27200
    },
    {
      "epoch": 1.5242136515138784,
      "grad_norm": 5.284707069396973,
      "learning_rate": 6.193872974942541e-05,
      "loss": 0.1443,
      "step": 27210
    },
    {
      "epoch": 1.5247738285298156,
      "grad_norm": 3.7138023376464844,
      "learning_rate": 6.192471551095914e-05,
      "loss": 0.1612,
      "step": 27220
    },
    {
      "epoch": 1.5253340055457525,
      "grad_norm": 1.7608715295791626,
      "learning_rate": 6.191070127249286e-05,
      "loss": 0.1687,
      "step": 27230
    },
    {
      "epoch": 1.5258941825616894,
      "grad_norm": 5.2457990646362305,
      "learning_rate": 6.189668703402658e-05,
      "loss": 0.1305,
      "step": 27240
    },
    {
      "epoch": 1.5264543595776265,
      "grad_norm": 4.100982189178467,
      "learning_rate": 6.188267279556029e-05,
      "loss": 0.1506,
      "step": 27250
    },
    {
      "epoch": 1.5270145365935637,
      "grad_norm": 4.291549205780029,
      "learning_rate": 6.1868658557094e-05,
      "loss": 0.1832,
      "step": 27260
    },
    {
      "epoch": 1.5275747136095006,
      "grad_norm": 3.8746819496154785,
      "learning_rate": 6.185464431862773e-05,
      "loss": 0.2094,
      "step": 27270
    },
    {
      "epoch": 1.5281348906254375,
      "grad_norm": 3.5017669200897217,
      "learning_rate": 6.184063008016144e-05,
      "loss": 0.1179,
      "step": 27280
    },
    {
      "epoch": 1.5286950676413746,
      "grad_norm": 2.5843400955200195,
      "learning_rate": 6.182661584169517e-05,
      "loss": 0.1458,
      "step": 27290
    },
    {
      "epoch": 1.5292552446573118,
      "grad_norm": 4.945356369018555,
      "learning_rate": 6.181260160322889e-05,
      "loss": 0.2061,
      "step": 27300
    },
    {
      "epoch": 1.529815421673249,
      "grad_norm": 2.831815242767334,
      "learning_rate": 6.17985873647626e-05,
      "loss": 0.2114,
      "step": 27310
    },
    {
      "epoch": 1.5303755986891858,
      "grad_norm": 7.522776126861572,
      "learning_rate": 6.178457312629632e-05,
      "loss": 0.3166,
      "step": 27320
    },
    {
      "epoch": 1.5309357757051227,
      "grad_norm": 4.17284631729126,
      "learning_rate": 6.177055888783004e-05,
      "loss": 0.1859,
      "step": 27330
    },
    {
      "epoch": 1.5314959527210599,
      "grad_norm": 6.1491594314575195,
      "learning_rate": 6.175654464936376e-05,
      "loss": 0.213,
      "step": 27340
    },
    {
      "epoch": 1.532056129736997,
      "grad_norm": 4.016927242279053,
      "learning_rate": 6.174253041089747e-05,
      "loss": 0.1387,
      "step": 27350
    },
    {
      "epoch": 1.532616306752934,
      "grad_norm": 7.4805121421813965,
      "learning_rate": 6.172851617243119e-05,
      "loss": 0.1883,
      "step": 27360
    },
    {
      "epoch": 1.5331764837688708,
      "grad_norm": 4.347542762756348,
      "learning_rate": 6.17145019339649e-05,
      "loss": 0.1383,
      "step": 27370
    },
    {
      "epoch": 1.533736660784808,
      "grad_norm": 4.582841873168945,
      "learning_rate": 6.170048769549863e-05,
      "loss": 0.155,
      "step": 27380
    },
    {
      "epoch": 1.534296837800745,
      "grad_norm": 3.4176793098449707,
      "learning_rate": 6.168647345703235e-05,
      "loss": 0.1515,
      "step": 27390
    },
    {
      "epoch": 1.534857014816682,
      "grad_norm": 3.8710718154907227,
      "learning_rate": 6.167245921856608e-05,
      "loss": 0.2501,
      "step": 27400
    },
    {
      "epoch": 1.5354171918326192,
      "grad_norm": 3.158663034439087,
      "learning_rate": 6.165844498009979e-05,
      "loss": 0.154,
      "step": 27410
    },
    {
      "epoch": 1.535977368848556,
      "grad_norm": 3.2875001430511475,
      "learning_rate": 6.16444307416335e-05,
      "loss": 0.1786,
      "step": 27420
    },
    {
      "epoch": 1.5365375458644932,
      "grad_norm": 1.0767643451690674,
      "learning_rate": 6.163041650316722e-05,
      "loss": 0.1163,
      "step": 27430
    },
    {
      "epoch": 1.5370977228804303,
      "grad_norm": 1.8096818923950195,
      "learning_rate": 6.161640226470093e-05,
      "loss": 0.2702,
      "step": 27440
    },
    {
      "epoch": 1.5376578998963673,
      "grad_norm": 2.3487160205841064,
      "learning_rate": 6.160238802623466e-05,
      "loss": 0.1134,
      "step": 27450
    },
    {
      "epoch": 1.5382180769123042,
      "grad_norm": 5.665498733520508,
      "learning_rate": 6.158837378776838e-05,
      "loss": 0.3159,
      "step": 27460
    },
    {
      "epoch": 1.5387782539282413,
      "grad_norm": 2.5961544513702393,
      "learning_rate": 6.157435954930209e-05,
      "loss": 0.2161,
      "step": 27470
    },
    {
      "epoch": 1.5393384309441784,
      "grad_norm": 4.6619110107421875,
      "learning_rate": 6.15603453108358e-05,
      "loss": 0.2028,
      "step": 27480
    },
    {
      "epoch": 1.5398986079601154,
      "grad_norm": 1.684461236000061,
      "learning_rate": 6.154633107236953e-05,
      "loss": 0.1561,
      "step": 27490
    },
    {
      "epoch": 1.5404587849760525,
      "grad_norm": 3.861999750137329,
      "learning_rate": 6.153231683390325e-05,
      "loss": 0.156,
      "step": 27500
    },
    {
      "epoch": 1.5410189619919894,
      "grad_norm": 3.569343090057373,
      "learning_rate": 6.151830259543698e-05,
      "loss": 0.1628,
      "step": 27510
    },
    {
      "epoch": 1.5415791390079265,
      "grad_norm": 1.8777964115142822,
      "learning_rate": 6.150428835697068e-05,
      "loss": 0.1752,
      "step": 27520
    },
    {
      "epoch": 1.5421393160238637,
      "grad_norm": 3.725459575653076,
      "learning_rate": 6.14902741185044e-05,
      "loss": 0.2456,
      "step": 27530
    },
    {
      "epoch": 1.5426994930398006,
      "grad_norm": 3.719280242919922,
      "learning_rate": 6.147625988003812e-05,
      "loss": 0.103,
      "step": 27540
    },
    {
      "epoch": 1.5432596700557375,
      "grad_norm": 2.1186068058013916,
      "learning_rate": 6.146224564157184e-05,
      "loss": 0.1874,
      "step": 27550
    },
    {
      "epoch": 1.5438198470716746,
      "grad_norm": 4.924456596374512,
      "learning_rate": 6.144823140310557e-05,
      "loss": 0.1528,
      "step": 27560
    },
    {
      "epoch": 1.5443800240876118,
      "grad_norm": 4.947965621948242,
      "learning_rate": 6.143421716463928e-05,
      "loss": 0.1754,
      "step": 27570
    },
    {
      "epoch": 1.5449402011035487,
      "grad_norm": 5.168821811676025,
      "learning_rate": 6.1420202926173e-05,
      "loss": 0.16,
      "step": 27580
    },
    {
      "epoch": 1.5455003781194856,
      "grad_norm": 3.885768175125122,
      "learning_rate": 6.140618868770671e-05,
      "loss": 0.2191,
      "step": 27590
    },
    {
      "epoch": 1.5460605551354227,
      "grad_norm": 4.551523208618164,
      "learning_rate": 6.139217444924044e-05,
      "loss": 0.2074,
      "step": 27600
    },
    {
      "epoch": 1.5466207321513599,
      "grad_norm": 5.428704261779785,
      "learning_rate": 6.137816021077415e-05,
      "loss": 0.2435,
      "step": 27610
    },
    {
      "epoch": 1.547180909167297,
      "grad_norm": 3.6908962726593018,
      "learning_rate": 6.136414597230787e-05,
      "loss": 0.1545,
      "step": 27620
    },
    {
      "epoch": 1.547741086183234,
      "grad_norm": 1.2787938117980957,
      "learning_rate": 6.135013173384158e-05,
      "loss": 0.1377,
      "step": 27630
    },
    {
      "epoch": 1.5483012631991708,
      "grad_norm": 7.252349853515625,
      "learning_rate": 6.13361174953753e-05,
      "loss": 0.2255,
      "step": 27640
    },
    {
      "epoch": 1.548861440215108,
      "grad_norm": 3.9919071197509766,
      "learning_rate": 6.132210325690902e-05,
      "loss": 0.1988,
      "step": 27650
    },
    {
      "epoch": 1.549421617231045,
      "grad_norm": 4.681173801422119,
      "learning_rate": 6.130808901844274e-05,
      "loss": 0.2119,
      "step": 27660
    },
    {
      "epoch": 1.549981794246982,
      "grad_norm": 3.030097007751465,
      "learning_rate": 6.129407477997647e-05,
      "loss": 0.1467,
      "step": 27670
    },
    {
      "epoch": 1.550541971262919,
      "grad_norm": 4.785409927368164,
      "learning_rate": 6.128006054151017e-05,
      "loss": 0.1016,
      "step": 27680
    },
    {
      "epoch": 1.551102148278856,
      "grad_norm": 8.089531898498535,
      "learning_rate": 6.12660463030439e-05,
      "loss": 0.1906,
      "step": 27690
    },
    {
      "epoch": 1.5516623252947932,
      "grad_norm": 2.946409225463867,
      "learning_rate": 6.125203206457761e-05,
      "loss": 0.1938,
      "step": 27700
    },
    {
      "epoch": 1.5522225023107303,
      "grad_norm": 4.028398513793945,
      "learning_rate": 6.123801782611133e-05,
      "loss": 0.2475,
      "step": 27710
    },
    {
      "epoch": 1.5527826793266672,
      "grad_norm": 5.708154678344727,
      "learning_rate": 6.122400358764506e-05,
      "loss": 0.1668,
      "step": 27720
    },
    {
      "epoch": 1.5533428563426042,
      "grad_norm": 4.423981189727783,
      "learning_rate": 6.120998934917877e-05,
      "loss": 0.2237,
      "step": 27730
    },
    {
      "epoch": 1.5539030333585413,
      "grad_norm": 3.0840673446655273,
      "learning_rate": 6.119597511071248e-05,
      "loss": 0.2051,
      "step": 27740
    },
    {
      "epoch": 1.5544632103744784,
      "grad_norm": 4.694064617156982,
      "learning_rate": 6.11819608722462e-05,
      "loss": 0.1618,
      "step": 27750
    },
    {
      "epoch": 1.5550233873904153,
      "grad_norm": 4.588138580322266,
      "learning_rate": 6.116794663377993e-05,
      "loss": 0.1395,
      "step": 27760
    },
    {
      "epoch": 1.5555835644063523,
      "grad_norm": 3.791398525238037,
      "learning_rate": 6.115393239531364e-05,
      "loss": 0.2277,
      "step": 27770
    },
    {
      "epoch": 1.5561437414222894,
      "grad_norm": 6.348287582397461,
      "learning_rate": 6.113991815684737e-05,
      "loss": 0.1933,
      "step": 27780
    },
    {
      "epoch": 1.5567039184382265,
      "grad_norm": 1.5190353393554688,
      "learning_rate": 6.112590391838107e-05,
      "loss": 0.1546,
      "step": 27790
    },
    {
      "epoch": 1.5572640954541637,
      "grad_norm": 2.8352248668670654,
      "learning_rate": 6.111188967991479e-05,
      "loss": 0.2261,
      "step": 27800
    },
    {
      "epoch": 1.5578242724701006,
      "grad_norm": 5.680578708648682,
      "learning_rate": 6.109787544144852e-05,
      "loss": 0.1971,
      "step": 27810
    },
    {
      "epoch": 1.5583844494860375,
      "grad_norm": 7.8726091384887695,
      "learning_rate": 6.108386120298223e-05,
      "loss": 0.2713,
      "step": 27820
    },
    {
      "epoch": 1.5589446265019746,
      "grad_norm": 3.7504382133483887,
      "learning_rate": 6.106984696451596e-05,
      "loss": 0.1445,
      "step": 27830
    },
    {
      "epoch": 1.5595048035179118,
      "grad_norm": 1.7213038206100464,
      "learning_rate": 6.105583272604967e-05,
      "loss": 0.2893,
      "step": 27840
    },
    {
      "epoch": 1.5600649805338487,
      "grad_norm": 4.841912746429443,
      "learning_rate": 6.104181848758339e-05,
      "loss": 0.1809,
      "step": 27850
    },
    {
      "epoch": 1.5606251575497856,
      "grad_norm": 3.340959310531616,
      "learning_rate": 6.10278042491171e-05,
      "loss": 0.2069,
      "step": 27860
    },
    {
      "epoch": 1.5611853345657227,
      "grad_norm": 4.975081443786621,
      "learning_rate": 6.101379001065083e-05,
      "loss": 0.1786,
      "step": 27870
    },
    {
      "epoch": 1.5617455115816599,
      "grad_norm": 5.031400680541992,
      "learning_rate": 6.0999775772184545e-05,
      "loss": 0.2099,
      "step": 27880
    },
    {
      "epoch": 1.562305688597597,
      "grad_norm": 3.1820411682128906,
      "learning_rate": 6.098576153371825e-05,
      "loss": 0.1687,
      "step": 27890
    },
    {
      "epoch": 1.562865865613534,
      "grad_norm": 4.200175762176514,
      "learning_rate": 6.097174729525198e-05,
      "loss": 0.2452,
      "step": 27900
    },
    {
      "epoch": 1.5634260426294708,
      "grad_norm": 4.034119606018066,
      "learning_rate": 6.0957733056785696e-05,
      "loss": 0.1511,
      "step": 27910
    },
    {
      "epoch": 1.563986219645408,
      "grad_norm": 1.5285009145736694,
      "learning_rate": 6.094371881831942e-05,
      "loss": 0.2094,
      "step": 27920
    },
    {
      "epoch": 1.564546396661345,
      "grad_norm": 3.9999356269836426,
      "learning_rate": 6.092970457985313e-05,
      "loss": 0.2021,
      "step": 27930
    },
    {
      "epoch": 1.565106573677282,
      "grad_norm": 3.3012125492095947,
      "learning_rate": 6.0915690341386854e-05,
      "loss": 0.2079,
      "step": 27940
    },
    {
      "epoch": 1.565666750693219,
      "grad_norm": 3.247418165206909,
      "learning_rate": 6.090167610292057e-05,
      "loss": 0.2034,
      "step": 27950
    },
    {
      "epoch": 1.566226927709156,
      "grad_norm": 5.3014702796936035,
      "learning_rate": 6.088766186445429e-05,
      "loss": 0.2207,
      "step": 27960
    },
    {
      "epoch": 1.5667871047250932,
      "grad_norm": 4.6758503913879395,
      "learning_rate": 6.0873647625988005e-05,
      "loss": 0.1826,
      "step": 27970
    },
    {
      "epoch": 1.5673472817410303,
      "grad_norm": 3.340125799179077,
      "learning_rate": 6.085963338752172e-05,
      "loss": 0.3834,
      "step": 27980
    },
    {
      "epoch": 1.5679074587569672,
      "grad_norm": 4.440649032592773,
      "learning_rate": 6.084561914905544e-05,
      "loss": 0.2485,
      "step": 27990
    },
    {
      "epoch": 1.5684676357729042,
      "grad_norm": 5.781813621520996,
      "learning_rate": 6.0831604910589156e-05,
      "loss": 0.1428,
      "step": 28000
    },
    {
      "epoch": 1.5690278127888413,
      "grad_norm": 7.5198163986206055,
      "learning_rate": 6.0817590672122884e-05,
      "loss": 0.1861,
      "step": 28010
    },
    {
      "epoch": 1.5695879898047784,
      "grad_norm": 1.1100146770477295,
      "learning_rate": 6.080357643365659e-05,
      "loss": 0.1278,
      "step": 28020
    },
    {
      "epoch": 1.5701481668207153,
      "grad_norm": 3.2249526977539062,
      "learning_rate": 6.078956219519032e-05,
      "loss": 0.1695,
      "step": 28030
    },
    {
      "epoch": 1.5707083438366523,
      "grad_norm": 5.501709461212158,
      "learning_rate": 6.0775547956724035e-05,
      "loss": 0.2177,
      "step": 28040
    },
    {
      "epoch": 1.5712685208525894,
      "grad_norm": 2.3122639656066895,
      "learning_rate": 6.076153371825776e-05,
      "loss": 0.1609,
      "step": 28050
    },
    {
      "epoch": 1.5718286978685265,
      "grad_norm": 6.188480377197266,
      "learning_rate": 6.074751947979147e-05,
      "loss": 0.2844,
      "step": 28060
    },
    {
      "epoch": 1.5723888748844634,
      "grad_norm": 3.4496114253997803,
      "learning_rate": 6.073350524132519e-05,
      "loss": 0.1675,
      "step": 28070
    },
    {
      "epoch": 1.5729490519004006,
      "grad_norm": 3.8777308464050293,
      "learning_rate": 6.071949100285891e-05,
      "loss": 0.1783,
      "step": 28080
    },
    {
      "epoch": 1.5735092289163375,
      "grad_norm": 4.375425338745117,
      "learning_rate": 6.070547676439262e-05,
      "loss": 0.1492,
      "step": 28090
    },
    {
      "epoch": 1.5740694059322746,
      "grad_norm": 4.802548885345459,
      "learning_rate": 6.0691462525926344e-05,
      "loss": 0.173,
      "step": 28100
    },
    {
      "epoch": 1.5746295829482118,
      "grad_norm": 1.4597363471984863,
      "learning_rate": 6.067744828746006e-05,
      "loss": 0.1851,
      "step": 28110
    },
    {
      "epoch": 1.5751897599641487,
      "grad_norm": 2.411611557006836,
      "learning_rate": 6.066343404899378e-05,
      "loss": 0.2079,
      "step": 28120
    },
    {
      "epoch": 1.5757499369800856,
      "grad_norm": 3.2286829948425293,
      "learning_rate": 6.0649419810527495e-05,
      "loss": 0.1688,
      "step": 28130
    },
    {
      "epoch": 1.5763101139960227,
      "grad_norm": 2.605319023132324,
      "learning_rate": 6.0635405572061224e-05,
      "loss": 0.2187,
      "step": 28140
    },
    {
      "epoch": 1.5768702910119599,
      "grad_norm": 2.6673319339752197,
      "learning_rate": 6.062139133359493e-05,
      "loss": 0.182,
      "step": 28150
    },
    {
      "epoch": 1.5774304680278968,
      "grad_norm": 3.4189131259918213,
      "learning_rate": 6.060737709512866e-05,
      "loss": 0.1798,
      "step": 28160
    },
    {
      "epoch": 1.577990645043834,
      "grad_norm": 3.8898909091949463,
      "learning_rate": 6.0593362856662375e-05,
      "loss": 0.1512,
      "step": 28170
    },
    {
      "epoch": 1.5785508220597708,
      "grad_norm": 2.865213632583618,
      "learning_rate": 6.057934861819608e-05,
      "loss": 0.2485,
      "step": 28180
    },
    {
      "epoch": 1.579110999075708,
      "grad_norm": 3.619072675704956,
      "learning_rate": 6.056533437972981e-05,
      "loss": 0.2822,
      "step": 28190
    },
    {
      "epoch": 1.579671176091645,
      "grad_norm": 3.2514097690582275,
      "learning_rate": 6.0551320141263526e-05,
      "loss": 0.2297,
      "step": 28200
    },
    {
      "epoch": 1.580231353107582,
      "grad_norm": 1.7236815690994263,
      "learning_rate": 6.053730590279725e-05,
      "loss": 0.1818,
      "step": 28210
    },
    {
      "epoch": 1.580791530123519,
      "grad_norm": 2.3660168647766113,
      "learning_rate": 6.052329166433096e-05,
      "loss": 0.16,
      "step": 28220
    },
    {
      "epoch": 1.581351707139456,
      "grad_norm": 5.4132843017578125,
      "learning_rate": 6.050927742586468e-05,
      "loss": 0.1426,
      "step": 28230
    },
    {
      "epoch": 1.5819118841553932,
      "grad_norm": 2.409301996231079,
      "learning_rate": 6.04952631873984e-05,
      "loss": 0.1797,
      "step": 28240
    },
    {
      "epoch": 1.58247206117133,
      "grad_norm": 1.3523790836334229,
      "learning_rate": 6.0481248948932126e-05,
      "loss": 0.1691,
      "step": 28250
    },
    {
      "epoch": 1.583032238187267,
      "grad_norm": 2.9187285900115967,
      "learning_rate": 6.0467234710465834e-05,
      "loss": 0.2304,
      "step": 28260
    },
    {
      "epoch": 1.5835924152032042,
      "grad_norm": 4.052648544311523,
      "learning_rate": 6.045322047199955e-05,
      "loss": 0.2358,
      "step": 28270
    },
    {
      "epoch": 1.5841525922191413,
      "grad_norm": 2.1680967807769775,
      "learning_rate": 6.043920623353328e-05,
      "loss": 0.1146,
      "step": 28280
    },
    {
      "epoch": 1.5847127692350784,
      "grad_norm": 4.0448737144470215,
      "learning_rate": 6.0425191995066985e-05,
      "loss": 0.1919,
      "step": 28290
    },
    {
      "epoch": 1.5852729462510153,
      "grad_norm": 4.511191368103027,
      "learning_rate": 6.0411177756600714e-05,
      "loss": 0.2025,
      "step": 28300
    },
    {
      "epoch": 1.5858331232669522,
      "grad_norm": 5.704319000244141,
      "learning_rate": 6.039716351813443e-05,
      "loss": 0.1773,
      "step": 28310
    },
    {
      "epoch": 1.5863933002828894,
      "grad_norm": 1.407021403312683,
      "learning_rate": 6.038314927966815e-05,
      "loss": 0.1777,
      "step": 28320
    },
    {
      "epoch": 1.5869534772988265,
      "grad_norm": 3.6204984188079834,
      "learning_rate": 6.0369135041201865e-05,
      "loss": 0.1704,
      "step": 28330
    },
    {
      "epoch": 1.5875136543147634,
      "grad_norm": 4.097997188568115,
      "learning_rate": 6.0355120802735586e-05,
      "loss": 0.2114,
      "step": 28340
    },
    {
      "epoch": 1.5880738313307003,
      "grad_norm": 4.873144149780273,
      "learning_rate": 6.03411065642693e-05,
      "loss": 0.1369,
      "step": 28350
    },
    {
      "epoch": 1.5886340083466375,
      "grad_norm": 2.9110310077667236,
      "learning_rate": 6.0327092325803016e-05,
      "loss": 0.2281,
      "step": 28360
    },
    {
      "epoch": 1.5891941853625746,
      "grad_norm": 1.71143639087677,
      "learning_rate": 6.031307808733674e-05,
      "loss": 0.1277,
      "step": 28370
    },
    {
      "epoch": 1.5897543623785118,
      "grad_norm": 6.396270751953125,
      "learning_rate": 6.029906384887045e-05,
      "loss": 0.2314,
      "step": 28380
    },
    {
      "epoch": 1.5903145393944487,
      "grad_norm": 1.0321321487426758,
      "learning_rate": 6.0285049610404173e-05,
      "loss": 0.1715,
      "step": 28390
    },
    {
      "epoch": 1.5908747164103856,
      "grad_norm": 3.241771697998047,
      "learning_rate": 6.027103537193789e-05,
      "loss": 0.1796,
      "step": 28400
    },
    {
      "epoch": 1.5914348934263227,
      "grad_norm": 1.4702494144439697,
      "learning_rate": 6.0257021133471616e-05,
      "loss": 0.1007,
      "step": 28410
    },
    {
      "epoch": 1.5919950704422599,
      "grad_norm": 4.114366054534912,
      "learning_rate": 6.0243006895005324e-05,
      "loss": 0.2104,
      "step": 28420
    },
    {
      "epoch": 1.5925552474581968,
      "grad_norm": 2.703097105026245,
      "learning_rate": 6.022899265653905e-05,
      "loss": 0.2315,
      "step": 28430
    },
    {
      "epoch": 1.5931154244741337,
      "grad_norm": 3.1966257095336914,
      "learning_rate": 6.021497841807277e-05,
      "loss": 0.2073,
      "step": 28440
    },
    {
      "epoch": 1.5936756014900708,
      "grad_norm": 3.5716042518615723,
      "learning_rate": 6.0200964179606475e-05,
      "loss": 0.2685,
      "step": 28450
    },
    {
      "epoch": 1.594235778506008,
      "grad_norm": 2.6467466354370117,
      "learning_rate": 6.0186949941140204e-05,
      "loss": 0.2499,
      "step": 28460
    },
    {
      "epoch": 1.594795955521945,
      "grad_norm": 3.263174295425415,
      "learning_rate": 6.017293570267392e-05,
      "loss": 0.1691,
      "step": 28470
    },
    {
      "epoch": 1.595356132537882,
      "grad_norm": 3.7866477966308594,
      "learning_rate": 6.015892146420764e-05,
      "loss": 0.1708,
      "step": 28480
    },
    {
      "epoch": 1.595916309553819,
      "grad_norm": 5.0490827560424805,
      "learning_rate": 6.0144907225741355e-05,
      "loss": 0.17,
      "step": 28490
    },
    {
      "epoch": 1.596476486569756,
      "grad_norm": 5.093950271606445,
      "learning_rate": 6.0130892987275076e-05,
      "loss": 0.1829,
      "step": 28500
    },
    {
      "epoch": 1.5970366635856932,
      "grad_norm": 1.9800629615783691,
      "learning_rate": 6.011687874880879e-05,
      "loss": 0.2117,
      "step": 28510
    },
    {
      "epoch": 1.59759684060163,
      "grad_norm": 1.6729931831359863,
      "learning_rate": 6.010286451034251e-05,
      "loss": 0.1589,
      "step": 28520
    },
    {
      "epoch": 1.598157017617567,
      "grad_norm": 5.3049821853637695,
      "learning_rate": 6.008885027187623e-05,
      "loss": 0.202,
      "step": 28530
    },
    {
      "epoch": 1.5987171946335041,
      "grad_norm": 1.2595207691192627,
      "learning_rate": 6.007483603340994e-05,
      "loss": 0.1513,
      "step": 28540
    },
    {
      "epoch": 1.5992773716494413,
      "grad_norm": 2.622735023498535,
      "learning_rate": 6.0060821794943664e-05,
      "loss": 0.1569,
      "step": 28550
    },
    {
      "epoch": 1.5998375486653784,
      "grad_norm": 2.3484673500061035,
      "learning_rate": 6.004680755647738e-05,
      "loss": 0.1492,
      "step": 28560
    },
    {
      "epoch": 1.6003977256813153,
      "grad_norm": 3.4048922061920166,
      "learning_rate": 6.0032793318011107e-05,
      "loss": 0.1435,
      "step": 28570
    },
    {
      "epoch": 1.6009579026972522,
      "grad_norm": 4.530910968780518,
      "learning_rate": 6.0018779079544815e-05,
      "loss": 0.2127,
      "step": 28580
    },
    {
      "epoch": 1.6015180797131894,
      "grad_norm": 5.043306827545166,
      "learning_rate": 6.000476484107854e-05,
      "loss": 0.163,
      "step": 28590
    },
    {
      "epoch": 1.6020782567291265,
      "grad_norm": 4.301031589508057,
      "learning_rate": 5.999075060261226e-05,
      "loss": 0.1845,
      "step": 28600
    },
    {
      "epoch": 1.6026384337450634,
      "grad_norm": 6.387328624725342,
      "learning_rate": 5.997673636414598e-05,
      "loss": 0.18,
      "step": 28610
    },
    {
      "epoch": 1.6031986107610003,
      "grad_norm": 2.584698438644409,
      "learning_rate": 5.9962722125679694e-05,
      "loss": 0.2128,
      "step": 28620
    },
    {
      "epoch": 1.6037587877769375,
      "grad_norm": 5.014176368713379,
      "learning_rate": 5.994870788721341e-05,
      "loss": 0.1806,
      "step": 28630
    },
    {
      "epoch": 1.6043189647928746,
      "grad_norm": 1.5157302618026733,
      "learning_rate": 5.993469364874713e-05,
      "loss": 0.125,
      "step": 28640
    },
    {
      "epoch": 1.6048791418088117,
      "grad_norm": 1.7647982835769653,
      "learning_rate": 5.9920679410280845e-05,
      "loss": 0.1606,
      "step": 28650
    },
    {
      "epoch": 1.6054393188247487,
      "grad_norm": 4.679053783416748,
      "learning_rate": 5.9906665171814566e-05,
      "loss": 0.1914,
      "step": 28660
    },
    {
      "epoch": 1.6059994958406856,
      "grad_norm": 2.1766293048858643,
      "learning_rate": 5.989265093334828e-05,
      "loss": 0.1188,
      "step": 28670
    },
    {
      "epoch": 1.6065596728566227,
      "grad_norm": 3.191532850265503,
      "learning_rate": 5.987863669488201e-05,
      "loss": 0.1796,
      "step": 28680
    },
    {
      "epoch": 1.6071198498725598,
      "grad_norm": 5.228457450866699,
      "learning_rate": 5.986462245641572e-05,
      "loss": 0.1825,
      "step": 28690
    },
    {
      "epoch": 1.6076800268884968,
      "grad_norm": 2.7021195888519287,
      "learning_rate": 5.9850608217949446e-05,
      "loss": 0.1595,
      "step": 28700
    },
    {
      "epoch": 1.6082402039044337,
      "grad_norm": 2.077871084213257,
      "learning_rate": 5.983659397948316e-05,
      "loss": 0.1134,
      "step": 28710
    },
    {
      "epoch": 1.6088003809203708,
      "grad_norm": 7.926705360412598,
      "learning_rate": 5.982257974101687e-05,
      "loss": 0.1433,
      "step": 28720
    },
    {
      "epoch": 1.609360557936308,
      "grad_norm": 2.7486608028411865,
      "learning_rate": 5.98085655025506e-05,
      "loss": 0.1464,
      "step": 28730
    },
    {
      "epoch": 1.6099207349522449,
      "grad_norm": 5.574065208435059,
      "learning_rate": 5.979455126408431e-05,
      "loss": 0.2841,
      "step": 28740
    },
    {
      "epoch": 1.610480911968182,
      "grad_norm": 2.156642198562622,
      "learning_rate": 5.978053702561803e-05,
      "loss": 0.1757,
      "step": 28750
    },
    {
      "epoch": 1.611041088984119,
      "grad_norm": 4.991570949554443,
      "learning_rate": 5.976652278715175e-05,
      "loss": 0.1885,
      "step": 28760
    },
    {
      "epoch": 1.611601266000056,
      "grad_norm": 1.7223691940307617,
      "learning_rate": 5.975250854868547e-05,
      "loss": 0.1312,
      "step": 28770
    },
    {
      "epoch": 1.6121614430159932,
      "grad_norm": 2.361581325531006,
      "learning_rate": 5.9738494310219184e-05,
      "loss": 0.1824,
      "step": 28780
    },
    {
      "epoch": 1.61272162003193,
      "grad_norm": 3.2461674213409424,
      "learning_rate": 5.9724480071752905e-05,
      "loss": 0.1513,
      "step": 28790
    },
    {
      "epoch": 1.613281797047867,
      "grad_norm": 1.8510717153549194,
      "learning_rate": 5.971046583328662e-05,
      "loss": 0.159,
      "step": 28800
    },
    {
      "epoch": 1.6138419740638041,
      "grad_norm": 2.0432136058807373,
      "learning_rate": 5.9696451594820335e-05,
      "loss": 0.142,
      "step": 28810
    },
    {
      "epoch": 1.6144021510797413,
      "grad_norm": 3.8634040355682373,
      "learning_rate": 5.9682437356354056e-05,
      "loss": 0.1942,
      "step": 28820
    },
    {
      "epoch": 1.6149623280956782,
      "grad_norm": 1.8612613677978516,
      "learning_rate": 5.966842311788777e-05,
      "loss": 0.1515,
      "step": 28830
    },
    {
      "epoch": 1.6155225051116153,
      "grad_norm": 2.22210431098938,
      "learning_rate": 5.96544088794215e-05,
      "loss": 0.1905,
      "step": 28840
    },
    {
      "epoch": 1.6160826821275522,
      "grad_norm": 5.632792949676514,
      "learning_rate": 5.964039464095521e-05,
      "loss": 0.1999,
      "step": 28850
    },
    {
      "epoch": 1.6166428591434894,
      "grad_norm": 5.636044502258301,
      "learning_rate": 5.9626380402488936e-05,
      "loss": 0.1901,
      "step": 28860
    },
    {
      "epoch": 1.6172030361594265,
      "grad_norm": 1.0206795930862427,
      "learning_rate": 5.961236616402265e-05,
      "loss": 0.2011,
      "step": 28870
    },
    {
      "epoch": 1.6177632131753634,
      "grad_norm": 5.770604133605957,
      "learning_rate": 5.959835192555637e-05,
      "loss": 0.1635,
      "step": 28880
    },
    {
      "epoch": 1.6183233901913003,
      "grad_norm": 4.6904988288879395,
      "learning_rate": 5.958433768709009e-05,
      "loss": 0.2664,
      "step": 28890
    },
    {
      "epoch": 1.6188835672072375,
      "grad_norm": 2.7915139198303223,
      "learning_rate": 5.95703234486238e-05,
      "loss": 0.2587,
      "step": 28900
    },
    {
      "epoch": 1.6194437442231746,
      "grad_norm": 4.893941402435303,
      "learning_rate": 5.955630921015752e-05,
      "loss": 0.1373,
      "step": 28910
    },
    {
      "epoch": 1.6200039212391115,
      "grad_norm": 1.995727300643921,
      "learning_rate": 5.954229497169124e-05,
      "loss": 0.2637,
      "step": 28920
    },
    {
      "epoch": 1.6205640982550484,
      "grad_norm": 7.498562335968018,
      "learning_rate": 5.952828073322496e-05,
      "loss": 0.1745,
      "step": 28930
    },
    {
      "epoch": 1.6211242752709856,
      "grad_norm": 4.182431697845459,
      "learning_rate": 5.9514266494758674e-05,
      "loss": 0.1892,
      "step": 28940
    },
    {
      "epoch": 1.6216844522869227,
      "grad_norm": 1.5221260786056519,
      "learning_rate": 5.95002522562924e-05,
      "loss": 0.1142,
      "step": 28950
    },
    {
      "epoch": 1.6222446293028598,
      "grad_norm": 3.721421957015991,
      "learning_rate": 5.948623801782611e-05,
      "loss": 0.1332,
      "step": 28960
    },
    {
      "epoch": 1.6228048063187968,
      "grad_norm": 1.7333409786224365,
      "learning_rate": 5.947222377935984e-05,
      "loss": 0.1265,
      "step": 28970
    },
    {
      "epoch": 1.6233649833347337,
      "grad_norm": 2.5332021713256836,
      "learning_rate": 5.945820954089355e-05,
      "loss": 0.1832,
      "step": 28980
    },
    {
      "epoch": 1.6239251603506708,
      "grad_norm": 1.1915138959884644,
      "learning_rate": 5.944419530242726e-05,
      "loss": 0.1512,
      "step": 28990
    },
    {
      "epoch": 1.624485337366608,
      "grad_norm": 1.3081022500991821,
      "learning_rate": 5.943018106396099e-05,
      "loss": 0.2032,
      "step": 29000
    },
    {
      "epoch": 1.6250455143825449,
      "grad_norm": 3.666809320449829,
      "learning_rate": 5.9416166825494704e-05,
      "loss": 0.1663,
      "step": 29010
    },
    {
      "epoch": 1.6256056913984818,
      "grad_norm": 5.574570178985596,
      "learning_rate": 5.9402152587028426e-05,
      "loss": 0.2254,
      "step": 29020
    },
    {
      "epoch": 1.626165868414419,
      "grad_norm": 3.595958948135376,
      "learning_rate": 5.938813834856214e-05,
      "loss": 0.1761,
      "step": 29030
    },
    {
      "epoch": 1.626726045430356,
      "grad_norm": 3.110926628112793,
      "learning_rate": 5.937412411009586e-05,
      "loss": 0.1298,
      "step": 29040
    },
    {
      "epoch": 1.6272862224462932,
      "grad_norm": 2.582850694656372,
      "learning_rate": 5.936010987162958e-05,
      "loss": 0.2362,
      "step": 29050
    },
    {
      "epoch": 1.62784639946223,
      "grad_norm": 2.77008056640625,
      "learning_rate": 5.93460956331633e-05,
      "loss": 0.1423,
      "step": 29060
    },
    {
      "epoch": 1.628406576478167,
      "grad_norm": 3.160299062728882,
      "learning_rate": 5.933208139469701e-05,
      "loss": 0.1902,
      "step": 29070
    },
    {
      "epoch": 1.6289667534941041,
      "grad_norm": 0.9427784085273743,
      "learning_rate": 5.931806715623073e-05,
      "loss": 0.2052,
      "step": 29080
    },
    {
      "epoch": 1.6295269305100413,
      "grad_norm": 4.701667785644531,
      "learning_rate": 5.930405291776445e-05,
      "loss": 0.1677,
      "step": 29090
    },
    {
      "epoch": 1.6300871075259782,
      "grad_norm": 6.035212993621826,
      "learning_rate": 5.9290038679298164e-05,
      "loss": 0.1901,
      "step": 29100
    },
    {
      "epoch": 1.630647284541915,
      "grad_norm": 2.4837515354156494,
      "learning_rate": 5.927602444083189e-05,
      "loss": 0.2068,
      "step": 29110
    },
    {
      "epoch": 1.6312074615578522,
      "grad_norm": 2.366013288497925,
      "learning_rate": 5.92620102023656e-05,
      "loss": 0.1396,
      "step": 29120
    },
    {
      "epoch": 1.6317676385737894,
      "grad_norm": 1.9438385963439941,
      "learning_rate": 5.924799596389933e-05,
      "loss": 0.1414,
      "step": 29130
    },
    {
      "epoch": 1.6323278155897265,
      "grad_norm": 1.6386772394180298,
      "learning_rate": 5.9233981725433043e-05,
      "loss": 0.2603,
      "step": 29140
    },
    {
      "epoch": 1.6328879926056634,
      "grad_norm": 7.524159908294678,
      "learning_rate": 5.9219967486966765e-05,
      "loss": 0.1536,
      "step": 29150
    },
    {
      "epoch": 1.6334481696216003,
      "grad_norm": 2.2820448875427246,
      "learning_rate": 5.920595324850048e-05,
      "loss": 0.2238,
      "step": 29160
    },
    {
      "epoch": 1.6340083466375375,
      "grad_norm": 2.022833824157715,
      "learning_rate": 5.91919390100342e-05,
      "loss": 0.2139,
      "step": 29170
    },
    {
      "epoch": 1.6345685236534746,
      "grad_norm": 4.678610801696777,
      "learning_rate": 5.9177924771567916e-05,
      "loss": 0.1689,
      "step": 29180
    },
    {
      "epoch": 1.6351287006694115,
      "grad_norm": 4.596405029296875,
      "learning_rate": 5.916391053310163e-05,
      "loss": 0.157,
      "step": 29190
    },
    {
      "epoch": 1.6356888776853484,
      "grad_norm": 5.045727252960205,
      "learning_rate": 5.914989629463535e-05,
      "loss": 0.2041,
      "step": 29200
    },
    {
      "epoch": 1.6362490547012856,
      "grad_norm": 1.4785304069519043,
      "learning_rate": 5.913588205616907e-05,
      "loss": 0.1432,
      "step": 29210
    },
    {
      "epoch": 1.6368092317172227,
      "grad_norm": 3.5352537631988525,
      "learning_rate": 5.912186781770279e-05,
      "loss": 0.1955,
      "step": 29220
    },
    {
      "epoch": 1.6373694087331598,
      "grad_norm": 3.7512943744659424,
      "learning_rate": 5.91078535792365e-05,
      "loss": 0.2115,
      "step": 29230
    },
    {
      "epoch": 1.6379295857490968,
      "grad_norm": 5.420690059661865,
      "learning_rate": 5.909383934077023e-05,
      "loss": 0.2098,
      "step": 29240
    },
    {
      "epoch": 1.6384897627650337,
      "grad_norm": 2.7167482376098633,
      "learning_rate": 5.907982510230394e-05,
      "loss": 0.1356,
      "step": 29250
    },
    {
      "epoch": 1.6390499397809708,
      "grad_norm": 1.087867259979248,
      "learning_rate": 5.906581086383767e-05,
      "loss": 0.1454,
      "step": 29260
    },
    {
      "epoch": 1.639610116796908,
      "grad_norm": 3.5433459281921387,
      "learning_rate": 5.905179662537138e-05,
      "loss": 0.1605,
      "step": 29270
    },
    {
      "epoch": 1.6401702938128448,
      "grad_norm": 1.121443510055542,
      "learning_rate": 5.903778238690509e-05,
      "loss": 0.1275,
      "step": 29280
    },
    {
      "epoch": 1.6407304708287818,
      "grad_norm": 4.861131191253662,
      "learning_rate": 5.902376814843882e-05,
      "loss": 0.1277,
      "step": 29290
    },
    {
      "epoch": 1.641290647844719,
      "grad_norm": 3.9183177947998047,
      "learning_rate": 5.9009753909972534e-05,
      "loss": 0.1921,
      "step": 29300
    },
    {
      "epoch": 1.641850824860656,
      "grad_norm": 4.959932804107666,
      "learning_rate": 5.8995739671506255e-05,
      "loss": 0.1746,
      "step": 29310
    },
    {
      "epoch": 1.6424110018765932,
      "grad_norm": 4.017457485198975,
      "learning_rate": 5.898172543303997e-05,
      "loss": 0.1745,
      "step": 29320
    },
    {
      "epoch": 1.64297117889253,
      "grad_norm": 4.115612030029297,
      "learning_rate": 5.896771119457369e-05,
      "loss": 0.1343,
      "step": 29330
    },
    {
      "epoch": 1.643531355908467,
      "grad_norm": 3.0943331718444824,
      "learning_rate": 5.8953696956107406e-05,
      "loss": 0.2349,
      "step": 29340
    },
    {
      "epoch": 1.6440915329244041,
      "grad_norm": 6.6015095710754395,
      "learning_rate": 5.8939682717641134e-05,
      "loss": 0.119,
      "step": 29350
    },
    {
      "epoch": 1.6446517099403413,
      "grad_norm": 6.714932918548584,
      "learning_rate": 5.892566847917484e-05,
      "loss": 0.1942,
      "step": 29360
    },
    {
      "epoch": 1.6452118869562782,
      "grad_norm": 4.270575523376465,
      "learning_rate": 5.891165424070856e-05,
      "loss": 0.2699,
      "step": 29370
    },
    {
      "epoch": 1.645772063972215,
      "grad_norm": 3.047206401824951,
      "learning_rate": 5.8897640002242285e-05,
      "loss": 0.1778,
      "step": 29380
    },
    {
      "epoch": 1.6463322409881522,
      "grad_norm": 4.568303108215332,
      "learning_rate": 5.888362576377599e-05,
      "loss": 0.1847,
      "step": 29390
    },
    {
      "epoch": 1.6468924180040894,
      "grad_norm": 4.076613426208496,
      "learning_rate": 5.886961152530972e-05,
      "loss": 0.1906,
      "step": 29400
    },
    {
      "epoch": 1.6474525950200263,
      "grad_norm": 3.3463010787963867,
      "learning_rate": 5.8855597286843436e-05,
      "loss": 0.1404,
      "step": 29410
    },
    {
      "epoch": 1.6480127720359634,
      "grad_norm": 2.821401596069336,
      "learning_rate": 5.884158304837716e-05,
      "loss": 0.1277,
      "step": 29420
    },
    {
      "epoch": 1.6485729490519003,
      "grad_norm": 4.1692728996276855,
      "learning_rate": 5.882756880991087e-05,
      "loss": 0.2507,
      "step": 29430
    },
    {
      "epoch": 1.6491331260678375,
      "grad_norm": 3.7897210121154785,
      "learning_rate": 5.8813554571444594e-05,
      "loss": 0.1569,
      "step": 29440
    },
    {
      "epoch": 1.6496933030837746,
      "grad_norm": 1.1011322736740112,
      "learning_rate": 5.879954033297831e-05,
      "loss": 0.1432,
      "step": 29450
    },
    {
      "epoch": 1.6502534800997115,
      "grad_norm": 4.875851154327393,
      "learning_rate": 5.8785526094512024e-05,
      "loss": 0.1663,
      "step": 29460
    },
    {
      "epoch": 1.6508136571156484,
      "grad_norm": 4.433757305145264,
      "learning_rate": 5.8771511856045745e-05,
      "loss": 0.1617,
      "step": 29470
    },
    {
      "epoch": 1.6513738341315856,
      "grad_norm": 4.924721717834473,
      "learning_rate": 5.875749761757946e-05,
      "loss": 0.1614,
      "step": 29480
    },
    {
      "epoch": 1.6519340111475227,
      "grad_norm": 0.9585012197494507,
      "learning_rate": 5.874348337911318e-05,
      "loss": 0.2483,
      "step": 29490
    },
    {
      "epoch": 1.6524941881634596,
      "grad_norm": 2.6624794006347656,
      "learning_rate": 5.8729469140646896e-05,
      "loss": 0.1993,
      "step": 29500
    },
    {
      "epoch": 1.6530543651793965,
      "grad_norm": 5.516581058502197,
      "learning_rate": 5.8715454902180624e-05,
      "loss": 0.233,
      "step": 29510
    },
    {
      "epoch": 1.6536145421953337,
      "grad_norm": 6.465648651123047,
      "learning_rate": 5.870144066371433e-05,
      "loss": 0.2331,
      "step": 29520
    },
    {
      "epoch": 1.6541747192112708,
      "grad_norm": 1.6682907342910767,
      "learning_rate": 5.868742642524806e-05,
      "loss": 0.1276,
      "step": 29530
    },
    {
      "epoch": 1.654734896227208,
      "grad_norm": 3.4182708263397217,
      "learning_rate": 5.8673412186781775e-05,
      "loss": 0.1568,
      "step": 29540
    },
    {
      "epoch": 1.6552950732431448,
      "grad_norm": 3.9237873554229736,
      "learning_rate": 5.8659397948315483e-05,
      "loss": 0.1879,
      "step": 29550
    },
    {
      "epoch": 1.6558552502590818,
      "grad_norm": 2.875505208969116,
      "learning_rate": 5.864538370984921e-05,
      "loss": 0.145,
      "step": 29560
    },
    {
      "epoch": 1.656415427275019,
      "grad_norm": 2.9792532920837402,
      "learning_rate": 5.8631369471382926e-05,
      "loss": 0.2264,
      "step": 29570
    },
    {
      "epoch": 1.656975604290956,
      "grad_norm": 2.0013253688812256,
      "learning_rate": 5.861735523291665e-05,
      "loss": 0.1432,
      "step": 29580
    },
    {
      "epoch": 1.657535781306893,
      "grad_norm": 2.230125904083252,
      "learning_rate": 5.860334099445036e-05,
      "loss": 0.1542,
      "step": 29590
    },
    {
      "epoch": 1.6580959583228299,
      "grad_norm": 3.383037567138672,
      "learning_rate": 5.8589326755984084e-05,
      "loss": 0.1463,
      "step": 29600
    },
    {
      "epoch": 1.658656135338767,
      "grad_norm": 2.9498190879821777,
      "learning_rate": 5.85753125175178e-05,
      "loss": 0.1773,
      "step": 29610
    },
    {
      "epoch": 1.6592163123547041,
      "grad_norm": 3.7990801334381104,
      "learning_rate": 5.856129827905153e-05,
      "loss": 0.3629,
      "step": 29620
    },
    {
      "epoch": 1.6597764893706413,
      "grad_norm": 1.9956214427947998,
      "learning_rate": 5.8547284040585235e-05,
      "loss": 0.1528,
      "step": 29630
    },
    {
      "epoch": 1.6603366663865782,
      "grad_norm": 1.7699296474456787,
      "learning_rate": 5.853326980211895e-05,
      "loss": 0.1762,
      "step": 29640
    },
    {
      "epoch": 1.660896843402515,
      "grad_norm": 1.2079086303710938,
      "learning_rate": 5.851925556365268e-05,
      "loss": 0.2214,
      "step": 29650
    },
    {
      "epoch": 1.6614570204184522,
      "grad_norm": 2.448927640914917,
      "learning_rate": 5.8505241325186386e-05,
      "loss": 0.2147,
      "step": 29660
    },
    {
      "epoch": 1.6620171974343894,
      "grad_norm": 6.341246604919434,
      "learning_rate": 5.8491227086720115e-05,
      "loss": 0.1964,
      "step": 29670
    },
    {
      "epoch": 1.6625773744503263,
      "grad_norm": 12.375382423400879,
      "learning_rate": 5.847721284825383e-05,
      "loss": 0.1614,
      "step": 29680
    },
    {
      "epoch": 1.6631375514662632,
      "grad_norm": 4.6314897537231445,
      "learning_rate": 5.846319860978755e-05,
      "loss": 0.1396,
      "step": 29690
    },
    {
      "epoch": 1.6636977284822003,
      "grad_norm": 2.0752973556518555,
      "learning_rate": 5.8449184371321266e-05,
      "loss": 0.1236,
      "step": 29700
    },
    {
      "epoch": 1.6642579054981375,
      "grad_norm": 3.3909876346588135,
      "learning_rate": 5.843517013285499e-05,
      "loss": 0.1908,
      "step": 29710
    },
    {
      "epoch": 1.6648180825140746,
      "grad_norm": 3.1882503032684326,
      "learning_rate": 5.84211558943887e-05,
      "loss": 0.1328,
      "step": 29720
    },
    {
      "epoch": 1.6653782595300115,
      "grad_norm": 3.7834651470184326,
      "learning_rate": 5.8407141655922417e-05,
      "loss": 0.1965,
      "step": 29730
    },
    {
      "epoch": 1.6659384365459484,
      "grad_norm": 2.7538561820983887,
      "learning_rate": 5.839312741745614e-05,
      "loss": 0.1857,
      "step": 29740
    },
    {
      "epoch": 1.6664986135618856,
      "grad_norm": 2.1498160362243652,
      "learning_rate": 5.837911317898985e-05,
      "loss": 0.1869,
      "step": 29750
    },
    {
      "epoch": 1.6670587905778227,
      "grad_norm": 2.219574451446533,
      "learning_rate": 5.8365098940523574e-05,
      "loss": 0.1116,
      "step": 29760
    },
    {
      "epoch": 1.6676189675937596,
      "grad_norm": 3.5631542205810547,
      "learning_rate": 5.835108470205729e-05,
      "loss": 0.1448,
      "step": 29770
    },
    {
      "epoch": 1.6681791446096965,
      "grad_norm": 3.5241777896881104,
      "learning_rate": 5.833707046359102e-05,
      "loss": 0.1489,
      "step": 29780
    },
    {
      "epoch": 1.6687393216256337,
      "grad_norm": 1.6467024087905884,
      "learning_rate": 5.8323056225124725e-05,
      "loss": 0.1576,
      "step": 29790
    },
    {
      "epoch": 1.6692994986415708,
      "grad_norm": 2.8919546604156494,
      "learning_rate": 5.8309041986658454e-05,
      "loss": 0.136,
      "step": 29800
    },
    {
      "epoch": 1.669859675657508,
      "grad_norm": 4.394136905670166,
      "learning_rate": 5.829502774819217e-05,
      "loss": 0.2521,
      "step": 29810
    },
    {
      "epoch": 1.6704198526734448,
      "grad_norm": 4.644876956939697,
      "learning_rate": 5.8281013509725876e-05,
      "loss": 0.1836,
      "step": 29820
    },
    {
      "epoch": 1.6709800296893818,
      "grad_norm": 0.8978593349456787,
      "learning_rate": 5.8266999271259605e-05,
      "loss": 0.0911,
      "step": 29830
    },
    {
      "epoch": 1.6715402067053189,
      "grad_norm": 4.198788166046143,
      "learning_rate": 5.825298503279332e-05,
      "loss": 0.2428,
      "step": 29840
    },
    {
      "epoch": 1.672100383721256,
      "grad_norm": 3.344512939453125,
      "learning_rate": 5.823897079432704e-05,
      "loss": 0.1524,
      "step": 29850
    },
    {
      "epoch": 1.672660560737193,
      "grad_norm": 4.765718936920166,
      "learning_rate": 5.8224956555860756e-05,
      "loss": 0.2383,
      "step": 29860
    },
    {
      "epoch": 1.6732207377531298,
      "grad_norm": 1.3541958332061768,
      "learning_rate": 5.821094231739448e-05,
      "loss": 0.1891,
      "step": 29870
    },
    {
      "epoch": 1.673780914769067,
      "grad_norm": 1.7836716175079346,
      "learning_rate": 5.819692807892819e-05,
      "loss": 0.1297,
      "step": 29880
    },
    {
      "epoch": 1.6743410917850041,
      "grad_norm": 4.828663349151611,
      "learning_rate": 5.8182913840461913e-05,
      "loss": 0.1967,
      "step": 29890
    },
    {
      "epoch": 1.6749012688009413,
      "grad_norm": 2.666327714920044,
      "learning_rate": 5.816889960199563e-05,
      "loss": 0.126,
      "step": 29900
    },
    {
      "epoch": 1.6754614458168782,
      "grad_norm": 2.568345308303833,
      "learning_rate": 5.815488536352934e-05,
      "loss": 0.2361,
      "step": 29910
    },
    {
      "epoch": 1.676021622832815,
      "grad_norm": 5.681777000427246,
      "learning_rate": 5.8140871125063064e-05,
      "loss": 0.2309,
      "step": 29920
    },
    {
      "epoch": 1.6765817998487522,
      "grad_norm": 3.118652105331421,
      "learning_rate": 5.812685688659678e-05,
      "loss": 0.1885,
      "step": 29930
    },
    {
      "epoch": 1.6771419768646894,
      "grad_norm": 6.750882148742676,
      "learning_rate": 5.811284264813051e-05,
      "loss": 0.2475,
      "step": 29940
    },
    {
      "epoch": 1.6777021538806263,
      "grad_norm": 1.8819310665130615,
      "learning_rate": 5.8098828409664215e-05,
      "loss": 0.1441,
      "step": 29950
    },
    {
      "epoch": 1.6782623308965632,
      "grad_norm": 5.16327428817749,
      "learning_rate": 5.8084814171197944e-05,
      "loss": 0.2695,
      "step": 29960
    },
    {
      "epoch": 1.6788225079125003,
      "grad_norm": 2.5685126781463623,
      "learning_rate": 5.807079993273166e-05,
      "loss": 0.1224,
      "step": 29970
    },
    {
      "epoch": 1.6793826849284375,
      "grad_norm": 4.763306617736816,
      "learning_rate": 5.805678569426538e-05,
      "loss": 0.1139,
      "step": 29980
    },
    {
      "epoch": 1.6799428619443746,
      "grad_norm": 2.763946056365967,
      "learning_rate": 5.8042771455799095e-05,
      "loss": 0.2027,
      "step": 29990
    },
    {
      "epoch": 1.6805030389603115,
      "grad_norm": 4.654021739959717,
      "learning_rate": 5.802875721733281e-05,
      "loss": 0.1753,
      "step": 30000
    },
    {
      "epoch": 1.6810632159762484,
      "grad_norm": 4.487274169921875,
      "learning_rate": 5.801474297886653e-05,
      "loss": 0.1534,
      "step": 30010
    },
    {
      "epoch": 1.6816233929921856,
      "grad_norm": 3.533737897872925,
      "learning_rate": 5.8000728740400246e-05,
      "loss": 0.1478,
      "step": 30020
    },
    {
      "epoch": 1.6821835700081227,
      "grad_norm": 7.13553524017334,
      "learning_rate": 5.798671450193397e-05,
      "loss": 0.1129,
      "step": 30030
    },
    {
      "epoch": 1.6827437470240596,
      "grad_norm": 5.556312084197998,
      "learning_rate": 5.797270026346768e-05,
      "loss": 0.128,
      "step": 30040
    },
    {
      "epoch": 1.6833039240399965,
      "grad_norm": 4.319295406341553,
      "learning_rate": 5.795868602500141e-05,
      "loss": 0.1236,
      "step": 30050
    },
    {
      "epoch": 1.6838641010559336,
      "grad_norm": 2.780775308609009,
      "learning_rate": 5.794467178653512e-05,
      "loss": 0.1988,
      "step": 30060
    },
    {
      "epoch": 1.6844242780718708,
      "grad_norm": 4.135161876678467,
      "learning_rate": 5.7930657548068847e-05,
      "loss": 0.1588,
      "step": 30070
    },
    {
      "epoch": 1.6849844550878077,
      "grad_norm": 4.666834354400635,
      "learning_rate": 5.791664330960256e-05,
      "loss": 0.1958,
      "step": 30080
    },
    {
      "epoch": 1.6855446321037448,
      "grad_norm": 3.675445556640625,
      "learning_rate": 5.790262907113627e-05,
      "loss": 0.1745,
      "step": 30090
    },
    {
      "epoch": 1.6861048091196817,
      "grad_norm": 1.109494686126709,
      "learning_rate": 5.788861483267e-05,
      "loss": 0.1908,
      "step": 30100
    },
    {
      "epoch": 1.6866649861356189,
      "grad_norm": 5.153343677520752,
      "learning_rate": 5.787460059420371e-05,
      "loss": 0.1683,
      "step": 30110
    },
    {
      "epoch": 1.687225163151556,
      "grad_norm": 2.8385119438171387,
      "learning_rate": 5.7860586355737434e-05,
      "loss": 0.193,
      "step": 30120
    },
    {
      "epoch": 1.687785340167493,
      "grad_norm": 2.920220136642456,
      "learning_rate": 5.784657211727115e-05,
      "loss": 0.1325,
      "step": 30130
    },
    {
      "epoch": 1.6883455171834298,
      "grad_norm": 0.9989436864852905,
      "learning_rate": 5.783255787880487e-05,
      "loss": 0.1401,
      "step": 30140
    },
    {
      "epoch": 1.688905694199367,
      "grad_norm": 5.364210605621338,
      "learning_rate": 5.7818543640338585e-05,
      "loss": 0.1941,
      "step": 30150
    },
    {
      "epoch": 1.6894658712153041,
      "grad_norm": 4.2056097984313965,
      "learning_rate": 5.7804529401872306e-05,
      "loss": 0.1609,
      "step": 30160
    },
    {
      "epoch": 1.690026048231241,
      "grad_norm": 4.645619869232178,
      "learning_rate": 5.779051516340602e-05,
      "loss": 0.1217,
      "step": 30170
    },
    {
      "epoch": 1.690586225247178,
      "grad_norm": 2.792459726333618,
      "learning_rate": 5.777650092493975e-05,
      "loss": 0.1904,
      "step": 30180
    },
    {
      "epoch": 1.691146402263115,
      "grad_norm": 2.0718228816986084,
      "learning_rate": 5.776248668647346e-05,
      "loss": 0.1845,
      "step": 30190
    },
    {
      "epoch": 1.6917065792790522,
      "grad_norm": 1.5574054718017578,
      "learning_rate": 5.774847244800717e-05,
      "loss": 0.1631,
      "step": 30200
    },
    {
      "epoch": 1.6922667562949893,
      "grad_norm": 1.6723179817199707,
      "learning_rate": 5.77344582095409e-05,
      "loss": 0.1626,
      "step": 30210
    },
    {
      "epoch": 1.6928269333109263,
      "grad_norm": 1.832242488861084,
      "learning_rate": 5.772044397107461e-05,
      "loss": 0.1367,
      "step": 30220
    },
    {
      "epoch": 1.6933871103268632,
      "grad_norm": 4.171330451965332,
      "learning_rate": 5.770642973260834e-05,
      "loss": 0.1642,
      "step": 30230
    },
    {
      "epoch": 1.6939472873428003,
      "grad_norm": 3.299534797668457,
      "learning_rate": 5.769241549414205e-05,
      "loss": 0.158,
      "step": 30240
    },
    {
      "epoch": 1.6945074643587374,
      "grad_norm": 5.768553733825684,
      "learning_rate": 5.767840125567577e-05,
      "loss": 0.208,
      "step": 30250
    },
    {
      "epoch": 1.6950676413746744,
      "grad_norm": 3.0473828315734863,
      "learning_rate": 5.766438701720949e-05,
      "loss": 0.1404,
      "step": 30260
    },
    {
      "epoch": 1.6956278183906113,
      "grad_norm": 3.6157169342041016,
      "learning_rate": 5.765037277874321e-05,
      "loss": 0.1963,
      "step": 30270
    },
    {
      "epoch": 1.6961879954065484,
      "grad_norm": 4.086698532104492,
      "learning_rate": 5.7636358540276924e-05,
      "loss": 0.184,
      "step": 30280
    },
    {
      "epoch": 1.6967481724224855,
      "grad_norm": 3.9615604877471924,
      "learning_rate": 5.762234430181064e-05,
      "loss": 0.1541,
      "step": 30290
    },
    {
      "epoch": 1.6973083494384227,
      "grad_norm": 0.7990846037864685,
      "learning_rate": 5.760833006334436e-05,
      "loss": 0.1508,
      "step": 30300
    },
    {
      "epoch": 1.6978685264543596,
      "grad_norm": 2.3192379474639893,
      "learning_rate": 5.7594315824878075e-05,
      "loss": 0.1799,
      "step": 30310
    },
    {
      "epoch": 1.6984287034702965,
      "grad_norm": 3.864006280899048,
      "learning_rate": 5.75803015864118e-05,
      "loss": 0.2273,
      "step": 30320
    },
    {
      "epoch": 1.6989888804862336,
      "grad_norm": 2.885857343673706,
      "learning_rate": 5.756628734794551e-05,
      "loss": 0.1562,
      "step": 30330
    },
    {
      "epoch": 1.6995490575021708,
      "grad_norm": 3.149648427963257,
      "learning_rate": 5.755227310947924e-05,
      "loss": 0.1609,
      "step": 30340
    },
    {
      "epoch": 1.7001092345181077,
      "grad_norm": 0.616328239440918,
      "learning_rate": 5.7538258871012954e-05,
      "loss": 0.1955,
      "step": 30350
    },
    {
      "epoch": 1.7006694115340446,
      "grad_norm": 3.5780680179595947,
      "learning_rate": 5.7524244632546676e-05,
      "loss": 0.1972,
      "step": 30360
    },
    {
      "epoch": 1.7012295885499817,
      "grad_norm": 3.369577646255493,
      "learning_rate": 5.751023039408039e-05,
      "loss": 0.1338,
      "step": 30370
    },
    {
      "epoch": 1.7017897655659189,
      "grad_norm": 2.049788236618042,
      "learning_rate": 5.7496216155614105e-05,
      "loss": 0.1424,
      "step": 30380
    },
    {
      "epoch": 1.702349942581856,
      "grad_norm": 2.9951093196868896,
      "learning_rate": 5.748220191714783e-05,
      "loss": 0.2403,
      "step": 30390
    },
    {
      "epoch": 1.702910119597793,
      "grad_norm": 4.33479118347168,
      "learning_rate": 5.746818767868154e-05,
      "loss": 0.1179,
      "step": 30400
    },
    {
      "epoch": 1.7034702966137298,
      "grad_norm": 2.29651141166687,
      "learning_rate": 5.745417344021526e-05,
      "loss": 0.2926,
      "step": 30410
    },
    {
      "epoch": 1.704030473629667,
      "grad_norm": 6.1574883460998535,
      "learning_rate": 5.744015920174898e-05,
      "loss": 0.2105,
      "step": 30420
    },
    {
      "epoch": 1.704590650645604,
      "grad_norm": 1.7556251287460327,
      "learning_rate": 5.74261449632827e-05,
      "loss": 0.1852,
      "step": 30430
    },
    {
      "epoch": 1.705150827661541,
      "grad_norm": 3.422382116317749,
      "learning_rate": 5.7412130724816414e-05,
      "loss": 0.173,
      "step": 30440
    },
    {
      "epoch": 1.705711004677478,
      "grad_norm": 2.9174957275390625,
      "learning_rate": 5.739811648635014e-05,
      "loss": 0.2094,
      "step": 30450
    },
    {
      "epoch": 1.706271181693415,
      "grad_norm": 3.0968947410583496,
      "learning_rate": 5.738410224788385e-05,
      "loss": 0.1399,
      "step": 30460
    },
    {
      "epoch": 1.7068313587093522,
      "grad_norm": 3.4098587036132812,
      "learning_rate": 5.7370088009417565e-05,
      "loss": 0.2278,
      "step": 30470
    },
    {
      "epoch": 1.7073915357252893,
      "grad_norm": 2.0791873931884766,
      "learning_rate": 5.735607377095129e-05,
      "loss": 0.196,
      "step": 30480
    },
    {
      "epoch": 1.7079517127412263,
      "grad_norm": 7.8441691398620605,
      "learning_rate": 5.7342059532485e-05,
      "loss": 0.2299,
      "step": 30490
    },
    {
      "epoch": 1.7085118897571632,
      "grad_norm": 2.034364938735962,
      "learning_rate": 5.732804529401873e-05,
      "loss": 0.1511,
      "step": 30500
    },
    {
      "epoch": 1.7090720667731003,
      "grad_norm": 4.65687894821167,
      "learning_rate": 5.7314031055552444e-05,
      "loss": 0.1507,
      "step": 30510
    },
    {
      "epoch": 1.7096322437890374,
      "grad_norm": 3.146073818206787,
      "learning_rate": 5.7300016817086166e-05,
      "loss": 0.152,
      "step": 30520
    },
    {
      "epoch": 1.7101924208049744,
      "grad_norm": 2.3331727981567383,
      "learning_rate": 5.728600257861988e-05,
      "loss": 0.1335,
      "step": 30530
    },
    {
      "epoch": 1.7107525978209113,
      "grad_norm": 1.279500126838684,
      "learning_rate": 5.72719883401536e-05,
      "loss": 0.1512,
      "step": 30540
    },
    {
      "epoch": 1.7113127748368484,
      "grad_norm": 4.2766594886779785,
      "learning_rate": 5.725797410168732e-05,
      "loss": 0.2195,
      "step": 30550
    },
    {
      "epoch": 1.7118729518527855,
      "grad_norm": 2.9098992347717285,
      "learning_rate": 5.724395986322103e-05,
      "loss": 0.2053,
      "step": 30560
    },
    {
      "epoch": 1.7124331288687227,
      "grad_norm": 1.6023861169815063,
      "learning_rate": 5.722994562475475e-05,
      "loss": 0.1936,
      "step": 30570
    },
    {
      "epoch": 1.7129933058846596,
      "grad_norm": 3.3565571308135986,
      "learning_rate": 5.721593138628847e-05,
      "loss": 0.2262,
      "step": 30580
    },
    {
      "epoch": 1.7135534829005965,
      "grad_norm": 1.6092058420181274,
      "learning_rate": 5.720191714782219e-05,
      "loss": 0.1533,
      "step": 30590
    },
    {
      "epoch": 1.7141136599165336,
      "grad_norm": 2.3709325790405273,
      "learning_rate": 5.7187902909355904e-05,
      "loss": 0.1405,
      "step": 30600
    },
    {
      "epoch": 1.7146738369324708,
      "grad_norm": 5.320470809936523,
      "learning_rate": 5.717388867088963e-05,
      "loss": 0.2474,
      "step": 30610
    },
    {
      "epoch": 1.7152340139484077,
      "grad_norm": 3.850409746170044,
      "learning_rate": 5.715987443242334e-05,
      "loss": 0.2583,
      "step": 30620
    },
    {
      "epoch": 1.7157941909643446,
      "grad_norm": 1.8823777437210083,
      "learning_rate": 5.714586019395707e-05,
      "loss": 0.2829,
      "step": 30630
    },
    {
      "epoch": 1.7163543679802817,
      "grad_norm": 3.3359391689300537,
      "learning_rate": 5.7131845955490783e-05,
      "loss": 0.1907,
      "step": 30640
    },
    {
      "epoch": 1.7169145449962189,
      "grad_norm": 3.377042531967163,
      "learning_rate": 5.711783171702449e-05,
      "loss": 0.162,
      "step": 30650
    },
    {
      "epoch": 1.717474722012156,
      "grad_norm": 4.246795177459717,
      "learning_rate": 5.710381747855822e-05,
      "loss": 0.1475,
      "step": 30660
    },
    {
      "epoch": 1.718034899028093,
      "grad_norm": 2.2163760662078857,
      "learning_rate": 5.7089803240091934e-05,
      "loss": 0.17,
      "step": 30670
    },
    {
      "epoch": 1.7185950760440298,
      "grad_norm": 5.856156826019287,
      "learning_rate": 5.7075789001625656e-05,
      "loss": 0.2733,
      "step": 30680
    },
    {
      "epoch": 1.719155253059967,
      "grad_norm": 4.474893093109131,
      "learning_rate": 5.706177476315937e-05,
      "loss": 0.4271,
      "step": 30690
    },
    {
      "epoch": 1.719715430075904,
      "grad_norm": 4.78483772277832,
      "learning_rate": 5.704776052469309e-05,
      "loss": 0.2104,
      "step": 30700
    },
    {
      "epoch": 1.720275607091841,
      "grad_norm": 2.2217307090759277,
      "learning_rate": 5.703374628622681e-05,
      "loss": 0.1631,
      "step": 30710
    },
    {
      "epoch": 1.720835784107778,
      "grad_norm": 3.268838882446289,
      "learning_rate": 5.7019732047760535e-05,
      "loss": 0.1429,
      "step": 30720
    },
    {
      "epoch": 1.721395961123715,
      "grad_norm": 1.1369545459747314,
      "learning_rate": 5.700571780929424e-05,
      "loss": 0.2137,
      "step": 30730
    },
    {
      "epoch": 1.7219561381396522,
      "grad_norm": 2.7630112171173096,
      "learning_rate": 5.699170357082796e-05,
      "loss": 0.1728,
      "step": 30740
    },
    {
      "epoch": 1.7225163151555891,
      "grad_norm": 7.6579718589782715,
      "learning_rate": 5.6977689332361686e-05,
      "loss": 0.1808,
      "step": 30750
    },
    {
      "epoch": 1.7230764921715263,
      "grad_norm": 5.316935062408447,
      "learning_rate": 5.6963675093895394e-05,
      "loss": 0.1435,
      "step": 30760
    },
    {
      "epoch": 1.7236366691874632,
      "grad_norm": 6.121695518493652,
      "learning_rate": 5.694966085542912e-05,
      "loss": 0.1389,
      "step": 30770
    },
    {
      "epoch": 1.7241968462034003,
      "grad_norm": 2.488396167755127,
      "learning_rate": 5.693564661696284e-05,
      "loss": 0.1732,
      "step": 30780
    },
    {
      "epoch": 1.7247570232193374,
      "grad_norm": 4.123736381530762,
      "learning_rate": 5.692163237849656e-05,
      "loss": 0.1725,
      "step": 30790
    },
    {
      "epoch": 1.7253172002352744,
      "grad_norm": 2.758765697479248,
      "learning_rate": 5.6907618140030274e-05,
      "loss": 0.1801,
      "step": 30800
    },
    {
      "epoch": 1.7258773772512113,
      "grad_norm": 5.916016101837158,
      "learning_rate": 5.6893603901563995e-05,
      "loss": 0.1954,
      "step": 30810
    },
    {
      "epoch": 1.7264375542671484,
      "grad_norm": 2.2224035263061523,
      "learning_rate": 5.687958966309771e-05,
      "loss": 0.1564,
      "step": 30820
    },
    {
      "epoch": 1.7269977312830855,
      "grad_norm": 4.23984956741333,
      "learning_rate": 5.6865575424631425e-05,
      "loss": 0.2099,
      "step": 30830
    },
    {
      "epoch": 1.7275579082990224,
      "grad_norm": 6.5950846672058105,
      "learning_rate": 5.6851561186165146e-05,
      "loss": 0.1366,
      "step": 30840
    },
    {
      "epoch": 1.7281180853149594,
      "grad_norm": 3.213955879211426,
      "learning_rate": 5.683754694769886e-05,
      "loss": 0.138,
      "step": 30850
    },
    {
      "epoch": 1.7286782623308965,
      "grad_norm": 4.7650251388549805,
      "learning_rate": 5.682353270923258e-05,
      "loss": 0.1912,
      "step": 30860
    },
    {
      "epoch": 1.7292384393468336,
      "grad_norm": 0.8856392502784729,
      "learning_rate": 5.68095184707663e-05,
      "loss": 0.1642,
      "step": 30870
    },
    {
      "epoch": 1.7297986163627708,
      "grad_norm": 2.579373598098755,
      "learning_rate": 5.6795504232300025e-05,
      "loss": 0.1843,
      "step": 30880
    },
    {
      "epoch": 1.7303587933787077,
      "grad_norm": 3.50209903717041,
      "learning_rate": 5.678148999383373e-05,
      "loss": 0.2314,
      "step": 30890
    },
    {
      "epoch": 1.7309189703946446,
      "grad_norm": 2.1980738639831543,
      "learning_rate": 5.676747575536746e-05,
      "loss": 0.2031,
      "step": 30900
    },
    {
      "epoch": 1.7314791474105817,
      "grad_norm": 2.69830322265625,
      "learning_rate": 5.6753461516901176e-05,
      "loss": 0.1675,
      "step": 30910
    },
    {
      "epoch": 1.7320393244265189,
      "grad_norm": 3.275424003601074,
      "learning_rate": 5.6739447278434884e-05,
      "loss": 0.1316,
      "step": 30920
    },
    {
      "epoch": 1.7325995014424558,
      "grad_norm": 2.7866909503936768,
      "learning_rate": 5.672543303996861e-05,
      "loss": 0.1685,
      "step": 30930
    },
    {
      "epoch": 1.7331596784583927,
      "grad_norm": 4.196068286895752,
      "learning_rate": 5.671141880150233e-05,
      "loss": 0.1654,
      "step": 30940
    },
    {
      "epoch": 1.7337198554743298,
      "grad_norm": 2.3053994178771973,
      "learning_rate": 5.669740456303605e-05,
      "loss": 0.1566,
      "step": 30950
    },
    {
      "epoch": 1.734280032490267,
      "grad_norm": 1.1379129886627197,
      "learning_rate": 5.6683390324569764e-05,
      "loss": 0.1413,
      "step": 30960
    },
    {
      "epoch": 1.734840209506204,
      "grad_norm": 5.02432918548584,
      "learning_rate": 5.6669376086103485e-05,
      "loss": 0.133,
      "step": 30970
    },
    {
      "epoch": 1.735400386522141,
      "grad_norm": 3.8542048931121826,
      "learning_rate": 5.66553618476372e-05,
      "loss": 0.2181,
      "step": 30980
    },
    {
      "epoch": 1.735960563538078,
      "grad_norm": 1.0897873640060425,
      "learning_rate": 5.664134760917093e-05,
      "loss": 0.1736,
      "step": 30990
    },
    {
      "epoch": 1.736520740554015,
      "grad_norm": 4.040518760681152,
      "learning_rate": 5.6627333370704636e-05,
      "loss": 0.1206,
      "step": 31000
    },
    {
      "epoch": 1.7370809175699522,
      "grad_norm": 4.094430446624756,
      "learning_rate": 5.661331913223835e-05,
      "loss": 0.1298,
      "step": 31010
    },
    {
      "epoch": 1.7376410945858891,
      "grad_norm": 2.4028687477111816,
      "learning_rate": 5.659930489377208e-05,
      "loss": 0.1613,
      "step": 31020
    },
    {
      "epoch": 1.738201271601826,
      "grad_norm": 3.117892265319824,
      "learning_rate": 5.658529065530579e-05,
      "loss": 0.1876,
      "step": 31030
    },
    {
      "epoch": 1.7387614486177632,
      "grad_norm": 5.925274848937988,
      "learning_rate": 5.6571276416839515e-05,
      "loss": 0.1665,
      "step": 31040
    },
    {
      "epoch": 1.7393216256337003,
      "grad_norm": 4.036971569061279,
      "learning_rate": 5.655726217837323e-05,
      "loss": 0.1608,
      "step": 31050
    },
    {
      "epoch": 1.7398818026496374,
      "grad_norm": 4.972134590148926,
      "learning_rate": 5.654324793990695e-05,
      "loss": 0.1829,
      "step": 31060
    },
    {
      "epoch": 1.7404419796655743,
      "grad_norm": 5.57438325881958,
      "learning_rate": 5.6529233701440666e-05,
      "loss": 0.1185,
      "step": 31070
    },
    {
      "epoch": 1.7410021566815113,
      "grad_norm": 1.6712990999221802,
      "learning_rate": 5.651521946297439e-05,
      "loss": 0.1968,
      "step": 31080
    },
    {
      "epoch": 1.7415623336974484,
      "grad_norm": 6.793785572052002,
      "learning_rate": 5.65012052245081e-05,
      "loss": 0.2456,
      "step": 31090
    },
    {
      "epoch": 1.7421225107133855,
      "grad_norm": 4.442314147949219,
      "learning_rate": 5.648719098604182e-05,
      "loss": 0.2025,
      "step": 31100
    },
    {
      "epoch": 1.7426826877293224,
      "grad_norm": 3.9424705505371094,
      "learning_rate": 5.647317674757554e-05,
      "loss": 0.1531,
      "step": 31110
    },
    {
      "epoch": 1.7432428647452594,
      "grad_norm": 1.564990520477295,
      "learning_rate": 5.6459162509109254e-05,
      "loss": 0.0961,
      "step": 31120
    },
    {
      "epoch": 1.7438030417611965,
      "grad_norm": 3.1941750049591064,
      "learning_rate": 5.6445148270642975e-05,
      "loss": 0.1358,
      "step": 31130
    },
    {
      "epoch": 1.7443632187771336,
      "grad_norm": 2.7921531200408936,
      "learning_rate": 5.643113403217669e-05,
      "loss": 0.2276,
      "step": 31140
    },
    {
      "epoch": 1.7449233957930708,
      "grad_norm": 1.690466284751892,
      "learning_rate": 5.641711979371042e-05,
      "loss": 0.1757,
      "step": 31150
    },
    {
      "epoch": 1.7454835728090077,
      "grad_norm": 2.7553415298461914,
      "learning_rate": 5.6403105555244126e-05,
      "loss": 0.233,
      "step": 31160
    },
    {
      "epoch": 1.7460437498249446,
      "grad_norm": 1.1184017658233643,
      "learning_rate": 5.6389091316777855e-05,
      "loss": 0.1329,
      "step": 31170
    },
    {
      "epoch": 1.7466039268408817,
      "grad_norm": 2.4715487957000732,
      "learning_rate": 5.637507707831157e-05,
      "loss": 0.1417,
      "step": 31180
    },
    {
      "epoch": 1.7471641038568189,
      "grad_norm": 2.363624095916748,
      "learning_rate": 5.636106283984528e-05,
      "loss": 0.1701,
      "step": 31190
    },
    {
      "epoch": 1.7477242808727558,
      "grad_norm": 4.102774620056152,
      "learning_rate": 5.6347048601379006e-05,
      "loss": 0.2147,
      "step": 31200
    },
    {
      "epoch": 1.7482844578886927,
      "grad_norm": 3.62148380279541,
      "learning_rate": 5.633303436291272e-05,
      "loss": 0.2134,
      "step": 31210
    },
    {
      "epoch": 1.7488446349046298,
      "grad_norm": 4.372802734375,
      "learning_rate": 5.631902012444644e-05,
      "loss": 0.1811,
      "step": 31220
    },
    {
      "epoch": 1.749404811920567,
      "grad_norm": 3.4491982460021973,
      "learning_rate": 5.6305005885980157e-05,
      "loss": 0.2201,
      "step": 31230
    },
    {
      "epoch": 1.749964988936504,
      "grad_norm": 4.746356010437012,
      "learning_rate": 5.629099164751388e-05,
      "loss": 0.2055,
      "step": 31240
    },
    {
      "epoch": 1.750525165952441,
      "grad_norm": 6.622012138366699,
      "learning_rate": 5.627697740904759e-05,
      "loss": 0.182,
      "step": 31250
    },
    {
      "epoch": 1.751085342968378,
      "grad_norm": 2.8790698051452637,
      "learning_rate": 5.6262963170581314e-05,
      "loss": 0.1653,
      "step": 31260
    },
    {
      "epoch": 1.751645519984315,
      "grad_norm": 2.408687114715576,
      "learning_rate": 5.624894893211503e-05,
      "loss": 0.181,
      "step": 31270
    },
    {
      "epoch": 1.7522056970002522,
      "grad_norm": 2.299257516860962,
      "learning_rate": 5.623493469364876e-05,
      "loss": 0.0942,
      "step": 31280
    },
    {
      "epoch": 1.752765874016189,
      "grad_norm": 1.0970269441604614,
      "learning_rate": 5.6220920455182465e-05,
      "loss": 0.0951,
      "step": 31290
    },
    {
      "epoch": 1.753326051032126,
      "grad_norm": 1.7828906774520874,
      "learning_rate": 5.620690621671618e-05,
      "loss": 0.1308,
      "step": 31300
    },
    {
      "epoch": 1.7538862280480632,
      "grad_norm": 1.084925889968872,
      "learning_rate": 5.619289197824991e-05,
      "loss": 0.1993,
      "step": 31310
    },
    {
      "epoch": 1.7544464050640003,
      "grad_norm": 2.4557178020477295,
      "learning_rate": 5.6178877739783616e-05,
      "loss": 0.1214,
      "step": 31320
    },
    {
      "epoch": 1.7550065820799372,
      "grad_norm": 4.059043884277344,
      "learning_rate": 5.6164863501317345e-05,
      "loss": 0.1413,
      "step": 31330
    },
    {
      "epoch": 1.7555667590958743,
      "grad_norm": 1.5792744159698486,
      "learning_rate": 5.615084926285106e-05,
      "loss": 0.1626,
      "step": 31340
    },
    {
      "epoch": 1.7561269361118113,
      "grad_norm": 5.447598934173584,
      "learning_rate": 5.613683502438478e-05,
      "loss": 0.1336,
      "step": 31350
    },
    {
      "epoch": 1.7566871131277484,
      "grad_norm": 2.8637430667877197,
      "learning_rate": 5.6122820785918496e-05,
      "loss": 0.133,
      "step": 31360
    },
    {
      "epoch": 1.7572472901436855,
      "grad_norm": 7.08648681640625,
      "learning_rate": 5.610880654745222e-05,
      "loss": 0.1567,
      "step": 31370
    },
    {
      "epoch": 1.7578074671596224,
      "grad_norm": 1.8333982229232788,
      "learning_rate": 5.609479230898593e-05,
      "loss": 0.1191,
      "step": 31380
    },
    {
      "epoch": 1.7583676441755594,
      "grad_norm": 5.578339576721191,
      "learning_rate": 5.608077807051965e-05,
      "loss": 0.189,
      "step": 31390
    },
    {
      "epoch": 1.7589278211914965,
      "grad_norm": 4.249574184417725,
      "learning_rate": 5.606676383205337e-05,
      "loss": 0.2901,
      "step": 31400
    },
    {
      "epoch": 1.7594879982074336,
      "grad_norm": 2.821753740310669,
      "learning_rate": 5.605274959358708e-05,
      "loss": 0.1315,
      "step": 31410
    },
    {
      "epoch": 1.7600481752233705,
      "grad_norm": 1.8652307987213135,
      "learning_rate": 5.603873535512081e-05,
      "loss": 0.109,
      "step": 31420
    },
    {
      "epoch": 1.7606083522393077,
      "grad_norm": 4.406703948974609,
      "learning_rate": 5.602472111665452e-05,
      "loss": 0.1139,
      "step": 31430
    },
    {
      "epoch": 1.7611685292552446,
      "grad_norm": 3.7222228050231934,
      "learning_rate": 5.601070687818825e-05,
      "loss": 0.2265,
      "step": 31440
    },
    {
      "epoch": 1.7617287062711817,
      "grad_norm": 0.9736186861991882,
      "learning_rate": 5.599669263972196e-05,
      "loss": 0.1934,
      "step": 31450
    },
    {
      "epoch": 1.7622888832871189,
      "grad_norm": 3.5202441215515137,
      "learning_rate": 5.5982678401255684e-05,
      "loss": 0.1302,
      "step": 31460
    },
    {
      "epoch": 1.7628490603030558,
      "grad_norm": 2.834681272506714,
      "learning_rate": 5.59686641627894e-05,
      "loss": 0.2072,
      "step": 31470
    },
    {
      "epoch": 1.7634092373189927,
      "grad_norm": 4.007800102233887,
      "learning_rate": 5.595464992432311e-05,
      "loss": 0.2253,
      "step": 31480
    },
    {
      "epoch": 1.7639694143349298,
      "grad_norm": 5.65718412399292,
      "learning_rate": 5.5940635685856835e-05,
      "loss": 0.1591,
      "step": 31490
    },
    {
      "epoch": 1.764529591350867,
      "grad_norm": 3.5699212551116943,
      "learning_rate": 5.592662144739055e-05,
      "loss": 0.1808,
      "step": 31500
    },
    {
      "epoch": 1.7650897683668039,
      "grad_norm": 3.3287172317504883,
      "learning_rate": 5.591260720892427e-05,
      "loss": 0.1505,
      "step": 31510
    },
    {
      "epoch": 1.7656499453827408,
      "grad_norm": 2.4362006187438965,
      "learning_rate": 5.5898592970457986e-05,
      "loss": 0.2544,
      "step": 31520
    },
    {
      "epoch": 1.766210122398678,
      "grad_norm": 3.8781914710998535,
      "learning_rate": 5.588457873199171e-05,
      "loss": 0.1525,
      "step": 31530
    },
    {
      "epoch": 1.766770299414615,
      "grad_norm": 3.121419906616211,
      "learning_rate": 5.587056449352542e-05,
      "loss": 0.1764,
      "step": 31540
    },
    {
      "epoch": 1.7673304764305522,
      "grad_norm": 5.982038497924805,
      "learning_rate": 5.585655025505915e-05,
      "loss": 0.2124,
      "step": 31550
    },
    {
      "epoch": 1.767890653446489,
      "grad_norm": 3.469053030014038,
      "learning_rate": 5.584253601659286e-05,
      "loss": 0.1042,
      "step": 31560
    },
    {
      "epoch": 1.768450830462426,
      "grad_norm": 0.9394048452377319,
      "learning_rate": 5.582852177812657e-05,
      "loss": 0.1587,
      "step": 31570
    },
    {
      "epoch": 1.7690110074783632,
      "grad_norm": 1.8187106847763062,
      "learning_rate": 5.58145075396603e-05,
      "loss": 0.0975,
      "step": 31580
    },
    {
      "epoch": 1.7695711844943003,
      "grad_norm": 6.664860248565674,
      "learning_rate": 5.580049330119401e-05,
      "loss": 0.1402,
      "step": 31590
    },
    {
      "epoch": 1.7701313615102372,
      "grad_norm": 4.1381659507751465,
      "learning_rate": 5.578647906272774e-05,
      "loss": 0.1993,
      "step": 31600
    },
    {
      "epoch": 1.7706915385261741,
      "grad_norm": 3.7208244800567627,
      "learning_rate": 5.577246482426145e-05,
      "loss": 0.1709,
      "step": 31610
    },
    {
      "epoch": 1.7712517155421112,
      "grad_norm": 2.2659192085266113,
      "learning_rate": 5.5758450585795174e-05,
      "loss": 0.2004,
      "step": 31620
    },
    {
      "epoch": 1.7718118925580484,
      "grad_norm": 5.326480865478516,
      "learning_rate": 5.574443634732889e-05,
      "loss": 0.1157,
      "step": 31630
    },
    {
      "epoch": 1.7723720695739855,
      "grad_norm": 1.1623793840408325,
      "learning_rate": 5.573042210886261e-05,
      "loss": 0.3269,
      "step": 31640
    },
    {
      "epoch": 1.7729322465899224,
      "grad_norm": 4.396239280700684,
      "learning_rate": 5.5716407870396325e-05,
      "loss": 0.1948,
      "step": 31650
    },
    {
      "epoch": 1.7734924236058593,
      "grad_norm": 6.644378662109375,
      "learning_rate": 5.570239363193004e-05,
      "loss": 0.1747,
      "step": 31660
    },
    {
      "epoch": 1.7740526006217965,
      "grad_norm": 5.210316181182861,
      "learning_rate": 5.568837939346376e-05,
      "loss": 0.222,
      "step": 31670
    },
    {
      "epoch": 1.7746127776377336,
      "grad_norm": 1.8244680166244507,
      "learning_rate": 5.5674365154997476e-05,
      "loss": 0.2243,
      "step": 31680
    },
    {
      "epoch": 1.7751729546536705,
      "grad_norm": 4.023155689239502,
      "learning_rate": 5.5660350916531204e-05,
      "loss": 0.2028,
      "step": 31690
    },
    {
      "epoch": 1.7757331316696074,
      "grad_norm": 3.394106388092041,
      "learning_rate": 5.564633667806491e-05,
      "loss": 0.1335,
      "step": 31700
    },
    {
      "epoch": 1.7762933086855446,
      "grad_norm": 4.75823450088501,
      "learning_rate": 5.563232243959864e-05,
      "loss": 0.1446,
      "step": 31710
    },
    {
      "epoch": 1.7768534857014817,
      "grad_norm": 3.00740385055542,
      "learning_rate": 5.5618308201132355e-05,
      "loss": 0.1932,
      "step": 31720
    },
    {
      "epoch": 1.7774136627174189,
      "grad_norm": 5.415266036987305,
      "learning_rate": 5.560429396266608e-05,
      "loss": 0.2162,
      "step": 31730
    },
    {
      "epoch": 1.7779738397333558,
      "grad_norm": 3.084944725036621,
      "learning_rate": 5.559027972419979e-05,
      "loss": 0.1166,
      "step": 31740
    },
    {
      "epoch": 1.7785340167492927,
      "grad_norm": 2.894977569580078,
      "learning_rate": 5.5576265485733506e-05,
      "loss": 0.1663,
      "step": 31750
    },
    {
      "epoch": 1.7790941937652298,
      "grad_norm": 1.4386179447174072,
      "learning_rate": 5.556225124726723e-05,
      "loss": 0.2512,
      "step": 31760
    },
    {
      "epoch": 1.779654370781167,
      "grad_norm": 1.5408328771591187,
      "learning_rate": 5.554823700880094e-05,
      "loss": 0.3233,
      "step": 31770
    },
    {
      "epoch": 1.7802145477971039,
      "grad_norm": 2.205204963684082,
      "learning_rate": 5.5534222770334664e-05,
      "loss": 0.2828,
      "step": 31780
    },
    {
      "epoch": 1.7807747248130408,
      "grad_norm": 2.2786741256713867,
      "learning_rate": 5.552020853186838e-05,
      "loss": 0.1407,
      "step": 31790
    },
    {
      "epoch": 1.781334901828978,
      "grad_norm": 4.68187141418457,
      "learning_rate": 5.55061942934021e-05,
      "loss": 0.2167,
      "step": 31800
    },
    {
      "epoch": 1.781895078844915,
      "grad_norm": 2.726745843887329,
      "learning_rate": 5.5492180054935815e-05,
      "loss": 0.1386,
      "step": 31810
    },
    {
      "epoch": 1.7824552558608522,
      "grad_norm": 2.2648348808288574,
      "learning_rate": 5.547816581646954e-05,
      "loss": 0.1738,
      "step": 31820
    },
    {
      "epoch": 1.783015432876789,
      "grad_norm": 1.553773045539856,
      "learning_rate": 5.546415157800325e-05,
      "loss": 0.1431,
      "step": 31830
    },
    {
      "epoch": 1.783575609892726,
      "grad_norm": 4.115767002105713,
      "learning_rate": 5.5450137339536966e-05,
      "loss": 0.2327,
      "step": 31840
    },
    {
      "epoch": 1.7841357869086631,
      "grad_norm": 1.9990293979644775,
      "learning_rate": 5.5436123101070694e-05,
      "loss": 0.1314,
      "step": 31850
    },
    {
      "epoch": 1.7846959639246003,
      "grad_norm": 4.943813323974609,
      "learning_rate": 5.54221088626044e-05,
      "loss": 0.1722,
      "step": 31860
    },
    {
      "epoch": 1.7852561409405372,
      "grad_norm": 3.2799248695373535,
      "learning_rate": 5.540809462413813e-05,
      "loss": 0.1698,
      "step": 31870
    },
    {
      "epoch": 1.785816317956474,
      "grad_norm": 2.881234884262085,
      "learning_rate": 5.5394080385671845e-05,
      "loss": 0.1266,
      "step": 31880
    },
    {
      "epoch": 1.7863764949724112,
      "grad_norm": 3.8912887573242188,
      "learning_rate": 5.538006614720557e-05,
      "loss": 0.1777,
      "step": 31890
    },
    {
      "epoch": 1.7869366719883484,
      "grad_norm": 7.531405448913574,
      "learning_rate": 5.536605190873928e-05,
      "loss": 0.174,
      "step": 31900
    },
    {
      "epoch": 1.7874968490042855,
      "grad_norm": 2.8812742233276367,
      "learning_rate": 5.5352037670273e-05,
      "loss": 0.1447,
      "step": 31910
    },
    {
      "epoch": 1.7880570260202224,
      "grad_norm": 4.85136079788208,
      "learning_rate": 5.533802343180672e-05,
      "loss": 0.1436,
      "step": 31920
    },
    {
      "epoch": 1.7886172030361593,
      "grad_norm": 2.6389856338500977,
      "learning_rate": 5.532400919334043e-05,
      "loss": 0.3072,
      "step": 31930
    },
    {
      "epoch": 1.7891773800520965,
      "grad_norm": 4.098440647125244,
      "learning_rate": 5.5309994954874154e-05,
      "loss": 0.171,
      "step": 31940
    },
    {
      "epoch": 1.7897375570680336,
      "grad_norm": 4.263071060180664,
      "learning_rate": 5.529598071640787e-05,
      "loss": 0.1277,
      "step": 31950
    },
    {
      "epoch": 1.7902977340839705,
      "grad_norm": 4.720503807067871,
      "learning_rate": 5.528196647794159e-05,
      "loss": 0.2092,
      "step": 31960
    },
    {
      "epoch": 1.7908579110999074,
      "grad_norm": 3.7818808555603027,
      "learning_rate": 5.5267952239475305e-05,
      "loss": 0.1892,
      "step": 31970
    },
    {
      "epoch": 1.7914180881158446,
      "grad_norm": 4.205569744110107,
      "learning_rate": 5.525393800100903e-05,
      "loss": 0.1164,
      "step": 31980
    },
    {
      "epoch": 1.7919782651317817,
      "grad_norm": 5.281345844268799,
      "learning_rate": 5.523992376254274e-05,
      "loss": 0.1779,
      "step": 31990
    },
    {
      "epoch": 1.7925384421477186,
      "grad_norm": 6.923809051513672,
      "learning_rate": 5.522590952407647e-05,
      "loss": 0.1816,
      "step": 32000
    },
    {
      "epoch": 1.7930986191636558,
      "grad_norm": 5.899164199829102,
      "learning_rate": 5.5211895285610184e-05,
      "loss": 0.2532,
      "step": 32010
    },
    {
      "epoch": 1.7936587961795927,
      "grad_norm": 2.593027114868164,
      "learning_rate": 5.519788104714389e-05,
      "loss": 0.1525,
      "step": 32020
    },
    {
      "epoch": 1.7942189731955298,
      "grad_norm": 2.1650469303131104,
      "learning_rate": 5.518386680867762e-05,
      "loss": 0.1205,
      "step": 32030
    },
    {
      "epoch": 1.794779150211467,
      "grad_norm": 4.036097049713135,
      "learning_rate": 5.5169852570211335e-05,
      "loss": 0.1797,
      "step": 32040
    },
    {
      "epoch": 1.7953393272274039,
      "grad_norm": 5.098970413208008,
      "learning_rate": 5.515583833174506e-05,
      "loss": 0.1218,
      "step": 32050
    },
    {
      "epoch": 1.7958995042433408,
      "grad_norm": 1.4790301322937012,
      "learning_rate": 5.514182409327877e-05,
      "loss": 0.176,
      "step": 32060
    },
    {
      "epoch": 1.796459681259278,
      "grad_norm": 3.227161407470703,
      "learning_rate": 5.512780985481249e-05,
      "loss": 0.1211,
      "step": 32070
    },
    {
      "epoch": 1.797019858275215,
      "grad_norm": 2.2968621253967285,
      "learning_rate": 5.511379561634621e-05,
      "loss": 0.0911,
      "step": 32080
    },
    {
      "epoch": 1.797580035291152,
      "grad_norm": 0.7532120943069458,
      "learning_rate": 5.5099781377879936e-05,
      "loss": 0.1525,
      "step": 32090
    },
    {
      "epoch": 1.798140212307089,
      "grad_norm": 1.3017702102661133,
      "learning_rate": 5.5085767139413644e-05,
      "loss": 0.1867,
      "step": 32100
    },
    {
      "epoch": 1.798700389323026,
      "grad_norm": 3.698637008666992,
      "learning_rate": 5.507175290094736e-05,
      "loss": 0.1768,
      "step": 32110
    },
    {
      "epoch": 1.7992605663389631,
      "grad_norm": 5.410356044769287,
      "learning_rate": 5.505773866248109e-05,
      "loss": 0.1829,
      "step": 32120
    },
    {
      "epoch": 1.7998207433549003,
      "grad_norm": 2.2790298461914062,
      "learning_rate": 5.5043724424014795e-05,
      "loss": 0.1564,
      "step": 32130
    },
    {
      "epoch": 1.8003809203708372,
      "grad_norm": 4.58261775970459,
      "learning_rate": 5.5029710185548523e-05,
      "loss": 0.1728,
      "step": 32140
    },
    {
      "epoch": 1.800941097386774,
      "grad_norm": 2.5689735412597656,
      "learning_rate": 5.501569594708224e-05,
      "loss": 0.1558,
      "step": 32150
    },
    {
      "epoch": 1.8015012744027112,
      "grad_norm": 6.607227325439453,
      "learning_rate": 5.500168170861596e-05,
      "loss": 0.2194,
      "step": 32160
    },
    {
      "epoch": 1.8020614514186484,
      "grad_norm": 4.873506546020508,
      "learning_rate": 5.4987667470149674e-05,
      "loss": 0.18,
      "step": 32170
    },
    {
      "epoch": 1.8026216284345853,
      "grad_norm": 4.0948944091796875,
      "learning_rate": 5.4973653231683396e-05,
      "loss": 0.0976,
      "step": 32180
    },
    {
      "epoch": 1.8031818054505222,
      "grad_norm": 4.43376350402832,
      "learning_rate": 5.495963899321711e-05,
      "loss": 0.15,
      "step": 32190
    },
    {
      "epoch": 1.8037419824664593,
      "grad_norm": 3.715090751647949,
      "learning_rate": 5.4945624754750825e-05,
      "loss": 0.1364,
      "step": 32200
    },
    {
      "epoch": 1.8043021594823965,
      "grad_norm": 5.359443187713623,
      "learning_rate": 5.493161051628455e-05,
      "loss": 0.1527,
      "step": 32210
    },
    {
      "epoch": 1.8048623364983336,
      "grad_norm": 3.92779803276062,
      "learning_rate": 5.491759627781826e-05,
      "loss": 0.1518,
      "step": 32220
    },
    {
      "epoch": 1.8054225135142705,
      "grad_norm": 2.2386057376861572,
      "learning_rate": 5.490358203935198e-05,
      "loss": 0.1598,
      "step": 32230
    },
    {
      "epoch": 1.8059826905302074,
      "grad_norm": 1.783931016921997,
      "learning_rate": 5.48895678008857e-05,
      "loss": 0.2687,
      "step": 32240
    },
    {
      "epoch": 1.8065428675461446,
      "grad_norm": 5.336971759796143,
      "learning_rate": 5.4875553562419426e-05,
      "loss": 0.2455,
      "step": 32250
    },
    {
      "epoch": 1.8071030445620817,
      "grad_norm": 2.3854475021362305,
      "learning_rate": 5.4861539323953134e-05,
      "loss": 0.2629,
      "step": 32260
    },
    {
      "epoch": 1.8076632215780186,
      "grad_norm": 4.238337993621826,
      "learning_rate": 5.484752508548686e-05,
      "loss": 0.1124,
      "step": 32270
    },
    {
      "epoch": 1.8082233985939555,
      "grad_norm": 2.0108964443206787,
      "learning_rate": 5.483351084702058e-05,
      "loss": 0.1402,
      "step": 32280
    },
    {
      "epoch": 1.8087835756098927,
      "grad_norm": 3.1715853214263916,
      "learning_rate": 5.4819496608554285e-05,
      "loss": 0.1665,
      "step": 32290
    },
    {
      "epoch": 1.8093437526258298,
      "grad_norm": 2.345092535018921,
      "learning_rate": 5.4805482370088014e-05,
      "loss": 0.1189,
      "step": 32300
    },
    {
      "epoch": 1.809903929641767,
      "grad_norm": 1.5126014947891235,
      "learning_rate": 5.479146813162173e-05,
      "loss": 0.1592,
      "step": 32310
    },
    {
      "epoch": 1.8104641066577039,
      "grad_norm": 4.169081687927246,
      "learning_rate": 5.477745389315545e-05,
      "loss": 0.1531,
      "step": 32320
    },
    {
      "epoch": 1.8110242836736408,
      "grad_norm": 2.8771891593933105,
      "learning_rate": 5.4763439654689165e-05,
      "loss": 0.1233,
      "step": 32330
    },
    {
      "epoch": 1.811584460689578,
      "grad_norm": 1.5059846639633179,
      "learning_rate": 5.4749425416222886e-05,
      "loss": 0.1638,
      "step": 32340
    },
    {
      "epoch": 1.812144637705515,
      "grad_norm": 1.471081018447876,
      "learning_rate": 5.47354111777566e-05,
      "loss": 0.1794,
      "step": 32350
    },
    {
      "epoch": 1.812704814721452,
      "grad_norm": 2.1022567749023438,
      "learning_rate": 5.472139693929033e-05,
      "loss": 0.1182,
      "step": 32360
    },
    {
      "epoch": 1.8132649917373889,
      "grad_norm": 2.704524278640747,
      "learning_rate": 5.470738270082404e-05,
      "loss": 0.1072,
      "step": 32370
    },
    {
      "epoch": 1.813825168753326,
      "grad_norm": 4.837814807891846,
      "learning_rate": 5.4693368462357765e-05,
      "loss": 0.1953,
      "step": 32380
    },
    {
      "epoch": 1.8143853457692631,
      "grad_norm": 4.157980442047119,
      "learning_rate": 5.467935422389148e-05,
      "loss": 0.2079,
      "step": 32390
    },
    {
      "epoch": 1.8149455227852003,
      "grad_norm": 4.858229160308838,
      "learning_rate": 5.466533998542519e-05,
      "loss": 0.1665,
      "step": 32400
    },
    {
      "epoch": 1.8155056998011372,
      "grad_norm": 3.663525342941284,
      "learning_rate": 5.4651325746958916e-05,
      "loss": 0.1182,
      "step": 32410
    },
    {
      "epoch": 1.816065876817074,
      "grad_norm": 3.2899575233459473,
      "learning_rate": 5.463731150849263e-05,
      "loss": 0.1628,
      "step": 32420
    },
    {
      "epoch": 1.8166260538330112,
      "grad_norm": 3.145437002182007,
      "learning_rate": 5.462329727002635e-05,
      "loss": 0.1607,
      "step": 32430
    },
    {
      "epoch": 1.8171862308489484,
      "grad_norm": 2.922194480895996,
      "learning_rate": 5.460928303156007e-05,
      "loss": 0.1633,
      "step": 32440
    },
    {
      "epoch": 1.8177464078648853,
      "grad_norm": 4.188393592834473,
      "learning_rate": 5.459526879309379e-05,
      "loss": 0.1723,
      "step": 32450
    },
    {
      "epoch": 1.8183065848808222,
      "grad_norm": 2.3704745769500732,
      "learning_rate": 5.4581254554627504e-05,
      "loss": 0.136,
      "step": 32460
    },
    {
      "epoch": 1.8188667618967593,
      "grad_norm": 6.172049522399902,
      "learning_rate": 5.4567240316161225e-05,
      "loss": 0.1333,
      "step": 32470
    },
    {
      "epoch": 1.8194269389126965,
      "grad_norm": 3.0668387413024902,
      "learning_rate": 5.455322607769494e-05,
      "loss": 0.1845,
      "step": 32480
    },
    {
      "epoch": 1.8199871159286336,
      "grad_norm": 0.9843103885650635,
      "learning_rate": 5.4539211839228655e-05,
      "loss": 0.1488,
      "step": 32490
    },
    {
      "epoch": 1.8205472929445705,
      "grad_norm": 5.349882125854492,
      "learning_rate": 5.4525197600762376e-05,
      "loss": 0.1447,
      "step": 32500
    },
    {
      "epoch": 1.8211074699605074,
      "grad_norm": 4.204653263092041,
      "learning_rate": 5.451118336229609e-05,
      "loss": 0.2273,
      "step": 32510
    },
    {
      "epoch": 1.8216676469764446,
      "grad_norm": 1.5228993892669678,
      "learning_rate": 5.449716912382982e-05,
      "loss": 0.1654,
      "step": 32520
    },
    {
      "epoch": 1.8222278239923817,
      "grad_norm": 2.180375814437866,
      "learning_rate": 5.448315488536353e-05,
      "loss": 0.1384,
      "step": 32530
    },
    {
      "epoch": 1.8227880010083186,
      "grad_norm": 3.866027355194092,
      "learning_rate": 5.4469140646897255e-05,
      "loss": 0.2713,
      "step": 32540
    },
    {
      "epoch": 1.8233481780242555,
      "grad_norm": 3.1523265838623047,
      "learning_rate": 5.445512640843097e-05,
      "loss": 0.1892,
      "step": 32550
    },
    {
      "epoch": 1.8239083550401927,
      "grad_norm": 3.515470266342163,
      "learning_rate": 5.444111216996469e-05,
      "loss": 0.1769,
      "step": 32560
    },
    {
      "epoch": 1.8244685320561298,
      "grad_norm": 2.819755792617798,
      "learning_rate": 5.4427097931498406e-05,
      "loss": 0.1094,
      "step": 32570
    },
    {
      "epoch": 1.825028709072067,
      "grad_norm": 3.791663885116577,
      "learning_rate": 5.441308369303212e-05,
      "loss": 0.2123,
      "step": 32580
    },
    {
      "epoch": 1.8255888860880038,
      "grad_norm": 3.1772608757019043,
      "learning_rate": 5.439906945456584e-05,
      "loss": 0.1324,
      "step": 32590
    },
    {
      "epoch": 1.8261490631039408,
      "grad_norm": 2.8793997764587402,
      "learning_rate": 5.438505521609956e-05,
      "loss": 0.1566,
      "step": 32600
    },
    {
      "epoch": 1.826709240119878,
      "grad_norm": 6.445951461791992,
      "learning_rate": 5.437104097763328e-05,
      "loss": 0.213,
      "step": 32610
    },
    {
      "epoch": 1.827269417135815,
      "grad_norm": 4.861263751983643,
      "learning_rate": 5.4357026739166994e-05,
      "loss": 0.2139,
      "step": 32620
    },
    {
      "epoch": 1.827829594151752,
      "grad_norm": 5.837805271148682,
      "learning_rate": 5.4343012500700715e-05,
      "loss": 0.1628,
      "step": 32630
    },
    {
      "epoch": 1.8283897711676889,
      "grad_norm": 3.46386456489563,
      "learning_rate": 5.432899826223443e-05,
      "loss": 0.1938,
      "step": 32640
    },
    {
      "epoch": 1.828949948183626,
      "grad_norm": 1.0990747213363647,
      "learning_rate": 5.431498402376816e-05,
      "loss": 0.1607,
      "step": 32650
    },
    {
      "epoch": 1.8295101251995631,
      "grad_norm": 1.570148229598999,
      "learning_rate": 5.4300969785301866e-05,
      "loss": 0.1112,
      "step": 32660
    },
    {
      "epoch": 1.8300703022155,
      "grad_norm": 2.4318785667419434,
      "learning_rate": 5.428695554683558e-05,
      "loss": 0.2842,
      "step": 32670
    },
    {
      "epoch": 1.8306304792314372,
      "grad_norm": 3.672711133956909,
      "learning_rate": 5.427294130836931e-05,
      "loss": 0.1577,
      "step": 32680
    },
    {
      "epoch": 1.831190656247374,
      "grad_norm": 1.6882847547531128,
      "learning_rate": 5.425892706990302e-05,
      "loss": 0.2096,
      "step": 32690
    },
    {
      "epoch": 1.8317508332633112,
      "grad_norm": 2.16147780418396,
      "learning_rate": 5.4244912831436746e-05,
      "loss": 0.2387,
      "step": 32700
    },
    {
      "epoch": 1.8323110102792484,
      "grad_norm": 1.1565203666687012,
      "learning_rate": 5.423089859297046e-05,
      "loss": 0.1951,
      "step": 32710
    },
    {
      "epoch": 1.8328711872951853,
      "grad_norm": 3.723921775817871,
      "learning_rate": 5.421688435450418e-05,
      "loss": 0.133,
      "step": 32720
    },
    {
      "epoch": 1.8334313643111222,
      "grad_norm": 4.354382038116455,
      "learning_rate": 5.4202870116037897e-05,
      "loss": 0.1618,
      "step": 32730
    },
    {
      "epoch": 1.8339915413270593,
      "grad_norm": 5.938557147979736,
      "learning_rate": 5.418885587757162e-05,
      "loss": 0.1669,
      "step": 32740
    },
    {
      "epoch": 1.8345517183429965,
      "grad_norm": 5.429024696350098,
      "learning_rate": 5.417484163910533e-05,
      "loss": 0.1608,
      "step": 32750
    },
    {
      "epoch": 1.8351118953589334,
      "grad_norm": 3.1275291442871094,
      "learning_rate": 5.416082740063905e-05,
      "loss": 0.1871,
      "step": 32760
    },
    {
      "epoch": 1.8356720723748705,
      "grad_norm": 1.9820646047592163,
      "learning_rate": 5.414681316217277e-05,
      "loss": 0.1297,
      "step": 32770
    },
    {
      "epoch": 1.8362322493908074,
      "grad_norm": 3.254295825958252,
      "learning_rate": 5.4132798923706484e-05,
      "loss": 0.2039,
      "step": 32780
    },
    {
      "epoch": 1.8367924264067446,
      "grad_norm": 2.9039268493652344,
      "learning_rate": 5.411878468524021e-05,
      "loss": 0.1888,
      "step": 32790
    },
    {
      "epoch": 1.8373526034226817,
      "grad_norm": 4.1149516105651855,
      "learning_rate": 5.410477044677392e-05,
      "loss": 0.2666,
      "step": 32800
    },
    {
      "epoch": 1.8379127804386186,
      "grad_norm": 3.8025598526000977,
      "learning_rate": 5.409075620830765e-05,
      "loss": 0.182,
      "step": 32810
    },
    {
      "epoch": 1.8384729574545555,
      "grad_norm": 3.9535250663757324,
      "learning_rate": 5.407674196984136e-05,
      "loss": 0.181,
      "step": 32820
    },
    {
      "epoch": 1.8390331344704927,
      "grad_norm": 5.208583831787109,
      "learning_rate": 5.4062727731375085e-05,
      "loss": 0.2738,
      "step": 32830
    },
    {
      "epoch": 1.8395933114864298,
      "grad_norm": 3.4326698780059814,
      "learning_rate": 5.40487134929088e-05,
      "loss": 0.107,
      "step": 32840
    },
    {
      "epoch": 1.8401534885023667,
      "grad_norm": 5.086480140686035,
      "learning_rate": 5.4034699254442514e-05,
      "loss": 0.1888,
      "step": 32850
    },
    {
      "epoch": 1.8407136655183036,
      "grad_norm": 4.561774730682373,
      "learning_rate": 5.4020685015976236e-05,
      "loss": 0.2179,
      "step": 32860
    },
    {
      "epoch": 1.8412738425342408,
      "grad_norm": 1.3456236124038696,
      "learning_rate": 5.400667077750995e-05,
      "loss": 0.119,
      "step": 32870
    },
    {
      "epoch": 1.841834019550178,
      "grad_norm": 3.4480860233306885,
      "learning_rate": 5.399265653904367e-05,
      "loss": 0.2224,
      "step": 32880
    },
    {
      "epoch": 1.842394196566115,
      "grad_norm": 2.0214362144470215,
      "learning_rate": 5.397864230057739e-05,
      "loss": 0.1282,
      "step": 32890
    },
    {
      "epoch": 1.842954373582052,
      "grad_norm": 3.0546605587005615,
      "learning_rate": 5.396462806211111e-05,
      "loss": 0.2518,
      "step": 32900
    },
    {
      "epoch": 1.8435145505979889,
      "grad_norm": 6.332855224609375,
      "learning_rate": 5.395061382364482e-05,
      "loss": 0.2533,
      "step": 32910
    },
    {
      "epoch": 1.844074727613926,
      "grad_norm": 3.9677462577819824,
      "learning_rate": 5.393659958517855e-05,
      "loss": 0.192,
      "step": 32920
    },
    {
      "epoch": 1.8446349046298631,
      "grad_norm": 3.8762366771698,
      "learning_rate": 5.392258534671226e-05,
      "loss": 0.1838,
      "step": 32930
    },
    {
      "epoch": 1.8451950816458,
      "grad_norm": 1.3506712913513184,
      "learning_rate": 5.3908571108245974e-05,
      "loss": 0.1903,
      "step": 32940
    },
    {
      "epoch": 1.845755258661737,
      "grad_norm": 4.191331386566162,
      "learning_rate": 5.38945568697797e-05,
      "loss": 0.2342,
      "step": 32950
    },
    {
      "epoch": 1.846315435677674,
      "grad_norm": 3.7360334396362305,
      "learning_rate": 5.388054263131341e-05,
      "loss": 0.1738,
      "step": 32960
    },
    {
      "epoch": 1.8468756126936112,
      "grad_norm": 5.314276695251465,
      "learning_rate": 5.386652839284714e-05,
      "loss": 0.1293,
      "step": 32970
    },
    {
      "epoch": 1.8474357897095484,
      "grad_norm": 3.5847301483154297,
      "learning_rate": 5.385251415438085e-05,
      "loss": 0.2372,
      "step": 32980
    },
    {
      "epoch": 1.8479959667254853,
      "grad_norm": 2.463111162185669,
      "learning_rate": 5.3838499915914575e-05,
      "loss": 0.1316,
      "step": 32990
    },
    {
      "epoch": 1.8485561437414222,
      "grad_norm": 5.706508636474609,
      "learning_rate": 5.382448567744829e-05,
      "loss": 0.1536,
      "step": 33000
    },
    {
      "epoch": 1.8491163207573593,
      "grad_norm": 3.20847487449646,
      "learning_rate": 5.381047143898201e-05,
      "loss": 0.1269,
      "step": 33010
    },
    {
      "epoch": 1.8496764977732965,
      "grad_norm": 1.478928565979004,
      "learning_rate": 5.3796457200515726e-05,
      "loss": 0.1692,
      "step": 33020
    },
    {
      "epoch": 1.8502366747892334,
      "grad_norm": 5.4319915771484375,
      "learning_rate": 5.378244296204944e-05,
      "loss": 0.2363,
      "step": 33030
    },
    {
      "epoch": 1.8507968518051703,
      "grad_norm": 1.8744306564331055,
      "learning_rate": 5.376842872358316e-05,
      "loss": 0.1667,
      "step": 33040
    },
    {
      "epoch": 1.8513570288211074,
      "grad_norm": 2.8594913482666016,
      "learning_rate": 5.375441448511688e-05,
      "loss": 0.1616,
      "step": 33050
    },
    {
      "epoch": 1.8519172058370446,
      "grad_norm": 3.7388505935668945,
      "learning_rate": 5.3740400246650605e-05,
      "loss": 0.1429,
      "step": 33060
    },
    {
      "epoch": 1.8524773828529817,
      "grad_norm": 4.223423957824707,
      "learning_rate": 5.372638600818431e-05,
      "loss": 0.2208,
      "step": 33070
    },
    {
      "epoch": 1.8530375598689186,
      "grad_norm": 3.378638982772827,
      "learning_rate": 5.371237176971804e-05,
      "loss": 0.302,
      "step": 33080
    },
    {
      "epoch": 1.8535977368848555,
      "grad_norm": 2.866773843765259,
      "learning_rate": 5.3698357531251756e-05,
      "loss": 0.2037,
      "step": 33090
    },
    {
      "epoch": 1.8541579139007927,
      "grad_norm": 7.393355846405029,
      "learning_rate": 5.368434329278548e-05,
      "loss": 0.1976,
      "step": 33100
    },
    {
      "epoch": 1.8547180909167298,
      "grad_norm": 1.6197566986083984,
      "learning_rate": 5.367032905431919e-05,
      "loss": 0.105,
      "step": 33110
    },
    {
      "epoch": 1.8552782679326667,
      "grad_norm": 4.191032886505127,
      "learning_rate": 5.365631481585291e-05,
      "loss": 0.1069,
      "step": 33120
    },
    {
      "epoch": 1.8558384449486036,
      "grad_norm": 1.6259052753448486,
      "learning_rate": 5.364230057738663e-05,
      "loss": 0.1652,
      "step": 33130
    },
    {
      "epoch": 1.8563986219645408,
      "grad_norm": 4.356235980987549,
      "learning_rate": 5.362828633892034e-05,
      "loss": 0.1834,
      "step": 33140
    },
    {
      "epoch": 1.8569587989804779,
      "grad_norm": 2.09749698638916,
      "learning_rate": 5.3614272100454065e-05,
      "loss": 0.1089,
      "step": 33150
    },
    {
      "epoch": 1.857518975996415,
      "grad_norm": 2.598808765411377,
      "learning_rate": 5.360025786198778e-05,
      "loss": 0.2331,
      "step": 33160
    },
    {
      "epoch": 1.858079153012352,
      "grad_norm": 4.056375503540039,
      "learning_rate": 5.35862436235215e-05,
      "loss": 0.1404,
      "step": 33170
    },
    {
      "epoch": 1.8586393300282888,
      "grad_norm": 4.781313896179199,
      "learning_rate": 5.3572229385055216e-05,
      "loss": 0.1374,
      "step": 33180
    },
    {
      "epoch": 1.859199507044226,
      "grad_norm": 3.8205854892730713,
      "learning_rate": 5.3558215146588944e-05,
      "loss": 0.1387,
      "step": 33190
    },
    {
      "epoch": 1.8597596840601631,
      "grad_norm": 5.646346092224121,
      "learning_rate": 5.354420090812265e-05,
      "loss": 0.2337,
      "step": 33200
    },
    {
      "epoch": 1.8603198610761,
      "grad_norm": 2.3894903659820557,
      "learning_rate": 5.353018666965637e-05,
      "loss": 0.1214,
      "step": 33210
    },
    {
      "epoch": 1.860880038092037,
      "grad_norm": 7.403929710388184,
      "learning_rate": 5.3516172431190095e-05,
      "loss": 0.1819,
      "step": 33220
    },
    {
      "epoch": 1.861440215107974,
      "grad_norm": 4.5055694580078125,
      "learning_rate": 5.35021581927238e-05,
      "loss": 0.1377,
      "step": 33230
    },
    {
      "epoch": 1.8620003921239112,
      "grad_norm": 3.8340048789978027,
      "learning_rate": 5.348814395425753e-05,
      "loss": 0.2014,
      "step": 33240
    },
    {
      "epoch": 1.8625605691398484,
      "grad_norm": 5.7365922927856445,
      "learning_rate": 5.3474129715791246e-05,
      "loss": 0.1856,
      "step": 33250
    },
    {
      "epoch": 1.8631207461557853,
      "grad_norm": 2.036271095275879,
      "learning_rate": 5.346011547732497e-05,
      "loss": 0.3183,
      "step": 33260
    },
    {
      "epoch": 1.8636809231717222,
      "grad_norm": 3.706874132156372,
      "learning_rate": 5.344610123885868e-05,
      "loss": 0.1151,
      "step": 33270
    },
    {
      "epoch": 1.8642411001876593,
      "grad_norm": 3.10208797454834,
      "learning_rate": 5.3432087000392404e-05,
      "loss": 0.1352,
      "step": 33280
    },
    {
      "epoch": 1.8648012772035965,
      "grad_norm": 2.879403829574585,
      "learning_rate": 5.341807276192612e-05,
      "loss": 0.1278,
      "step": 33290
    },
    {
      "epoch": 1.8653614542195334,
      "grad_norm": 3.556870460510254,
      "learning_rate": 5.3404058523459833e-05,
      "loss": 0.1307,
      "step": 33300
    },
    {
      "epoch": 1.8659216312354703,
      "grad_norm": 2.947126865386963,
      "learning_rate": 5.3390044284993555e-05,
      "loss": 0.1904,
      "step": 33310
    },
    {
      "epoch": 1.8664818082514074,
      "grad_norm": 1.6092381477355957,
      "learning_rate": 5.337603004652727e-05,
      "loss": 0.1791,
      "step": 33320
    },
    {
      "epoch": 1.8670419852673446,
      "grad_norm": 4.996896266937256,
      "learning_rate": 5.336201580806099e-05,
      "loss": 0.1116,
      "step": 33330
    },
    {
      "epoch": 1.8676021622832815,
      "grad_norm": 4.999267101287842,
      "learning_rate": 5.3348001569594706e-05,
      "loss": 0.1535,
      "step": 33340
    },
    {
      "epoch": 1.8681623392992186,
      "grad_norm": 6.043344497680664,
      "learning_rate": 5.3333987331128434e-05,
      "loss": 0.2335,
      "step": 33350
    },
    {
      "epoch": 1.8687225163151555,
      "grad_norm": 5.388620853424072,
      "learning_rate": 5.331997309266214e-05,
      "loss": 0.1844,
      "step": 33360
    },
    {
      "epoch": 1.8692826933310926,
      "grad_norm": 1.680941104888916,
      "learning_rate": 5.330595885419587e-05,
      "loss": 0.1737,
      "step": 33370
    },
    {
      "epoch": 1.8698428703470298,
      "grad_norm": 4.675735950469971,
      "learning_rate": 5.3291944615729585e-05,
      "loss": 0.1204,
      "step": 33380
    },
    {
      "epoch": 1.8704030473629667,
      "grad_norm": 3.4135360717773438,
      "learning_rate": 5.327793037726331e-05,
      "loss": 0.1155,
      "step": 33390
    },
    {
      "epoch": 1.8709632243789036,
      "grad_norm": 4.8214430809021,
      "learning_rate": 5.326391613879702e-05,
      "loss": 0.1802,
      "step": 33400
    },
    {
      "epoch": 1.8715234013948407,
      "grad_norm": 3.3535425662994385,
      "learning_rate": 5.3249901900330736e-05,
      "loss": 0.1924,
      "step": 33410
    },
    {
      "epoch": 1.8720835784107779,
      "grad_norm": 2.9427902698516846,
      "learning_rate": 5.323588766186446e-05,
      "loss": 0.1355,
      "step": 33420
    },
    {
      "epoch": 1.8726437554267148,
      "grad_norm": 3.1273398399353027,
      "learning_rate": 5.322187342339817e-05,
      "loss": 0.1205,
      "step": 33430
    },
    {
      "epoch": 1.873203932442652,
      "grad_norm": 5.157545566558838,
      "learning_rate": 5.3207859184931894e-05,
      "loss": 0.1853,
      "step": 33440
    },
    {
      "epoch": 1.8737641094585888,
      "grad_norm": 3.2538843154907227,
      "learning_rate": 5.319384494646561e-05,
      "loss": 0.1682,
      "step": 33450
    },
    {
      "epoch": 1.874324286474526,
      "grad_norm": 4.441575527191162,
      "learning_rate": 5.317983070799934e-05,
      "loss": 0.2152,
      "step": 33460
    },
    {
      "epoch": 1.8748844634904631,
      "grad_norm": 5.906880855560303,
      "learning_rate": 5.3165816469533045e-05,
      "loss": 0.2351,
      "step": 33470
    },
    {
      "epoch": 1.8754446405064,
      "grad_norm": 2.637803316116333,
      "learning_rate": 5.315180223106677e-05,
      "loss": 0.1833,
      "step": 33480
    },
    {
      "epoch": 1.876004817522337,
      "grad_norm": 3.200709581375122,
      "learning_rate": 5.313778799260049e-05,
      "loss": 0.1022,
      "step": 33490
    },
    {
      "epoch": 1.876564994538274,
      "grad_norm": 2.4917449951171875,
      "learning_rate": 5.3123773754134196e-05,
      "loss": 0.1869,
      "step": 33500
    },
    {
      "epoch": 1.8771251715542112,
      "grad_norm": 1.598864197731018,
      "learning_rate": 5.3109759515667924e-05,
      "loss": 0.1376,
      "step": 33510
    },
    {
      "epoch": 1.8776853485701481,
      "grad_norm": 4.750253200531006,
      "learning_rate": 5.309574527720164e-05,
      "loss": 0.1569,
      "step": 33520
    },
    {
      "epoch": 1.878245525586085,
      "grad_norm": 3.9269542694091797,
      "learning_rate": 5.308173103873536e-05,
      "loss": 0.186,
      "step": 33530
    },
    {
      "epoch": 1.8788057026020222,
      "grad_norm": 3.3930771350860596,
      "learning_rate": 5.3067716800269075e-05,
      "loss": 0.2582,
      "step": 33540
    },
    {
      "epoch": 1.8793658796179593,
      "grad_norm": 3.2554728984832764,
      "learning_rate": 5.30537025618028e-05,
      "loss": 0.2175,
      "step": 33550
    },
    {
      "epoch": 1.8799260566338964,
      "grad_norm": 2.7848570346832275,
      "learning_rate": 5.303968832333651e-05,
      "loss": 0.1902,
      "step": 33560
    },
    {
      "epoch": 1.8804862336498334,
      "grad_norm": 2.9942479133605957,
      "learning_rate": 5.302567408487023e-05,
      "loss": 0.2837,
      "step": 33570
    },
    {
      "epoch": 1.8810464106657703,
      "grad_norm": 1.6247652769088745,
      "learning_rate": 5.301165984640395e-05,
      "loss": 0.1735,
      "step": 33580
    },
    {
      "epoch": 1.8816065876817074,
      "grad_norm": 3.2846710681915283,
      "learning_rate": 5.299764560793766e-05,
      "loss": 0.152,
      "step": 33590
    },
    {
      "epoch": 1.8821667646976445,
      "grad_norm": 2.5887646675109863,
      "learning_rate": 5.2983631369471384e-05,
      "loss": 0.1338,
      "step": 33600
    },
    {
      "epoch": 1.8827269417135815,
      "grad_norm": 3.002805233001709,
      "learning_rate": 5.29696171310051e-05,
      "loss": 0.2597,
      "step": 33610
    },
    {
      "epoch": 1.8832871187295184,
      "grad_norm": 2.0196707248687744,
      "learning_rate": 5.295560289253883e-05,
      "loss": 0.1078,
      "step": 33620
    },
    {
      "epoch": 1.8838472957454555,
      "grad_norm": 2.1384329795837402,
      "learning_rate": 5.2941588654072535e-05,
      "loss": 0.1187,
      "step": 33630
    },
    {
      "epoch": 1.8844074727613926,
      "grad_norm": 2.149402618408203,
      "learning_rate": 5.2927574415606263e-05,
      "loss": 0.1114,
      "step": 33640
    },
    {
      "epoch": 1.8849676497773298,
      "grad_norm": 5.605396747589111,
      "learning_rate": 5.291356017713998e-05,
      "loss": 0.1103,
      "step": 33650
    },
    {
      "epoch": 1.8855278267932667,
      "grad_norm": 1.1852210760116577,
      "learning_rate": 5.28995459386737e-05,
      "loss": 0.1346,
      "step": 33660
    },
    {
      "epoch": 1.8860880038092036,
      "grad_norm": 7.405263900756836,
      "learning_rate": 5.2885531700207414e-05,
      "loss": 0.1969,
      "step": 33670
    },
    {
      "epoch": 1.8866481808251407,
      "grad_norm": 3.8261806964874268,
      "learning_rate": 5.287151746174113e-05,
      "loss": 0.1462,
      "step": 33680
    },
    {
      "epoch": 1.8872083578410779,
      "grad_norm": 3.001465082168579,
      "learning_rate": 5.285750322327485e-05,
      "loss": 0.1988,
      "step": 33690
    },
    {
      "epoch": 1.8877685348570148,
      "grad_norm": 3.4518635272979736,
      "learning_rate": 5.2843488984808565e-05,
      "loss": 0.1393,
      "step": 33700
    },
    {
      "epoch": 1.8883287118729517,
      "grad_norm": 1.6781070232391357,
      "learning_rate": 5.282947474634229e-05,
      "loss": 0.2705,
      "step": 33710
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 3.5930850505828857,
      "learning_rate": 5.2815460507876e-05,
      "loss": 0.2221,
      "step": 33720
    },
    {
      "epoch": 1.889449065904826,
      "grad_norm": 1.832603931427002,
      "learning_rate": 5.280144626940973e-05,
      "loss": 0.1315,
      "step": 33730
    },
    {
      "epoch": 1.890009242920763,
      "grad_norm": 4.378277778625488,
      "learning_rate": 5.278743203094344e-05,
      "loss": 0.2114,
      "step": 33740
    },
    {
      "epoch": 1.8905694199367,
      "grad_norm": 6.613366603851318,
      "learning_rate": 5.2773417792477166e-05,
      "loss": 0.188,
      "step": 33750
    },
    {
      "epoch": 1.891129596952637,
      "grad_norm": 1.398424744606018,
      "learning_rate": 5.275940355401088e-05,
      "loss": 0.1212,
      "step": 33760
    },
    {
      "epoch": 1.891689773968574,
      "grad_norm": 4.111019134521484,
      "learning_rate": 5.274538931554459e-05,
      "loss": 0.1338,
      "step": 33770
    },
    {
      "epoch": 1.8922499509845112,
      "grad_norm": 6.283969402313232,
      "learning_rate": 5.273137507707832e-05,
      "loss": 0.1952,
      "step": 33780
    },
    {
      "epoch": 1.8928101280004481,
      "grad_norm": 1.9019253253936768,
      "learning_rate": 5.271736083861203e-05,
      "loss": 0.1449,
      "step": 33790
    },
    {
      "epoch": 1.893370305016385,
      "grad_norm": 2.382704019546509,
      "learning_rate": 5.2703346600145754e-05,
      "loss": 0.1065,
      "step": 33800
    },
    {
      "epoch": 1.8939304820323222,
      "grad_norm": 6.44360876083374,
      "learning_rate": 5.268933236167947e-05,
      "loss": 0.1852,
      "step": 33810
    },
    {
      "epoch": 1.8944906590482593,
      "grad_norm": 2.799529552459717,
      "learning_rate": 5.267531812321319e-05,
      "loss": 0.1608,
      "step": 33820
    },
    {
      "epoch": 1.8950508360641964,
      "grad_norm": 2.8303215503692627,
      "learning_rate": 5.2661303884746905e-05,
      "loss": 0.1147,
      "step": 33830
    },
    {
      "epoch": 1.8956110130801334,
      "grad_norm": 2.5727431774139404,
      "learning_rate": 5.2647289646280626e-05,
      "loss": 0.1671,
      "step": 33840
    },
    {
      "epoch": 1.8961711900960703,
      "grad_norm": 1.8381675481796265,
      "learning_rate": 5.263327540781434e-05,
      "loss": 0.1809,
      "step": 33850
    },
    {
      "epoch": 1.8967313671120074,
      "grad_norm": 1.9466443061828613,
      "learning_rate": 5.2619261169348056e-05,
      "loss": 0.0931,
      "step": 33860
    },
    {
      "epoch": 1.8972915441279445,
      "grad_norm": 0.8997418284416199,
      "learning_rate": 5.260524693088178e-05,
      "loss": 0.1078,
      "step": 33870
    },
    {
      "epoch": 1.8978517211438815,
      "grad_norm": 2.6409499645233154,
      "learning_rate": 5.259123269241549e-05,
      "loss": 0.152,
      "step": 33880
    },
    {
      "epoch": 1.8984118981598184,
      "grad_norm": 3.0442728996276855,
      "learning_rate": 5.257721845394922e-05,
      "loss": 0.1844,
      "step": 33890
    },
    {
      "epoch": 1.8989720751757555,
      "grad_norm": 3.3169643878936768,
      "learning_rate": 5.256320421548293e-05,
      "loss": 0.1075,
      "step": 33900
    },
    {
      "epoch": 1.8995322521916926,
      "grad_norm": 4.342371463775635,
      "learning_rate": 5.2549189977016656e-05,
      "loss": 0.1145,
      "step": 33910
    },
    {
      "epoch": 1.9000924292076298,
      "grad_norm": 4.783419609069824,
      "learning_rate": 5.253517573855037e-05,
      "loss": 0.1987,
      "step": 33920
    },
    {
      "epoch": 1.9006526062235667,
      "grad_norm": 4.4052839279174805,
      "learning_rate": 5.252116150008409e-05,
      "loss": 0.2245,
      "step": 33930
    },
    {
      "epoch": 1.9012127832395036,
      "grad_norm": 4.0532917976379395,
      "learning_rate": 5.250714726161781e-05,
      "loss": 0.1304,
      "step": 33940
    },
    {
      "epoch": 1.9017729602554407,
      "grad_norm": 3.685927152633667,
      "learning_rate": 5.249313302315152e-05,
      "loss": 0.1527,
      "step": 33950
    },
    {
      "epoch": 1.9023331372713779,
      "grad_norm": 3.9787352085113525,
      "learning_rate": 5.2479118784685244e-05,
      "loss": 0.1509,
      "step": 33960
    },
    {
      "epoch": 1.9028933142873148,
      "grad_norm": 4.261077404022217,
      "learning_rate": 5.246510454621896e-05,
      "loss": 0.1536,
      "step": 33970
    },
    {
      "epoch": 1.9034534913032517,
      "grad_norm": 1.8368020057678223,
      "learning_rate": 5.245109030775268e-05,
      "loss": 0.3335,
      "step": 33980
    },
    {
      "epoch": 1.9040136683191888,
      "grad_norm": 1.2198013067245483,
      "learning_rate": 5.2437076069286395e-05,
      "loss": 0.1716,
      "step": 33990
    },
    {
      "epoch": 1.904573845335126,
      "grad_norm": 1.348741888999939,
      "learning_rate": 5.2423061830820116e-05,
      "loss": 0.1072,
      "step": 34000
    },
    {
      "epoch": 1.9051340223510629,
      "grad_norm": 4.30763578414917,
      "learning_rate": 5.240904759235383e-05,
      "loss": 0.1144,
      "step": 34010
    },
    {
      "epoch": 1.905694199367,
      "grad_norm": 3.2487006187438965,
      "learning_rate": 5.239503335388756e-05,
      "loss": 0.165,
      "step": 34020
    },
    {
      "epoch": 1.906254376382937,
      "grad_norm": 3.3669934272766113,
      "learning_rate": 5.238101911542127e-05,
      "loss": 0.1998,
      "step": 34030
    },
    {
      "epoch": 1.906814553398874,
      "grad_norm": 2.4797475337982178,
      "learning_rate": 5.236700487695498e-05,
      "loss": 0.2353,
      "step": 34040
    },
    {
      "epoch": 1.9073747304148112,
      "grad_norm": 3.779768705368042,
      "learning_rate": 5.235299063848871e-05,
      "loss": 0.1397,
      "step": 34050
    },
    {
      "epoch": 1.9079349074307481,
      "grad_norm": 4.261595249176025,
      "learning_rate": 5.233897640002242e-05,
      "loss": 0.2093,
      "step": 34060
    },
    {
      "epoch": 1.908495084446685,
      "grad_norm": 3.3781936168670654,
      "learning_rate": 5.2324962161556146e-05,
      "loss": 0.2922,
      "step": 34070
    },
    {
      "epoch": 1.9090552614626222,
      "grad_norm": 6.115586280822754,
      "learning_rate": 5.231094792308986e-05,
      "loss": 0.1211,
      "step": 34080
    },
    {
      "epoch": 1.9096154384785593,
      "grad_norm": 1.263181447982788,
      "learning_rate": 5.229693368462358e-05,
      "loss": 0.2805,
      "step": 34090
    },
    {
      "epoch": 1.9101756154944962,
      "grad_norm": 0.7287585139274597,
      "learning_rate": 5.22829194461573e-05,
      "loss": 0.1738,
      "step": 34100
    },
    {
      "epoch": 1.9107357925104334,
      "grad_norm": 3.43247127532959,
      "learning_rate": 5.226890520769102e-05,
      "loss": 0.1369,
      "step": 34110
    },
    {
      "epoch": 1.9112959695263703,
      "grad_norm": 2.3964223861694336,
      "learning_rate": 5.2254890969224734e-05,
      "loss": 0.1388,
      "step": 34120
    },
    {
      "epoch": 1.9118561465423074,
      "grad_norm": 1.0559804439544678,
      "learning_rate": 5.224087673075845e-05,
      "loss": 0.15,
      "step": 34130
    },
    {
      "epoch": 1.9124163235582445,
      "grad_norm": 3.1065874099731445,
      "learning_rate": 5.222686249229217e-05,
      "loss": 0.1035,
      "step": 34140
    },
    {
      "epoch": 1.9129765005741814,
      "grad_norm": 3.2473533153533936,
      "learning_rate": 5.2212848253825885e-05,
      "loss": 0.2063,
      "step": 34150
    },
    {
      "epoch": 1.9135366775901184,
      "grad_norm": 4.375254154205322,
      "learning_rate": 5.219883401535961e-05,
      "loss": 0.1646,
      "step": 34160
    },
    {
      "epoch": 1.9140968546060555,
      "grad_norm": 1.8356428146362305,
      "learning_rate": 5.218481977689332e-05,
      "loss": 0.1353,
      "step": 34170
    },
    {
      "epoch": 1.9146570316219926,
      "grad_norm": 2.125304937362671,
      "learning_rate": 5.217080553842705e-05,
      "loss": 0.196,
      "step": 34180
    },
    {
      "epoch": 1.9152172086379295,
      "grad_norm": 4.020051956176758,
      "learning_rate": 5.2156791299960764e-05,
      "loss": 0.1096,
      "step": 34190
    },
    {
      "epoch": 1.9157773856538665,
      "grad_norm": 3.731201171875,
      "learning_rate": 5.2142777061494486e-05,
      "loss": 0.1833,
      "step": 34200
    },
    {
      "epoch": 1.9163375626698036,
      "grad_norm": 5.128891944885254,
      "learning_rate": 5.21287628230282e-05,
      "loss": 0.1619,
      "step": 34210
    },
    {
      "epoch": 1.9168977396857407,
      "grad_norm": 3.7377591133117676,
      "learning_rate": 5.2114748584561915e-05,
      "loss": 0.1357,
      "step": 34220
    },
    {
      "epoch": 1.9174579167016779,
      "grad_norm": 5.999131679534912,
      "learning_rate": 5.2100734346095637e-05,
      "loss": 0.1563,
      "step": 34230
    },
    {
      "epoch": 1.9180180937176148,
      "grad_norm": 2.517448663711548,
      "learning_rate": 5.208672010762935e-05,
      "loss": 0.1146,
      "step": 34240
    },
    {
      "epoch": 1.9185782707335517,
      "grad_norm": 2.3330087661743164,
      "learning_rate": 5.207270586916307e-05,
      "loss": 0.1588,
      "step": 34250
    },
    {
      "epoch": 1.9191384477494888,
      "grad_norm": 2.932199001312256,
      "learning_rate": 5.205869163069679e-05,
      "loss": 0.2257,
      "step": 34260
    },
    {
      "epoch": 1.919698624765426,
      "grad_norm": 6.89320182800293,
      "learning_rate": 5.204467739223051e-05,
      "loss": 0.2218,
      "step": 34270
    },
    {
      "epoch": 1.9202588017813629,
      "grad_norm": 2.417029619216919,
      "learning_rate": 5.2030663153764224e-05,
      "loss": 0.2551,
      "step": 34280
    },
    {
      "epoch": 1.9208189787972998,
      "grad_norm": 2.018400192260742,
      "learning_rate": 5.201664891529795e-05,
      "loss": 0.1652,
      "step": 34290
    },
    {
      "epoch": 1.921379155813237,
      "grad_norm": 5.067285060882568,
      "learning_rate": 5.200263467683166e-05,
      "loss": 0.1781,
      "step": 34300
    },
    {
      "epoch": 1.921939332829174,
      "grad_norm": 2.015864849090576,
      "learning_rate": 5.1988620438365375e-05,
      "loss": 0.2263,
      "step": 34310
    },
    {
      "epoch": 1.9224995098451112,
      "grad_norm": 5.1581645011901855,
      "learning_rate": 5.19746061998991e-05,
      "loss": 0.198,
      "step": 34320
    },
    {
      "epoch": 1.9230596868610481,
      "grad_norm": 2.752466917037964,
      "learning_rate": 5.196059196143281e-05,
      "loss": 0.1773,
      "step": 34330
    },
    {
      "epoch": 1.923619863876985,
      "grad_norm": 0.7110176682472229,
      "learning_rate": 5.194657772296654e-05,
      "loss": 0.1665,
      "step": 34340
    },
    {
      "epoch": 1.9241800408929222,
      "grad_norm": 1.75381600856781,
      "learning_rate": 5.1932563484500254e-05,
      "loss": 0.2034,
      "step": 34350
    },
    {
      "epoch": 1.9247402179088593,
      "grad_norm": 3.802464008331299,
      "learning_rate": 5.1918549246033976e-05,
      "loss": 0.1334,
      "step": 34360
    },
    {
      "epoch": 1.9253003949247962,
      "grad_norm": 6.126594066619873,
      "learning_rate": 5.190453500756769e-05,
      "loss": 0.1842,
      "step": 34370
    },
    {
      "epoch": 1.9258605719407331,
      "grad_norm": 3.379896879196167,
      "learning_rate": 5.189052076910141e-05,
      "loss": 0.227,
      "step": 34380
    },
    {
      "epoch": 1.9264207489566703,
      "grad_norm": 1.384171724319458,
      "learning_rate": 5.187650653063513e-05,
      "loss": 0.1057,
      "step": 34390
    },
    {
      "epoch": 1.9269809259726074,
      "grad_norm": 2.2500860691070557,
      "learning_rate": 5.186249229216884e-05,
      "loss": 0.1173,
      "step": 34400
    },
    {
      "epoch": 1.9275411029885445,
      "grad_norm": 4.805517196655273,
      "learning_rate": 5.184847805370256e-05,
      "loss": 0.1277,
      "step": 34410
    },
    {
      "epoch": 1.9281012800044814,
      "grad_norm": 1.623451828956604,
      "learning_rate": 5.183446381523628e-05,
      "loss": 0.122,
      "step": 34420
    },
    {
      "epoch": 1.9286614570204184,
      "grad_norm": 4.004408836364746,
      "learning_rate": 5.1820449576770006e-05,
      "loss": 0.2407,
      "step": 34430
    },
    {
      "epoch": 1.9292216340363555,
      "grad_norm": 1.53684663772583,
      "learning_rate": 5.1806435338303714e-05,
      "loss": 0.1455,
      "step": 34440
    },
    {
      "epoch": 1.9297818110522926,
      "grad_norm": 4.0590925216674805,
      "learning_rate": 5.179242109983744e-05,
      "loss": 0.1285,
      "step": 34450
    },
    {
      "epoch": 1.9303419880682295,
      "grad_norm": 3.18117356300354,
      "learning_rate": 5.177840686137116e-05,
      "loss": 0.1332,
      "step": 34460
    },
    {
      "epoch": 1.9309021650841665,
      "grad_norm": 1.5047483444213867,
      "learning_rate": 5.176439262290488e-05,
      "loss": 0.1632,
      "step": 34470
    },
    {
      "epoch": 1.9314623421001036,
      "grad_norm": 4.135133266448975,
      "learning_rate": 5.175037838443859e-05,
      "loss": 0.1488,
      "step": 34480
    },
    {
      "epoch": 1.9320225191160407,
      "grad_norm": 3.1974287033081055,
      "learning_rate": 5.1736364145972315e-05,
      "loss": 0.2664,
      "step": 34490
    },
    {
      "epoch": 1.9325826961319779,
      "grad_norm": 2.005805253982544,
      "learning_rate": 5.172234990750603e-05,
      "loss": 0.1653,
      "step": 34500
    },
    {
      "epoch": 1.9331428731479148,
      "grad_norm": 4.106363296508789,
      "learning_rate": 5.1708335669039744e-05,
      "loss": 0.1496,
      "step": 34510
    },
    {
      "epoch": 1.9337030501638517,
      "grad_norm": 5.375769138336182,
      "learning_rate": 5.1694321430573466e-05,
      "loss": 0.1832,
      "step": 34520
    },
    {
      "epoch": 1.9342632271797888,
      "grad_norm": 3.4433369636535645,
      "learning_rate": 5.168030719210718e-05,
      "loss": 0.1423,
      "step": 34530
    },
    {
      "epoch": 1.934823404195726,
      "grad_norm": 4.119915008544922,
      "learning_rate": 5.16662929536409e-05,
      "loss": 0.1557,
      "step": 34540
    },
    {
      "epoch": 1.9353835812116629,
      "grad_norm": 3.2204415798187256,
      "learning_rate": 5.165227871517462e-05,
      "loss": 0.2478,
      "step": 34550
    },
    {
      "epoch": 1.9359437582275998,
      "grad_norm": 4.63081693649292,
      "learning_rate": 5.1638264476708345e-05,
      "loss": 0.1522,
      "step": 34560
    },
    {
      "epoch": 1.936503935243537,
      "grad_norm": 0.7239029407501221,
      "learning_rate": 5.162425023824205e-05,
      "loss": 0.1652,
      "step": 34570
    },
    {
      "epoch": 1.937064112259474,
      "grad_norm": 2.1573097705841064,
      "learning_rate": 5.161023599977578e-05,
      "loss": 0.1974,
      "step": 34580
    },
    {
      "epoch": 1.9376242892754112,
      "grad_norm": 2.6544549465179443,
      "learning_rate": 5.1596221761309496e-05,
      "loss": 0.2172,
      "step": 34590
    },
    {
      "epoch": 1.938184466291348,
      "grad_norm": 4.498708724975586,
      "learning_rate": 5.1582207522843204e-05,
      "loss": 0.1585,
      "step": 34600
    },
    {
      "epoch": 1.938744643307285,
      "grad_norm": 2.690551280975342,
      "learning_rate": 5.156819328437693e-05,
      "loss": 0.1541,
      "step": 34610
    },
    {
      "epoch": 1.9393048203232222,
      "grad_norm": 3.6275906562805176,
      "learning_rate": 5.155417904591065e-05,
      "loss": 0.1718,
      "step": 34620
    },
    {
      "epoch": 1.9398649973391593,
      "grad_norm": 2.750455141067505,
      "learning_rate": 5.154016480744437e-05,
      "loss": 0.1251,
      "step": 34630
    },
    {
      "epoch": 1.9404251743550962,
      "grad_norm": 1.920960783958435,
      "learning_rate": 5.152615056897808e-05,
      "loss": 0.0972,
      "step": 34640
    },
    {
      "epoch": 1.9409853513710331,
      "grad_norm": 3.0424883365631104,
      "learning_rate": 5.1512136330511805e-05,
      "loss": 0.1298,
      "step": 34650
    },
    {
      "epoch": 1.9415455283869703,
      "grad_norm": 3.741586923599243,
      "learning_rate": 5.149812209204552e-05,
      "loss": 0.3398,
      "step": 34660
    },
    {
      "epoch": 1.9421057054029074,
      "grad_norm": 4.0010786056518555,
      "learning_rate": 5.148410785357924e-05,
      "loss": 0.1095,
      "step": 34670
    },
    {
      "epoch": 1.9426658824188443,
      "grad_norm": 2.7501962184906006,
      "learning_rate": 5.1470093615112956e-05,
      "loss": 0.1056,
      "step": 34680
    },
    {
      "epoch": 1.9432260594347814,
      "grad_norm": 1.292137861251831,
      "learning_rate": 5.145607937664667e-05,
      "loss": 0.2074,
      "step": 34690
    },
    {
      "epoch": 1.9437862364507184,
      "grad_norm": 0.5664548873901367,
      "learning_rate": 5.144206513818039e-05,
      "loss": 0.1356,
      "step": 34700
    },
    {
      "epoch": 1.9443464134666555,
      "grad_norm": 2.154402017593384,
      "learning_rate": 5.142805089971411e-05,
      "loss": 0.1469,
      "step": 34710
    },
    {
      "epoch": 1.9449065904825926,
      "grad_norm": 1.479720115661621,
      "learning_rate": 5.1414036661247835e-05,
      "loss": 0.0816,
      "step": 34720
    },
    {
      "epoch": 1.9454667674985295,
      "grad_norm": 3.581078052520752,
      "learning_rate": 5.140002242278154e-05,
      "loss": 0.1899,
      "step": 34730
    },
    {
      "epoch": 1.9460269445144665,
      "grad_norm": 2.536377191543579,
      "learning_rate": 5.138600818431527e-05,
      "loss": 0.1313,
      "step": 34740
    },
    {
      "epoch": 1.9465871215304036,
      "grad_norm": 3.109327554702759,
      "learning_rate": 5.1371993945848986e-05,
      "loss": 0.142,
      "step": 34750
    },
    {
      "epoch": 1.9471472985463407,
      "grad_norm": 6.120450973510742,
      "learning_rate": 5.135797970738271e-05,
      "loss": 0.1561,
      "step": 34760
    },
    {
      "epoch": 1.9477074755622776,
      "grad_norm": 1.491744875907898,
      "learning_rate": 5.134396546891642e-05,
      "loss": 0.1432,
      "step": 34770
    },
    {
      "epoch": 1.9482676525782148,
      "grad_norm": 3.1526730060577393,
      "learning_rate": 5.132995123045014e-05,
      "loss": 0.2514,
      "step": 34780
    },
    {
      "epoch": 1.9488278295941517,
      "grad_norm": 2.2298951148986816,
      "learning_rate": 5.131593699198386e-05,
      "loss": 0.0877,
      "step": 34790
    },
    {
      "epoch": 1.9493880066100888,
      "grad_norm": 1.630509853363037,
      "learning_rate": 5.1301922753517573e-05,
      "loss": 0.1443,
      "step": 34800
    },
    {
      "epoch": 1.949948183626026,
      "grad_norm": 1.7149577140808105,
      "learning_rate": 5.1287908515051295e-05,
      "loss": 0.2083,
      "step": 34810
    },
    {
      "epoch": 1.9505083606419629,
      "grad_norm": 3.410646915435791,
      "learning_rate": 5.127389427658501e-05,
      "loss": 0.1673,
      "step": 34820
    },
    {
      "epoch": 1.9510685376578998,
      "grad_norm": 3.8829171657562256,
      "learning_rate": 5.125988003811874e-05,
      "loss": 0.182,
      "step": 34830
    },
    {
      "epoch": 1.951628714673837,
      "grad_norm": 1.7265756130218506,
      "learning_rate": 5.1245865799652446e-05,
      "loss": 0.1957,
      "step": 34840
    },
    {
      "epoch": 1.952188891689774,
      "grad_norm": 1.8505760431289673,
      "learning_rate": 5.1231851561186174e-05,
      "loss": 0.2027,
      "step": 34850
    },
    {
      "epoch": 1.952749068705711,
      "grad_norm": 1.5200624465942383,
      "learning_rate": 5.121783732271989e-05,
      "loss": 0.1558,
      "step": 34860
    },
    {
      "epoch": 1.9533092457216479,
      "grad_norm": 2.8886635303497314,
      "learning_rate": 5.12038230842536e-05,
      "loss": 0.17,
      "step": 34870
    },
    {
      "epoch": 1.953869422737585,
      "grad_norm": 3.975980520248413,
      "learning_rate": 5.1189808845787325e-05,
      "loss": 0.1114,
      "step": 34880
    },
    {
      "epoch": 1.9544295997535222,
      "grad_norm": 1.593979001045227,
      "learning_rate": 5.117579460732104e-05,
      "loss": 0.1315,
      "step": 34890
    },
    {
      "epoch": 1.9549897767694593,
      "grad_norm": 3.9798238277435303,
      "learning_rate": 5.116178036885476e-05,
      "loss": 0.1153,
      "step": 34900
    },
    {
      "epoch": 1.9555499537853962,
      "grad_norm": 2.8729450702667236,
      "learning_rate": 5.1147766130388476e-05,
      "loss": 0.1027,
      "step": 34910
    },
    {
      "epoch": 1.9561101308013331,
      "grad_norm": 2.0824685096740723,
      "learning_rate": 5.11337518919222e-05,
      "loss": 0.1359,
      "step": 34920
    },
    {
      "epoch": 1.9566703078172702,
      "grad_norm": 6.285609722137451,
      "learning_rate": 5.111973765345591e-05,
      "loss": 0.1692,
      "step": 34930
    },
    {
      "epoch": 1.9572304848332074,
      "grad_norm": 3.7705540657043457,
      "learning_rate": 5.1105723414989634e-05,
      "loss": 0.1799,
      "step": 34940
    },
    {
      "epoch": 1.9577906618491443,
      "grad_norm": 2.845599889755249,
      "learning_rate": 5.109170917652335e-05,
      "loss": 0.1119,
      "step": 34950
    },
    {
      "epoch": 1.9583508388650812,
      "grad_norm": 2.2369818687438965,
      "learning_rate": 5.1077694938057064e-05,
      "loss": 0.2527,
      "step": 34960
    },
    {
      "epoch": 1.9589110158810183,
      "grad_norm": 3.493173122406006,
      "learning_rate": 5.1063680699590785e-05,
      "loss": 0.1742,
      "step": 34970
    },
    {
      "epoch": 1.9594711928969555,
      "grad_norm": 2.8106393814086914,
      "learning_rate": 5.10496664611245e-05,
      "loss": 0.127,
      "step": 34980
    },
    {
      "epoch": 1.9600313699128926,
      "grad_norm": 2.1779019832611084,
      "learning_rate": 5.103565222265823e-05,
      "loss": 0.1965,
      "step": 34990
    },
    {
      "epoch": 1.9605915469288295,
      "grad_norm": 3.8571255207061768,
      "learning_rate": 5.1021637984191936e-05,
      "loss": 0.1227,
      "step": 35000
    },
    {
      "epoch": 1.9611517239447664,
      "grad_norm": 2.189707040786743,
      "learning_rate": 5.1007623745725664e-05,
      "loss": 0.1265,
      "step": 35010
    },
    {
      "epoch": 1.9617119009607036,
      "grad_norm": 2.940727710723877,
      "learning_rate": 5.099360950725938e-05,
      "loss": 0.12,
      "step": 35020
    },
    {
      "epoch": 1.9622720779766407,
      "grad_norm": 2.418870687484741,
      "learning_rate": 5.09795952687931e-05,
      "loss": 0.3153,
      "step": 35030
    },
    {
      "epoch": 1.9628322549925776,
      "grad_norm": 5.081405162811279,
      "learning_rate": 5.0965581030326815e-05,
      "loss": 0.1172,
      "step": 35040
    },
    {
      "epoch": 1.9633924320085145,
      "grad_norm": 4.153882026672363,
      "learning_rate": 5.095156679186053e-05,
      "loss": 0.2185,
      "step": 35050
    },
    {
      "epoch": 1.9639526090244517,
      "grad_norm": 2.367180109024048,
      "learning_rate": 5.093755255339425e-05,
      "loss": 0.1563,
      "step": 35060
    },
    {
      "epoch": 1.9645127860403888,
      "grad_norm": 5.788562774658203,
      "learning_rate": 5.0923538314927966e-05,
      "loss": 0.1505,
      "step": 35070
    },
    {
      "epoch": 1.965072963056326,
      "grad_norm": 2.038343906402588,
      "learning_rate": 5.090952407646169e-05,
      "loss": 0.1606,
      "step": 35080
    },
    {
      "epoch": 1.9656331400722629,
      "grad_norm": 2.709127902984619,
      "learning_rate": 5.08955098379954e-05,
      "loss": 0.1889,
      "step": 35090
    },
    {
      "epoch": 1.9661933170881998,
      "grad_norm": 2.5630719661712646,
      "learning_rate": 5.088149559952913e-05,
      "loss": 0.1362,
      "step": 35100
    },
    {
      "epoch": 1.966753494104137,
      "grad_norm": 5.509594440460205,
      "learning_rate": 5.086748136106284e-05,
      "loss": 0.1996,
      "step": 35110
    },
    {
      "epoch": 1.967313671120074,
      "grad_norm": 2.229586601257324,
      "learning_rate": 5.085346712259657e-05,
      "loss": 0.1687,
      "step": 35120
    },
    {
      "epoch": 1.967873848136011,
      "grad_norm": 2.549872875213623,
      "learning_rate": 5.083945288413028e-05,
      "loss": 0.1304,
      "step": 35130
    },
    {
      "epoch": 1.9684340251519479,
      "grad_norm": 1.156537652015686,
      "learning_rate": 5.082543864566399e-05,
      "loss": 0.1154,
      "step": 35140
    },
    {
      "epoch": 1.968994202167885,
      "grad_norm": 2.5220725536346436,
      "learning_rate": 5.081142440719772e-05,
      "loss": 0.1274,
      "step": 35150
    },
    {
      "epoch": 1.9695543791838221,
      "grad_norm": 3.855743885040283,
      "learning_rate": 5.079741016873143e-05,
      "loss": 0.2277,
      "step": 35160
    },
    {
      "epoch": 1.9701145561997593,
      "grad_norm": 2.7367775440216064,
      "learning_rate": 5.0783395930265154e-05,
      "loss": 0.1336,
      "step": 35170
    },
    {
      "epoch": 1.9706747332156962,
      "grad_norm": 2.5560874938964844,
      "learning_rate": 5.076938169179887e-05,
      "loss": 0.1638,
      "step": 35180
    },
    {
      "epoch": 1.971234910231633,
      "grad_norm": 5.223056316375732,
      "learning_rate": 5.075536745333259e-05,
      "loss": 0.2012,
      "step": 35190
    },
    {
      "epoch": 1.9717950872475702,
      "grad_norm": 2.4326534271240234,
      "learning_rate": 5.0741353214866305e-05,
      "loss": 0.1277,
      "step": 35200
    },
    {
      "epoch": 1.9723552642635074,
      "grad_norm": 1.1892343759536743,
      "learning_rate": 5.072733897640003e-05,
      "loss": 0.1083,
      "step": 35210
    },
    {
      "epoch": 1.9729154412794443,
      "grad_norm": 1.1001049280166626,
      "learning_rate": 5.071332473793374e-05,
      "loss": 0.2538,
      "step": 35220
    },
    {
      "epoch": 1.9734756182953812,
      "grad_norm": 2.6087498664855957,
      "learning_rate": 5.0699310499467456e-05,
      "loss": 0.1105,
      "step": 35230
    },
    {
      "epoch": 1.9740357953113183,
      "grad_norm": 1.6321463584899902,
      "learning_rate": 5.068529626100118e-05,
      "loss": 0.2073,
      "step": 35240
    },
    {
      "epoch": 1.9745959723272555,
      "grad_norm": 3.5162482261657715,
      "learning_rate": 5.067128202253489e-05,
      "loss": 0.3517,
      "step": 35250
    },
    {
      "epoch": 1.9751561493431926,
      "grad_norm": 3.9438669681549072,
      "learning_rate": 5.065726778406862e-05,
      "loss": 0.174,
      "step": 35260
    },
    {
      "epoch": 1.9757163263591295,
      "grad_norm": 2.8507819175720215,
      "learning_rate": 5.064325354560233e-05,
      "loss": 0.2245,
      "step": 35270
    },
    {
      "epoch": 1.9762765033750664,
      "grad_norm": 1.7507576942443848,
      "learning_rate": 5.062923930713606e-05,
      "loss": 0.1199,
      "step": 35280
    },
    {
      "epoch": 1.9768366803910036,
      "grad_norm": 3.8768153190612793,
      "learning_rate": 5.061522506866977e-05,
      "loss": 0.2137,
      "step": 35290
    },
    {
      "epoch": 1.9773968574069407,
      "grad_norm": 4.074046611785889,
      "learning_rate": 5.0601210830203494e-05,
      "loss": 0.1902,
      "step": 35300
    },
    {
      "epoch": 1.9779570344228776,
      "grad_norm": 2.4426827430725098,
      "learning_rate": 5.058719659173721e-05,
      "loss": 0.3008,
      "step": 35310
    },
    {
      "epoch": 1.9785172114388145,
      "grad_norm": 1.5235254764556885,
      "learning_rate": 5.057318235327092e-05,
      "loss": 0.1586,
      "step": 35320
    },
    {
      "epoch": 1.9790773884547517,
      "grad_norm": 3.2862513065338135,
      "learning_rate": 5.0559168114804645e-05,
      "loss": 0.1481,
      "step": 35330
    },
    {
      "epoch": 1.9796375654706888,
      "grad_norm": 3.4489853382110596,
      "learning_rate": 5.054515387633836e-05,
      "loss": 0.1431,
      "step": 35340
    },
    {
      "epoch": 1.9801977424866257,
      "grad_norm": 2.7270238399505615,
      "learning_rate": 5.053113963787208e-05,
      "loss": 0.1625,
      "step": 35350
    },
    {
      "epoch": 1.9807579195025629,
      "grad_norm": 0.9992182850837708,
      "learning_rate": 5.0517125399405796e-05,
      "loss": 0.1187,
      "step": 35360
    },
    {
      "epoch": 1.9813180965184998,
      "grad_norm": 4.763944149017334,
      "learning_rate": 5.050311116093952e-05,
      "loss": 0.1414,
      "step": 35370
    },
    {
      "epoch": 1.981878273534437,
      "grad_norm": 3.4743056297302246,
      "learning_rate": 5.048909692247323e-05,
      "loss": 0.1644,
      "step": 35380
    },
    {
      "epoch": 1.982438450550374,
      "grad_norm": 3.6340091228485107,
      "learning_rate": 5.047508268400696e-05,
      "loss": 0.1145,
      "step": 35390
    },
    {
      "epoch": 1.982998627566311,
      "grad_norm": 1.9460978507995605,
      "learning_rate": 5.046106844554067e-05,
      "loss": 0.1853,
      "step": 35400
    },
    {
      "epoch": 1.9835588045822479,
      "grad_norm": 2.853827953338623,
      "learning_rate": 5.044705420707438e-05,
      "loss": 0.1694,
      "step": 35410
    },
    {
      "epoch": 1.984118981598185,
      "grad_norm": 4.159029006958008,
      "learning_rate": 5.043303996860811e-05,
      "loss": 0.1472,
      "step": 35420
    },
    {
      "epoch": 1.9846791586141221,
      "grad_norm": 3.745155096054077,
      "learning_rate": 5.041902573014182e-05,
      "loss": 0.1836,
      "step": 35430
    },
    {
      "epoch": 1.985239335630059,
      "grad_norm": 5.43074369430542,
      "learning_rate": 5.040501149167555e-05,
      "loss": 0.1384,
      "step": 35440
    },
    {
      "epoch": 1.9857995126459962,
      "grad_norm": 3.211261510848999,
      "learning_rate": 5.039099725320926e-05,
      "loss": 0.1074,
      "step": 35450
    },
    {
      "epoch": 1.986359689661933,
      "grad_norm": 6.390030384063721,
      "learning_rate": 5.0376983014742984e-05,
      "loss": 0.2228,
      "step": 35460
    },
    {
      "epoch": 1.9869198666778702,
      "grad_norm": 1.8912018537521362,
      "learning_rate": 5.03629687762767e-05,
      "loss": 0.172,
      "step": 35470
    },
    {
      "epoch": 1.9874800436938074,
      "grad_norm": 1.3747421503067017,
      "learning_rate": 5.034895453781042e-05,
      "loss": 0.1267,
      "step": 35480
    },
    {
      "epoch": 1.9880402207097443,
      "grad_norm": 1.3394604921340942,
      "learning_rate": 5.0334940299344135e-05,
      "loss": 0.112,
      "step": 35490
    },
    {
      "epoch": 1.9886003977256812,
      "grad_norm": 2.174619197845459,
      "learning_rate": 5.032092606087785e-05,
      "loss": 0.092,
      "step": 35500
    },
    {
      "epoch": 1.9891605747416183,
      "grad_norm": 4.316507339477539,
      "learning_rate": 5.030691182241157e-05,
      "loss": 0.0988,
      "step": 35510
    },
    {
      "epoch": 1.9897207517575555,
      "grad_norm": 3.646125555038452,
      "learning_rate": 5.0292897583945286e-05,
      "loss": 0.213,
      "step": 35520
    },
    {
      "epoch": 1.9902809287734924,
      "grad_norm": 3.7200019359588623,
      "learning_rate": 5.0278883345479014e-05,
      "loss": 0.123,
      "step": 35530
    },
    {
      "epoch": 1.9908411057894293,
      "grad_norm": 3.144578695297241,
      "learning_rate": 5.026486910701272e-05,
      "loss": 0.2517,
      "step": 35540
    },
    {
      "epoch": 1.9914012828053664,
      "grad_norm": 3.7575066089630127,
      "learning_rate": 5.025085486854645e-05,
      "loss": 0.0974,
      "step": 35550
    },
    {
      "epoch": 1.9919614598213036,
      "grad_norm": 2.8197059631347656,
      "learning_rate": 5.0236840630080165e-05,
      "loss": 0.169,
      "step": 35560
    },
    {
      "epoch": 1.9925216368372407,
      "grad_norm": 1.8182835578918457,
      "learning_rate": 5.0222826391613886e-05,
      "loss": 0.1441,
      "step": 35570
    },
    {
      "epoch": 1.9930818138531776,
      "grad_norm": 8.63253116607666,
      "learning_rate": 5.02088121531476e-05,
      "loss": 0.1423,
      "step": 35580
    },
    {
      "epoch": 1.9936419908691145,
      "grad_norm": 5.09137487411499,
      "learning_rate": 5.019479791468132e-05,
      "loss": 0.1857,
      "step": 35590
    },
    {
      "epoch": 1.9942021678850517,
      "grad_norm": 2.110576629638672,
      "learning_rate": 5.018078367621504e-05,
      "loss": 0.1582,
      "step": 35600
    },
    {
      "epoch": 1.9947623449009888,
      "grad_norm": 1.9224607944488525,
      "learning_rate": 5.016676943774875e-05,
      "loss": 0.1267,
      "step": 35610
    },
    {
      "epoch": 1.9953225219169257,
      "grad_norm": 4.619019031524658,
      "learning_rate": 5.0152755199282474e-05,
      "loss": 0.1538,
      "step": 35620
    },
    {
      "epoch": 1.9958826989328626,
      "grad_norm": 1.94721519947052,
      "learning_rate": 5.013874096081619e-05,
      "loss": 0.2058,
      "step": 35630
    },
    {
      "epoch": 1.9964428759487998,
      "grad_norm": 0.8519774079322815,
      "learning_rate": 5.012472672234991e-05,
      "loss": 0.1596,
      "step": 35640
    },
    {
      "epoch": 1.997003052964737,
      "grad_norm": 3.4449641704559326,
      "learning_rate": 5.0110712483883625e-05,
      "loss": 0.1885,
      "step": 35650
    },
    {
      "epoch": 1.997563229980674,
      "grad_norm": 4.189281940460205,
      "learning_rate": 5.009669824541735e-05,
      "loss": 0.2335,
      "step": 35660
    },
    {
      "epoch": 1.998123406996611,
      "grad_norm": 5.911552906036377,
      "learning_rate": 5.008268400695106e-05,
      "loss": 0.2629,
      "step": 35670
    },
    {
      "epoch": 1.9986835840125479,
      "grad_norm": 1.908169150352478,
      "learning_rate": 5.006866976848479e-05,
      "loss": 0.1321,
      "step": 35680
    },
    {
      "epoch": 1.999243761028485,
      "grad_norm": 3.114729166030884,
      "learning_rate": 5.0054655530018504e-05,
      "loss": 0.0983,
      "step": 35690
    },
    {
      "epoch": 1.9998039380444221,
      "grad_norm": 4.186099529266357,
      "learning_rate": 5.004064129155221e-05,
      "loss": 0.1838,
      "step": 35700
    },
    {
      "epoch": 2.0003361062095624,
      "grad_norm": 2.312624931335449,
      "learning_rate": 5.002662705308594e-05,
      "loss": 0.2007,
      "step": 35710
    },
    {
      "epoch": 2.000896283225499,
      "grad_norm": 4.791044235229492,
      "learning_rate": 5.0012612814619655e-05,
      "loss": 0.1962,
      "step": 35720
    },
    {
      "epoch": 2.001456460241436,
      "grad_norm": 1.2320414781570435,
      "learning_rate": 4.999859857615337e-05,
      "loss": 0.1496,
      "step": 35730
    },
    {
      "epoch": 2.0020166372573733,
      "grad_norm": 1.7001419067382812,
      "learning_rate": 4.998458433768709e-05,
      "loss": 0.1238,
      "step": 35740
    },
    {
      "epoch": 2.0025768142733105,
      "grad_norm": 3.0190529823303223,
      "learning_rate": 4.9970570099220806e-05,
      "loss": 0.228,
      "step": 35750
    },
    {
      "epoch": 2.0031369912892476,
      "grad_norm": 3.3747377395629883,
      "learning_rate": 4.995655586075453e-05,
      "loss": 0.125,
      "step": 35760
    },
    {
      "epoch": 2.0036971683051843,
      "grad_norm": 2.6783995628356934,
      "learning_rate": 4.994254162228825e-05,
      "loss": 0.1818,
      "step": 35770
    },
    {
      "epoch": 2.0042573453211214,
      "grad_norm": 4.561264514923096,
      "learning_rate": 4.9928527383821964e-05,
      "loss": 0.1545,
      "step": 35780
    },
    {
      "epoch": 2.0048175223370586,
      "grad_norm": 1.7310196161270142,
      "learning_rate": 4.9914513145355685e-05,
      "loss": 0.195,
      "step": 35790
    },
    {
      "epoch": 2.0053776993529957,
      "grad_norm": 1.8930060863494873,
      "learning_rate": 4.990049890688941e-05,
      "loss": 0.1445,
      "step": 35800
    },
    {
      "epoch": 2.0059378763689324,
      "grad_norm": 3.1685969829559326,
      "learning_rate": 4.988648466842312e-05,
      "loss": 0.1849,
      "step": 35810
    },
    {
      "epoch": 2.0064980533848695,
      "grad_norm": 2.9958043098449707,
      "learning_rate": 4.987247042995684e-05,
      "loss": 0.167,
      "step": 35820
    },
    {
      "epoch": 2.0070582304008067,
      "grad_norm": 4.011610507965088,
      "learning_rate": 4.985845619149056e-05,
      "loss": 0.1398,
      "step": 35830
    },
    {
      "epoch": 2.007618407416744,
      "grad_norm": 4.454493045806885,
      "learning_rate": 4.984444195302427e-05,
      "loss": 0.3057,
      "step": 35840
    },
    {
      "epoch": 2.008178584432681,
      "grad_norm": 1.7347959280014038,
      "learning_rate": 4.9830427714557994e-05,
      "loss": 0.115,
      "step": 35850
    },
    {
      "epoch": 2.0087387614486176,
      "grad_norm": 3.9841206073760986,
      "learning_rate": 4.981641347609171e-05,
      "loss": 0.1023,
      "step": 35860
    },
    {
      "epoch": 2.0092989384645548,
      "grad_norm": 3.3207671642303467,
      "learning_rate": 4.980239923762543e-05,
      "loss": 0.2079,
      "step": 35870
    },
    {
      "epoch": 2.009859115480492,
      "grad_norm": 1.199925422668457,
      "learning_rate": 4.978838499915915e-05,
      "loss": 0.1251,
      "step": 35880
    },
    {
      "epoch": 2.010419292496429,
      "grad_norm": 2.2507123947143555,
      "learning_rate": 4.977437076069287e-05,
      "loss": 0.1795,
      "step": 35890
    },
    {
      "epoch": 2.0109794695123657,
      "grad_norm": 2.447545289993286,
      "learning_rate": 4.976035652222659e-05,
      "loss": 0.1846,
      "step": 35900
    },
    {
      "epoch": 2.011539646528303,
      "grad_norm": 6.1627116203308105,
      "learning_rate": 4.97463422837603e-05,
      "loss": 0.1502,
      "step": 35910
    },
    {
      "epoch": 2.01209982354424,
      "grad_norm": 1.956589937210083,
      "learning_rate": 4.973232804529402e-05,
      "loss": 0.1694,
      "step": 35920
    },
    {
      "epoch": 2.012660000560177,
      "grad_norm": 2.331993579864502,
      "learning_rate": 4.971831380682774e-05,
      "loss": 0.1278,
      "step": 35930
    },
    {
      "epoch": 2.0132201775761143,
      "grad_norm": 5.270256042480469,
      "learning_rate": 4.9704299568361454e-05,
      "loss": 0.2515,
      "step": 35940
    },
    {
      "epoch": 2.013780354592051,
      "grad_norm": 3.990377187728882,
      "learning_rate": 4.9690285329895175e-05,
      "loss": 0.2091,
      "step": 35950
    },
    {
      "epoch": 2.014340531607988,
      "grad_norm": 3.089811325073242,
      "learning_rate": 4.96762710914289e-05,
      "loss": 0.1852,
      "step": 35960
    },
    {
      "epoch": 2.0149007086239252,
      "grad_norm": 1.4785929918289185,
      "learning_rate": 4.966225685296261e-05,
      "loss": 0.2091,
      "step": 35970
    },
    {
      "epoch": 2.0154608856398624,
      "grad_norm": 3.5306992530822754,
      "learning_rate": 4.964824261449633e-05,
      "loss": 0.1217,
      "step": 35980
    },
    {
      "epoch": 2.016021062655799,
      "grad_norm": 3.297426700592041,
      "learning_rate": 4.963422837603005e-05,
      "loss": 0.1359,
      "step": 35990
    },
    {
      "epoch": 2.016581239671736,
      "grad_norm": 2.6415107250213623,
      "learning_rate": 4.962021413756377e-05,
      "loss": 0.1692,
      "step": 36000
    },
    {
      "epoch": 2.0171414166876733,
      "grad_norm": 3.824174642562866,
      "learning_rate": 4.9606199899097484e-05,
      "loss": 0.2974,
      "step": 36010
    },
    {
      "epoch": 2.0177015937036105,
      "grad_norm": 3.08809232711792,
      "learning_rate": 4.95921856606312e-05,
      "loss": 0.1164,
      "step": 36020
    },
    {
      "epoch": 2.0182617707195476,
      "grad_norm": 4.042666912078857,
      "learning_rate": 4.957817142216492e-05,
      "loss": 0.1773,
      "step": 36030
    },
    {
      "epoch": 2.0188219477354843,
      "grad_norm": 3.4376463890075684,
      "learning_rate": 4.956415718369864e-05,
      "loss": 0.1629,
      "step": 36040
    },
    {
      "epoch": 2.0193821247514214,
      "grad_norm": 3.8971076011657715,
      "learning_rate": 4.955014294523236e-05,
      "loss": 0.0892,
      "step": 36050
    },
    {
      "epoch": 2.0199423017673586,
      "grad_norm": 2.2440879344940186,
      "learning_rate": 4.953612870676608e-05,
      "loss": 0.0838,
      "step": 36060
    },
    {
      "epoch": 2.0205024787832957,
      "grad_norm": 3.8327744007110596,
      "learning_rate": 4.952211446829979e-05,
      "loss": 0.1322,
      "step": 36070
    },
    {
      "epoch": 2.0210626557992324,
      "grad_norm": 5.420738220214844,
      "learning_rate": 4.9508100229833515e-05,
      "loss": 0.1487,
      "step": 36080
    },
    {
      "epoch": 2.0216228328151695,
      "grad_norm": 3.250230550765991,
      "learning_rate": 4.9494085991367236e-05,
      "loss": 0.126,
      "step": 36090
    },
    {
      "epoch": 2.0221830098311067,
      "grad_norm": 3.191310405731201,
      "learning_rate": 4.9480071752900944e-05,
      "loss": 0.1284,
      "step": 36100
    },
    {
      "epoch": 2.022743186847044,
      "grad_norm": 5.505771160125732,
      "learning_rate": 4.9466057514434666e-05,
      "loss": 0.108,
      "step": 36110
    },
    {
      "epoch": 2.0233033638629805,
      "grad_norm": 0.986077070236206,
      "learning_rate": 4.945204327596839e-05,
      "loss": 0.1805,
      "step": 36120
    },
    {
      "epoch": 2.0238635408789176,
      "grad_norm": 3.2597124576568604,
      "learning_rate": 4.94380290375021e-05,
      "loss": 0.1979,
      "step": 36130
    },
    {
      "epoch": 2.0244237178948548,
      "grad_norm": 3.7576117515563965,
      "learning_rate": 4.942401479903582e-05,
      "loss": 0.1711,
      "step": 36140
    },
    {
      "epoch": 2.024983894910792,
      "grad_norm": 7.4840474128723145,
      "learning_rate": 4.9410000560569545e-05,
      "loss": 0.1287,
      "step": 36150
    },
    {
      "epoch": 2.025544071926729,
      "grad_norm": 4.122438430786133,
      "learning_rate": 4.939598632210326e-05,
      "loss": 0.1704,
      "step": 36160
    },
    {
      "epoch": 2.0261042489426657,
      "grad_norm": 4.393864631652832,
      "learning_rate": 4.938197208363698e-05,
      "loss": 0.1403,
      "step": 36170
    },
    {
      "epoch": 2.026664425958603,
      "grad_norm": 4.750946998596191,
      "learning_rate": 4.9367957845170696e-05,
      "loss": 0.1254,
      "step": 36180
    },
    {
      "epoch": 2.02722460297454,
      "grad_norm": 2.1155521869659424,
      "learning_rate": 4.935394360670441e-05,
      "loss": 0.1784,
      "step": 36190
    },
    {
      "epoch": 2.027784779990477,
      "grad_norm": 4.2018914222717285,
      "learning_rate": 4.933992936823813e-05,
      "loss": 0.1437,
      "step": 36200
    },
    {
      "epoch": 2.028344957006414,
      "grad_norm": 4.824227809906006,
      "learning_rate": 4.932591512977185e-05,
      "loss": 0.1889,
      "step": 36210
    },
    {
      "epoch": 2.028905134022351,
      "grad_norm": 1.9598042964935303,
      "learning_rate": 4.931190089130557e-05,
      "loss": 0.1152,
      "step": 36220
    },
    {
      "epoch": 2.029465311038288,
      "grad_norm": 4.573408126831055,
      "learning_rate": 4.929788665283929e-05,
      "loss": 0.138,
      "step": 36230
    },
    {
      "epoch": 2.0300254880542252,
      "grad_norm": 1.4601929187774658,
      "learning_rate": 4.9283872414373005e-05,
      "loss": 0.0884,
      "step": 36240
    },
    {
      "epoch": 2.0305856650701624,
      "grad_norm": 3.5514774322509766,
      "learning_rate": 4.9269858175906726e-05,
      "loss": 0.073,
      "step": 36250
    },
    {
      "epoch": 2.031145842086099,
      "grad_norm": 1.290791630744934,
      "learning_rate": 4.925584393744044e-05,
      "loss": 0.1081,
      "step": 36260
    },
    {
      "epoch": 2.031706019102036,
      "grad_norm": 1.5596648454666138,
      "learning_rate": 4.924182969897416e-05,
      "loss": 0.1852,
      "step": 36270
    },
    {
      "epoch": 2.0322661961179733,
      "grad_norm": 4.6247663497924805,
      "learning_rate": 4.922781546050788e-05,
      "loss": 0.1417,
      "step": 36280
    },
    {
      "epoch": 2.0328263731339105,
      "grad_norm": 3.631913185119629,
      "learning_rate": 4.921380122204159e-05,
      "loss": 0.1195,
      "step": 36290
    },
    {
      "epoch": 2.033386550149847,
      "grad_norm": 5.322076320648193,
      "learning_rate": 4.9199786983575313e-05,
      "loss": 0.184,
      "step": 36300
    },
    {
      "epoch": 2.0339467271657843,
      "grad_norm": 3.058321952819824,
      "learning_rate": 4.9185772745109035e-05,
      "loss": 0.1042,
      "step": 36310
    },
    {
      "epoch": 2.0345069041817214,
      "grad_norm": 4.645498275756836,
      "learning_rate": 4.917175850664275e-05,
      "loss": 0.1701,
      "step": 36320
    },
    {
      "epoch": 2.0350670811976586,
      "grad_norm": 3.7281923294067383,
      "learning_rate": 4.915774426817647e-05,
      "loss": 0.1758,
      "step": 36330
    },
    {
      "epoch": 2.0356272582135957,
      "grad_norm": 4.841441631317139,
      "learning_rate": 4.9143730029710186e-05,
      "loss": 0.1418,
      "step": 36340
    },
    {
      "epoch": 2.0361874352295324,
      "grad_norm": 3.931960344314575,
      "learning_rate": 4.912971579124391e-05,
      "loss": 0.112,
      "step": 36350
    },
    {
      "epoch": 2.0367476122454695,
      "grad_norm": 2.9139859676361084,
      "learning_rate": 4.911570155277763e-05,
      "loss": 0.1706,
      "step": 36360
    },
    {
      "epoch": 2.0373077892614067,
      "grad_norm": 7.952676773071289,
      "learning_rate": 4.9101687314311344e-05,
      "loss": 0.1545,
      "step": 36370
    },
    {
      "epoch": 2.037867966277344,
      "grad_norm": 2.4913535118103027,
      "learning_rate": 4.908767307584506e-05,
      "loss": 0.1487,
      "step": 36380
    },
    {
      "epoch": 2.0384281432932805,
      "grad_norm": 2.7944397926330566,
      "learning_rate": 4.907365883737878e-05,
      "loss": 0.1303,
      "step": 36390
    },
    {
      "epoch": 2.0389883203092176,
      "grad_norm": 1.6701937913894653,
      "learning_rate": 4.9059644598912495e-05,
      "loss": 0.1768,
      "step": 36400
    },
    {
      "epoch": 2.0395484973251548,
      "grad_norm": 1.589247703552246,
      "learning_rate": 4.9045630360446216e-05,
      "loss": 0.161,
      "step": 36410
    },
    {
      "epoch": 2.040108674341092,
      "grad_norm": 2.2999613285064697,
      "learning_rate": 4.903161612197993e-05,
      "loss": 0.1562,
      "step": 36420
    },
    {
      "epoch": 2.040668851357029,
      "grad_norm": 3.325054407119751,
      "learning_rate": 4.901760188351365e-05,
      "loss": 0.1052,
      "step": 36430
    },
    {
      "epoch": 2.0412290283729657,
      "grad_norm": 2.2510287761688232,
      "learning_rate": 4.9003587645047374e-05,
      "loss": 0.1722,
      "step": 36440
    },
    {
      "epoch": 2.041789205388903,
      "grad_norm": 5.6713457107543945,
      "learning_rate": 4.898957340658109e-05,
      "loss": 0.1511,
      "step": 36450
    },
    {
      "epoch": 2.04234938240484,
      "grad_norm": 3.732299327850342,
      "learning_rate": 4.897555916811481e-05,
      "loss": 0.1408,
      "step": 36460
    },
    {
      "epoch": 2.042909559420777,
      "grad_norm": 5.601471424102783,
      "learning_rate": 4.8961544929648525e-05,
      "loss": 0.1311,
      "step": 36470
    },
    {
      "epoch": 2.043469736436714,
      "grad_norm": 3.0584068298339844,
      "learning_rate": 4.894753069118224e-05,
      "loss": 0.2009,
      "step": 36480
    },
    {
      "epoch": 2.044029913452651,
      "grad_norm": 5.778555870056152,
      "learning_rate": 4.893351645271596e-05,
      "loss": 0.1334,
      "step": 36490
    },
    {
      "epoch": 2.044590090468588,
      "grad_norm": 2.6399331092834473,
      "learning_rate": 4.891950221424968e-05,
      "loss": 0.2653,
      "step": 36500
    },
    {
      "epoch": 2.045150267484525,
      "grad_norm": 1.205413579940796,
      "learning_rate": 4.89054879757834e-05,
      "loss": 0.144,
      "step": 36510
    },
    {
      "epoch": 2.0457104445004624,
      "grad_norm": 3.965721607208252,
      "learning_rate": 4.889147373731712e-05,
      "loss": 0.2426,
      "step": 36520
    },
    {
      "epoch": 2.046270621516399,
      "grad_norm": 2.123655319213867,
      "learning_rate": 4.8877459498850834e-05,
      "loss": 0.1441,
      "step": 36530
    },
    {
      "epoch": 2.046830798532336,
      "grad_norm": 4.449999809265137,
      "learning_rate": 4.8863445260384555e-05,
      "loss": 0.119,
      "step": 36540
    },
    {
      "epoch": 2.0473909755482733,
      "grad_norm": 1.9088401794433594,
      "learning_rate": 4.884943102191828e-05,
      "loss": 0.1409,
      "step": 36550
    },
    {
      "epoch": 2.0479511525642105,
      "grad_norm": 4.027937412261963,
      "learning_rate": 4.8835416783451985e-05,
      "loss": 0.2065,
      "step": 36560
    },
    {
      "epoch": 2.048511329580147,
      "grad_norm": 1.8373619318008423,
      "learning_rate": 4.8821402544985706e-05,
      "loss": 0.1334,
      "step": 36570
    },
    {
      "epoch": 2.0490715065960843,
      "grad_norm": 4.053055763244629,
      "learning_rate": 4.880738830651943e-05,
      "loss": 0.1467,
      "step": 36580
    },
    {
      "epoch": 2.0496316836120214,
      "grad_norm": 2.208766222000122,
      "learning_rate": 4.879337406805314e-05,
      "loss": 0.1319,
      "step": 36590
    },
    {
      "epoch": 2.0501918606279586,
      "grad_norm": 2.869330644607544,
      "learning_rate": 4.8779359829586864e-05,
      "loss": 0.1329,
      "step": 36600
    },
    {
      "epoch": 2.0507520376438957,
      "grad_norm": 5.006513595581055,
      "learning_rate": 4.876534559112058e-05,
      "loss": 0.1315,
      "step": 36610
    },
    {
      "epoch": 2.0513122146598324,
      "grad_norm": 5.325945854187012,
      "learning_rate": 4.87513313526543e-05,
      "loss": 0.1843,
      "step": 36620
    },
    {
      "epoch": 2.0518723916757695,
      "grad_norm": 3.118091344833374,
      "learning_rate": 4.873731711418802e-05,
      "loss": 0.1194,
      "step": 36630
    },
    {
      "epoch": 2.0524325686917066,
      "grad_norm": 3.1424825191497803,
      "learning_rate": 4.872330287572174e-05,
      "loss": 0.1649,
      "step": 36640
    },
    {
      "epoch": 2.052992745707644,
      "grad_norm": 2.9938580989837646,
      "learning_rate": 4.870928863725545e-05,
      "loss": 0.1365,
      "step": 36650
    },
    {
      "epoch": 2.0535529227235805,
      "grad_norm": 6.414351463317871,
      "learning_rate": 4.869527439878917e-05,
      "loss": 0.266,
      "step": 36660
    },
    {
      "epoch": 2.0541130997395176,
      "grad_norm": 2.2118608951568604,
      "learning_rate": 4.868126016032289e-05,
      "loss": 0.1565,
      "step": 36670
    },
    {
      "epoch": 2.0546732767554547,
      "grad_norm": 2.009371280670166,
      "learning_rate": 4.866724592185661e-05,
      "loss": 0.0944,
      "step": 36680
    },
    {
      "epoch": 2.055233453771392,
      "grad_norm": 4.788317680358887,
      "learning_rate": 4.8653231683390324e-05,
      "loss": 0.1101,
      "step": 36690
    },
    {
      "epoch": 2.0557936307873286,
      "grad_norm": 6.804869174957275,
      "learning_rate": 4.8639217444924045e-05,
      "loss": 0.2411,
      "step": 36700
    },
    {
      "epoch": 2.0563538078032657,
      "grad_norm": 4.658665657043457,
      "learning_rate": 4.862520320645777e-05,
      "loss": 0.094,
      "step": 36710
    },
    {
      "epoch": 2.056913984819203,
      "grad_norm": 1.3248335123062134,
      "learning_rate": 4.861118896799148e-05,
      "loss": 0.1247,
      "step": 36720
    },
    {
      "epoch": 2.05747416183514,
      "grad_norm": 5.022470951080322,
      "learning_rate": 4.85971747295252e-05,
      "loss": 0.1029,
      "step": 36730
    },
    {
      "epoch": 2.058034338851077,
      "grad_norm": 3.423145294189453,
      "learning_rate": 4.858316049105892e-05,
      "loss": 0.1194,
      "step": 36740
    },
    {
      "epoch": 2.058594515867014,
      "grad_norm": 3.3057985305786133,
      "learning_rate": 4.856914625259263e-05,
      "loss": 0.1574,
      "step": 36750
    },
    {
      "epoch": 2.059154692882951,
      "grad_norm": 1.842679738998413,
      "learning_rate": 4.8555132014126354e-05,
      "loss": 0.1017,
      "step": 36760
    },
    {
      "epoch": 2.059714869898888,
      "grad_norm": 4.498553276062012,
      "learning_rate": 4.854111777566007e-05,
      "loss": 0.1222,
      "step": 36770
    },
    {
      "epoch": 2.060275046914825,
      "grad_norm": 2.628234386444092,
      "learning_rate": 4.852710353719379e-05,
      "loss": 0.1904,
      "step": 36780
    },
    {
      "epoch": 2.060835223930762,
      "grad_norm": 2.8149380683898926,
      "learning_rate": 4.851308929872751e-05,
      "loss": 0.1651,
      "step": 36790
    },
    {
      "epoch": 2.061395400946699,
      "grad_norm": 4.1445841789245605,
      "learning_rate": 4.849907506026123e-05,
      "loss": 0.1684,
      "step": 36800
    },
    {
      "epoch": 2.061955577962636,
      "grad_norm": 3.531944751739502,
      "learning_rate": 4.848506082179495e-05,
      "loss": 0.1865,
      "step": 36810
    },
    {
      "epoch": 2.0625157549785733,
      "grad_norm": 4.612893104553223,
      "learning_rate": 4.847104658332867e-05,
      "loss": 0.1276,
      "step": 36820
    },
    {
      "epoch": 2.0630759319945104,
      "grad_norm": 1.4808948040008545,
      "learning_rate": 4.845703234486238e-05,
      "loss": 0.0938,
      "step": 36830
    },
    {
      "epoch": 2.063636109010447,
      "grad_norm": 6.212050914764404,
      "learning_rate": 4.84430181063961e-05,
      "loss": 0.1626,
      "step": 36840
    },
    {
      "epoch": 2.0641962860263843,
      "grad_norm": 1.9028905630111694,
      "learning_rate": 4.842900386792982e-05,
      "loss": 0.1538,
      "step": 36850
    },
    {
      "epoch": 2.0647564630423214,
      "grad_norm": 0.8161979913711548,
      "learning_rate": 4.8414989629463536e-05,
      "loss": 0.1212,
      "step": 36860
    },
    {
      "epoch": 2.0653166400582585,
      "grad_norm": 2.019394636154175,
      "learning_rate": 4.840097539099726e-05,
      "loss": 0.1738,
      "step": 36870
    },
    {
      "epoch": 2.0658768170741952,
      "grad_norm": 6.034072399139404,
      "learning_rate": 4.838696115253097e-05,
      "loss": 0.1524,
      "step": 36880
    },
    {
      "epoch": 2.0664369940901324,
      "grad_norm": 4.246789932250977,
      "learning_rate": 4.837294691406469e-05,
      "loss": 0.1506,
      "step": 36890
    },
    {
      "epoch": 2.0669971711060695,
      "grad_norm": 2.6336874961853027,
      "learning_rate": 4.8358932675598415e-05,
      "loss": 0.1796,
      "step": 36900
    },
    {
      "epoch": 2.0675573481220066,
      "grad_norm": 0.874274492263794,
      "learning_rate": 4.834491843713213e-05,
      "loss": 0.109,
      "step": 36910
    },
    {
      "epoch": 2.068117525137944,
      "grad_norm": 4.5108489990234375,
      "learning_rate": 4.833090419866585e-05,
      "loss": 0.1661,
      "step": 36920
    },
    {
      "epoch": 2.0686777021538805,
      "grad_norm": 2.334320068359375,
      "learning_rate": 4.8316889960199566e-05,
      "loss": 0.196,
      "step": 36930
    },
    {
      "epoch": 2.0692378791698176,
      "grad_norm": 1.0651036500930786,
      "learning_rate": 4.830287572173328e-05,
      "loss": 0.1639,
      "step": 36940
    },
    {
      "epoch": 2.0697980561857547,
      "grad_norm": 2.989231824874878,
      "learning_rate": 4.8288861483267e-05,
      "loss": 0.1335,
      "step": 36950
    },
    {
      "epoch": 2.070358233201692,
      "grad_norm": 2.2135300636291504,
      "learning_rate": 4.827484724480072e-05,
      "loss": 0.1225,
      "step": 36960
    },
    {
      "epoch": 2.0709184102176286,
      "grad_norm": 3.1102707386016846,
      "learning_rate": 4.826083300633444e-05,
      "loss": 0.1849,
      "step": 36970
    },
    {
      "epoch": 2.0714785872335657,
      "grad_norm": 2.9246068000793457,
      "learning_rate": 4.824681876786816e-05,
      "loss": 0.2232,
      "step": 36980
    },
    {
      "epoch": 2.072038764249503,
      "grad_norm": 3.2901244163513184,
      "learning_rate": 4.8232804529401875e-05,
      "loss": 0.1565,
      "step": 36990
    },
    {
      "epoch": 2.07259894126544,
      "grad_norm": 3.1640329360961914,
      "learning_rate": 4.8218790290935596e-05,
      "loss": 0.1295,
      "step": 37000
    },
    {
      "epoch": 2.073159118281377,
      "grad_norm": 2.8191945552825928,
      "learning_rate": 4.820477605246931e-05,
      "loss": 0.1641,
      "step": 37010
    },
    {
      "epoch": 2.073719295297314,
      "grad_norm": 1.4493629932403564,
      "learning_rate": 4.8190761814003026e-05,
      "loss": 0.1438,
      "step": 37020
    },
    {
      "epoch": 2.074279472313251,
      "grad_norm": 0.9382480382919312,
      "learning_rate": 4.817674757553675e-05,
      "loss": 0.0867,
      "step": 37030
    },
    {
      "epoch": 2.074839649329188,
      "grad_norm": 4.689262866973877,
      "learning_rate": 4.816273333707046e-05,
      "loss": 0.1291,
      "step": 37040
    },
    {
      "epoch": 2.075399826345125,
      "grad_norm": 4.7521209716796875,
      "learning_rate": 4.8148719098604183e-05,
      "loss": 0.2125,
      "step": 37050
    },
    {
      "epoch": 2.075960003361062,
      "grad_norm": 4.727580547332764,
      "learning_rate": 4.8134704860137905e-05,
      "loss": 0.1703,
      "step": 37060
    },
    {
      "epoch": 2.076520180376999,
      "grad_norm": 3.767122983932495,
      "learning_rate": 4.812069062167162e-05,
      "loss": 0.1974,
      "step": 37070
    },
    {
      "epoch": 2.077080357392936,
      "grad_norm": 0.8469663262367249,
      "learning_rate": 4.810667638320534e-05,
      "loss": 0.1607,
      "step": 37080
    },
    {
      "epoch": 2.0776405344088733,
      "grad_norm": 1.1334218978881836,
      "learning_rate": 4.8092662144739056e-05,
      "loss": 0.2445,
      "step": 37090
    },
    {
      "epoch": 2.0782007114248104,
      "grad_norm": 3.1126153469085693,
      "learning_rate": 4.807864790627278e-05,
      "loss": 0.1863,
      "step": 37100
    },
    {
      "epoch": 2.078760888440747,
      "grad_norm": 5.262630462646484,
      "learning_rate": 4.806463366780649e-05,
      "loss": 0.1448,
      "step": 37110
    },
    {
      "epoch": 2.0793210654566843,
      "grad_norm": 1.5840286016464233,
      "learning_rate": 4.805061942934021e-05,
      "loss": 0.1015,
      "step": 37120
    },
    {
      "epoch": 2.0798812424726214,
      "grad_norm": 3.647780418395996,
      "learning_rate": 4.803660519087393e-05,
      "loss": 0.1256,
      "step": 37130
    },
    {
      "epoch": 2.0804414194885585,
      "grad_norm": 1.3982115983963013,
      "learning_rate": 4.802259095240765e-05,
      "loss": 0.122,
      "step": 37140
    },
    {
      "epoch": 2.0810015965044952,
      "grad_norm": 5.602316856384277,
      "learning_rate": 4.8008576713941365e-05,
      "loss": 0.2063,
      "step": 37150
    },
    {
      "epoch": 2.0815617735204324,
      "grad_norm": 2.3448915481567383,
      "learning_rate": 4.7994562475475086e-05,
      "loss": 0.1735,
      "step": 37160
    },
    {
      "epoch": 2.0821219505363695,
      "grad_norm": 3.415586233139038,
      "learning_rate": 4.798054823700881e-05,
      "loss": 0.1162,
      "step": 37170
    },
    {
      "epoch": 2.0826821275523066,
      "grad_norm": 3.4279260635375977,
      "learning_rate": 4.796653399854252e-05,
      "loss": 0.1643,
      "step": 37180
    },
    {
      "epoch": 2.0832423045682438,
      "grad_norm": 2.297386646270752,
      "learning_rate": 4.7952519760076244e-05,
      "loss": 0.1086,
      "step": 37190
    },
    {
      "epoch": 2.0838024815841805,
      "grad_norm": 2.591615676879883,
      "learning_rate": 4.793850552160996e-05,
      "loss": 0.1791,
      "step": 37200
    },
    {
      "epoch": 2.0843626586001176,
      "grad_norm": 2.384197473526001,
      "learning_rate": 4.7924491283143674e-05,
      "loss": 0.1433,
      "step": 37210
    },
    {
      "epoch": 2.0849228356160547,
      "grad_norm": 4.514084815979004,
      "learning_rate": 4.7910477044677395e-05,
      "loss": 0.0934,
      "step": 37220
    },
    {
      "epoch": 2.085483012631992,
      "grad_norm": 1.0920830965042114,
      "learning_rate": 4.789646280621111e-05,
      "loss": 0.2418,
      "step": 37230
    },
    {
      "epoch": 2.0860431896479286,
      "grad_norm": 1.3164637088775635,
      "learning_rate": 4.788244856774483e-05,
      "loss": 0.174,
      "step": 37240
    },
    {
      "epoch": 2.0866033666638657,
      "grad_norm": 5.0511579513549805,
      "learning_rate": 4.786843432927855e-05,
      "loss": 0.1596,
      "step": 37250
    },
    {
      "epoch": 2.087163543679803,
      "grad_norm": 4.588644981384277,
      "learning_rate": 4.785442009081227e-05,
      "loss": 0.106,
      "step": 37260
    },
    {
      "epoch": 2.08772372069574,
      "grad_norm": 2.7767491340637207,
      "learning_rate": 4.784040585234599e-05,
      "loss": 0.149,
      "step": 37270
    },
    {
      "epoch": 2.088283897711677,
      "grad_norm": 1.929636001586914,
      "learning_rate": 4.7826391613879704e-05,
      "loss": 0.0979,
      "step": 37280
    },
    {
      "epoch": 2.088844074727614,
      "grad_norm": 3.2527518272399902,
      "learning_rate": 4.781237737541342e-05,
      "loss": 0.1031,
      "step": 37290
    },
    {
      "epoch": 2.089404251743551,
      "grad_norm": 0.77618008852005,
      "learning_rate": 4.779836313694714e-05,
      "loss": 0.1322,
      "step": 37300
    },
    {
      "epoch": 2.089964428759488,
      "grad_norm": 1.8869736194610596,
      "learning_rate": 4.7784348898480855e-05,
      "loss": 0.2035,
      "step": 37310
    },
    {
      "epoch": 2.090524605775425,
      "grad_norm": 1.528295636177063,
      "learning_rate": 4.7770334660014576e-05,
      "loss": 0.147,
      "step": 37320
    },
    {
      "epoch": 2.091084782791362,
      "grad_norm": 3.9730265140533447,
      "learning_rate": 4.77563204215483e-05,
      "loss": 0.1646,
      "step": 37330
    },
    {
      "epoch": 2.091644959807299,
      "grad_norm": 5.055266380310059,
      "learning_rate": 4.774230618308201e-05,
      "loss": 0.1861,
      "step": 37340
    },
    {
      "epoch": 2.092205136823236,
      "grad_norm": 2.7463817596435547,
      "learning_rate": 4.7728291944615734e-05,
      "loss": 0.0993,
      "step": 37350
    },
    {
      "epoch": 2.0927653138391733,
      "grad_norm": 3.3069801330566406,
      "learning_rate": 4.771427770614945e-05,
      "loss": 0.088,
      "step": 37360
    },
    {
      "epoch": 2.0933254908551104,
      "grad_norm": 4.436494827270508,
      "learning_rate": 4.770026346768317e-05,
      "loss": 0.1251,
      "step": 37370
    },
    {
      "epoch": 2.093885667871047,
      "grad_norm": 1.2549525499343872,
      "learning_rate": 4.7686249229216885e-05,
      "loss": 0.1465,
      "step": 37380
    },
    {
      "epoch": 2.0944458448869843,
      "grad_norm": 3.0339162349700928,
      "learning_rate": 4.76722349907506e-05,
      "loss": 0.1877,
      "step": 37390
    },
    {
      "epoch": 2.0950060219029214,
      "grad_norm": 4.249268531799316,
      "learning_rate": 4.765822075228432e-05,
      "loss": 0.1703,
      "step": 37400
    },
    {
      "epoch": 2.0955661989188585,
      "grad_norm": 1.9852591753005981,
      "learning_rate": 4.764420651381804e-05,
      "loss": 0.1164,
      "step": 37410
    },
    {
      "epoch": 2.0961263759347952,
      "grad_norm": 4.369726181030273,
      "learning_rate": 4.763019227535176e-05,
      "loss": 0.1707,
      "step": 37420
    },
    {
      "epoch": 2.0966865529507324,
      "grad_norm": 4.199390888214111,
      "learning_rate": 4.761617803688548e-05,
      "loss": 0.1449,
      "step": 37430
    },
    {
      "epoch": 2.0972467299666695,
      "grad_norm": 4.848065376281738,
      "learning_rate": 4.7602163798419194e-05,
      "loss": 0.1888,
      "step": 37440
    },
    {
      "epoch": 2.0978069069826066,
      "grad_norm": 3.5171680450439453,
      "learning_rate": 4.7588149559952915e-05,
      "loss": 0.1395,
      "step": 37450
    },
    {
      "epoch": 2.0983670839985433,
      "grad_norm": 0.9695559144020081,
      "learning_rate": 4.757413532148664e-05,
      "loss": 0.1429,
      "step": 37460
    },
    {
      "epoch": 2.0989272610144805,
      "grad_norm": 4.109314918518066,
      "learning_rate": 4.756012108302035e-05,
      "loss": 0.1156,
      "step": 37470
    },
    {
      "epoch": 2.0994874380304176,
      "grad_norm": 2.3519577980041504,
      "learning_rate": 4.7546106844554066e-05,
      "loss": 0.1996,
      "step": 37480
    },
    {
      "epoch": 2.1000476150463547,
      "grad_norm": 3.778836488723755,
      "learning_rate": 4.753209260608779e-05,
      "loss": 0.1517,
      "step": 37490
    },
    {
      "epoch": 2.100607792062292,
      "grad_norm": 3.5639994144439697,
      "learning_rate": 4.75180783676215e-05,
      "loss": 0.1982,
      "step": 37500
    },
    {
      "epoch": 2.1011679690782286,
      "grad_norm": 5.698246002197266,
      "learning_rate": 4.7504064129155224e-05,
      "loss": 0.2228,
      "step": 37510
    },
    {
      "epoch": 2.1017281460941657,
      "grad_norm": 4.015478610992432,
      "learning_rate": 4.7490049890688946e-05,
      "loss": 0.1188,
      "step": 37520
    },
    {
      "epoch": 2.102288323110103,
      "grad_norm": 4.347174644470215,
      "learning_rate": 4.747603565222266e-05,
      "loss": 0.1562,
      "step": 37530
    },
    {
      "epoch": 2.10284850012604,
      "grad_norm": 5.494474411010742,
      "learning_rate": 4.746202141375638e-05,
      "loss": 0.113,
      "step": 37540
    },
    {
      "epoch": 2.1034086771419767,
      "grad_norm": 3.0765206813812256,
      "learning_rate": 4.74480071752901e-05,
      "loss": 0.1172,
      "step": 37550
    },
    {
      "epoch": 2.103968854157914,
      "grad_norm": 3.0349814891815186,
      "learning_rate": 4.743399293682382e-05,
      "loss": 0.117,
      "step": 37560
    },
    {
      "epoch": 2.104529031173851,
      "grad_norm": 3.4102869033813477,
      "learning_rate": 4.741997869835753e-05,
      "loss": 0.1315,
      "step": 37570
    },
    {
      "epoch": 2.105089208189788,
      "grad_norm": 5.063694953918457,
      "learning_rate": 4.740596445989125e-05,
      "loss": 0.2049,
      "step": 37580
    },
    {
      "epoch": 2.105649385205725,
      "grad_norm": 3.7366080284118652,
      "learning_rate": 4.739195022142497e-05,
      "loss": 0.1561,
      "step": 37590
    },
    {
      "epoch": 2.106209562221662,
      "grad_norm": 4.115206718444824,
      "learning_rate": 4.737793598295869e-05,
      "loss": 0.1359,
      "step": 37600
    },
    {
      "epoch": 2.106769739237599,
      "grad_norm": 5.659516334533691,
      "learning_rate": 4.7363921744492406e-05,
      "loss": 0.16,
      "step": 37610
    },
    {
      "epoch": 2.107329916253536,
      "grad_norm": 5.325099945068359,
      "learning_rate": 4.734990750602613e-05,
      "loss": 0.1542,
      "step": 37620
    },
    {
      "epoch": 2.1078900932694733,
      "grad_norm": 1.7925318479537964,
      "learning_rate": 4.733589326755984e-05,
      "loss": 0.1729,
      "step": 37630
    },
    {
      "epoch": 2.10845027028541,
      "grad_norm": 2.5996272563934326,
      "learning_rate": 4.732187902909356e-05,
      "loss": 0.1281,
      "step": 37640
    },
    {
      "epoch": 2.109010447301347,
      "grad_norm": 1.9832698106765747,
      "learning_rate": 4.7307864790627285e-05,
      "loss": 0.2051,
      "step": 37650
    },
    {
      "epoch": 2.1095706243172843,
      "grad_norm": 1.8835736513137817,
      "learning_rate": 4.729385055216099e-05,
      "loss": 0.2367,
      "step": 37660
    },
    {
      "epoch": 2.1101308013332214,
      "grad_norm": 4.807738304138184,
      "learning_rate": 4.7279836313694714e-05,
      "loss": 0.2586,
      "step": 37670
    },
    {
      "epoch": 2.1106909783491585,
      "grad_norm": 1.0115101337432861,
      "learning_rate": 4.7265822075228436e-05,
      "loss": 0.1027,
      "step": 37680
    },
    {
      "epoch": 2.1112511553650952,
      "grad_norm": 2.789233684539795,
      "learning_rate": 4.725180783676215e-05,
      "loss": 0.139,
      "step": 37690
    },
    {
      "epoch": 2.1118113323810324,
      "grad_norm": 4.025214195251465,
      "learning_rate": 4.723779359829587e-05,
      "loss": 0.2251,
      "step": 37700
    },
    {
      "epoch": 2.1123715093969695,
      "grad_norm": 5.014869689941406,
      "learning_rate": 4.722377935982959e-05,
      "loss": 0.1651,
      "step": 37710
    },
    {
      "epoch": 2.1129316864129066,
      "grad_norm": 1.120585560798645,
      "learning_rate": 4.720976512136331e-05,
      "loss": 0.1493,
      "step": 37720
    },
    {
      "epoch": 2.1134918634288433,
      "grad_norm": 2.326214551925659,
      "learning_rate": 4.719575088289703e-05,
      "loss": 0.1214,
      "step": 37730
    },
    {
      "epoch": 2.1140520404447805,
      "grad_norm": 1.442252278327942,
      "learning_rate": 4.7181736644430745e-05,
      "loss": 0.2909,
      "step": 37740
    },
    {
      "epoch": 2.1146122174607176,
      "grad_norm": 2.6948301792144775,
      "learning_rate": 4.716772240596446e-05,
      "loss": 0.1023,
      "step": 37750
    },
    {
      "epoch": 2.1151723944766547,
      "grad_norm": 4.286096096038818,
      "learning_rate": 4.715370816749818e-05,
      "loss": 0.1311,
      "step": 37760
    },
    {
      "epoch": 2.115732571492592,
      "grad_norm": 0.9707351922988892,
      "learning_rate": 4.7139693929031896e-05,
      "loss": 0.0955,
      "step": 37770
    },
    {
      "epoch": 2.1162927485085286,
      "grad_norm": 4.015649318695068,
      "learning_rate": 4.712567969056562e-05,
      "loss": 0.1867,
      "step": 37780
    },
    {
      "epoch": 2.1168529255244657,
      "grad_norm": 1.4142725467681885,
      "learning_rate": 4.711166545209933e-05,
      "loss": 0.1584,
      "step": 37790
    },
    {
      "epoch": 2.117413102540403,
      "grad_norm": 1.6225395202636719,
      "learning_rate": 4.7097651213633053e-05,
      "loss": 0.1195,
      "step": 37800
    },
    {
      "epoch": 2.11797327955634,
      "grad_norm": 3.432793140411377,
      "learning_rate": 4.7083636975166775e-05,
      "loss": 0.197,
      "step": 37810
    },
    {
      "epoch": 2.1185334565722767,
      "grad_norm": 1.475960612297058,
      "learning_rate": 4.706962273670049e-05,
      "loss": 0.109,
      "step": 37820
    },
    {
      "epoch": 2.119093633588214,
      "grad_norm": 3.3292760848999023,
      "learning_rate": 4.705560849823421e-05,
      "loss": 0.2435,
      "step": 37830
    },
    {
      "epoch": 2.119653810604151,
      "grad_norm": 2.0546212196350098,
      "learning_rate": 4.7041594259767926e-05,
      "loss": 0.1034,
      "step": 37840
    },
    {
      "epoch": 2.120213987620088,
      "grad_norm": 1.8900935649871826,
      "learning_rate": 4.702758002130164e-05,
      "loss": 0.1709,
      "step": 37850
    },
    {
      "epoch": 2.120774164636025,
      "grad_norm": 4.701302528381348,
      "learning_rate": 4.701356578283536e-05,
      "loss": 0.1401,
      "step": 37860
    },
    {
      "epoch": 2.121334341651962,
      "grad_norm": 4.777538299560547,
      "learning_rate": 4.6999551544369084e-05,
      "loss": 0.1027,
      "step": 37870
    },
    {
      "epoch": 2.121894518667899,
      "grad_norm": 2.5940489768981934,
      "learning_rate": 4.69855373059028e-05,
      "loss": 0.1102,
      "step": 37880
    },
    {
      "epoch": 2.122454695683836,
      "grad_norm": 3.171840190887451,
      "learning_rate": 4.697152306743652e-05,
      "loss": 0.2011,
      "step": 37890
    },
    {
      "epoch": 2.1230148726997733,
      "grad_norm": 3.712322950363159,
      "learning_rate": 4.6957508828970235e-05,
      "loss": 0.1624,
      "step": 37900
    },
    {
      "epoch": 2.12357504971571,
      "grad_norm": 3.2505013942718506,
      "learning_rate": 4.6943494590503956e-05,
      "loss": 0.1627,
      "step": 37910
    },
    {
      "epoch": 2.124135226731647,
      "grad_norm": 2.046767234802246,
      "learning_rate": 4.692948035203768e-05,
      "loss": 0.0894,
      "step": 37920
    },
    {
      "epoch": 2.1246954037475843,
      "grad_norm": 1.6592929363250732,
      "learning_rate": 4.691546611357139e-05,
      "loss": 0.1299,
      "step": 37930
    },
    {
      "epoch": 2.1252555807635214,
      "grad_norm": 3.2583107948303223,
      "learning_rate": 4.690145187510511e-05,
      "loss": 0.1853,
      "step": 37940
    },
    {
      "epoch": 2.125815757779458,
      "grad_norm": 1.4274706840515137,
      "learning_rate": 4.688743763663883e-05,
      "loss": 0.1275,
      "step": 37950
    },
    {
      "epoch": 2.126375934795395,
      "grad_norm": 4.260022163391113,
      "learning_rate": 4.6873423398172544e-05,
      "loss": 0.1104,
      "step": 37960
    },
    {
      "epoch": 2.1269361118113324,
      "grad_norm": 1.5282082557678223,
      "learning_rate": 4.6859409159706265e-05,
      "loss": 0.1828,
      "step": 37970
    },
    {
      "epoch": 2.1274962888272695,
      "grad_norm": 1.5845855474472046,
      "learning_rate": 4.684539492123998e-05,
      "loss": 0.1172,
      "step": 37980
    },
    {
      "epoch": 2.1280564658432066,
      "grad_norm": 2.8799240589141846,
      "learning_rate": 4.68313806827737e-05,
      "loss": 0.1191,
      "step": 37990
    },
    {
      "epoch": 2.1286166428591433,
      "grad_norm": 5.036952018737793,
      "learning_rate": 4.681736644430742e-05,
      "loss": 0.1851,
      "step": 38000
    },
    {
      "epoch": 2.1291768198750805,
      "grad_norm": 5.786324977874756,
      "learning_rate": 4.680335220584114e-05,
      "loss": 0.1631,
      "step": 38010
    },
    {
      "epoch": 2.1297369968910176,
      "grad_norm": 4.315108299255371,
      "learning_rate": 4.678933796737486e-05,
      "loss": 0.207,
      "step": 38020
    },
    {
      "epoch": 2.1302971739069547,
      "grad_norm": 2.6404306888580322,
      "learning_rate": 4.6775323728908574e-05,
      "loss": 0.2502,
      "step": 38030
    },
    {
      "epoch": 2.1308573509228914,
      "grad_norm": 4.5520405769348145,
      "learning_rate": 4.676130949044229e-05,
      "loss": 0.1567,
      "step": 38040
    },
    {
      "epoch": 2.1314175279388285,
      "grad_norm": 7.13132381439209,
      "learning_rate": 4.674729525197601e-05,
      "loss": 0.1344,
      "step": 38050
    },
    {
      "epoch": 2.1319777049547657,
      "grad_norm": 3.6482996940612793,
      "learning_rate": 4.6733281013509725e-05,
      "loss": 0.1096,
      "step": 38060
    },
    {
      "epoch": 2.132537881970703,
      "grad_norm": 2.8514883518218994,
      "learning_rate": 4.6719266775043446e-05,
      "loss": 0.1594,
      "step": 38070
    },
    {
      "epoch": 2.13309805898664,
      "grad_norm": 3.560744524002075,
      "learning_rate": 4.670525253657717e-05,
      "loss": 0.1678,
      "step": 38080
    },
    {
      "epoch": 2.1336582360025766,
      "grad_norm": 6.054569244384766,
      "learning_rate": 4.669123829811088e-05,
      "loss": 0.1437,
      "step": 38090
    },
    {
      "epoch": 2.134218413018514,
      "grad_norm": 3.325239658355713,
      "learning_rate": 4.6677224059644604e-05,
      "loss": 0.0912,
      "step": 38100
    },
    {
      "epoch": 2.134778590034451,
      "grad_norm": 2.234010696411133,
      "learning_rate": 4.666320982117832e-05,
      "loss": 0.1215,
      "step": 38110
    },
    {
      "epoch": 2.135338767050388,
      "grad_norm": 3.3480942249298096,
      "learning_rate": 4.6649195582712034e-05,
      "loss": 0.1647,
      "step": 38120
    },
    {
      "epoch": 2.1358989440663247,
      "grad_norm": 4.139359474182129,
      "learning_rate": 4.6635181344245755e-05,
      "loss": 0.1501,
      "step": 38130
    },
    {
      "epoch": 2.136459121082262,
      "grad_norm": 1.5670169591903687,
      "learning_rate": 4.662116710577947e-05,
      "loss": 0.196,
      "step": 38140
    },
    {
      "epoch": 2.137019298098199,
      "grad_norm": 3.56881046295166,
      "learning_rate": 4.660715286731319e-05,
      "loss": 0.1362,
      "step": 38150
    },
    {
      "epoch": 2.137579475114136,
      "grad_norm": 3.9783148765563965,
      "learning_rate": 4.659313862884691e-05,
      "loss": 0.177,
      "step": 38160
    },
    {
      "epoch": 2.1381396521300733,
      "grad_norm": 3.491170883178711,
      "learning_rate": 4.657912439038063e-05,
      "loss": 0.1724,
      "step": 38170
    },
    {
      "epoch": 2.13869982914601,
      "grad_norm": 3.1176180839538574,
      "learning_rate": 4.656511015191435e-05,
      "loss": 0.0781,
      "step": 38180
    },
    {
      "epoch": 2.139260006161947,
      "grad_norm": 1.9700161218643188,
      "learning_rate": 4.6551095913448064e-05,
      "loss": 0.1767,
      "step": 38190
    },
    {
      "epoch": 2.1398201831778842,
      "grad_norm": 5.364343643188477,
      "learning_rate": 4.6537081674981785e-05,
      "loss": 0.1004,
      "step": 38200
    },
    {
      "epoch": 2.1403803601938214,
      "grad_norm": 4.289059162139893,
      "learning_rate": 4.65230674365155e-05,
      "loss": 0.1493,
      "step": 38210
    },
    {
      "epoch": 2.140940537209758,
      "grad_norm": 3.5651321411132812,
      "learning_rate": 4.650905319804922e-05,
      "loss": 0.1245,
      "step": 38220
    },
    {
      "epoch": 2.141500714225695,
      "grad_norm": 1.9663952589035034,
      "learning_rate": 4.6495038959582936e-05,
      "loss": 0.1216,
      "step": 38230
    },
    {
      "epoch": 2.1420608912416323,
      "grad_norm": 1.8790069818496704,
      "learning_rate": 4.648102472111666e-05,
      "loss": 0.2635,
      "step": 38240
    },
    {
      "epoch": 2.1426210682575695,
      "grad_norm": 2.7303521633148193,
      "learning_rate": 4.646701048265037e-05,
      "loss": 0.0879,
      "step": 38250
    },
    {
      "epoch": 2.1431812452735066,
      "grad_norm": 1.0261610746383667,
      "learning_rate": 4.6452996244184094e-05,
      "loss": 0.1066,
      "step": 38260
    },
    {
      "epoch": 2.1437414222894433,
      "grad_norm": 5.481390953063965,
      "learning_rate": 4.6438982005717816e-05,
      "loss": 0.1567,
      "step": 38270
    },
    {
      "epoch": 2.1443015993053804,
      "grad_norm": 2.672433614730835,
      "learning_rate": 4.642496776725153e-05,
      "loss": 0.15,
      "step": 38280
    },
    {
      "epoch": 2.1448617763213176,
      "grad_norm": 0.9116891026496887,
      "learning_rate": 4.641095352878525e-05,
      "loss": 0.0992,
      "step": 38290
    },
    {
      "epoch": 2.1454219533372547,
      "grad_norm": 6.905259609222412,
      "learning_rate": 4.639693929031897e-05,
      "loss": 0.1963,
      "step": 38300
    },
    {
      "epoch": 2.1459821303531914,
      "grad_norm": 1.728319525718689,
      "learning_rate": 4.638292505185268e-05,
      "loss": 0.1575,
      "step": 38310
    },
    {
      "epoch": 2.1465423073691285,
      "grad_norm": 3.9092414379119873,
      "learning_rate": 4.63689108133864e-05,
      "loss": 0.1532,
      "step": 38320
    },
    {
      "epoch": 2.1471024843850657,
      "grad_norm": 1.9304850101470947,
      "learning_rate": 4.635489657492012e-05,
      "loss": 0.0975,
      "step": 38330
    },
    {
      "epoch": 2.147662661401003,
      "grad_norm": 3.92707896232605,
      "learning_rate": 4.634088233645384e-05,
      "loss": 0.126,
      "step": 38340
    },
    {
      "epoch": 2.14822283841694,
      "grad_norm": 3.5727145671844482,
      "learning_rate": 4.632686809798756e-05,
      "loss": 0.1148,
      "step": 38350
    },
    {
      "epoch": 2.1487830154328766,
      "grad_norm": 3.5917038917541504,
      "learning_rate": 4.6312853859521276e-05,
      "loss": 0.1881,
      "step": 38360
    },
    {
      "epoch": 2.1493431924488138,
      "grad_norm": 4.13530969619751,
      "learning_rate": 4.6298839621055e-05,
      "loss": 0.1123,
      "step": 38370
    },
    {
      "epoch": 2.149903369464751,
      "grad_norm": 3.828702211380005,
      "learning_rate": 4.628482538258871e-05,
      "loss": 0.1335,
      "step": 38380
    },
    {
      "epoch": 2.150463546480688,
      "grad_norm": 2.5610616207122803,
      "learning_rate": 4.6270811144122427e-05,
      "loss": 0.1602,
      "step": 38390
    },
    {
      "epoch": 2.1510237234966247,
      "grad_norm": 3.0033180713653564,
      "learning_rate": 4.625679690565615e-05,
      "loss": 0.2322,
      "step": 38400
    },
    {
      "epoch": 2.151583900512562,
      "grad_norm": 1.5962228775024414,
      "learning_rate": 4.624278266718986e-05,
      "loss": 0.1291,
      "step": 38410
    },
    {
      "epoch": 2.152144077528499,
      "grad_norm": 1.7528362274169922,
      "learning_rate": 4.6228768428723584e-05,
      "loss": 0.1383,
      "step": 38420
    },
    {
      "epoch": 2.152704254544436,
      "grad_norm": 2.2187864780426025,
      "learning_rate": 4.6214754190257306e-05,
      "loss": 0.1395,
      "step": 38430
    },
    {
      "epoch": 2.1532644315603733,
      "grad_norm": 2.481443405151367,
      "learning_rate": 4.620073995179102e-05,
      "loss": 0.0969,
      "step": 38440
    },
    {
      "epoch": 2.15382460857631,
      "grad_norm": 3.005678415298462,
      "learning_rate": 4.618672571332474e-05,
      "loss": 0.1148,
      "step": 38450
    },
    {
      "epoch": 2.154384785592247,
      "grad_norm": 2.6404998302459717,
      "learning_rate": 4.617271147485846e-05,
      "loss": 0.1521,
      "step": 38460
    },
    {
      "epoch": 2.1549449626081842,
      "grad_norm": 1.2910053730010986,
      "learning_rate": 4.615869723639218e-05,
      "loss": 0.1419,
      "step": 38470
    },
    {
      "epoch": 2.1555051396241214,
      "grad_norm": 1.4483360052108765,
      "learning_rate": 4.61446829979259e-05,
      "loss": 0.0882,
      "step": 38480
    },
    {
      "epoch": 2.156065316640058,
      "grad_norm": 2.7783901691436768,
      "learning_rate": 4.613066875945961e-05,
      "loss": 0.094,
      "step": 38490
    },
    {
      "epoch": 2.156625493655995,
      "grad_norm": 1.947737455368042,
      "learning_rate": 4.611665452099333e-05,
      "loss": 0.1079,
      "step": 38500
    },
    {
      "epoch": 2.1571856706719323,
      "grad_norm": 6.815512657165527,
      "learning_rate": 4.610264028252705e-05,
      "loss": 0.3042,
      "step": 38510
    },
    {
      "epoch": 2.1577458476878695,
      "grad_norm": 4.292049884796143,
      "learning_rate": 4.6088626044060766e-05,
      "loss": 0.1618,
      "step": 38520
    },
    {
      "epoch": 2.1583060247038066,
      "grad_norm": 5.1354570388793945,
      "learning_rate": 4.607461180559449e-05,
      "loss": 0.0936,
      "step": 38530
    },
    {
      "epoch": 2.1588662017197433,
      "grad_norm": 2.748790740966797,
      "learning_rate": 4.60605975671282e-05,
      "loss": 0.1989,
      "step": 38540
    },
    {
      "epoch": 2.1594263787356804,
      "grad_norm": 1.784439206123352,
      "learning_rate": 4.6046583328661923e-05,
      "loss": 0.1359,
      "step": 38550
    },
    {
      "epoch": 2.1599865557516176,
      "grad_norm": 3.949110507965088,
      "learning_rate": 4.6032569090195645e-05,
      "loss": 0.1629,
      "step": 38560
    },
    {
      "epoch": 2.1605467327675547,
      "grad_norm": 2.8766541481018066,
      "learning_rate": 4.601855485172936e-05,
      "loss": 0.1443,
      "step": 38570
    },
    {
      "epoch": 2.1611069097834914,
      "grad_norm": 1.3295003175735474,
      "learning_rate": 4.6004540613263074e-05,
      "loss": 0.1231,
      "step": 38580
    },
    {
      "epoch": 2.1616670867994285,
      "grad_norm": 1.212770938873291,
      "learning_rate": 4.5990526374796796e-05,
      "loss": 0.132,
      "step": 38590
    },
    {
      "epoch": 2.1622272638153657,
      "grad_norm": 4.31952428817749,
      "learning_rate": 4.597651213633051e-05,
      "loss": 0.2196,
      "step": 38600
    },
    {
      "epoch": 2.162787440831303,
      "grad_norm": 1.496761679649353,
      "learning_rate": 4.596249789786423e-05,
      "loss": 0.1565,
      "step": 38610
    },
    {
      "epoch": 2.16334761784724,
      "grad_norm": 0.7882899641990662,
      "learning_rate": 4.5948483659397954e-05,
      "loss": 0.1533,
      "step": 38620
    },
    {
      "epoch": 2.1639077948631766,
      "grad_norm": 1.5414026975631714,
      "learning_rate": 4.593446942093167e-05,
      "loss": 0.1051,
      "step": 38630
    },
    {
      "epoch": 2.1644679718791138,
      "grad_norm": 1.8413200378417969,
      "learning_rate": 4.592045518246539e-05,
      "loss": 0.2115,
      "step": 38640
    },
    {
      "epoch": 2.165028148895051,
      "grad_norm": 4.3518476486206055,
      "learning_rate": 4.5906440943999105e-05,
      "loss": 0.1263,
      "step": 38650
    },
    {
      "epoch": 2.165588325910988,
      "grad_norm": 1.7576508522033691,
      "learning_rate": 4.5892426705532826e-05,
      "loss": 0.2208,
      "step": 38660
    },
    {
      "epoch": 2.1661485029269247,
      "grad_norm": 3.417466402053833,
      "learning_rate": 4.587841246706654e-05,
      "loss": 0.1033,
      "step": 38670
    },
    {
      "epoch": 2.166708679942862,
      "grad_norm": 3.74324631690979,
      "learning_rate": 4.5864398228600256e-05,
      "loss": 0.1215,
      "step": 38680
    },
    {
      "epoch": 2.167268856958799,
      "grad_norm": 4.776632308959961,
      "learning_rate": 4.585038399013398e-05,
      "loss": 0.0795,
      "step": 38690
    },
    {
      "epoch": 2.167829033974736,
      "grad_norm": 2.285212278366089,
      "learning_rate": 4.58363697516677e-05,
      "loss": 0.1453,
      "step": 38700
    },
    {
      "epoch": 2.1683892109906733,
      "grad_norm": 2.2877285480499268,
      "learning_rate": 4.5822355513201414e-05,
      "loss": 0.1806,
      "step": 38710
    },
    {
      "epoch": 2.16894938800661,
      "grad_norm": 4.37170934677124,
      "learning_rate": 4.5808341274735135e-05,
      "loss": 0.1791,
      "step": 38720
    },
    {
      "epoch": 2.169509565022547,
      "grad_norm": 1.626447319984436,
      "learning_rate": 4.579432703626885e-05,
      "loss": 0.162,
      "step": 38730
    },
    {
      "epoch": 2.1700697420384842,
      "grad_norm": 3.8353118896484375,
      "learning_rate": 4.578031279780257e-05,
      "loss": 0.1075,
      "step": 38740
    },
    {
      "epoch": 2.1706299190544214,
      "grad_norm": 4.363757610321045,
      "learning_rate": 4.576629855933629e-05,
      "loss": 0.1966,
      "step": 38750
    },
    {
      "epoch": 2.171190096070358,
      "grad_norm": 1.179065227508545,
      "learning_rate": 4.575228432087e-05,
      "loss": 0.1534,
      "step": 38760
    },
    {
      "epoch": 2.171750273086295,
      "grad_norm": 2.015645742416382,
      "learning_rate": 4.573827008240372e-05,
      "loss": 0.1654,
      "step": 38770
    },
    {
      "epoch": 2.1723104501022323,
      "grad_norm": 2.7719335556030273,
      "learning_rate": 4.5724255843937444e-05,
      "loss": 0.1799,
      "step": 38780
    },
    {
      "epoch": 2.1728706271181695,
      "grad_norm": 1.0275766849517822,
      "learning_rate": 4.571024160547116e-05,
      "loss": 0.1264,
      "step": 38790
    },
    {
      "epoch": 2.1734308041341066,
      "grad_norm": 1.5253514051437378,
      "learning_rate": 4.569622736700488e-05,
      "loss": 0.1814,
      "step": 38800
    },
    {
      "epoch": 2.1739909811500433,
      "grad_norm": 5.589142799377441,
      "learning_rate": 4.5682213128538595e-05,
      "loss": 0.1502,
      "step": 38810
    },
    {
      "epoch": 2.1745511581659804,
      "grad_norm": 3.558183193206787,
      "learning_rate": 4.5668198890072316e-05,
      "loss": 0.1256,
      "step": 38820
    },
    {
      "epoch": 2.1751113351819176,
      "grad_norm": 1.8122087717056274,
      "learning_rate": 4.565418465160604e-05,
      "loss": 0.1288,
      "step": 38830
    },
    {
      "epoch": 2.1756715121978547,
      "grad_norm": 3.363210678100586,
      "learning_rate": 4.564017041313975e-05,
      "loss": 0.1337,
      "step": 38840
    },
    {
      "epoch": 2.1762316892137914,
      "grad_norm": 3.05279278755188,
      "learning_rate": 4.562615617467347e-05,
      "loss": 0.1331,
      "step": 38850
    },
    {
      "epoch": 2.1767918662297285,
      "grad_norm": 5.475992679595947,
      "learning_rate": 4.561214193620719e-05,
      "loss": 0.2425,
      "step": 38860
    },
    {
      "epoch": 2.1773520432456657,
      "grad_norm": 3.8061938285827637,
      "learning_rate": 4.5598127697740904e-05,
      "loss": 0.1401,
      "step": 38870
    },
    {
      "epoch": 2.177912220261603,
      "grad_norm": 1.967200517654419,
      "learning_rate": 4.5584113459274625e-05,
      "loss": 0.1147,
      "step": 38880
    },
    {
      "epoch": 2.1784723972775395,
      "grad_norm": 2.1025397777557373,
      "learning_rate": 4.557009922080834e-05,
      "loss": 0.1216,
      "step": 38890
    },
    {
      "epoch": 2.1790325742934766,
      "grad_norm": 2.7494709491729736,
      "learning_rate": 4.555608498234206e-05,
      "loss": 0.1034,
      "step": 38900
    },
    {
      "epoch": 2.1795927513094138,
      "grad_norm": 2.316150426864624,
      "learning_rate": 4.554207074387578e-05,
      "loss": 0.1991,
      "step": 38910
    },
    {
      "epoch": 2.180152928325351,
      "grad_norm": 3.569279670715332,
      "learning_rate": 4.55280565054095e-05,
      "loss": 0.2011,
      "step": 38920
    },
    {
      "epoch": 2.180713105341288,
      "grad_norm": 2.7936489582061768,
      "learning_rate": 4.551404226694322e-05,
      "loss": 0.1809,
      "step": 38930
    },
    {
      "epoch": 2.1812732823572247,
      "grad_norm": 3.3877601623535156,
      "learning_rate": 4.5500028028476934e-05,
      "loss": 0.1815,
      "step": 38940
    },
    {
      "epoch": 2.181833459373162,
      "grad_norm": 3.112877368927002,
      "learning_rate": 4.548601379001065e-05,
      "loss": 0.0962,
      "step": 38950
    },
    {
      "epoch": 2.182393636389099,
      "grad_norm": 7.492257118225098,
      "learning_rate": 4.547199955154437e-05,
      "loss": 0.1872,
      "step": 38960
    },
    {
      "epoch": 2.182953813405036,
      "grad_norm": 2.6862218379974365,
      "learning_rate": 4.545798531307809e-05,
      "loss": 0.1395,
      "step": 38970
    },
    {
      "epoch": 2.183513990420973,
      "grad_norm": 4.318006992340088,
      "learning_rate": 4.5443971074611806e-05,
      "loss": 0.1765,
      "step": 38980
    },
    {
      "epoch": 2.18407416743691,
      "grad_norm": 4.547475337982178,
      "learning_rate": 4.542995683614553e-05,
      "loss": 0.1163,
      "step": 38990
    },
    {
      "epoch": 2.184634344452847,
      "grad_norm": 3.0253219604492188,
      "learning_rate": 4.541594259767924e-05,
      "loss": 0.1312,
      "step": 39000
    },
    {
      "epoch": 2.1851945214687842,
      "grad_norm": 4.777474880218506,
      "learning_rate": 4.5401928359212964e-05,
      "loss": 0.2389,
      "step": 39010
    },
    {
      "epoch": 2.1857546984847214,
      "grad_norm": 6.439064025878906,
      "learning_rate": 4.5387914120746686e-05,
      "loss": 0.1389,
      "step": 39020
    },
    {
      "epoch": 2.186314875500658,
      "grad_norm": 2.669126272201538,
      "learning_rate": 4.53738998822804e-05,
      "loss": 0.2103,
      "step": 39030
    },
    {
      "epoch": 2.186875052516595,
      "grad_norm": 3.1149516105651855,
      "learning_rate": 4.5359885643814115e-05,
      "loss": 0.1386,
      "step": 39040
    },
    {
      "epoch": 2.1874352295325323,
      "grad_norm": 4.397765159606934,
      "learning_rate": 4.534587140534784e-05,
      "loss": 0.1409,
      "step": 39050
    },
    {
      "epoch": 2.1879954065484695,
      "grad_norm": 3.182677984237671,
      "learning_rate": 4.533185716688155e-05,
      "loss": 0.1205,
      "step": 39060
    },
    {
      "epoch": 2.188555583564406,
      "grad_norm": 1.3309500217437744,
      "learning_rate": 4.531784292841527e-05,
      "loss": 0.1792,
      "step": 39070
    },
    {
      "epoch": 2.1891157605803433,
      "grad_norm": 0.9033207893371582,
      "learning_rate": 4.530382868994899e-05,
      "loss": 0.0928,
      "step": 39080
    },
    {
      "epoch": 2.1896759375962804,
      "grad_norm": 3.6557400226593018,
      "learning_rate": 4.528981445148271e-05,
      "loss": 0.0865,
      "step": 39090
    },
    {
      "epoch": 2.1902361146122176,
      "grad_norm": 5.524266242980957,
      "learning_rate": 4.527580021301643e-05,
      "loss": 0.1434,
      "step": 39100
    },
    {
      "epoch": 2.1907962916281547,
      "grad_norm": 1.4416815042495728,
      "learning_rate": 4.5261785974550146e-05,
      "loss": 0.111,
      "step": 39110
    },
    {
      "epoch": 2.1913564686440914,
      "grad_norm": 4.315492630004883,
      "learning_rate": 4.524777173608387e-05,
      "loss": 0.1739,
      "step": 39120
    },
    {
      "epoch": 2.1919166456600285,
      "grad_norm": 3.298799514770508,
      "learning_rate": 4.523375749761758e-05,
      "loss": 0.1049,
      "step": 39130
    },
    {
      "epoch": 2.1924768226759657,
      "grad_norm": 4.771182060241699,
      "learning_rate": 4.5219743259151297e-05,
      "loss": 0.1016,
      "step": 39140
    },
    {
      "epoch": 2.193036999691903,
      "grad_norm": 4.672995567321777,
      "learning_rate": 4.520572902068502e-05,
      "loss": 0.1284,
      "step": 39150
    },
    {
      "epoch": 2.1935971767078395,
      "grad_norm": 2.7891135215759277,
      "learning_rate": 4.519171478221873e-05,
      "loss": 0.1296,
      "step": 39160
    },
    {
      "epoch": 2.1941573537237766,
      "grad_norm": 2.148998498916626,
      "learning_rate": 4.5177700543752454e-05,
      "loss": 0.1331,
      "step": 39170
    },
    {
      "epoch": 2.1947175307397138,
      "grad_norm": 5.707194805145264,
      "learning_rate": 4.5163686305286176e-05,
      "loss": 0.1389,
      "step": 39180
    },
    {
      "epoch": 2.195277707755651,
      "grad_norm": 3.282634973526001,
      "learning_rate": 4.514967206681989e-05,
      "loss": 0.1836,
      "step": 39190
    },
    {
      "epoch": 2.1958378847715876,
      "grad_norm": 1.8065261840820312,
      "learning_rate": 4.513565782835361e-05,
      "loss": 0.1222,
      "step": 39200
    },
    {
      "epoch": 2.1963980617875247,
      "grad_norm": 3.1033518314361572,
      "learning_rate": 4.512164358988733e-05,
      "loss": 0.177,
      "step": 39210
    },
    {
      "epoch": 2.196958238803462,
      "grad_norm": 1.5290396213531494,
      "learning_rate": 4.510762935142104e-05,
      "loss": 0.095,
      "step": 39220
    },
    {
      "epoch": 2.197518415819399,
      "grad_norm": 7.08622932434082,
      "learning_rate": 4.509361511295476e-05,
      "loss": 0.1115,
      "step": 39230
    },
    {
      "epoch": 2.198078592835336,
      "grad_norm": 4.12978458404541,
      "learning_rate": 4.507960087448848e-05,
      "loss": 0.1563,
      "step": 39240
    },
    {
      "epoch": 2.198638769851273,
      "grad_norm": 4.761347770690918,
      "learning_rate": 4.50655866360222e-05,
      "loss": 0.1353,
      "step": 39250
    },
    {
      "epoch": 2.19919894686721,
      "grad_norm": 3.622100353240967,
      "learning_rate": 4.505157239755592e-05,
      "loss": 0.1542,
      "step": 39260
    },
    {
      "epoch": 2.199759123883147,
      "grad_norm": 2.173424005508423,
      "learning_rate": 4.5037558159089636e-05,
      "loss": 0.114,
      "step": 39270
    },
    {
      "epoch": 2.2003193008990842,
      "grad_norm": 5.371133327484131,
      "learning_rate": 4.502354392062336e-05,
      "loss": 0.1356,
      "step": 39280
    },
    {
      "epoch": 2.200879477915021,
      "grad_norm": 1.2298270463943481,
      "learning_rate": 4.500952968215708e-05,
      "loss": 0.0895,
      "step": 39290
    },
    {
      "epoch": 2.201439654930958,
      "grad_norm": 4.306396961212158,
      "learning_rate": 4.4995515443690793e-05,
      "loss": 0.1584,
      "step": 39300
    },
    {
      "epoch": 2.201999831946895,
      "grad_norm": 2.030146360397339,
      "learning_rate": 4.498150120522451e-05,
      "loss": 0.1213,
      "step": 39310
    },
    {
      "epoch": 2.2025600089628323,
      "grad_norm": 3.950157880783081,
      "learning_rate": 4.496748696675823e-05,
      "loss": 0.1314,
      "step": 39320
    },
    {
      "epoch": 2.2031201859787695,
      "grad_norm": 3.468104362487793,
      "learning_rate": 4.4953472728291944e-05,
      "loss": 0.1788,
      "step": 39330
    },
    {
      "epoch": 2.203680362994706,
      "grad_norm": 2.412179470062256,
      "learning_rate": 4.4939458489825666e-05,
      "loss": 0.1484,
      "step": 39340
    },
    {
      "epoch": 2.2042405400106433,
      "grad_norm": 3.968843460083008,
      "learning_rate": 4.492544425135938e-05,
      "loss": 0.1273,
      "step": 39350
    },
    {
      "epoch": 2.2048007170265804,
      "grad_norm": 5.056290149688721,
      "learning_rate": 4.49114300128931e-05,
      "loss": 0.1467,
      "step": 39360
    },
    {
      "epoch": 2.2053608940425176,
      "grad_norm": 1.605745553970337,
      "learning_rate": 4.4897415774426824e-05,
      "loss": 0.1209,
      "step": 39370
    },
    {
      "epoch": 2.2059210710584543,
      "grad_norm": 4.759862422943115,
      "learning_rate": 4.488340153596054e-05,
      "loss": 0.1266,
      "step": 39380
    },
    {
      "epoch": 2.2064812480743914,
      "grad_norm": 1.8434367179870605,
      "learning_rate": 4.486938729749426e-05,
      "loss": 0.1152,
      "step": 39390
    },
    {
      "epoch": 2.2070414250903285,
      "grad_norm": 0.9287374019622803,
      "learning_rate": 4.4855373059027975e-05,
      "loss": 0.0983,
      "step": 39400
    },
    {
      "epoch": 2.2076016021062657,
      "grad_norm": 1.2159684896469116,
      "learning_rate": 4.484135882056169e-05,
      "loss": 0.1198,
      "step": 39410
    },
    {
      "epoch": 2.208161779122203,
      "grad_norm": 0.9066674709320068,
      "learning_rate": 4.482734458209541e-05,
      "loss": 0.1309,
      "step": 39420
    },
    {
      "epoch": 2.2087219561381395,
      "grad_norm": 1.3971589803695679,
      "learning_rate": 4.4813330343629126e-05,
      "loss": 0.0866,
      "step": 39430
    },
    {
      "epoch": 2.2092821331540766,
      "grad_norm": 5.148815631866455,
      "learning_rate": 4.479931610516285e-05,
      "loss": 0.1463,
      "step": 39440
    },
    {
      "epoch": 2.2098423101700138,
      "grad_norm": 1.1819334030151367,
      "learning_rate": 4.478530186669657e-05,
      "loss": 0.1377,
      "step": 39450
    },
    {
      "epoch": 2.210402487185951,
      "grad_norm": 3.437511444091797,
      "learning_rate": 4.4771287628230284e-05,
      "loss": 0.1558,
      "step": 39460
    },
    {
      "epoch": 2.2109626642018876,
      "grad_norm": 3.0675292015075684,
      "learning_rate": 4.4757273389764005e-05,
      "loss": 0.1066,
      "step": 39470
    },
    {
      "epoch": 2.2115228412178247,
      "grad_norm": 2.5960159301757812,
      "learning_rate": 4.474325915129772e-05,
      "loss": 0.1815,
      "step": 39480
    },
    {
      "epoch": 2.212083018233762,
      "grad_norm": 4.151734828948975,
      "learning_rate": 4.4729244912831435e-05,
      "loss": 0.1956,
      "step": 39490
    },
    {
      "epoch": 2.212643195249699,
      "grad_norm": 2.810922384262085,
      "learning_rate": 4.4715230674365156e-05,
      "loss": 0.1315,
      "step": 39500
    },
    {
      "epoch": 2.213203372265636,
      "grad_norm": 5.0237202644348145,
      "learning_rate": 4.470121643589887e-05,
      "loss": 0.1471,
      "step": 39510
    },
    {
      "epoch": 2.213763549281573,
      "grad_norm": 3.575263023376465,
      "learning_rate": 4.468720219743259e-05,
      "loss": 0.1243,
      "step": 39520
    },
    {
      "epoch": 2.21432372629751,
      "grad_norm": 5.0478997230529785,
      "learning_rate": 4.4673187958966314e-05,
      "loss": 0.2247,
      "step": 39530
    },
    {
      "epoch": 2.214883903313447,
      "grad_norm": 2.048691987991333,
      "learning_rate": 4.465917372050003e-05,
      "loss": 0.1223,
      "step": 39540
    },
    {
      "epoch": 2.2154440803293842,
      "grad_norm": 3.482104778289795,
      "learning_rate": 4.464515948203375e-05,
      "loss": 0.1396,
      "step": 39550
    },
    {
      "epoch": 2.216004257345321,
      "grad_norm": 5.3932647705078125,
      "learning_rate": 4.4631145243567465e-05,
      "loss": 0.166,
      "step": 39560
    },
    {
      "epoch": 2.216564434361258,
      "grad_norm": 1.2139694690704346,
      "learning_rate": 4.4617131005101186e-05,
      "loss": 0.192,
      "step": 39570
    },
    {
      "epoch": 2.217124611377195,
      "grad_norm": 6.8442182540893555,
      "learning_rate": 4.460311676663491e-05,
      "loss": 0.1944,
      "step": 39580
    },
    {
      "epoch": 2.2176847883931323,
      "grad_norm": 2.850309371948242,
      "learning_rate": 4.4589102528168616e-05,
      "loss": 0.2611,
      "step": 39590
    },
    {
      "epoch": 2.2182449654090695,
      "grad_norm": 2.844186544418335,
      "learning_rate": 4.457508828970234e-05,
      "loss": 0.1267,
      "step": 39600
    },
    {
      "epoch": 2.218805142425006,
      "grad_norm": 5.128359317779541,
      "learning_rate": 4.456107405123606e-05,
      "loss": 0.1388,
      "step": 39610
    },
    {
      "epoch": 2.2193653194409433,
      "grad_norm": 3.1304023265838623,
      "learning_rate": 4.4547059812769774e-05,
      "loss": 0.1206,
      "step": 39620
    },
    {
      "epoch": 2.2199254964568804,
      "grad_norm": 5.343831539154053,
      "learning_rate": 4.4533045574303495e-05,
      "loss": 0.1734,
      "step": 39630
    },
    {
      "epoch": 2.2204856734728176,
      "grad_norm": 2.1583986282348633,
      "learning_rate": 4.451903133583722e-05,
      "loss": 0.1515,
      "step": 39640
    },
    {
      "epoch": 2.2210458504887542,
      "grad_norm": 5.015814781188965,
      "learning_rate": 4.450501709737093e-05,
      "loss": 0.1344,
      "step": 39650
    },
    {
      "epoch": 2.2216060275046914,
      "grad_norm": 4.059277534484863,
      "learning_rate": 4.449100285890465e-05,
      "loss": 0.1376,
      "step": 39660
    },
    {
      "epoch": 2.2221662045206285,
      "grad_norm": 1.7838982343673706,
      "learning_rate": 4.447698862043837e-05,
      "loss": 0.1401,
      "step": 39670
    },
    {
      "epoch": 2.2227263815365657,
      "grad_norm": 4.3776631355285645,
      "learning_rate": 4.446297438197208e-05,
      "loss": 0.1215,
      "step": 39680
    },
    {
      "epoch": 2.223286558552503,
      "grad_norm": 2.058474063873291,
      "learning_rate": 4.4448960143505804e-05,
      "loss": 0.1468,
      "step": 39690
    },
    {
      "epoch": 2.2238467355684395,
      "grad_norm": 1.9484840631484985,
      "learning_rate": 4.443494590503952e-05,
      "loss": 0.14,
      "step": 39700
    },
    {
      "epoch": 2.2244069125843766,
      "grad_norm": 1.7226130962371826,
      "learning_rate": 4.442093166657324e-05,
      "loss": 0.1326,
      "step": 39710
    },
    {
      "epoch": 2.2249670896003138,
      "grad_norm": 3.81595778465271,
      "learning_rate": 4.440691742810696e-05,
      "loss": 0.129,
      "step": 39720
    },
    {
      "epoch": 2.225527266616251,
      "grad_norm": 6.103896141052246,
      "learning_rate": 4.4392903189640676e-05,
      "loss": 0.1424,
      "step": 39730
    },
    {
      "epoch": 2.2260874436321876,
      "grad_norm": 2.680026054382324,
      "learning_rate": 4.43788889511744e-05,
      "loss": 0.1081,
      "step": 39740
    },
    {
      "epoch": 2.2266476206481247,
      "grad_norm": 1.2652727365493774,
      "learning_rate": 4.436487471270811e-05,
      "loss": 0.2111,
      "step": 39750
    },
    {
      "epoch": 2.227207797664062,
      "grad_norm": 2.8308794498443604,
      "learning_rate": 4.4350860474241834e-05,
      "loss": 0.1121,
      "step": 39760
    },
    {
      "epoch": 2.227767974679999,
      "grad_norm": 3.669326066970825,
      "learning_rate": 4.433684623577555e-05,
      "loss": 0.1002,
      "step": 39770
    },
    {
      "epoch": 2.228328151695936,
      "grad_norm": 1.928963541984558,
      "learning_rate": 4.4322831997309264e-05,
      "loss": 0.1798,
      "step": 39780
    },
    {
      "epoch": 2.228888328711873,
      "grad_norm": 0.8007656931877136,
      "learning_rate": 4.4308817758842985e-05,
      "loss": 0.1455,
      "step": 39790
    },
    {
      "epoch": 2.22944850572781,
      "grad_norm": 0.7353696227073669,
      "learning_rate": 4.429480352037671e-05,
      "loss": 0.1352,
      "step": 39800
    },
    {
      "epoch": 2.230008682743747,
      "grad_norm": 3.714324474334717,
      "learning_rate": 4.428078928191042e-05,
      "loss": 0.1132,
      "step": 39810
    },
    {
      "epoch": 2.230568859759684,
      "grad_norm": 4.377589702606201,
      "learning_rate": 4.426677504344414e-05,
      "loss": 0.105,
      "step": 39820
    },
    {
      "epoch": 2.231129036775621,
      "grad_norm": 3.91961407661438,
      "learning_rate": 4.425276080497786e-05,
      "loss": 0.143,
      "step": 39830
    },
    {
      "epoch": 2.231689213791558,
      "grad_norm": 0.8678056001663208,
      "learning_rate": 4.423874656651158e-05,
      "loss": 0.1588,
      "step": 39840
    },
    {
      "epoch": 2.232249390807495,
      "grad_norm": 5.475912570953369,
      "learning_rate": 4.42247323280453e-05,
      "loss": 0.1681,
      "step": 39850
    },
    {
      "epoch": 2.2328095678234323,
      "grad_norm": 3.4405248165130615,
      "learning_rate": 4.421071808957901e-05,
      "loss": 0.1415,
      "step": 39860
    },
    {
      "epoch": 2.2333697448393695,
      "grad_norm": 1.8127484321594238,
      "learning_rate": 4.419670385111273e-05,
      "loss": 0.0894,
      "step": 39870
    },
    {
      "epoch": 2.233929921855306,
      "grad_norm": 3.842193126678467,
      "learning_rate": 4.418268961264645e-05,
      "loss": 0.1443,
      "step": 39880
    },
    {
      "epoch": 2.2344900988712433,
      "grad_norm": 5.576990127563477,
      "learning_rate": 4.4168675374180167e-05,
      "loss": 0.1292,
      "step": 39890
    },
    {
      "epoch": 2.2350502758871804,
      "grad_norm": 3.4774274826049805,
      "learning_rate": 4.415466113571389e-05,
      "loss": 0.1593,
      "step": 39900
    },
    {
      "epoch": 2.2356104529031176,
      "grad_norm": 3.3252360820770264,
      "learning_rate": 4.41406468972476e-05,
      "loss": 0.213,
      "step": 39910
    },
    {
      "epoch": 2.2361706299190542,
      "grad_norm": 2.960115671157837,
      "learning_rate": 4.4126632658781324e-05,
      "loss": 0.2075,
      "step": 39920
    },
    {
      "epoch": 2.2367308069349914,
      "grad_norm": 4.225352764129639,
      "learning_rate": 4.4112618420315046e-05,
      "loss": 0.1371,
      "step": 39930
    },
    {
      "epoch": 2.2372909839509285,
      "grad_norm": 4.370233058929443,
      "learning_rate": 4.409860418184876e-05,
      "loss": 0.203,
      "step": 39940
    },
    {
      "epoch": 2.2378511609668656,
      "grad_norm": 1.9088624715805054,
      "learning_rate": 4.4084589943382475e-05,
      "loss": 0.1694,
      "step": 39950
    },
    {
      "epoch": 2.238411337982803,
      "grad_norm": 3.0448403358459473,
      "learning_rate": 4.40705757049162e-05,
      "loss": 0.1688,
      "step": 39960
    },
    {
      "epoch": 2.2389715149987395,
      "grad_norm": 1.7519246339797974,
      "learning_rate": 4.405656146644991e-05,
      "loss": 0.1285,
      "step": 39970
    },
    {
      "epoch": 2.2395316920146766,
      "grad_norm": 1.4923428297042847,
      "learning_rate": 4.404254722798363e-05,
      "loss": 0.1726,
      "step": 39980
    },
    {
      "epoch": 2.2400918690306137,
      "grad_norm": 4.7628560066223145,
      "learning_rate": 4.4028532989517355e-05,
      "loss": 0.2861,
      "step": 39990
    },
    {
      "epoch": 2.240652046046551,
      "grad_norm": 3.4356157779693604,
      "learning_rate": 4.401451875105107e-05,
      "loss": 0.1404,
      "step": 40000
    },
    {
      "epoch": 2.2412122230624876,
      "grad_norm": 3.8132553100585938,
      "learning_rate": 4.400050451258479e-05,
      "loss": 0.1044,
      "step": 40010
    },
    {
      "epoch": 2.2417724000784247,
      "grad_norm": 2.1249821186065674,
      "learning_rate": 4.3986490274118506e-05,
      "loss": 0.1137,
      "step": 40020
    },
    {
      "epoch": 2.242332577094362,
      "grad_norm": 4.563406944274902,
      "learning_rate": 4.397247603565223e-05,
      "loss": 0.1406,
      "step": 40030
    },
    {
      "epoch": 2.242892754110299,
      "grad_norm": 0.7095766663551331,
      "learning_rate": 4.395846179718594e-05,
      "loss": 0.157,
      "step": 40040
    },
    {
      "epoch": 2.243452931126236,
      "grad_norm": 0.7606394290924072,
      "learning_rate": 4.394444755871966e-05,
      "loss": 0.1095,
      "step": 40050
    },
    {
      "epoch": 2.244013108142173,
      "grad_norm": 3.573399066925049,
      "learning_rate": 4.393043332025338e-05,
      "loss": 0.1468,
      "step": 40060
    },
    {
      "epoch": 2.24457328515811,
      "grad_norm": 4.832782745361328,
      "learning_rate": 4.39164190817871e-05,
      "loss": 0.159,
      "step": 40070
    },
    {
      "epoch": 2.245133462174047,
      "grad_norm": 5.002338409423828,
      "learning_rate": 4.3902404843320814e-05,
      "loss": 0.2537,
      "step": 40080
    },
    {
      "epoch": 2.245693639189984,
      "grad_norm": 1.1150224208831787,
      "learning_rate": 4.3888390604854536e-05,
      "loss": 0.1141,
      "step": 40090
    },
    {
      "epoch": 2.246253816205921,
      "grad_norm": 2.607011318206787,
      "learning_rate": 4.387437636638825e-05,
      "loss": 0.1365,
      "step": 40100
    },
    {
      "epoch": 2.246813993221858,
      "grad_norm": 2.0414953231811523,
      "learning_rate": 4.386036212792197e-05,
      "loss": 0.1567,
      "step": 40110
    },
    {
      "epoch": 2.247374170237795,
      "grad_norm": 1.1496800184249878,
      "learning_rate": 4.3846347889455694e-05,
      "loss": 0.1389,
      "step": 40120
    },
    {
      "epoch": 2.2479343472537323,
      "grad_norm": 3.0891339778900146,
      "learning_rate": 4.383233365098941e-05,
      "loss": 0.1514,
      "step": 40130
    },
    {
      "epoch": 2.2484945242696694,
      "grad_norm": 1.7732013463974,
      "learning_rate": 4.381831941252312e-05,
      "loss": 0.197,
      "step": 40140
    },
    {
      "epoch": 2.249054701285606,
      "grad_norm": 5.565792560577393,
      "learning_rate": 4.3804305174056845e-05,
      "loss": 0.1651,
      "step": 40150
    },
    {
      "epoch": 2.2496148783015433,
      "grad_norm": 6.552117347717285,
      "learning_rate": 4.379029093559056e-05,
      "loss": 0.2054,
      "step": 40160
    },
    {
      "epoch": 2.2501750553174804,
      "grad_norm": 4.588650703430176,
      "learning_rate": 4.377627669712428e-05,
      "loss": 0.1735,
      "step": 40170
    },
    {
      "epoch": 2.2507352323334175,
      "grad_norm": 3.1275510787963867,
      "learning_rate": 4.3762262458657996e-05,
      "loss": 0.1086,
      "step": 40180
    },
    {
      "epoch": 2.2512954093493542,
      "grad_norm": 3.17598819732666,
      "learning_rate": 4.374824822019172e-05,
      "loss": 0.1122,
      "step": 40190
    },
    {
      "epoch": 2.2518555863652914,
      "grad_norm": 4.268261909484863,
      "learning_rate": 4.373423398172544e-05,
      "loss": 0.2039,
      "step": 40200
    },
    {
      "epoch": 2.2524157633812285,
      "grad_norm": 3.8306198120117188,
      "learning_rate": 4.3720219743259154e-05,
      "loss": 0.1225,
      "step": 40210
    },
    {
      "epoch": 2.2529759403971656,
      "grad_norm": 1.9589484930038452,
      "learning_rate": 4.3706205504792875e-05,
      "loss": 0.1188,
      "step": 40220
    },
    {
      "epoch": 2.253536117413103,
      "grad_norm": 2.913649559020996,
      "learning_rate": 4.369219126632659e-05,
      "loss": 0.1809,
      "step": 40230
    },
    {
      "epoch": 2.2540962944290395,
      "grad_norm": 2.9960289001464844,
      "learning_rate": 4.3678177027860305e-05,
      "loss": 0.1105,
      "step": 40240
    },
    {
      "epoch": 2.2546564714449766,
      "grad_norm": 4.816039085388184,
      "learning_rate": 4.3664162789394026e-05,
      "loss": 0.2087,
      "step": 40250
    },
    {
      "epoch": 2.2552166484609137,
      "grad_norm": 2.9336886405944824,
      "learning_rate": 4.365014855092774e-05,
      "loss": 0.1098,
      "step": 40260
    },
    {
      "epoch": 2.255776825476851,
      "grad_norm": 2.966557741165161,
      "learning_rate": 4.363613431246146e-05,
      "loss": 0.1124,
      "step": 40270
    },
    {
      "epoch": 2.2563370024927876,
      "grad_norm": 5.803834915161133,
      "learning_rate": 4.3622120073995184e-05,
      "loss": 0.2411,
      "step": 40280
    },
    {
      "epoch": 2.2568971795087247,
      "grad_norm": 0.8241926431655884,
      "learning_rate": 4.36081058355289e-05,
      "loss": 0.0914,
      "step": 40290
    },
    {
      "epoch": 2.257457356524662,
      "grad_norm": 7.0580878257751465,
      "learning_rate": 4.359409159706262e-05,
      "loss": 0.1866,
      "step": 40300
    },
    {
      "epoch": 2.258017533540599,
      "grad_norm": 2.073474884033203,
      "learning_rate": 4.358007735859634e-05,
      "loss": 0.2033,
      "step": 40310
    },
    {
      "epoch": 2.258577710556536,
      "grad_norm": 4.333353519439697,
      "learning_rate": 4.356606312013005e-05,
      "loss": 0.1432,
      "step": 40320
    },
    {
      "epoch": 2.259137887572473,
      "grad_norm": 2.4829816818237305,
      "learning_rate": 4.355204888166377e-05,
      "loss": 0.1804,
      "step": 40330
    },
    {
      "epoch": 2.25969806458841,
      "grad_norm": 2.1246516704559326,
      "learning_rate": 4.353803464319749e-05,
      "loss": 0.1712,
      "step": 40340
    },
    {
      "epoch": 2.260258241604347,
      "grad_norm": 4.926733016967773,
      "learning_rate": 4.352402040473121e-05,
      "loss": 0.102,
      "step": 40350
    },
    {
      "epoch": 2.2608184186202838,
      "grad_norm": 3.4379923343658447,
      "learning_rate": 4.351000616626493e-05,
      "loss": 0.2432,
      "step": 40360
    },
    {
      "epoch": 2.261378595636221,
      "grad_norm": 1.8202106952667236,
      "learning_rate": 4.3495991927798644e-05,
      "loss": 0.139,
      "step": 40370
    },
    {
      "epoch": 2.261938772652158,
      "grad_norm": 2.630495071411133,
      "learning_rate": 4.3481977689332365e-05,
      "loss": 0.204,
      "step": 40380
    },
    {
      "epoch": 2.262498949668095,
      "grad_norm": 2.008186101913452,
      "learning_rate": 4.346796345086609e-05,
      "loss": 0.2343,
      "step": 40390
    },
    {
      "epoch": 2.2630591266840323,
      "grad_norm": 3.4960925579071045,
      "learning_rate": 4.34539492123998e-05,
      "loss": 0.169,
      "step": 40400
    },
    {
      "epoch": 2.263619303699969,
      "grad_norm": 1.4549108743667603,
      "learning_rate": 4.3439934973933516e-05,
      "loss": 0.0781,
      "step": 40410
    },
    {
      "epoch": 2.264179480715906,
      "grad_norm": 3.2240071296691895,
      "learning_rate": 4.342592073546724e-05,
      "loss": 0.1342,
      "step": 40420
    },
    {
      "epoch": 2.2647396577318433,
      "grad_norm": 1.9892845153808594,
      "learning_rate": 4.341190649700095e-05,
      "loss": 0.1583,
      "step": 40430
    },
    {
      "epoch": 2.2652998347477804,
      "grad_norm": 2.126762866973877,
      "learning_rate": 4.3397892258534674e-05,
      "loss": 0.1548,
      "step": 40440
    },
    {
      "epoch": 2.265860011763717,
      "grad_norm": 2.3152830600738525,
      "learning_rate": 4.338387802006839e-05,
      "loss": 0.1389,
      "step": 40450
    },
    {
      "epoch": 2.2664201887796542,
      "grad_norm": 1.9729582071304321,
      "learning_rate": 4.336986378160211e-05,
      "loss": 0.148,
      "step": 40460
    },
    {
      "epoch": 2.2669803657955914,
      "grad_norm": 1.5302393436431885,
      "learning_rate": 4.335584954313583e-05,
      "loss": 0.094,
      "step": 40470
    },
    {
      "epoch": 2.2675405428115285,
      "grad_norm": 2.381978988647461,
      "learning_rate": 4.3341835304669546e-05,
      "loss": 0.1053,
      "step": 40480
    },
    {
      "epoch": 2.2681007198274656,
      "grad_norm": 7.55615758895874,
      "learning_rate": 4.332782106620327e-05,
      "loss": 0.1591,
      "step": 40490
    },
    {
      "epoch": 2.2686608968434023,
      "grad_norm": 3.583143472671509,
      "learning_rate": 4.331380682773698e-05,
      "loss": 0.1312,
      "step": 40500
    },
    {
      "epoch": 2.2692210738593395,
      "grad_norm": 3.3328773975372314,
      "learning_rate": 4.32997925892707e-05,
      "loss": 0.2056,
      "step": 40510
    },
    {
      "epoch": 2.2697812508752766,
      "grad_norm": 6.46142053604126,
      "learning_rate": 4.328577835080442e-05,
      "loss": 0.1599,
      "step": 40520
    },
    {
      "epoch": 2.2703414278912137,
      "grad_norm": 1.4902440309524536,
      "learning_rate": 4.3271764112338134e-05,
      "loss": 0.1602,
      "step": 40530
    },
    {
      "epoch": 2.2709016049071504,
      "grad_norm": 3.577073335647583,
      "learning_rate": 4.3257749873871855e-05,
      "loss": 0.1164,
      "step": 40540
    },
    {
      "epoch": 2.2714617819230876,
      "grad_norm": 1.669396162033081,
      "learning_rate": 4.324373563540558e-05,
      "loss": 0.1343,
      "step": 40550
    },
    {
      "epoch": 2.2720219589390247,
      "grad_norm": 2.6210553646087646,
      "learning_rate": 4.322972139693929e-05,
      "loss": 0.1267,
      "step": 40560
    },
    {
      "epoch": 2.272582135954962,
      "grad_norm": 4.9458842277526855,
      "learning_rate": 4.321570715847301e-05,
      "loss": 0.0935,
      "step": 40570
    },
    {
      "epoch": 2.273142312970899,
      "grad_norm": 3.049813985824585,
      "learning_rate": 4.320169292000673e-05,
      "loss": 0.1656,
      "step": 40580
    },
    {
      "epoch": 2.2737024899868357,
      "grad_norm": 4.549896240234375,
      "learning_rate": 4.318767868154045e-05,
      "loss": 0.1108,
      "step": 40590
    },
    {
      "epoch": 2.274262667002773,
      "grad_norm": 1.4258710145950317,
      "learning_rate": 4.3173664443074164e-05,
      "loss": 0.17,
      "step": 40600
    },
    {
      "epoch": 2.27482284401871,
      "grad_norm": 2.4446630477905273,
      "learning_rate": 4.315965020460788e-05,
      "loss": 0.184,
      "step": 40610
    },
    {
      "epoch": 2.275383021034647,
      "grad_norm": 4.839625835418701,
      "learning_rate": 4.31456359661416e-05,
      "loss": 0.1275,
      "step": 40620
    },
    {
      "epoch": 2.2759431980505838,
      "grad_norm": 1.6053569316864014,
      "learning_rate": 4.313162172767532e-05,
      "loss": 0.1312,
      "step": 40630
    },
    {
      "epoch": 2.276503375066521,
      "grad_norm": 5.064828395843506,
      "learning_rate": 4.3117607489209037e-05,
      "loss": 0.1318,
      "step": 40640
    },
    {
      "epoch": 2.277063552082458,
      "grad_norm": 2.6575303077697754,
      "learning_rate": 4.310359325074276e-05,
      "loss": 0.142,
      "step": 40650
    },
    {
      "epoch": 2.277623729098395,
      "grad_norm": 5.695626735687256,
      "learning_rate": 4.308957901227648e-05,
      "loss": 0.2321,
      "step": 40660
    },
    {
      "epoch": 2.2781839061143323,
      "grad_norm": 2.997382402420044,
      "learning_rate": 4.3075564773810194e-05,
      "loss": 0.1571,
      "step": 40670
    },
    {
      "epoch": 2.278744083130269,
      "grad_norm": 3.824866533279419,
      "learning_rate": 4.3061550535343916e-05,
      "loss": 0.1432,
      "step": 40680
    },
    {
      "epoch": 2.279304260146206,
      "grad_norm": 1.7407063245773315,
      "learning_rate": 4.304753629687763e-05,
      "loss": 0.1223,
      "step": 40690
    },
    {
      "epoch": 2.2798644371621433,
      "grad_norm": 5.073419094085693,
      "learning_rate": 4.3033522058411345e-05,
      "loss": 0.17,
      "step": 40700
    },
    {
      "epoch": 2.2804246141780804,
      "grad_norm": 1.9369109869003296,
      "learning_rate": 4.301950781994507e-05,
      "loss": 0.1486,
      "step": 40710
    },
    {
      "epoch": 2.280984791194017,
      "grad_norm": 5.507804870605469,
      "learning_rate": 4.300549358147878e-05,
      "loss": 0.2299,
      "step": 40720
    },
    {
      "epoch": 2.2815449682099542,
      "grad_norm": 3.2386937141418457,
      "learning_rate": 4.29914793430125e-05,
      "loss": 0.0998,
      "step": 40730
    },
    {
      "epoch": 2.2821051452258914,
      "grad_norm": 1.0635950565338135,
      "learning_rate": 4.2977465104546225e-05,
      "loss": 0.1075,
      "step": 40740
    },
    {
      "epoch": 2.2826653222418285,
      "grad_norm": 2.8725860118865967,
      "learning_rate": 4.296345086607994e-05,
      "loss": 0.1975,
      "step": 40750
    },
    {
      "epoch": 2.2832254992577656,
      "grad_norm": 5.318774223327637,
      "learning_rate": 4.294943662761366e-05,
      "loss": 0.1791,
      "step": 40760
    },
    {
      "epoch": 2.2837856762737023,
      "grad_norm": 1.5511058568954468,
      "learning_rate": 4.2935422389147376e-05,
      "loss": 0.1947,
      "step": 40770
    },
    {
      "epoch": 2.2843458532896395,
      "grad_norm": 5.12503719329834,
      "learning_rate": 4.292140815068109e-05,
      "loss": 0.1784,
      "step": 40780
    },
    {
      "epoch": 2.2849060303055766,
      "grad_norm": 2.5047190189361572,
      "learning_rate": 4.290739391221481e-05,
      "loss": 0.1156,
      "step": 40790
    },
    {
      "epoch": 2.2854662073215137,
      "grad_norm": 1.9141925573349,
      "learning_rate": 4.289337967374853e-05,
      "loss": 0.1132,
      "step": 40800
    },
    {
      "epoch": 2.2860263843374504,
      "grad_norm": 1.8675892353057861,
      "learning_rate": 4.287936543528225e-05,
      "loss": 0.1139,
      "step": 40810
    },
    {
      "epoch": 2.2865865613533876,
      "grad_norm": 3.8175764083862305,
      "learning_rate": 4.286535119681597e-05,
      "loss": 0.1847,
      "step": 40820
    },
    {
      "epoch": 2.2871467383693247,
      "grad_norm": 2.997257709503174,
      "learning_rate": 4.2851336958349684e-05,
      "loss": 0.1756,
      "step": 40830
    },
    {
      "epoch": 2.287706915385262,
      "grad_norm": 0.7970640063285828,
      "learning_rate": 4.2837322719883406e-05,
      "loss": 0.1031,
      "step": 40840
    },
    {
      "epoch": 2.288267092401199,
      "grad_norm": 1.3477953672409058,
      "learning_rate": 4.282330848141712e-05,
      "loss": 0.1732,
      "step": 40850
    },
    {
      "epoch": 2.2888272694171357,
      "grad_norm": 0.8565126061439514,
      "learning_rate": 4.280929424295084e-05,
      "loss": 0.1664,
      "step": 40860
    },
    {
      "epoch": 2.289387446433073,
      "grad_norm": 1.4243533611297607,
      "learning_rate": 4.279528000448456e-05,
      "loss": 0.1321,
      "step": 40870
    },
    {
      "epoch": 2.28994762344901,
      "grad_norm": 2.244292974472046,
      "learning_rate": 4.278126576601827e-05,
      "loss": 0.263,
      "step": 40880
    },
    {
      "epoch": 2.290507800464947,
      "grad_norm": 4.1382036209106445,
      "learning_rate": 4.276725152755199e-05,
      "loss": 0.2323,
      "step": 40890
    },
    {
      "epoch": 2.2910679774808838,
      "grad_norm": 2.4084243774414062,
      "learning_rate": 4.2753237289085715e-05,
      "loss": 0.16,
      "step": 40900
    },
    {
      "epoch": 2.291628154496821,
      "grad_norm": 4.639557361602783,
      "learning_rate": 4.273922305061943e-05,
      "loss": 0.1519,
      "step": 40910
    },
    {
      "epoch": 2.292188331512758,
      "grad_norm": 1.903428077697754,
      "learning_rate": 4.272520881215315e-05,
      "loss": 0.1251,
      "step": 40920
    },
    {
      "epoch": 2.292748508528695,
      "grad_norm": 1.087368369102478,
      "learning_rate": 4.2711194573686866e-05,
      "loss": 0.1707,
      "step": 40930
    },
    {
      "epoch": 2.2933086855446323,
      "grad_norm": 1.4775731563568115,
      "learning_rate": 4.269718033522059e-05,
      "loss": 0.1292,
      "step": 40940
    },
    {
      "epoch": 2.293868862560569,
      "grad_norm": 1.984878420829773,
      "learning_rate": 4.268316609675431e-05,
      "loss": 0.1485,
      "step": 40950
    },
    {
      "epoch": 2.294429039576506,
      "grad_norm": 4.23030948638916,
      "learning_rate": 4.266915185828802e-05,
      "loss": 0.1674,
      "step": 40960
    },
    {
      "epoch": 2.2949892165924433,
      "grad_norm": 1.8690491914749146,
      "learning_rate": 4.265513761982174e-05,
      "loss": 0.103,
      "step": 40970
    },
    {
      "epoch": 2.2955493936083804,
      "grad_norm": 1.8591383695602417,
      "learning_rate": 4.264112338135546e-05,
      "loss": 0.1439,
      "step": 40980
    },
    {
      "epoch": 2.296109570624317,
      "grad_norm": 3.7204415798187256,
      "learning_rate": 4.2627109142889175e-05,
      "loss": 0.1401,
      "step": 40990
    },
    {
      "epoch": 2.2966697476402542,
      "grad_norm": 3.4846365451812744,
      "learning_rate": 4.2613094904422896e-05,
      "loss": 0.1166,
      "step": 41000
    },
    {
      "epoch": 2.2972299246561914,
      "grad_norm": 3.522272825241089,
      "learning_rate": 4.259908066595662e-05,
      "loss": 0.1529,
      "step": 41010
    },
    {
      "epoch": 2.2977901016721285,
      "grad_norm": 3.549231767654419,
      "learning_rate": 4.258506642749033e-05,
      "loss": 0.127,
      "step": 41020
    },
    {
      "epoch": 2.2983502786880656,
      "grad_norm": 2.0094759464263916,
      "learning_rate": 4.2571052189024054e-05,
      "loss": 0.1439,
      "step": 41030
    },
    {
      "epoch": 2.2989104557040023,
      "grad_norm": 3.045227527618408,
      "learning_rate": 4.255703795055777e-05,
      "loss": 0.1234,
      "step": 41040
    },
    {
      "epoch": 2.2994706327199395,
      "grad_norm": 1.6947696208953857,
      "learning_rate": 4.254302371209148e-05,
      "loss": 0.1407,
      "step": 41050
    },
    {
      "epoch": 2.3000308097358766,
      "grad_norm": 2.972550630569458,
      "learning_rate": 4.2529009473625205e-05,
      "loss": 0.0955,
      "step": 41060
    },
    {
      "epoch": 2.3005909867518137,
      "grad_norm": 3.6407437324523926,
      "learning_rate": 4.251499523515892e-05,
      "loss": 0.146,
      "step": 41070
    },
    {
      "epoch": 2.3011511637677504,
      "grad_norm": 2.0197677612304688,
      "learning_rate": 4.250098099669264e-05,
      "loss": 0.1509,
      "step": 41080
    },
    {
      "epoch": 2.3017113407836876,
      "grad_norm": 5.4230546951293945,
      "learning_rate": 4.248696675822636e-05,
      "loss": 0.1024,
      "step": 41090
    },
    {
      "epoch": 2.3022715177996247,
      "grad_norm": 1.8999274969100952,
      "learning_rate": 4.247295251976008e-05,
      "loss": 0.1366,
      "step": 41100
    },
    {
      "epoch": 2.302831694815562,
      "grad_norm": 4.383832931518555,
      "learning_rate": 4.24589382812938e-05,
      "loss": 0.1783,
      "step": 41110
    },
    {
      "epoch": 2.303391871831499,
      "grad_norm": 5.950786113739014,
      "learning_rate": 4.2444924042827514e-05,
      "loss": 0.1671,
      "step": 41120
    },
    {
      "epoch": 2.3039520488474357,
      "grad_norm": 3.674811363220215,
      "learning_rate": 4.2430909804361235e-05,
      "loss": 0.1257,
      "step": 41130
    },
    {
      "epoch": 2.304512225863373,
      "grad_norm": 4.4308648109436035,
      "learning_rate": 4.241689556589496e-05,
      "loss": 0.1291,
      "step": 41140
    },
    {
      "epoch": 2.30507240287931,
      "grad_norm": 2.941067934036255,
      "learning_rate": 4.2402881327428665e-05,
      "loss": 0.1204,
      "step": 41150
    },
    {
      "epoch": 2.305632579895247,
      "grad_norm": 5.252900123596191,
      "learning_rate": 4.2388867088962386e-05,
      "loss": 0.1594,
      "step": 41160
    },
    {
      "epoch": 2.3061927569111837,
      "grad_norm": 1.863329291343689,
      "learning_rate": 4.237485285049611e-05,
      "loss": 0.1386,
      "step": 41170
    },
    {
      "epoch": 2.306752933927121,
      "grad_norm": 0.8738759160041809,
      "learning_rate": 4.236083861202982e-05,
      "loss": 0.1183,
      "step": 41180
    },
    {
      "epoch": 2.307313110943058,
      "grad_norm": 1.434605360031128,
      "learning_rate": 4.2346824373563544e-05,
      "loss": 0.0925,
      "step": 41190
    },
    {
      "epoch": 2.307873287958995,
      "grad_norm": 4.961337566375732,
      "learning_rate": 4.233281013509726e-05,
      "loss": 0.1295,
      "step": 41200
    },
    {
      "epoch": 2.3084334649749323,
      "grad_norm": 1.5607444047927856,
      "learning_rate": 4.231879589663098e-05,
      "loss": 0.1855,
      "step": 41210
    },
    {
      "epoch": 2.308993641990869,
      "grad_norm": 5.446300029754639,
      "learning_rate": 4.23047816581647e-05,
      "loss": 0.1433,
      "step": 41220
    },
    {
      "epoch": 2.309553819006806,
      "grad_norm": 1.666441559791565,
      "learning_rate": 4.2290767419698416e-05,
      "loss": 0.0922,
      "step": 41230
    },
    {
      "epoch": 2.3101139960227433,
      "grad_norm": 4.074128150939941,
      "learning_rate": 4.227675318123213e-05,
      "loss": 0.1675,
      "step": 41240
    },
    {
      "epoch": 2.3106741730386804,
      "grad_norm": 6.752700328826904,
      "learning_rate": 4.226273894276585e-05,
      "loss": 0.1183,
      "step": 41250
    },
    {
      "epoch": 2.311234350054617,
      "grad_norm": 3.7797110080718994,
      "learning_rate": 4.224872470429957e-05,
      "loss": 0.1624,
      "step": 41260
    },
    {
      "epoch": 2.311794527070554,
      "grad_norm": 3.336521863937378,
      "learning_rate": 4.223471046583329e-05,
      "loss": 0.1593,
      "step": 41270
    },
    {
      "epoch": 2.3123547040864914,
      "grad_norm": 5.103033065795898,
      "learning_rate": 4.2220696227367004e-05,
      "loss": 0.219,
      "step": 41280
    },
    {
      "epoch": 2.3129148811024285,
      "grad_norm": 1.7764555215835571,
      "learning_rate": 4.2206681988900725e-05,
      "loss": 0.2136,
      "step": 41290
    },
    {
      "epoch": 2.3134750581183656,
      "grad_norm": 4.3366594314575195,
      "learning_rate": 4.219266775043445e-05,
      "loss": 0.1768,
      "step": 41300
    },
    {
      "epoch": 2.3140352351343023,
      "grad_norm": 4.868058204650879,
      "learning_rate": 4.217865351196816e-05,
      "loss": 0.2175,
      "step": 41310
    },
    {
      "epoch": 2.3145954121502395,
      "grad_norm": 2.4305477142333984,
      "learning_rate": 4.216463927350188e-05,
      "loss": 0.1598,
      "step": 41320
    },
    {
      "epoch": 2.3151555891661766,
      "grad_norm": 2.1816885471343994,
      "learning_rate": 4.21506250350356e-05,
      "loss": 0.147,
      "step": 41330
    },
    {
      "epoch": 2.3157157661821137,
      "grad_norm": 1.2286286354064941,
      "learning_rate": 4.213661079656931e-05,
      "loss": 0.1237,
      "step": 41340
    },
    {
      "epoch": 2.3162759431980504,
      "grad_norm": 1.9087719917297363,
      "learning_rate": 4.2122596558103034e-05,
      "loss": 0.0937,
      "step": 41350
    },
    {
      "epoch": 2.3168361202139875,
      "grad_norm": 3.0653600692749023,
      "learning_rate": 4.2108582319636756e-05,
      "loss": 0.2013,
      "step": 41360
    },
    {
      "epoch": 2.3173962972299247,
      "grad_norm": 1.8632344007492065,
      "learning_rate": 4.209456808117047e-05,
      "loss": 0.13,
      "step": 41370
    },
    {
      "epoch": 2.317956474245862,
      "grad_norm": 1.8710639476776123,
      "learning_rate": 4.208055384270419e-05,
      "loss": 0.1129,
      "step": 41380
    },
    {
      "epoch": 2.318516651261799,
      "grad_norm": 1.3090399503707886,
      "learning_rate": 4.2066539604237907e-05,
      "loss": 0.2273,
      "step": 41390
    },
    {
      "epoch": 2.3190768282777356,
      "grad_norm": 1.2387241125106812,
      "learning_rate": 4.205252536577163e-05,
      "loss": 0.1344,
      "step": 41400
    },
    {
      "epoch": 2.319637005293673,
      "grad_norm": 3.797771453857422,
      "learning_rate": 4.203851112730535e-05,
      "loss": 0.1546,
      "step": 41410
    },
    {
      "epoch": 2.32019718230961,
      "grad_norm": 2.5620954036712646,
      "learning_rate": 4.202449688883906e-05,
      "loss": 0.1846,
      "step": 41420
    },
    {
      "epoch": 2.320757359325547,
      "grad_norm": 3.801851987838745,
      "learning_rate": 4.201048265037278e-05,
      "loss": 0.1191,
      "step": 41430
    },
    {
      "epoch": 2.3213175363414837,
      "grad_norm": 3.6574127674102783,
      "learning_rate": 4.19964684119065e-05,
      "loss": 0.1573,
      "step": 41440
    },
    {
      "epoch": 2.321877713357421,
      "grad_norm": 2.067139148712158,
      "learning_rate": 4.1982454173440215e-05,
      "loss": 0.1279,
      "step": 41450
    },
    {
      "epoch": 2.322437890373358,
      "grad_norm": 4.441597938537598,
      "learning_rate": 4.196843993497394e-05,
      "loss": 0.1104,
      "step": 41460
    },
    {
      "epoch": 2.322998067389295,
      "grad_norm": 0.6525494456291199,
      "learning_rate": 4.195442569650765e-05,
      "loss": 0.1812,
      "step": 41470
    },
    {
      "epoch": 2.3235582444052323,
      "grad_norm": 2.9903950691223145,
      "learning_rate": 4.194041145804137e-05,
      "loss": 0.0854,
      "step": 41480
    },
    {
      "epoch": 2.324118421421169,
      "grad_norm": 2.534827470779419,
      "learning_rate": 4.1926397219575095e-05,
      "loss": 0.1576,
      "step": 41490
    },
    {
      "epoch": 2.324678598437106,
      "grad_norm": 1.692987322807312,
      "learning_rate": 4.191238298110881e-05,
      "loss": 0.1048,
      "step": 41500
    },
    {
      "epoch": 2.3252387754530432,
      "grad_norm": 2.025717258453369,
      "learning_rate": 4.1898368742642524e-05,
      "loss": 0.1059,
      "step": 41510
    },
    {
      "epoch": 2.3257989524689804,
      "grad_norm": 2.8646907806396484,
      "learning_rate": 4.1884354504176246e-05,
      "loss": 0.1263,
      "step": 41520
    },
    {
      "epoch": 2.326359129484917,
      "grad_norm": 3.626664638519287,
      "learning_rate": 4.187034026570996e-05,
      "loss": 0.1278,
      "step": 41530
    },
    {
      "epoch": 2.326919306500854,
      "grad_norm": 2.7220757007598877,
      "learning_rate": 4.185632602724368e-05,
      "loss": 0.1524,
      "step": 41540
    },
    {
      "epoch": 2.3274794835167913,
      "grad_norm": 4.533795356750488,
      "learning_rate": 4.18423117887774e-05,
      "loss": 0.1556,
      "step": 41550
    },
    {
      "epoch": 2.3280396605327285,
      "grad_norm": 1.8563655614852905,
      "learning_rate": 4.182829755031112e-05,
      "loss": 0.12,
      "step": 41560
    },
    {
      "epoch": 2.3285998375486656,
      "grad_norm": 2.5288712978363037,
      "learning_rate": 4.181428331184484e-05,
      "loss": 0.1688,
      "step": 41570
    },
    {
      "epoch": 2.3291600145646023,
      "grad_norm": 1.7144075632095337,
      "learning_rate": 4.1800269073378554e-05,
      "loss": 0.1333,
      "step": 41580
    },
    {
      "epoch": 2.3297201915805394,
      "grad_norm": 3.247314929962158,
      "learning_rate": 4.1786254834912276e-05,
      "loss": 0.2527,
      "step": 41590
    },
    {
      "epoch": 2.3302803685964766,
      "grad_norm": 6.955153942108154,
      "learning_rate": 4.177224059644599e-05,
      "loss": 0.1581,
      "step": 41600
    },
    {
      "epoch": 2.3308405456124137,
      "grad_norm": 2.7659711837768555,
      "learning_rate": 4.1758226357979705e-05,
      "loss": 0.1671,
      "step": 41610
    },
    {
      "epoch": 2.3314007226283504,
      "grad_norm": 2.8801615238189697,
      "learning_rate": 4.174421211951343e-05,
      "loss": 0.1146,
      "step": 41620
    },
    {
      "epoch": 2.3319608996442875,
      "grad_norm": 2.828896999359131,
      "learning_rate": 4.173019788104714e-05,
      "loss": 0.0918,
      "step": 41630
    },
    {
      "epoch": 2.3325210766602247,
      "grad_norm": 3.2261974811553955,
      "learning_rate": 4.171618364258086e-05,
      "loss": 0.1611,
      "step": 41640
    },
    {
      "epoch": 2.333081253676162,
      "grad_norm": 3.620776653289795,
      "learning_rate": 4.1702169404114585e-05,
      "loss": 0.1234,
      "step": 41650
    },
    {
      "epoch": 2.333641430692099,
      "grad_norm": 1.1525222063064575,
      "learning_rate": 4.16881551656483e-05,
      "loss": 0.1396,
      "step": 41660
    },
    {
      "epoch": 2.3342016077080356,
      "grad_norm": 3.6010186672210693,
      "learning_rate": 4.167414092718202e-05,
      "loss": 0.1318,
      "step": 41670
    },
    {
      "epoch": 2.3347617847239728,
      "grad_norm": 2.2317020893096924,
      "learning_rate": 4.166012668871574e-05,
      "loss": 0.0934,
      "step": 41680
    },
    {
      "epoch": 2.33532196173991,
      "grad_norm": 2.0668318271636963,
      "learning_rate": 4.164611245024946e-05,
      "loss": 0.1934,
      "step": 41690
    },
    {
      "epoch": 2.3358821387558466,
      "grad_norm": 4.555378437042236,
      "learning_rate": 4.163209821178317e-05,
      "loss": 0.1431,
      "step": 41700
    },
    {
      "epoch": 2.3364423157717837,
      "grad_norm": 2.223701000213623,
      "learning_rate": 4.1618083973316894e-05,
      "loss": 0.1438,
      "step": 41710
    },
    {
      "epoch": 2.337002492787721,
      "grad_norm": 3.0293688774108887,
      "learning_rate": 4.160406973485061e-05,
      "loss": 0.1377,
      "step": 41720
    },
    {
      "epoch": 2.337562669803658,
      "grad_norm": 1.4825465679168701,
      "learning_rate": 4.159005549638433e-05,
      "loss": 0.1196,
      "step": 41730
    },
    {
      "epoch": 2.338122846819595,
      "grad_norm": 3.9172005653381348,
      "learning_rate": 4.1576041257918045e-05,
      "loss": 0.1587,
      "step": 41740
    },
    {
      "epoch": 2.338683023835532,
      "grad_norm": 1.328188180923462,
      "learning_rate": 4.1562027019451766e-05,
      "loss": 0.1503,
      "step": 41750
    },
    {
      "epoch": 2.339243200851469,
      "grad_norm": 4.685403823852539,
      "learning_rate": 4.154801278098549e-05,
      "loss": 0.1487,
      "step": 41760
    },
    {
      "epoch": 2.339803377867406,
      "grad_norm": 1.6693880558013916,
      "learning_rate": 4.15339985425192e-05,
      "loss": 0.1415,
      "step": 41770
    },
    {
      "epoch": 2.3403635548833432,
      "grad_norm": 1.8911324739456177,
      "learning_rate": 4.1519984304052924e-05,
      "loss": 0.1837,
      "step": 41780
    },
    {
      "epoch": 2.34092373189928,
      "grad_norm": 1.2025917768478394,
      "learning_rate": 4.150597006558664e-05,
      "loss": 0.146,
      "step": 41790
    },
    {
      "epoch": 2.341483908915217,
      "grad_norm": 3.9924190044403076,
      "learning_rate": 4.149195582712035e-05,
      "loss": 0.0905,
      "step": 41800
    },
    {
      "epoch": 2.342044085931154,
      "grad_norm": 7.9837470054626465,
      "learning_rate": 4.1477941588654075e-05,
      "loss": 0.1892,
      "step": 41810
    },
    {
      "epoch": 2.3426042629470913,
      "grad_norm": 4.332564830780029,
      "learning_rate": 4.146392735018779e-05,
      "loss": 0.1224,
      "step": 41820
    },
    {
      "epoch": 2.3431644399630285,
      "grad_norm": 1.993104100227356,
      "learning_rate": 4.144991311172151e-05,
      "loss": 0.1258,
      "step": 41830
    },
    {
      "epoch": 2.343724616978965,
      "grad_norm": 1.0580542087554932,
      "learning_rate": 4.143589887325523e-05,
      "loss": 0.0896,
      "step": 41840
    },
    {
      "epoch": 2.3442847939949023,
      "grad_norm": 3.7568814754486084,
      "learning_rate": 4.142188463478895e-05,
      "loss": 0.1285,
      "step": 41850
    },
    {
      "epoch": 2.3448449710108394,
      "grad_norm": 2.7240068912506104,
      "learning_rate": 4.140787039632267e-05,
      "loss": 0.1413,
      "step": 41860
    },
    {
      "epoch": 2.3454051480267766,
      "grad_norm": 5.735942840576172,
      "learning_rate": 4.1393856157856384e-05,
      "loss": 0.2186,
      "step": 41870
    },
    {
      "epoch": 2.3459653250427133,
      "grad_norm": 3.0115225315093994,
      "learning_rate": 4.13798419193901e-05,
      "loss": 0.1228,
      "step": 41880
    },
    {
      "epoch": 2.3465255020586504,
      "grad_norm": 2.592163324356079,
      "learning_rate": 4.136582768092382e-05,
      "loss": 0.1368,
      "step": 41890
    },
    {
      "epoch": 2.3470856790745875,
      "grad_norm": 1.755820631980896,
      "learning_rate": 4.1351813442457535e-05,
      "loss": 0.1399,
      "step": 41900
    },
    {
      "epoch": 2.3476458560905247,
      "grad_norm": 5.0599365234375,
      "learning_rate": 4.1337799203991256e-05,
      "loss": 0.1628,
      "step": 41910
    },
    {
      "epoch": 2.348206033106462,
      "grad_norm": 2.8937249183654785,
      "learning_rate": 4.132378496552498e-05,
      "loss": 0.1212,
      "step": 41920
    },
    {
      "epoch": 2.3487662101223985,
      "grad_norm": 3.3459129333496094,
      "learning_rate": 4.130977072705869e-05,
      "loss": 0.0929,
      "step": 41930
    },
    {
      "epoch": 2.3493263871383356,
      "grad_norm": 3.172381639480591,
      "learning_rate": 4.1295756488592414e-05,
      "loss": 0.1228,
      "step": 41940
    },
    {
      "epoch": 2.3498865641542728,
      "grad_norm": 1.5718798637390137,
      "learning_rate": 4.128174225012613e-05,
      "loss": 0.1249,
      "step": 41950
    },
    {
      "epoch": 2.35044674117021,
      "grad_norm": 2.2756102085113525,
      "learning_rate": 4.126772801165985e-05,
      "loss": 0.1426,
      "step": 41960
    },
    {
      "epoch": 2.3510069181861466,
      "grad_norm": 0.823295533657074,
      "learning_rate": 4.1253713773193565e-05,
      "loss": 0.1245,
      "step": 41970
    },
    {
      "epoch": 2.3515670952020837,
      "grad_norm": 0.7383208274841309,
      "learning_rate": 4.123969953472728e-05,
      "loss": 0.1163,
      "step": 41980
    },
    {
      "epoch": 2.352127272218021,
      "grad_norm": 3.762951612472534,
      "learning_rate": 4.1225685296261e-05,
      "loss": 0.1148,
      "step": 41990
    },
    {
      "epoch": 2.352687449233958,
      "grad_norm": 1.1382545232772827,
      "learning_rate": 4.121167105779472e-05,
      "loss": 0.1676,
      "step": 42000
    },
    {
      "epoch": 2.353247626249895,
      "grad_norm": 3.192795515060425,
      "learning_rate": 4.119765681932844e-05,
      "loss": 0.2577,
      "step": 42010
    },
    {
      "epoch": 2.353807803265832,
      "grad_norm": 5.428462505340576,
      "learning_rate": 4.118364258086216e-05,
      "loss": 0.1537,
      "step": 42020
    },
    {
      "epoch": 2.354367980281769,
      "grad_norm": 4.512852191925049,
      "learning_rate": 4.116962834239588e-05,
      "loss": 0.235,
      "step": 42030
    },
    {
      "epoch": 2.354928157297706,
      "grad_norm": 5.095263481140137,
      "learning_rate": 4.1155614103929595e-05,
      "loss": 0.1883,
      "step": 42040
    },
    {
      "epoch": 2.3554883343136432,
      "grad_norm": 3.529611349105835,
      "learning_rate": 4.114159986546332e-05,
      "loss": 0.2574,
      "step": 42050
    },
    {
      "epoch": 2.35604851132958,
      "grad_norm": 4.478390216827393,
      "learning_rate": 4.112758562699703e-05,
      "loss": 0.1812,
      "step": 42060
    },
    {
      "epoch": 2.356608688345517,
      "grad_norm": 3.4121618270874023,
      "learning_rate": 4.1113571388530746e-05,
      "loss": 0.1695,
      "step": 42070
    },
    {
      "epoch": 2.357168865361454,
      "grad_norm": 0.7472324967384338,
      "learning_rate": 4.109955715006447e-05,
      "loss": 0.1246,
      "step": 42080
    },
    {
      "epoch": 2.3577290423773913,
      "grad_norm": 5.380659103393555,
      "learning_rate": 4.108554291159818e-05,
      "loss": 0.1759,
      "step": 42090
    },
    {
      "epoch": 2.3582892193933285,
      "grad_norm": 2.9636642932891846,
      "learning_rate": 4.1071528673131904e-05,
      "loss": 0.1566,
      "step": 42100
    },
    {
      "epoch": 2.358849396409265,
      "grad_norm": 1.4569512605667114,
      "learning_rate": 4.1057514434665626e-05,
      "loss": 0.0935,
      "step": 42110
    },
    {
      "epoch": 2.3594095734252023,
      "grad_norm": 1.9015483856201172,
      "learning_rate": 4.104350019619934e-05,
      "loss": 0.1519,
      "step": 42120
    },
    {
      "epoch": 2.3599697504411394,
      "grad_norm": 1.466253399848938,
      "learning_rate": 4.102948595773306e-05,
      "loss": 0.1311,
      "step": 42130
    },
    {
      "epoch": 2.3605299274570766,
      "grad_norm": 2.2953710556030273,
      "learning_rate": 4.1015471719266777e-05,
      "loss": 0.1488,
      "step": 42140
    },
    {
      "epoch": 2.3610901044730133,
      "grad_norm": 0.7387888431549072,
      "learning_rate": 4.100145748080049e-05,
      "loss": 0.2122,
      "step": 42150
    },
    {
      "epoch": 2.3616502814889504,
      "grad_norm": 1.235511302947998,
      "learning_rate": 4.098744324233421e-05,
      "loss": 0.1924,
      "step": 42160
    },
    {
      "epoch": 2.3622104585048875,
      "grad_norm": 5.291430950164795,
      "learning_rate": 4.097342900386793e-05,
      "loss": 0.1562,
      "step": 42170
    },
    {
      "epoch": 2.3627706355208247,
      "grad_norm": 0.8174666166305542,
      "learning_rate": 4.095941476540165e-05,
      "loss": 0.1699,
      "step": 42180
    },
    {
      "epoch": 2.363330812536762,
      "grad_norm": 1.892755389213562,
      "learning_rate": 4.094540052693537e-05,
      "loss": 0.1054,
      "step": 42190
    },
    {
      "epoch": 2.3638909895526985,
      "grad_norm": 1.6252707242965698,
      "learning_rate": 4.0931386288469085e-05,
      "loss": 0.0944,
      "step": 42200
    },
    {
      "epoch": 2.3644511665686356,
      "grad_norm": 1.005671501159668,
      "learning_rate": 4.091737205000281e-05,
      "loss": 0.1183,
      "step": 42210
    },
    {
      "epoch": 2.3650113435845728,
      "grad_norm": 2.57706618309021,
      "learning_rate": 4.090335781153652e-05,
      "loss": 0.1189,
      "step": 42220
    },
    {
      "epoch": 2.36557152060051,
      "grad_norm": 3.2699339389801025,
      "learning_rate": 4.088934357307024e-05,
      "loss": 0.1697,
      "step": 42230
    },
    {
      "epoch": 2.3661316976164466,
      "grad_norm": 3.0867598056793213,
      "learning_rate": 4.0875329334603965e-05,
      "loss": 0.1215,
      "step": 42240
    },
    {
      "epoch": 2.3666918746323837,
      "grad_norm": 2.145328998565674,
      "learning_rate": 4.086131509613767e-05,
      "loss": 0.1231,
      "step": 42250
    },
    {
      "epoch": 2.367252051648321,
      "grad_norm": 1.9386152029037476,
      "learning_rate": 4.0847300857671394e-05,
      "loss": 0.0908,
      "step": 42260
    },
    {
      "epoch": 2.367812228664258,
      "grad_norm": 1.6446640491485596,
      "learning_rate": 4.0833286619205116e-05,
      "loss": 0.113,
      "step": 42270
    },
    {
      "epoch": 2.368372405680195,
      "grad_norm": 8.181124687194824,
      "learning_rate": 4.081927238073883e-05,
      "loss": 0.1597,
      "step": 42280
    },
    {
      "epoch": 2.368932582696132,
      "grad_norm": 2.168691635131836,
      "learning_rate": 4.080525814227255e-05,
      "loss": 0.1374,
      "step": 42290
    },
    {
      "epoch": 2.369492759712069,
      "grad_norm": 4.85173225402832,
      "learning_rate": 4.079124390380627e-05,
      "loss": 0.1401,
      "step": 42300
    },
    {
      "epoch": 2.370052936728006,
      "grad_norm": 6.420787334442139,
      "learning_rate": 4.077722966533999e-05,
      "loss": 0.1508,
      "step": 42310
    },
    {
      "epoch": 2.3706131137439432,
      "grad_norm": 1.128556728363037,
      "learning_rate": 4.076321542687371e-05,
      "loss": 0.0967,
      "step": 42320
    },
    {
      "epoch": 2.37117329075988,
      "grad_norm": 2.9318325519561768,
      "learning_rate": 4.0749201188407424e-05,
      "loss": 0.1293,
      "step": 42330
    },
    {
      "epoch": 2.371733467775817,
      "grad_norm": 2.6026222705841064,
      "learning_rate": 4.073518694994114e-05,
      "loss": 0.1297,
      "step": 42340
    },
    {
      "epoch": 2.372293644791754,
      "grad_norm": 3.7849338054656982,
      "learning_rate": 4.072117271147486e-05,
      "loss": 0.14,
      "step": 42350
    },
    {
      "epoch": 2.3728538218076913,
      "grad_norm": 3.150916337966919,
      "learning_rate": 4.0707158473008575e-05,
      "loss": 0.1513,
      "step": 42360
    },
    {
      "epoch": 2.3734139988236285,
      "grad_norm": 1.537105679512024,
      "learning_rate": 4.06931442345423e-05,
      "loss": 0.0915,
      "step": 42370
    },
    {
      "epoch": 2.373974175839565,
      "grad_norm": 4.454941749572754,
      "learning_rate": 4.067912999607602e-05,
      "loss": 0.1064,
      "step": 42380
    },
    {
      "epoch": 2.3745343528555023,
      "grad_norm": 2.0152909755706787,
      "learning_rate": 4.066511575760973e-05,
      "loss": 0.1603,
      "step": 42390
    },
    {
      "epoch": 2.3750945298714394,
      "grad_norm": 5.335593223571777,
      "learning_rate": 4.0651101519143455e-05,
      "loss": 0.173,
      "step": 42400
    },
    {
      "epoch": 2.3756547068873766,
      "grad_norm": 3.3962416648864746,
      "learning_rate": 4.063708728067717e-05,
      "loss": 0.117,
      "step": 42410
    },
    {
      "epoch": 2.3762148839033133,
      "grad_norm": 2.968782663345337,
      "learning_rate": 4.062307304221089e-05,
      "loss": 0.127,
      "step": 42420
    },
    {
      "epoch": 2.3767750609192504,
      "grad_norm": 4.8912763595581055,
      "learning_rate": 4.0609058803744606e-05,
      "loss": 0.1356,
      "step": 42430
    },
    {
      "epoch": 2.3773352379351875,
      "grad_norm": 2.1651854515075684,
      "learning_rate": 4.059504456527832e-05,
      "loss": 0.0999,
      "step": 42440
    },
    {
      "epoch": 2.3778954149511247,
      "grad_norm": 6.182384490966797,
      "learning_rate": 4.058103032681204e-05,
      "loss": 0.1936,
      "step": 42450
    },
    {
      "epoch": 2.378455591967062,
      "grad_norm": 3.674875259399414,
      "learning_rate": 4.0567016088345764e-05,
      "loss": 0.139,
      "step": 42460
    },
    {
      "epoch": 2.3790157689829985,
      "grad_norm": 4.730157375335693,
      "learning_rate": 4.055300184987948e-05,
      "loss": 0.1147,
      "step": 42470
    },
    {
      "epoch": 2.3795759459989356,
      "grad_norm": 2.047330379486084,
      "learning_rate": 4.05389876114132e-05,
      "loss": 0.1606,
      "step": 42480
    },
    {
      "epoch": 2.3801361230148728,
      "grad_norm": 2.655431032180786,
      "learning_rate": 4.0524973372946915e-05,
      "loss": 0.2183,
      "step": 42490
    },
    {
      "epoch": 2.38069630003081,
      "grad_norm": 2.2406435012817383,
      "learning_rate": 4.0510959134480636e-05,
      "loss": 0.1058,
      "step": 42500
    },
    {
      "epoch": 2.3812564770467466,
      "grad_norm": 3.8469650745391846,
      "learning_rate": 4.049694489601436e-05,
      "loss": 0.1263,
      "step": 42510
    },
    {
      "epoch": 2.3818166540626837,
      "grad_norm": 1.9867042303085327,
      "learning_rate": 4.0482930657548066e-05,
      "loss": 0.1519,
      "step": 42520
    },
    {
      "epoch": 2.382376831078621,
      "grad_norm": 1.4869067668914795,
      "learning_rate": 4.046891641908179e-05,
      "loss": 0.1632,
      "step": 42530
    },
    {
      "epoch": 2.382937008094558,
      "grad_norm": 4.3625946044921875,
      "learning_rate": 4.045490218061551e-05,
      "loss": 0.1107,
      "step": 42540
    },
    {
      "epoch": 2.383497185110495,
      "grad_norm": 5.933204650878906,
      "learning_rate": 4.044088794214922e-05,
      "loss": 0.2102,
      "step": 42550
    },
    {
      "epoch": 2.384057362126432,
      "grad_norm": 2.170111894607544,
      "learning_rate": 4.0426873703682945e-05,
      "loss": 0.1518,
      "step": 42560
    },
    {
      "epoch": 2.384617539142369,
      "grad_norm": 1.9488118886947632,
      "learning_rate": 4.041285946521666e-05,
      "loss": 0.124,
      "step": 42570
    },
    {
      "epoch": 2.385177716158306,
      "grad_norm": 4.35560941696167,
      "learning_rate": 4.039884522675038e-05,
      "loss": 0.1582,
      "step": 42580
    },
    {
      "epoch": 2.3857378931742432,
      "grad_norm": 2.4488894939422607,
      "learning_rate": 4.03848309882841e-05,
      "loss": 0.1508,
      "step": 42590
    },
    {
      "epoch": 2.38629807019018,
      "grad_norm": 3.79835844039917,
      "learning_rate": 4.037081674981782e-05,
      "loss": 0.1679,
      "step": 42600
    },
    {
      "epoch": 2.386858247206117,
      "grad_norm": 2.0313541889190674,
      "learning_rate": 4.035680251135153e-05,
      "loss": 0.1048,
      "step": 42610
    },
    {
      "epoch": 2.387418424222054,
      "grad_norm": 1.847442865371704,
      "learning_rate": 4.0342788272885254e-05,
      "loss": 0.1849,
      "step": 42620
    },
    {
      "epoch": 2.3879786012379913,
      "grad_norm": 5.072696208953857,
      "learning_rate": 4.032877403441897e-05,
      "loss": 0.1205,
      "step": 42630
    },
    {
      "epoch": 2.3885387782539285,
      "grad_norm": 3.968748092651367,
      "learning_rate": 4.031475979595269e-05,
      "loss": 0.165,
      "step": 42640
    },
    {
      "epoch": 2.389098955269865,
      "grad_norm": 5.316502094268799,
      "learning_rate": 4.0300745557486405e-05,
      "loss": 0.1135,
      "step": 42650
    },
    {
      "epoch": 2.3896591322858023,
      "grad_norm": 3.7323408126831055,
      "learning_rate": 4.0286731319020126e-05,
      "loss": 0.2111,
      "step": 42660
    },
    {
      "epoch": 2.3902193093017394,
      "grad_norm": 5.0679755210876465,
      "learning_rate": 4.027271708055385e-05,
      "loss": 0.1802,
      "step": 42670
    },
    {
      "epoch": 2.3907794863176766,
      "grad_norm": 2.4598777294158936,
      "learning_rate": 4.025870284208756e-05,
      "loss": 0.0675,
      "step": 42680
    },
    {
      "epoch": 2.3913396633336133,
      "grad_norm": 2.254636526107788,
      "learning_rate": 4.0244688603621284e-05,
      "loss": 0.1228,
      "step": 42690
    },
    {
      "epoch": 2.3918998403495504,
      "grad_norm": 1.4990373849868774,
      "learning_rate": 4.0230674365155e-05,
      "loss": 0.2228,
      "step": 42700
    },
    {
      "epoch": 2.3924600173654875,
      "grad_norm": 3.6055655479431152,
      "learning_rate": 4.0216660126688713e-05,
      "loss": 0.164,
      "step": 42710
    },
    {
      "epoch": 2.3930201943814247,
      "grad_norm": 1.0669652223587036,
      "learning_rate": 4.0202645888222435e-05,
      "loss": 0.1345,
      "step": 42720
    },
    {
      "epoch": 2.393580371397362,
      "grad_norm": 4.424293518066406,
      "learning_rate": 4.0188631649756156e-05,
      "loss": 0.115,
      "step": 42730
    },
    {
      "epoch": 2.3941405484132985,
      "grad_norm": 4.574793815612793,
      "learning_rate": 4.017461741128987e-05,
      "loss": 0.1206,
      "step": 42740
    },
    {
      "epoch": 2.3947007254292356,
      "grad_norm": 3.7225372791290283,
      "learning_rate": 4.016060317282359e-05,
      "loss": 0.1348,
      "step": 42750
    },
    {
      "epoch": 2.3952609024451728,
      "grad_norm": 1.6847248077392578,
      "learning_rate": 4.014658893435731e-05,
      "loss": 0.2109,
      "step": 42760
    },
    {
      "epoch": 2.39582107946111,
      "grad_norm": 2.696345329284668,
      "learning_rate": 4.013257469589103e-05,
      "loss": 0.1379,
      "step": 42770
    },
    {
      "epoch": 2.3963812564770466,
      "grad_norm": 0.5276878476142883,
      "learning_rate": 4.011856045742475e-05,
      "loss": 0.0987,
      "step": 42780
    },
    {
      "epoch": 2.3969414334929837,
      "grad_norm": 1.5392478704452515,
      "learning_rate": 4.0104546218958465e-05,
      "loss": 0.1851,
      "step": 42790
    },
    {
      "epoch": 2.397501610508921,
      "grad_norm": 2.6611709594726562,
      "learning_rate": 4.009053198049218e-05,
      "loss": 0.1825,
      "step": 42800
    },
    {
      "epoch": 2.398061787524858,
      "grad_norm": 4.288579940795898,
      "learning_rate": 4.00765177420259e-05,
      "loss": 0.147,
      "step": 42810
    },
    {
      "epoch": 2.398621964540795,
      "grad_norm": 2.4451076984405518,
      "learning_rate": 4.0062503503559616e-05,
      "loss": 0.1991,
      "step": 42820
    },
    {
      "epoch": 2.399182141556732,
      "grad_norm": 3.3000800609588623,
      "learning_rate": 4.004848926509334e-05,
      "loss": 0.1468,
      "step": 42830
    },
    {
      "epoch": 2.399742318572669,
      "grad_norm": 1.294890284538269,
      "learning_rate": 4.003447502662705e-05,
      "loss": 0.0857,
      "step": 42840
    },
    {
      "epoch": 2.400302495588606,
      "grad_norm": 4.472443103790283,
      "learning_rate": 4.0020460788160774e-05,
      "loss": 0.1185,
      "step": 42850
    },
    {
      "epoch": 2.4008626726045432,
      "grad_norm": 2.0202834606170654,
      "learning_rate": 4.0006446549694496e-05,
      "loss": 0.1166,
      "step": 42860
    },
    {
      "epoch": 2.40142284962048,
      "grad_norm": 0.6603901982307434,
      "learning_rate": 3.999243231122821e-05,
      "loss": 0.127,
      "step": 42870
    },
    {
      "epoch": 2.401983026636417,
      "grad_norm": 3.3841137886047363,
      "learning_rate": 3.997841807276193e-05,
      "loss": 0.1002,
      "step": 42880
    },
    {
      "epoch": 2.402543203652354,
      "grad_norm": 3.8561904430389404,
      "learning_rate": 3.9964403834295647e-05,
      "loss": 0.1068,
      "step": 42890
    },
    {
      "epoch": 2.4031033806682913,
      "grad_norm": 2.968909740447998,
      "learning_rate": 3.995038959582936e-05,
      "loss": 0.1046,
      "step": 42900
    },
    {
      "epoch": 2.4036635576842285,
      "grad_norm": 4.179520130157471,
      "learning_rate": 3.993637535736308e-05,
      "loss": 0.1064,
      "step": 42910
    },
    {
      "epoch": 2.404223734700165,
      "grad_norm": 5.234282493591309,
      "learning_rate": 3.99223611188968e-05,
      "loss": 0.2081,
      "step": 42920
    },
    {
      "epoch": 2.4047839117161023,
      "grad_norm": 7.462259769439697,
      "learning_rate": 3.990834688043052e-05,
      "loss": 0.1484,
      "step": 42930
    },
    {
      "epoch": 2.4053440887320394,
      "grad_norm": 2.965891122817993,
      "learning_rate": 3.989433264196424e-05,
      "loss": 0.1206,
      "step": 42940
    },
    {
      "epoch": 2.405904265747976,
      "grad_norm": 2.401970863342285,
      "learning_rate": 3.9880318403497955e-05,
      "loss": 0.1,
      "step": 42950
    },
    {
      "epoch": 2.4064644427639132,
      "grad_norm": 3.373554229736328,
      "learning_rate": 3.986630416503168e-05,
      "loss": 0.1497,
      "step": 42960
    },
    {
      "epoch": 2.4070246197798504,
      "grad_norm": 1.2633816003799438,
      "learning_rate": 3.985228992656539e-05,
      "loss": 0.1255,
      "step": 42970
    },
    {
      "epoch": 2.4075847967957875,
      "grad_norm": 4.042367458343506,
      "learning_rate": 3.9838275688099106e-05,
      "loss": 0.154,
      "step": 42980
    },
    {
      "epoch": 2.4081449738117247,
      "grad_norm": 3.5891520977020264,
      "learning_rate": 3.982426144963283e-05,
      "loss": 0.1238,
      "step": 42990
    },
    {
      "epoch": 2.408705150827662,
      "grad_norm": 2.3778958320617676,
      "learning_rate": 3.981024721116654e-05,
      "loss": 0.1211,
      "step": 43000
    },
    {
      "epoch": 2.4092653278435985,
      "grad_norm": 2.4585392475128174,
      "learning_rate": 3.9796232972700264e-05,
      "loss": 0.1225,
      "step": 43010
    },
    {
      "epoch": 2.4098255048595356,
      "grad_norm": 3.298072576522827,
      "learning_rate": 3.9782218734233986e-05,
      "loss": 0.1128,
      "step": 43020
    },
    {
      "epoch": 2.4103856818754728,
      "grad_norm": 3.2020673751831055,
      "learning_rate": 3.97682044957677e-05,
      "loss": 0.157,
      "step": 43030
    },
    {
      "epoch": 2.4109458588914094,
      "grad_norm": 0.6607831120491028,
      "learning_rate": 3.975419025730142e-05,
      "loss": 0.1324,
      "step": 43040
    },
    {
      "epoch": 2.4115060359073466,
      "grad_norm": 3.8338146209716797,
      "learning_rate": 3.9740176018835143e-05,
      "loss": 0.1485,
      "step": 43050
    },
    {
      "epoch": 2.4120662129232837,
      "grad_norm": 6.302176475524902,
      "learning_rate": 3.972616178036886e-05,
      "loss": 0.1661,
      "step": 43060
    },
    {
      "epoch": 2.412626389939221,
      "grad_norm": 2.8471648693084717,
      "learning_rate": 3.971214754190257e-05,
      "loss": 0.1517,
      "step": 43070
    },
    {
      "epoch": 2.413186566955158,
      "grad_norm": 5.371256351470947,
      "learning_rate": 3.9698133303436294e-05,
      "loss": 0.1939,
      "step": 43080
    },
    {
      "epoch": 2.4137467439710947,
      "grad_norm": 1.796656608581543,
      "learning_rate": 3.968411906497001e-05,
      "loss": 0.1484,
      "step": 43090
    },
    {
      "epoch": 2.414306920987032,
      "grad_norm": 0.7035080790519714,
      "learning_rate": 3.967010482650373e-05,
      "loss": 0.1828,
      "step": 43100
    },
    {
      "epoch": 2.414867098002969,
      "grad_norm": 3.3950436115264893,
      "learning_rate": 3.9656090588037445e-05,
      "loss": 0.1451,
      "step": 43110
    },
    {
      "epoch": 2.415427275018906,
      "grad_norm": 3.5725603103637695,
      "learning_rate": 3.964207634957117e-05,
      "loss": 0.1949,
      "step": 43120
    },
    {
      "epoch": 2.4159874520348428,
      "grad_norm": 2.6228227615356445,
      "learning_rate": 3.962806211110489e-05,
      "loss": 0.2024,
      "step": 43130
    },
    {
      "epoch": 2.41654762905078,
      "grad_norm": 3.631546974182129,
      "learning_rate": 3.96140478726386e-05,
      "loss": 0.086,
      "step": 43140
    },
    {
      "epoch": 2.417107806066717,
      "grad_norm": 1.8136000633239746,
      "learning_rate": 3.9600033634172325e-05,
      "loss": 0.0812,
      "step": 43150
    },
    {
      "epoch": 2.417667983082654,
      "grad_norm": 2.4730310440063477,
      "learning_rate": 3.958601939570604e-05,
      "loss": 0.1479,
      "step": 43160
    },
    {
      "epoch": 2.4182281600985913,
      "grad_norm": 1.5469329357147217,
      "learning_rate": 3.9572005157239754e-05,
      "loss": 0.1821,
      "step": 43170
    },
    {
      "epoch": 2.418788337114528,
      "grad_norm": 0.7572112083435059,
      "learning_rate": 3.9557990918773476e-05,
      "loss": 0.1639,
      "step": 43180
    },
    {
      "epoch": 2.419348514130465,
      "grad_norm": 2.3881285190582275,
      "learning_rate": 3.954397668030719e-05,
      "loss": 0.1521,
      "step": 43190
    },
    {
      "epoch": 2.4199086911464023,
      "grad_norm": 1.8735519647598267,
      "learning_rate": 3.952996244184091e-05,
      "loss": 0.1265,
      "step": 43200
    },
    {
      "epoch": 2.4204688681623394,
      "grad_norm": 2.7155585289001465,
      "learning_rate": 3.9515948203374634e-05,
      "loss": 0.1313,
      "step": 43210
    },
    {
      "epoch": 2.421029045178276,
      "grad_norm": 2.3232128620147705,
      "learning_rate": 3.950193396490835e-05,
      "loss": 0.1643,
      "step": 43220
    },
    {
      "epoch": 2.4215892221942132,
      "grad_norm": 3.3621084690093994,
      "learning_rate": 3.948791972644207e-05,
      "loss": 0.1333,
      "step": 43230
    },
    {
      "epoch": 2.4221493992101504,
      "grad_norm": 0.9192138910293579,
      "learning_rate": 3.9473905487975785e-05,
      "loss": 0.2229,
      "step": 43240
    },
    {
      "epoch": 2.4227095762260875,
      "grad_norm": 0.8897711634635925,
      "learning_rate": 3.94598912495095e-05,
      "loss": 0.1296,
      "step": 43250
    },
    {
      "epoch": 2.4232697532420246,
      "grad_norm": 1.9499698877334595,
      "learning_rate": 3.944587701104322e-05,
      "loss": 0.1855,
      "step": 43260
    },
    {
      "epoch": 2.4238299302579613,
      "grad_norm": 2.7049036026000977,
      "learning_rate": 3.9431862772576936e-05,
      "loss": 0.1628,
      "step": 43270
    },
    {
      "epoch": 2.4243901072738985,
      "grad_norm": 2.289799690246582,
      "learning_rate": 3.941784853411066e-05,
      "loss": 0.27,
      "step": 43280
    },
    {
      "epoch": 2.4249502842898356,
      "grad_norm": 2.806429624557495,
      "learning_rate": 3.940383429564438e-05,
      "loss": 0.168,
      "step": 43290
    },
    {
      "epoch": 2.4255104613057727,
      "grad_norm": 3.042628765106201,
      "learning_rate": 3.938982005717809e-05,
      "loss": 0.174,
      "step": 43300
    },
    {
      "epoch": 2.4260706383217094,
      "grad_norm": 3.937108039855957,
      "learning_rate": 3.9375805818711815e-05,
      "loss": 0.1747,
      "step": 43310
    },
    {
      "epoch": 2.4266308153376466,
      "grad_norm": 0.6979052424430847,
      "learning_rate": 3.936179158024553e-05,
      "loss": 0.1046,
      "step": 43320
    },
    {
      "epoch": 2.4271909923535837,
      "grad_norm": 1.294817328453064,
      "learning_rate": 3.934777734177925e-05,
      "loss": 0.1554,
      "step": 43330
    },
    {
      "epoch": 2.427751169369521,
      "grad_norm": 2.8531131744384766,
      "learning_rate": 3.933376310331297e-05,
      "loss": 0.1301,
      "step": 43340
    },
    {
      "epoch": 2.428311346385458,
      "grad_norm": 1.020552635192871,
      "learning_rate": 3.931974886484668e-05,
      "loss": 0.14,
      "step": 43350
    },
    {
      "epoch": 2.4288715234013947,
      "grad_norm": 5.506194591522217,
      "learning_rate": 3.93057346263804e-05,
      "loss": 0.2199,
      "step": 43360
    },
    {
      "epoch": 2.429431700417332,
      "grad_norm": 3.9104061126708984,
      "learning_rate": 3.9291720387914124e-05,
      "loss": 0.1064,
      "step": 43370
    },
    {
      "epoch": 2.429991877433269,
      "grad_norm": 1.4562591314315796,
      "learning_rate": 3.927770614944784e-05,
      "loss": 0.2148,
      "step": 43380
    },
    {
      "epoch": 2.430552054449206,
      "grad_norm": 2.87190842628479,
      "learning_rate": 3.926369191098156e-05,
      "loss": 0.1806,
      "step": 43390
    },
    {
      "epoch": 2.4311122314651428,
      "grad_norm": 3.7680323123931885,
      "learning_rate": 3.924967767251528e-05,
      "loss": 0.1202,
      "step": 43400
    },
    {
      "epoch": 2.43167240848108,
      "grad_norm": 3.61422061920166,
      "learning_rate": 3.9235663434048996e-05,
      "loss": 0.1612,
      "step": 43410
    },
    {
      "epoch": 2.432232585497017,
      "grad_norm": 2.228118896484375,
      "learning_rate": 3.922164919558272e-05,
      "loss": 0.1194,
      "step": 43420
    },
    {
      "epoch": 2.432792762512954,
      "grad_norm": 4.07897424697876,
      "learning_rate": 3.920763495711643e-05,
      "loss": 0.1582,
      "step": 43430
    },
    {
      "epoch": 2.4333529395288913,
      "grad_norm": 3.88382887840271,
      "learning_rate": 3.919362071865015e-05,
      "loss": 0.1109,
      "step": 43440
    },
    {
      "epoch": 2.433913116544828,
      "grad_norm": 2.0425291061401367,
      "learning_rate": 3.917960648018387e-05,
      "loss": 0.1368,
      "step": 43450
    },
    {
      "epoch": 2.434473293560765,
      "grad_norm": 4.814454555511475,
      "learning_rate": 3.9165592241717583e-05,
      "loss": 0.1068,
      "step": 43460
    },
    {
      "epoch": 2.4350334705767023,
      "grad_norm": 2.896411657333374,
      "learning_rate": 3.9151578003251305e-05,
      "loss": 0.2328,
      "step": 43470
    },
    {
      "epoch": 2.4355936475926394,
      "grad_norm": 5.195570945739746,
      "learning_rate": 3.9137563764785026e-05,
      "loss": 0.1694,
      "step": 43480
    },
    {
      "epoch": 2.436153824608576,
      "grad_norm": 1.5734399557113647,
      "learning_rate": 3.912354952631874e-05,
      "loss": 0.1193,
      "step": 43490
    },
    {
      "epoch": 2.4367140016245132,
      "grad_norm": 1.7160706520080566,
      "learning_rate": 3.910953528785246e-05,
      "loss": 0.2573,
      "step": 43500
    },
    {
      "epoch": 2.4372741786404504,
      "grad_norm": 3.626859426498413,
      "learning_rate": 3.909552104938618e-05,
      "loss": 0.0916,
      "step": 43510
    },
    {
      "epoch": 2.4378343556563875,
      "grad_norm": 3.1579372882843018,
      "learning_rate": 3.90815068109199e-05,
      "loss": 0.1409,
      "step": 43520
    },
    {
      "epoch": 2.4383945326723246,
      "grad_norm": 1.0050811767578125,
      "learning_rate": 3.9067492572453614e-05,
      "loss": 0.1119,
      "step": 43530
    },
    {
      "epoch": 2.4389547096882613,
      "grad_norm": 1.19960355758667,
      "learning_rate": 3.905347833398733e-05,
      "loss": 0.1137,
      "step": 43540
    },
    {
      "epoch": 2.4395148867041985,
      "grad_norm": 3.6616101264953613,
      "learning_rate": 3.903946409552105e-05,
      "loss": 0.1172,
      "step": 43550
    },
    {
      "epoch": 2.4400750637201356,
      "grad_norm": 4.016157150268555,
      "learning_rate": 3.902544985705477e-05,
      "loss": 0.1656,
      "step": 43560
    },
    {
      "epoch": 2.4406352407360727,
      "grad_norm": 4.455755233764648,
      "learning_rate": 3.9011435618588486e-05,
      "loss": 0.1029,
      "step": 43570
    },
    {
      "epoch": 2.4411954177520094,
      "grad_norm": 1.2748160362243652,
      "learning_rate": 3.899742138012221e-05,
      "loss": 0.1397,
      "step": 43580
    },
    {
      "epoch": 2.4417555947679466,
      "grad_norm": 2.4204177856445312,
      "learning_rate": 3.898340714165592e-05,
      "loss": 0.1125,
      "step": 43590
    },
    {
      "epoch": 2.4423157717838837,
      "grad_norm": 1.1275726556777954,
      "learning_rate": 3.8969392903189644e-05,
      "loss": 0.1432,
      "step": 43600
    },
    {
      "epoch": 2.442875948799821,
      "grad_norm": 4.45682430267334,
      "learning_rate": 3.8955378664723366e-05,
      "loss": 0.1744,
      "step": 43610
    },
    {
      "epoch": 2.443436125815758,
      "grad_norm": 1.814902663230896,
      "learning_rate": 3.8941364426257074e-05,
      "loss": 0.1913,
      "step": 43620
    },
    {
      "epoch": 2.4439963028316947,
      "grad_norm": 0.9805145263671875,
      "learning_rate": 3.8927350187790795e-05,
      "loss": 0.0903,
      "step": 43630
    },
    {
      "epoch": 2.444556479847632,
      "grad_norm": 4.319526195526123,
      "learning_rate": 3.8913335949324517e-05,
      "loss": 0.1391,
      "step": 43640
    },
    {
      "epoch": 2.445116656863569,
      "grad_norm": 1.982744574546814,
      "learning_rate": 3.889932171085823e-05,
      "loss": 0.1274,
      "step": 43650
    },
    {
      "epoch": 2.445676833879506,
      "grad_norm": 1.3704211711883545,
      "learning_rate": 3.888530747239195e-05,
      "loss": 0.1277,
      "step": 43660
    },
    {
      "epoch": 2.4462370108954428,
      "grad_norm": 4.516005992889404,
      "learning_rate": 3.887129323392567e-05,
      "loss": 0.106,
      "step": 43670
    },
    {
      "epoch": 2.44679718791138,
      "grad_norm": 1.955283522605896,
      "learning_rate": 3.885727899545939e-05,
      "loss": 0.1084,
      "step": 43680
    },
    {
      "epoch": 2.447357364927317,
      "grad_norm": 4.2760910987854,
      "learning_rate": 3.884326475699311e-05,
      "loss": 0.1434,
      "step": 43690
    },
    {
      "epoch": 2.447917541943254,
      "grad_norm": 4.080873489379883,
      "learning_rate": 3.8829250518526825e-05,
      "loss": 0.0974,
      "step": 43700
    },
    {
      "epoch": 2.4484777189591913,
      "grad_norm": 3.043874979019165,
      "learning_rate": 3.881523628006054e-05,
      "loss": 0.1451,
      "step": 43710
    },
    {
      "epoch": 2.449037895975128,
      "grad_norm": 4.50651741027832,
      "learning_rate": 3.880122204159426e-05,
      "loss": 0.104,
      "step": 43720
    },
    {
      "epoch": 2.449598072991065,
      "grad_norm": 3.7574291229248047,
      "learning_rate": 3.8787207803127976e-05,
      "loss": 0.196,
      "step": 43730
    },
    {
      "epoch": 2.4501582500070023,
      "grad_norm": 4.161116123199463,
      "learning_rate": 3.87731935646617e-05,
      "loss": 0.1129,
      "step": 43740
    },
    {
      "epoch": 2.4507184270229394,
      "grad_norm": 3.327559471130371,
      "learning_rate": 3.875917932619542e-05,
      "loss": 0.1076,
      "step": 43750
    },
    {
      "epoch": 2.451278604038876,
      "grad_norm": 1.2013472318649292,
      "learning_rate": 3.8745165087729134e-05,
      "loss": 0.1124,
      "step": 43760
    },
    {
      "epoch": 2.4518387810548132,
      "grad_norm": 1.2532354593276978,
      "learning_rate": 3.8731150849262856e-05,
      "loss": 0.1554,
      "step": 43770
    },
    {
      "epoch": 2.4523989580707504,
      "grad_norm": 2.2093605995178223,
      "learning_rate": 3.871713661079657e-05,
      "loss": 0.1517,
      "step": 43780
    },
    {
      "epoch": 2.4529591350866875,
      "grad_norm": 2.6895968914031982,
      "learning_rate": 3.870312237233029e-05,
      "loss": 0.1833,
      "step": 43790
    },
    {
      "epoch": 2.4535193121026246,
      "grad_norm": 4.268205165863037,
      "learning_rate": 3.8689108133864013e-05,
      "loss": 0.1011,
      "step": 43800
    },
    {
      "epoch": 2.4540794891185613,
      "grad_norm": 1.1002572774887085,
      "learning_rate": 3.867509389539772e-05,
      "loss": 0.1251,
      "step": 43810
    },
    {
      "epoch": 2.4546396661344985,
      "grad_norm": 1.9945247173309326,
      "learning_rate": 3.866107965693144e-05,
      "loss": 0.1174,
      "step": 43820
    },
    {
      "epoch": 2.4551998431504356,
      "grad_norm": 1.2263315916061401,
      "learning_rate": 3.8647065418465164e-05,
      "loss": 0.105,
      "step": 43830
    },
    {
      "epoch": 2.4557600201663727,
      "grad_norm": 2.7595367431640625,
      "learning_rate": 3.863305117999888e-05,
      "loss": 0.1776,
      "step": 43840
    },
    {
      "epoch": 2.4563201971823094,
      "grad_norm": 2.584479570388794,
      "learning_rate": 3.86190369415326e-05,
      "loss": 0.2231,
      "step": 43850
    },
    {
      "epoch": 2.4568803741982466,
      "grad_norm": 1.4545406103134155,
      "learning_rate": 3.8605022703066315e-05,
      "loss": 0.109,
      "step": 43860
    },
    {
      "epoch": 2.4574405512141837,
      "grad_norm": 6.770106315612793,
      "learning_rate": 3.859100846460004e-05,
      "loss": 0.1642,
      "step": 43870
    },
    {
      "epoch": 2.458000728230121,
      "grad_norm": 4.626778602600098,
      "learning_rate": 3.857699422613376e-05,
      "loss": 0.2166,
      "step": 43880
    },
    {
      "epoch": 2.458560905246058,
      "grad_norm": 4.188451290130615,
      "learning_rate": 3.856297998766747e-05,
      "loss": 0.1558,
      "step": 43890
    },
    {
      "epoch": 2.4591210822619947,
      "grad_norm": 0.5631417632102966,
      "learning_rate": 3.854896574920119e-05,
      "loss": 0.0996,
      "step": 43900
    },
    {
      "epoch": 2.459681259277932,
      "grad_norm": 4.453567981719971,
      "learning_rate": 3.853495151073491e-05,
      "loss": 0.1888,
      "step": 43910
    },
    {
      "epoch": 2.460241436293869,
      "grad_norm": 6.746283054351807,
      "learning_rate": 3.8520937272268624e-05,
      "loss": 0.2326,
      "step": 43920
    },
    {
      "epoch": 2.460801613309806,
      "grad_norm": 4.61957311630249,
      "learning_rate": 3.8506923033802346e-05,
      "loss": 0.1714,
      "step": 43930
    },
    {
      "epoch": 2.4613617903257428,
      "grad_norm": 0.7871787548065186,
      "learning_rate": 3.849290879533606e-05,
      "loss": 0.1202,
      "step": 43940
    },
    {
      "epoch": 2.46192196734168,
      "grad_norm": 2.211790084838867,
      "learning_rate": 3.847889455686978e-05,
      "loss": 0.0945,
      "step": 43950
    },
    {
      "epoch": 2.462482144357617,
      "grad_norm": 3.1805357933044434,
      "learning_rate": 3.8464880318403504e-05,
      "loss": 0.1422,
      "step": 43960
    },
    {
      "epoch": 2.463042321373554,
      "grad_norm": 3.9268364906311035,
      "learning_rate": 3.845086607993722e-05,
      "loss": 0.3003,
      "step": 43970
    },
    {
      "epoch": 2.4636024983894913,
      "grad_norm": 3.1115269660949707,
      "learning_rate": 3.843685184147094e-05,
      "loss": 0.1137,
      "step": 43980
    },
    {
      "epoch": 2.464162675405428,
      "grad_norm": 4.902919769287109,
      "learning_rate": 3.8422837603004655e-05,
      "loss": 0.1378,
      "step": 43990
    },
    {
      "epoch": 2.464722852421365,
      "grad_norm": 3.2837207317352295,
      "learning_rate": 3.840882336453837e-05,
      "loss": 0.1034,
      "step": 44000
    },
    {
      "epoch": 2.4652830294373023,
      "grad_norm": 1.076744556427002,
      "learning_rate": 3.839480912607209e-05,
      "loss": 0.1098,
      "step": 44010
    },
    {
      "epoch": 2.4658432064532394,
      "grad_norm": 2.9561574459075928,
      "learning_rate": 3.8380794887605806e-05,
      "loss": 0.1418,
      "step": 44020
    },
    {
      "epoch": 2.466403383469176,
      "grad_norm": 6.7188310623168945,
      "learning_rate": 3.836678064913953e-05,
      "loss": 0.1653,
      "step": 44030
    },
    {
      "epoch": 2.4669635604851132,
      "grad_norm": 2.1631734371185303,
      "learning_rate": 3.835276641067325e-05,
      "loss": 0.0931,
      "step": 44040
    },
    {
      "epoch": 2.4675237375010504,
      "grad_norm": 3.302461862564087,
      "learning_rate": 3.833875217220696e-05,
      "loss": 0.1658,
      "step": 44050
    },
    {
      "epoch": 2.4680839145169875,
      "grad_norm": 2.5497477054595947,
      "learning_rate": 3.8324737933740685e-05,
      "loss": 0.1317,
      "step": 44060
    },
    {
      "epoch": 2.4686440915329246,
      "grad_norm": 2.586946725845337,
      "learning_rate": 3.8310723695274406e-05,
      "loss": 0.1342,
      "step": 44070
    },
    {
      "epoch": 2.4692042685488613,
      "grad_norm": 2.206963300704956,
      "learning_rate": 3.8296709456808114e-05,
      "loss": 0.0883,
      "step": 44080
    },
    {
      "epoch": 2.4697644455647985,
      "grad_norm": 0.876697301864624,
      "learning_rate": 3.8282695218341836e-05,
      "loss": 0.2224,
      "step": 44090
    },
    {
      "epoch": 2.4703246225807356,
      "grad_norm": 1.1817353963851929,
      "learning_rate": 3.826868097987556e-05,
      "loss": 0.12,
      "step": 44100
    },
    {
      "epoch": 2.4708847995966727,
      "grad_norm": 3.8670613765716553,
      "learning_rate": 3.825466674140927e-05,
      "loss": 0.1383,
      "step": 44110
    },
    {
      "epoch": 2.4714449766126094,
      "grad_norm": 3.398247241973877,
      "learning_rate": 3.8240652502942994e-05,
      "loss": 0.1354,
      "step": 44120
    },
    {
      "epoch": 2.4720051536285466,
      "grad_norm": 2.740061044692993,
      "learning_rate": 3.822663826447671e-05,
      "loss": 0.1611,
      "step": 44130
    },
    {
      "epoch": 2.4725653306444837,
      "grad_norm": 1.8010846376419067,
      "learning_rate": 3.821262402601043e-05,
      "loss": 0.1826,
      "step": 44140
    },
    {
      "epoch": 2.473125507660421,
      "grad_norm": 0.8562396764755249,
      "learning_rate": 3.819860978754415e-05,
      "loss": 0.0949,
      "step": 44150
    },
    {
      "epoch": 2.473685684676358,
      "grad_norm": 3.558734655380249,
      "learning_rate": 3.8184595549077866e-05,
      "loss": 0.2013,
      "step": 44160
    },
    {
      "epoch": 2.4742458616922947,
      "grad_norm": 2.4812064170837402,
      "learning_rate": 3.817058131061158e-05,
      "loss": 0.2292,
      "step": 44170
    },
    {
      "epoch": 2.474806038708232,
      "grad_norm": 1.6789098978042603,
      "learning_rate": 3.81565670721453e-05,
      "loss": 0.0953,
      "step": 44180
    },
    {
      "epoch": 2.475366215724169,
      "grad_norm": 4.833126068115234,
      "learning_rate": 3.814255283367902e-05,
      "loss": 0.1288,
      "step": 44190
    },
    {
      "epoch": 2.475926392740106,
      "grad_norm": 2.8461341857910156,
      "learning_rate": 3.812853859521274e-05,
      "loss": 0.1003,
      "step": 44200
    },
    {
      "epoch": 2.4764865697560428,
      "grad_norm": 1.696580171585083,
      "learning_rate": 3.8114524356746453e-05,
      "loss": 0.1515,
      "step": 44210
    },
    {
      "epoch": 2.47704674677198,
      "grad_norm": 3.871264696121216,
      "learning_rate": 3.8100510118280175e-05,
      "loss": 0.212,
      "step": 44220
    },
    {
      "epoch": 2.477606923787917,
      "grad_norm": 1.9505575895309448,
      "learning_rate": 3.8086495879813896e-05,
      "loss": 0.1436,
      "step": 44230
    },
    {
      "epoch": 2.478167100803854,
      "grad_norm": 2.496406078338623,
      "learning_rate": 3.807248164134761e-05,
      "loss": 0.1274,
      "step": 44240
    },
    {
      "epoch": 2.4787272778197913,
      "grad_norm": 1.4041469097137451,
      "learning_rate": 3.805846740288133e-05,
      "loss": 0.1759,
      "step": 44250
    },
    {
      "epoch": 2.479287454835728,
      "grad_norm": 1.137082815170288,
      "learning_rate": 3.804445316441505e-05,
      "loss": 0.1364,
      "step": 44260
    },
    {
      "epoch": 2.479847631851665,
      "grad_norm": 1.0922446250915527,
      "learning_rate": 3.803043892594876e-05,
      "loss": 0.1101,
      "step": 44270
    },
    {
      "epoch": 2.4804078088676023,
      "grad_norm": 1.0665663480758667,
      "learning_rate": 3.8016424687482484e-05,
      "loss": 0.1024,
      "step": 44280
    },
    {
      "epoch": 2.480967985883539,
      "grad_norm": 4.753188133239746,
      "learning_rate": 3.80024104490162e-05,
      "loss": 0.1707,
      "step": 44290
    },
    {
      "epoch": 2.481528162899476,
      "grad_norm": 3.758371591567993,
      "learning_rate": 3.798839621054992e-05,
      "loss": 0.168,
      "step": 44300
    },
    {
      "epoch": 2.4820883399154132,
      "grad_norm": 2.984072685241699,
      "learning_rate": 3.797438197208364e-05,
      "loss": 0.1337,
      "step": 44310
    },
    {
      "epoch": 2.4826485169313504,
      "grad_norm": 2.780200719833374,
      "learning_rate": 3.7960367733617356e-05,
      "loss": 0.1182,
      "step": 44320
    },
    {
      "epoch": 2.4832086939472875,
      "grad_norm": 3.281470775604248,
      "learning_rate": 3.794635349515108e-05,
      "loss": 0.1352,
      "step": 44330
    },
    {
      "epoch": 2.4837688709632246,
      "grad_norm": 3.792926788330078,
      "learning_rate": 3.793233925668479e-05,
      "loss": 0.097,
      "step": 44340
    },
    {
      "epoch": 2.4843290479791613,
      "grad_norm": 1.9221230745315552,
      "learning_rate": 3.7918325018218514e-05,
      "loss": 0.1814,
      "step": 44350
    },
    {
      "epoch": 2.4848892249950985,
      "grad_norm": 1.1738606691360474,
      "learning_rate": 3.790431077975223e-05,
      "loss": 0.1191,
      "step": 44360
    },
    {
      "epoch": 2.4854494020110356,
      "grad_norm": 3.934211254119873,
      "learning_rate": 3.7890296541285944e-05,
      "loss": 0.14,
      "step": 44370
    },
    {
      "epoch": 2.4860095790269723,
      "grad_norm": 1.3655911684036255,
      "learning_rate": 3.7876282302819665e-05,
      "loss": 0.1496,
      "step": 44380
    },
    {
      "epoch": 2.4865697560429094,
      "grad_norm": 4.281754493713379,
      "learning_rate": 3.7862268064353387e-05,
      "loss": 0.1375,
      "step": 44390
    },
    {
      "epoch": 2.4871299330588466,
      "grad_norm": 3.6342599391937256,
      "learning_rate": 3.78482538258871e-05,
      "loss": 0.1378,
      "step": 44400
    },
    {
      "epoch": 2.4876901100747837,
      "grad_norm": 4.127793312072754,
      "learning_rate": 3.783423958742082e-05,
      "loss": 0.1237,
      "step": 44410
    },
    {
      "epoch": 2.488250287090721,
      "grad_norm": 2.3431153297424316,
      "learning_rate": 3.7820225348954544e-05,
      "loss": 0.106,
      "step": 44420
    },
    {
      "epoch": 2.4888104641066575,
      "grad_norm": 1.3534049987792969,
      "learning_rate": 3.780621111048826e-05,
      "loss": 0.1223,
      "step": 44430
    },
    {
      "epoch": 2.4893706411225947,
      "grad_norm": 4.6360578536987305,
      "learning_rate": 3.779219687202198e-05,
      "loss": 0.1929,
      "step": 44440
    },
    {
      "epoch": 2.489930818138532,
      "grad_norm": 1.7704213857650757,
      "learning_rate": 3.7778182633555695e-05,
      "loss": 0.1134,
      "step": 44450
    },
    {
      "epoch": 2.490490995154469,
      "grad_norm": 1.495784878730774,
      "learning_rate": 3.776416839508941e-05,
      "loss": 0.1379,
      "step": 44460
    },
    {
      "epoch": 2.4910511721704056,
      "grad_norm": 8.179576873779297,
      "learning_rate": 3.775015415662313e-05,
      "loss": 0.1811,
      "step": 44470
    },
    {
      "epoch": 2.4916113491863427,
      "grad_norm": 1.931348204612732,
      "learning_rate": 3.7736139918156846e-05,
      "loss": 0.1296,
      "step": 44480
    },
    {
      "epoch": 2.49217152620228,
      "grad_norm": 0.6212190389633179,
      "learning_rate": 3.772212567969057e-05,
      "loss": 0.1274,
      "step": 44490
    },
    {
      "epoch": 2.492731703218217,
      "grad_norm": 3.8018617630004883,
      "learning_rate": 3.770811144122429e-05,
      "loss": 0.1025,
      "step": 44500
    },
    {
      "epoch": 2.493291880234154,
      "grad_norm": 3.2952206134796143,
      "learning_rate": 3.7694097202758004e-05,
      "loss": 0.1698,
      "step": 44510
    },
    {
      "epoch": 2.493852057250091,
      "grad_norm": 1.822878122329712,
      "learning_rate": 3.7680082964291726e-05,
      "loss": 0.1353,
      "step": 44520
    },
    {
      "epoch": 2.494412234266028,
      "grad_norm": 3.4261393547058105,
      "learning_rate": 3.766606872582544e-05,
      "loss": 0.1182,
      "step": 44530
    },
    {
      "epoch": 2.494972411281965,
      "grad_norm": 3.1327428817749023,
      "learning_rate": 3.7652054487359155e-05,
      "loss": 0.1238,
      "step": 44540
    },
    {
      "epoch": 2.4955325882979023,
      "grad_norm": 4.5268964767456055,
      "learning_rate": 3.763804024889288e-05,
      "loss": 0.1116,
      "step": 44550
    },
    {
      "epoch": 2.496092765313839,
      "grad_norm": 4.64710807800293,
      "learning_rate": 3.762402601042659e-05,
      "loss": 0.2128,
      "step": 44560
    },
    {
      "epoch": 2.496652942329776,
      "grad_norm": 0.9365460276603699,
      "learning_rate": 3.761001177196031e-05,
      "loss": 0.1364,
      "step": 44570
    },
    {
      "epoch": 2.497213119345713,
      "grad_norm": 4.171837329864502,
      "learning_rate": 3.7595997533494034e-05,
      "loss": 0.1618,
      "step": 44580
    },
    {
      "epoch": 2.4977732963616504,
      "grad_norm": 3.4789910316467285,
      "learning_rate": 3.758198329502775e-05,
      "loss": 0.183,
      "step": 44590
    },
    {
      "epoch": 2.4983334733775875,
      "grad_norm": 4.666084289550781,
      "learning_rate": 3.756796905656147e-05,
      "loss": 0.1146,
      "step": 44600
    },
    {
      "epoch": 2.498893650393524,
      "grad_norm": 0.7882868647575378,
      "learning_rate": 3.7553954818095185e-05,
      "loss": 0.131,
      "step": 44610
    },
    {
      "epoch": 2.4994538274094613,
      "grad_norm": 2.904435396194458,
      "learning_rate": 3.753994057962891e-05,
      "loss": 0.1528,
      "step": 44620
    },
    {
      "epoch": 2.5000140044253985,
      "grad_norm": 2.396272659301758,
      "learning_rate": 3.752592634116262e-05,
      "loss": 0.0908,
      "step": 44630
    },
    {
      "epoch": 2.5005741814413356,
      "grad_norm": 0.7423126101493835,
      "learning_rate": 3.7511912102696336e-05,
      "loss": 0.1526,
      "step": 44640
    },
    {
      "epoch": 2.5011343584572723,
      "grad_norm": 5.482360363006592,
      "learning_rate": 3.749789786423006e-05,
      "loss": 0.1111,
      "step": 44650
    },
    {
      "epoch": 2.5016945354732094,
      "grad_norm": 1.3910205364227295,
      "learning_rate": 3.748388362576378e-05,
      "loss": 0.1229,
      "step": 44660
    },
    {
      "epoch": 2.5022547124891465,
      "grad_norm": 2.0121257305145264,
      "learning_rate": 3.7469869387297494e-05,
      "loss": 0.2391,
      "step": 44670
    },
    {
      "epoch": 2.5028148895050837,
      "grad_norm": 2.679079055786133,
      "learning_rate": 3.7455855148831216e-05,
      "loss": 0.1045,
      "step": 44680
    },
    {
      "epoch": 2.503375066521021,
      "grad_norm": 1.5794346332550049,
      "learning_rate": 3.744184091036493e-05,
      "loss": 0.1509,
      "step": 44690
    },
    {
      "epoch": 2.503935243536958,
      "grad_norm": 3.6958467960357666,
      "learning_rate": 3.742782667189865e-05,
      "loss": 0.1002,
      "step": 44700
    },
    {
      "epoch": 2.5044954205528946,
      "grad_norm": 7.183586120605469,
      "learning_rate": 3.7413812433432374e-05,
      "loss": 0.165,
      "step": 44710
    },
    {
      "epoch": 2.505055597568832,
      "grad_norm": 2.4085307121276855,
      "learning_rate": 3.739979819496608e-05,
      "loss": 0.085,
      "step": 44720
    },
    {
      "epoch": 2.505615774584769,
      "grad_norm": 4.512083530426025,
      "learning_rate": 3.73857839564998e-05,
      "loss": 0.1338,
      "step": 44730
    },
    {
      "epoch": 2.5061759516007056,
      "grad_norm": 2.253704309463501,
      "learning_rate": 3.7371769718033525e-05,
      "loss": 0.1668,
      "step": 44740
    },
    {
      "epoch": 2.5067361286166427,
      "grad_norm": 2.593015432357788,
      "learning_rate": 3.735775547956724e-05,
      "loss": 0.1269,
      "step": 44750
    },
    {
      "epoch": 2.50729630563258,
      "grad_norm": 2.0138323307037354,
      "learning_rate": 3.734374124110096e-05,
      "loss": 0.1586,
      "step": 44760
    },
    {
      "epoch": 2.507856482648517,
      "grad_norm": 3.2139697074890137,
      "learning_rate": 3.732972700263468e-05,
      "loss": 0.1457,
      "step": 44770
    },
    {
      "epoch": 2.508416659664454,
      "grad_norm": 1.5431263446807861,
      "learning_rate": 3.73157127641684e-05,
      "loss": 0.148,
      "step": 44780
    },
    {
      "epoch": 2.508976836680391,
      "grad_norm": 0.7827576398849487,
      "learning_rate": 3.730169852570212e-05,
      "loss": 0.0746,
      "step": 44790
    },
    {
      "epoch": 2.509537013696328,
      "grad_norm": 2.684438943862915,
      "learning_rate": 3.728768428723583e-05,
      "loss": 0.0956,
      "step": 44800
    },
    {
      "epoch": 2.510097190712265,
      "grad_norm": 0.7947391867637634,
      "learning_rate": 3.727367004876955e-05,
      "loss": 0.0979,
      "step": 44810
    },
    {
      "epoch": 2.5106573677282022,
      "grad_norm": 2.034419298171997,
      "learning_rate": 3.725965581030327e-05,
      "loss": 0.1817,
      "step": 44820
    },
    {
      "epoch": 2.511217544744139,
      "grad_norm": 4.30657958984375,
      "learning_rate": 3.7245641571836984e-05,
      "loss": 0.1207,
      "step": 44830
    },
    {
      "epoch": 2.511777721760076,
      "grad_norm": 4.726189136505127,
      "learning_rate": 3.7231627333370706e-05,
      "loss": 0.1493,
      "step": 44840
    },
    {
      "epoch": 2.512337898776013,
      "grad_norm": 0.7427204847335815,
      "learning_rate": 3.721761309490443e-05,
      "loss": 0.1291,
      "step": 44850
    },
    {
      "epoch": 2.5128980757919503,
      "grad_norm": 2.159318447113037,
      "learning_rate": 3.720359885643814e-05,
      "loss": 0.1106,
      "step": 44860
    },
    {
      "epoch": 2.5134582528078875,
      "grad_norm": 1.9092652797698975,
      "learning_rate": 3.7189584617971864e-05,
      "loss": 0.1027,
      "step": 44870
    },
    {
      "epoch": 2.514018429823824,
      "grad_norm": 5.135552406311035,
      "learning_rate": 3.717557037950558e-05,
      "loss": 0.1526,
      "step": 44880
    },
    {
      "epoch": 2.5145786068397613,
      "grad_norm": 4.676260471343994,
      "learning_rate": 3.71615561410393e-05,
      "loss": 0.1626,
      "step": 44890
    },
    {
      "epoch": 2.5151387838556984,
      "grad_norm": 2.14176869392395,
      "learning_rate": 3.714754190257302e-05,
      "loss": 0.1575,
      "step": 44900
    },
    {
      "epoch": 2.5156989608716356,
      "grad_norm": 2.480591058731079,
      "learning_rate": 3.713352766410673e-05,
      "loss": 0.0818,
      "step": 44910
    },
    {
      "epoch": 2.5162591378875723,
      "grad_norm": 2.8043670654296875,
      "learning_rate": 3.711951342564045e-05,
      "loss": 0.21,
      "step": 44920
    },
    {
      "epoch": 2.5168193149035094,
      "grad_norm": 3.814277172088623,
      "learning_rate": 3.710549918717417e-05,
      "loss": 0.1656,
      "step": 44930
    },
    {
      "epoch": 2.5173794919194465,
      "grad_norm": 1.709423542022705,
      "learning_rate": 3.709148494870789e-05,
      "loss": 0.1868,
      "step": 44940
    },
    {
      "epoch": 2.5179396689353837,
      "grad_norm": 2.3415067195892334,
      "learning_rate": 3.707747071024161e-05,
      "loss": 0.1393,
      "step": 44950
    },
    {
      "epoch": 2.518499845951321,
      "grad_norm": 3.387205123901367,
      "learning_rate": 3.7063456471775323e-05,
      "loss": 0.1619,
      "step": 44960
    },
    {
      "epoch": 2.5190600229672575,
      "grad_norm": 1.1767176389694214,
      "learning_rate": 3.7049442233309045e-05,
      "loss": 0.1602,
      "step": 44970
    },
    {
      "epoch": 2.5196201999831946,
      "grad_norm": 3.690704345703125,
      "learning_rate": 3.7035427994842766e-05,
      "loss": 0.1717,
      "step": 44980
    },
    {
      "epoch": 2.5201803769991318,
      "grad_norm": 2.537235975265503,
      "learning_rate": 3.702141375637648e-05,
      "loss": 0.175,
      "step": 44990
    },
    {
      "epoch": 2.520740554015069,
      "grad_norm": 5.397322654724121,
      "learning_rate": 3.7007399517910196e-05,
      "loss": 0.1713,
      "step": 45000
    },
    {
      "epoch": 2.5213007310310056,
      "grad_norm": 2.433654546737671,
      "learning_rate": 3.699338527944392e-05,
      "loss": 0.0995,
      "step": 45010
    },
    {
      "epoch": 2.5218609080469427,
      "grad_norm": 4.128536701202393,
      "learning_rate": 3.697937104097763e-05,
      "loss": 0.1215,
      "step": 45020
    },
    {
      "epoch": 2.52242108506288,
      "grad_norm": 2.6890275478363037,
      "learning_rate": 3.6965356802511354e-05,
      "loss": 0.1029,
      "step": 45030
    },
    {
      "epoch": 2.522981262078817,
      "grad_norm": 3.5307371616363525,
      "learning_rate": 3.695134256404507e-05,
      "loss": 0.216,
      "step": 45040
    },
    {
      "epoch": 2.523541439094754,
      "grad_norm": 4.3316144943237305,
      "learning_rate": 3.693732832557879e-05,
      "loss": 0.0958,
      "step": 45050
    },
    {
      "epoch": 2.524101616110691,
      "grad_norm": 3.316119909286499,
      "learning_rate": 3.692331408711251e-05,
      "loss": 0.1242,
      "step": 45060
    },
    {
      "epoch": 2.524661793126628,
      "grad_norm": 1.7070554494857788,
      "learning_rate": 3.6909299848646226e-05,
      "loss": 0.1092,
      "step": 45070
    },
    {
      "epoch": 2.525221970142565,
      "grad_norm": 2.970341205596924,
      "learning_rate": 3.689528561017995e-05,
      "loss": 0.0992,
      "step": 45080
    },
    {
      "epoch": 2.5257821471585022,
      "grad_norm": 3.7800323963165283,
      "learning_rate": 3.688127137171366e-05,
      "loss": 0.1694,
      "step": 45090
    },
    {
      "epoch": 2.526342324174439,
      "grad_norm": 1.4213355779647827,
      "learning_rate": 3.686725713324738e-05,
      "loss": 0.149,
      "step": 45100
    },
    {
      "epoch": 2.526902501190376,
      "grad_norm": 1.2744312286376953,
      "learning_rate": 3.68532428947811e-05,
      "loss": 0.0754,
      "step": 45110
    },
    {
      "epoch": 2.527462678206313,
      "grad_norm": 5.199069976806641,
      "learning_rate": 3.683922865631482e-05,
      "loss": 0.1708,
      "step": 45120
    },
    {
      "epoch": 2.5280228552222503,
      "grad_norm": 4.199945449829102,
      "learning_rate": 3.6825214417848535e-05,
      "loss": 0.143,
      "step": 45130
    },
    {
      "epoch": 2.5285830322381875,
      "grad_norm": 3.871371030807495,
      "learning_rate": 3.6811200179382257e-05,
      "loss": 0.1802,
      "step": 45140
    },
    {
      "epoch": 2.529143209254124,
      "grad_norm": 4.709897518157959,
      "learning_rate": 3.679718594091597e-05,
      "loss": 0.1762,
      "step": 45150
    },
    {
      "epoch": 2.5297033862700613,
      "grad_norm": 1.7257241010665894,
      "learning_rate": 3.678317170244969e-05,
      "loss": 0.0831,
      "step": 45160
    },
    {
      "epoch": 2.5302635632859984,
      "grad_norm": 2.7555878162384033,
      "learning_rate": 3.6769157463983414e-05,
      "loss": 0.1204,
      "step": 45170
    },
    {
      "epoch": 2.5308237403019356,
      "grad_norm": 2.1629722118377686,
      "learning_rate": 3.675514322551712e-05,
      "loss": 0.1661,
      "step": 45180
    },
    {
      "epoch": 2.5313839173178723,
      "grad_norm": 3.5017178058624268,
      "learning_rate": 3.6741128987050844e-05,
      "loss": 0.1389,
      "step": 45190
    },
    {
      "epoch": 2.5319440943338094,
      "grad_norm": 1.4922808408737183,
      "learning_rate": 3.6727114748584565e-05,
      "loss": 0.1031,
      "step": 45200
    },
    {
      "epoch": 2.5325042713497465,
      "grad_norm": 1.475371241569519,
      "learning_rate": 3.671310051011828e-05,
      "loss": 0.1301,
      "step": 45210
    },
    {
      "epoch": 2.5330644483656837,
      "grad_norm": 0.6145961284637451,
      "learning_rate": 3.6699086271652e-05,
      "loss": 0.0876,
      "step": 45220
    },
    {
      "epoch": 2.533624625381621,
      "grad_norm": 1.060032606124878,
      "learning_rate": 3.6685072033185716e-05,
      "loss": 0.1091,
      "step": 45230
    },
    {
      "epoch": 2.5341848023975575,
      "grad_norm": 4.671512126922607,
      "learning_rate": 3.667105779471944e-05,
      "loss": 0.1121,
      "step": 45240
    },
    {
      "epoch": 2.5347449794134946,
      "grad_norm": 1.8789665699005127,
      "learning_rate": 3.665704355625316e-05,
      "loss": 0.0985,
      "step": 45250
    },
    {
      "epoch": 2.5353051564294318,
      "grad_norm": 3.8252816200256348,
      "learning_rate": 3.6643029317786874e-05,
      "loss": 0.1437,
      "step": 45260
    },
    {
      "epoch": 2.5358653334453685,
      "grad_norm": 2.617281198501587,
      "learning_rate": 3.662901507932059e-05,
      "loss": 0.0877,
      "step": 45270
    },
    {
      "epoch": 2.5364255104613056,
      "grad_norm": 1.7479017972946167,
      "learning_rate": 3.661500084085431e-05,
      "loss": 0.2008,
      "step": 45280
    },
    {
      "epoch": 2.5369856874772427,
      "grad_norm": 5.716577053070068,
      "learning_rate": 3.6600986602388025e-05,
      "loss": 0.1334,
      "step": 45290
    },
    {
      "epoch": 2.53754586449318,
      "grad_norm": 3.8374571800231934,
      "learning_rate": 3.658697236392175e-05,
      "loss": 0.1157,
      "step": 45300
    },
    {
      "epoch": 2.538106041509117,
      "grad_norm": 5.678882598876953,
      "learning_rate": 3.657295812545546e-05,
      "loss": 0.1053,
      "step": 45310
    },
    {
      "epoch": 2.538666218525054,
      "grad_norm": 2.7852096557617188,
      "learning_rate": 3.655894388698918e-05,
      "loss": 0.0974,
      "step": 45320
    },
    {
      "epoch": 2.539226395540991,
      "grad_norm": 2.461603879928589,
      "learning_rate": 3.6544929648522904e-05,
      "loss": 0.1747,
      "step": 45330
    },
    {
      "epoch": 2.539786572556928,
      "grad_norm": 3.5038914680480957,
      "learning_rate": 3.653091541005662e-05,
      "loss": 0.1547,
      "step": 45340
    },
    {
      "epoch": 2.540346749572865,
      "grad_norm": 2.5736377239227295,
      "learning_rate": 3.651690117159034e-05,
      "loss": 0.1564,
      "step": 45350
    },
    {
      "epoch": 2.540906926588802,
      "grad_norm": 5.315398693084717,
      "learning_rate": 3.6502886933124055e-05,
      "loss": 0.1568,
      "step": 45360
    },
    {
      "epoch": 2.541467103604739,
      "grad_norm": 5.657803058624268,
      "learning_rate": 3.648887269465777e-05,
      "loss": 0.1703,
      "step": 45370
    },
    {
      "epoch": 2.542027280620676,
      "grad_norm": 3.4885141849517822,
      "learning_rate": 3.647485845619149e-05,
      "loss": 0.1758,
      "step": 45380
    },
    {
      "epoch": 2.542587457636613,
      "grad_norm": 0.7665538787841797,
      "learning_rate": 3.6460844217725206e-05,
      "loss": 0.1529,
      "step": 45390
    },
    {
      "epoch": 2.5431476346525503,
      "grad_norm": 4.792475700378418,
      "learning_rate": 3.644682997925893e-05,
      "loss": 0.1447,
      "step": 45400
    },
    {
      "epoch": 2.5437078116684875,
      "grad_norm": 1.4740314483642578,
      "learning_rate": 3.643281574079265e-05,
      "loss": 0.0991,
      "step": 45410
    },
    {
      "epoch": 2.544267988684424,
      "grad_norm": 1.20604407787323,
      "learning_rate": 3.6418801502326364e-05,
      "loss": 0.1184,
      "step": 45420
    },
    {
      "epoch": 2.5448281657003613,
      "grad_norm": 1.2643024921417236,
      "learning_rate": 3.6404787263860086e-05,
      "loss": 0.1546,
      "step": 45430
    },
    {
      "epoch": 2.5453883427162984,
      "grad_norm": 1.1746102571487427,
      "learning_rate": 3.639077302539381e-05,
      "loss": 0.128,
      "step": 45440
    },
    {
      "epoch": 2.545948519732235,
      "grad_norm": 1.7965672016143799,
      "learning_rate": 3.637675878692752e-05,
      "loss": 0.2122,
      "step": 45450
    },
    {
      "epoch": 2.5465086967481723,
      "grad_norm": 4.133667469024658,
      "learning_rate": 3.636274454846124e-05,
      "loss": 0.1689,
      "step": 45460
    },
    {
      "epoch": 2.5470688737641094,
      "grad_norm": 1.0247751474380493,
      "learning_rate": 3.634873030999496e-05,
      "loss": 0.1818,
      "step": 45470
    },
    {
      "epoch": 2.5476290507800465,
      "grad_norm": 2.8744094371795654,
      "learning_rate": 3.633471607152867e-05,
      "loss": 0.1901,
      "step": 45480
    },
    {
      "epoch": 2.5481892277959837,
      "grad_norm": 4.213192462921143,
      "learning_rate": 3.6320701833062395e-05,
      "loss": 0.1302,
      "step": 45490
    },
    {
      "epoch": 2.548749404811921,
      "grad_norm": 3.5884597301483154,
      "learning_rate": 3.630668759459611e-05,
      "loss": 0.1176,
      "step": 45500
    },
    {
      "epoch": 2.5493095818278575,
      "grad_norm": 1.7652413845062256,
      "learning_rate": 3.629267335612983e-05,
      "loss": 0.1004,
      "step": 45510
    },
    {
      "epoch": 2.5498697588437946,
      "grad_norm": 1.201770305633545,
      "learning_rate": 3.627865911766355e-05,
      "loss": 0.1306,
      "step": 45520
    },
    {
      "epoch": 2.5504299358597318,
      "grad_norm": 0.7721417546272278,
      "learning_rate": 3.626464487919727e-05,
      "loss": 0.1017,
      "step": 45530
    },
    {
      "epoch": 2.5509901128756685,
      "grad_norm": 1.4568980932235718,
      "learning_rate": 3.625063064073099e-05,
      "loss": 0.1249,
      "step": 45540
    },
    {
      "epoch": 2.5515502898916056,
      "grad_norm": 3.217935800552368,
      "learning_rate": 3.62366164022647e-05,
      "loss": 0.1552,
      "step": 45550
    },
    {
      "epoch": 2.5521104669075427,
      "grad_norm": 4.753571033477783,
      "learning_rate": 3.622260216379842e-05,
      "loss": 0.1372,
      "step": 45560
    },
    {
      "epoch": 2.55267064392348,
      "grad_norm": 3.378497362136841,
      "learning_rate": 3.620858792533214e-05,
      "loss": 0.0885,
      "step": 45570
    },
    {
      "epoch": 2.553230820939417,
      "grad_norm": 4.6737260818481445,
      "learning_rate": 3.6194573686865854e-05,
      "loss": 0.1087,
      "step": 45580
    },
    {
      "epoch": 2.553790997955354,
      "grad_norm": 3.51369047164917,
      "learning_rate": 3.6180559448399576e-05,
      "loss": 0.1027,
      "step": 45590
    },
    {
      "epoch": 2.554351174971291,
      "grad_norm": 1.994228720664978,
      "learning_rate": 3.61665452099333e-05,
      "loss": 0.1194,
      "step": 45600
    },
    {
      "epoch": 2.554911351987228,
      "grad_norm": 1.9605525732040405,
      "learning_rate": 3.615253097146701e-05,
      "loss": 0.127,
      "step": 45610
    },
    {
      "epoch": 2.555471529003165,
      "grad_norm": 1.729617714881897,
      "learning_rate": 3.6138516733000734e-05,
      "loss": 0.1034,
      "step": 45620
    },
    {
      "epoch": 2.556031706019102,
      "grad_norm": 3.3589909076690674,
      "learning_rate": 3.612450249453445e-05,
      "loss": 0.135,
      "step": 45630
    },
    {
      "epoch": 2.556591883035039,
      "grad_norm": 1.3049821853637695,
      "learning_rate": 3.611048825606816e-05,
      "loss": 0.1492,
      "step": 45640
    },
    {
      "epoch": 2.557152060050976,
      "grad_norm": 1.8350493907928467,
      "learning_rate": 3.6096474017601885e-05,
      "loss": 0.133,
      "step": 45650
    },
    {
      "epoch": 2.557712237066913,
      "grad_norm": 1.7512456178665161,
      "learning_rate": 3.60824597791356e-05,
      "loss": 0.2556,
      "step": 45660
    },
    {
      "epoch": 2.5582724140828503,
      "grad_norm": 1.97072434425354,
      "learning_rate": 3.606844554066932e-05,
      "loss": 0.1277,
      "step": 45670
    },
    {
      "epoch": 2.5588325910987875,
      "grad_norm": 2.649399757385254,
      "learning_rate": 3.605443130220304e-05,
      "loss": 0.1371,
      "step": 45680
    },
    {
      "epoch": 2.559392768114724,
      "grad_norm": 3.072340965270996,
      "learning_rate": 3.604041706373676e-05,
      "loss": 0.1345,
      "step": 45690
    },
    {
      "epoch": 2.5599529451306613,
      "grad_norm": 1.2082161903381348,
      "learning_rate": 3.602640282527048e-05,
      "loss": 0.154,
      "step": 45700
    },
    {
      "epoch": 2.5605131221465984,
      "grad_norm": 2.75596284866333,
      "learning_rate": 3.6012388586804193e-05,
      "loss": 0.0864,
      "step": 45710
    },
    {
      "epoch": 2.561073299162535,
      "grad_norm": 3.6193292140960693,
      "learning_rate": 3.5998374348337915e-05,
      "loss": 0.1631,
      "step": 45720
    },
    {
      "epoch": 2.5616334761784723,
      "grad_norm": 2.3422255516052246,
      "learning_rate": 3.598436010987163e-05,
      "loss": 0.157,
      "step": 45730
    },
    {
      "epoch": 2.5621936531944094,
      "grad_norm": 4.486083030700684,
      "learning_rate": 3.5970345871405344e-05,
      "loss": 0.1648,
      "step": 45740
    },
    {
      "epoch": 2.5627538302103465,
      "grad_norm": 1.5209027528762817,
      "learning_rate": 3.5956331632939066e-05,
      "loss": 0.117,
      "step": 45750
    },
    {
      "epoch": 2.5633140072262837,
      "grad_norm": 2.973404884338379,
      "learning_rate": 3.594231739447279e-05,
      "loss": 0.1107,
      "step": 45760
    },
    {
      "epoch": 2.563874184242221,
      "grad_norm": 1.3788000345230103,
      "learning_rate": 3.59283031560065e-05,
      "loss": 0.1913,
      "step": 45770
    },
    {
      "epoch": 2.5644343612581575,
      "grad_norm": 2.371053457260132,
      "learning_rate": 3.5914288917540224e-05,
      "loss": 0.1166,
      "step": 45780
    },
    {
      "epoch": 2.5649945382740946,
      "grad_norm": 1.766150712966919,
      "learning_rate": 3.5900274679073945e-05,
      "loss": 0.1317,
      "step": 45790
    },
    {
      "epoch": 2.5655547152900318,
      "grad_norm": 4.301491737365723,
      "learning_rate": 3.588626044060766e-05,
      "loss": 0.2212,
      "step": 45800
    },
    {
      "epoch": 2.5661148923059685,
      "grad_norm": 1.2616688013076782,
      "learning_rate": 3.587224620214138e-05,
      "loss": 0.2169,
      "step": 45810
    },
    {
      "epoch": 2.5666750693219056,
      "grad_norm": 2.421616554260254,
      "learning_rate": 3.5858231963675096e-05,
      "loss": 0.1378,
      "step": 45820
    },
    {
      "epoch": 2.5672352463378427,
      "grad_norm": 4.01230525970459,
      "learning_rate": 3.584421772520881e-05,
      "loss": 0.1581,
      "step": 45830
    },
    {
      "epoch": 2.56779542335378,
      "grad_norm": 2.0088319778442383,
      "learning_rate": 3.583020348674253e-05,
      "loss": 0.1312,
      "step": 45840
    },
    {
      "epoch": 2.568355600369717,
      "grad_norm": 2.7810163497924805,
      "learning_rate": 3.581618924827625e-05,
      "loss": 0.1349,
      "step": 45850
    },
    {
      "epoch": 2.568915777385654,
      "grad_norm": 4.837591171264648,
      "learning_rate": 3.580217500980997e-05,
      "loss": 0.2046,
      "step": 45860
    },
    {
      "epoch": 2.569475954401591,
      "grad_norm": 4.2366437911987305,
      "learning_rate": 3.578816077134369e-05,
      "loss": 0.0915,
      "step": 45870
    },
    {
      "epoch": 2.570036131417528,
      "grad_norm": 5.991317272186279,
      "learning_rate": 3.5774146532877405e-05,
      "loss": 0.2294,
      "step": 45880
    },
    {
      "epoch": 2.570596308433465,
      "grad_norm": 1.8597451448440552,
      "learning_rate": 3.5760132294411127e-05,
      "loss": 0.1004,
      "step": 45890
    },
    {
      "epoch": 2.571156485449402,
      "grad_norm": 1.1718761920928955,
      "learning_rate": 3.574611805594484e-05,
      "loss": 0.1817,
      "step": 45900
    },
    {
      "epoch": 2.571716662465339,
      "grad_norm": 1.7530817985534668,
      "learning_rate": 3.5732103817478556e-05,
      "loss": 0.1162,
      "step": 45910
    },
    {
      "epoch": 2.572276839481276,
      "grad_norm": 3.2528443336486816,
      "learning_rate": 3.571808957901228e-05,
      "loss": 0.1382,
      "step": 45920
    },
    {
      "epoch": 2.572837016497213,
      "grad_norm": 1.4844481945037842,
      "learning_rate": 3.570407534054599e-05,
      "loss": 0.1326,
      "step": 45930
    },
    {
      "epoch": 2.5733971935131503,
      "grad_norm": 1.3864686489105225,
      "learning_rate": 3.5690061102079714e-05,
      "loss": 0.1062,
      "step": 45940
    },
    {
      "epoch": 2.5739573705290875,
      "grad_norm": 1.015647530555725,
      "learning_rate": 3.5676046863613435e-05,
      "loss": 0.151,
      "step": 45950
    },
    {
      "epoch": 2.574517547545024,
      "grad_norm": 5.332280158996582,
      "learning_rate": 3.566203262514715e-05,
      "loss": 0.1532,
      "step": 45960
    },
    {
      "epoch": 2.5750777245609613,
      "grad_norm": 2.6904540061950684,
      "learning_rate": 3.564801838668087e-05,
      "loss": 0.0974,
      "step": 45970
    },
    {
      "epoch": 2.5756379015768984,
      "grad_norm": 1.9172029495239258,
      "learning_rate": 3.5634004148214586e-05,
      "loss": 0.1494,
      "step": 45980
    },
    {
      "epoch": 2.576198078592835,
      "grad_norm": 2.1871957778930664,
      "learning_rate": 3.561998990974831e-05,
      "loss": 0.1553,
      "step": 45990
    },
    {
      "epoch": 2.5767582556087723,
      "grad_norm": 2.7606005668640137,
      "learning_rate": 3.560597567128203e-05,
      "loss": 0.2016,
      "step": 46000
    },
    {
      "epoch": 2.5773184326247094,
      "grad_norm": 4.987969398498535,
      "learning_rate": 3.559196143281574e-05,
      "loss": 0.161,
      "step": 46010
    },
    {
      "epoch": 2.5778786096406465,
      "grad_norm": 3.04286789894104,
      "learning_rate": 3.557794719434946e-05,
      "loss": 0.1587,
      "step": 46020
    },
    {
      "epoch": 2.5784387866565837,
      "grad_norm": 2.390798330307007,
      "learning_rate": 3.556393295588318e-05,
      "loss": 0.1297,
      "step": 46030
    },
    {
      "epoch": 2.5789989636725204,
      "grad_norm": 3.835247278213501,
      "learning_rate": 3.5549918717416895e-05,
      "loss": 0.1322,
      "step": 46040
    },
    {
      "epoch": 2.5795591406884575,
      "grad_norm": 3.9720449447631836,
      "learning_rate": 3.553590447895062e-05,
      "loss": 0.1518,
      "step": 46050
    },
    {
      "epoch": 2.5801193177043946,
      "grad_norm": 2.162660837173462,
      "learning_rate": 3.552189024048433e-05,
      "loss": 0.1049,
      "step": 46060
    },
    {
      "epoch": 2.5806794947203318,
      "grad_norm": 1.4526209831237793,
      "learning_rate": 3.550787600201805e-05,
      "loss": 0.125,
      "step": 46070
    },
    {
      "epoch": 2.5812396717362684,
      "grad_norm": 3.130048990249634,
      "learning_rate": 3.5493861763551774e-05,
      "loss": 0.1333,
      "step": 46080
    },
    {
      "epoch": 2.5817998487522056,
      "grad_norm": 3.5117013454437256,
      "learning_rate": 3.547984752508549e-05,
      "loss": 0.1456,
      "step": 46090
    },
    {
      "epoch": 2.5823600257681427,
      "grad_norm": 5.134433746337891,
      "learning_rate": 3.5465833286619204e-05,
      "loss": 0.1596,
      "step": 46100
    },
    {
      "epoch": 2.58292020278408,
      "grad_norm": 2.7436509132385254,
      "learning_rate": 3.5451819048152925e-05,
      "loss": 0.1826,
      "step": 46110
    },
    {
      "epoch": 2.583480379800017,
      "grad_norm": 1.0114235877990723,
      "learning_rate": 3.543780480968664e-05,
      "loss": 0.0853,
      "step": 46120
    },
    {
      "epoch": 2.5840405568159537,
      "grad_norm": 4.9498186111450195,
      "learning_rate": 3.542379057122036e-05,
      "loss": 0.1624,
      "step": 46130
    },
    {
      "epoch": 2.584600733831891,
      "grad_norm": 6.4076714515686035,
      "learning_rate": 3.540977633275408e-05,
      "loss": 0.1193,
      "step": 46140
    },
    {
      "epoch": 2.585160910847828,
      "grad_norm": 2.7177248001098633,
      "learning_rate": 3.53957620942878e-05,
      "loss": 0.1553,
      "step": 46150
    },
    {
      "epoch": 2.585721087863765,
      "grad_norm": 6.597067832946777,
      "learning_rate": 3.538174785582152e-05,
      "loss": 0.1407,
      "step": 46160
    },
    {
      "epoch": 2.586281264879702,
      "grad_norm": 3.2362208366394043,
      "learning_rate": 3.5367733617355234e-05,
      "loss": 0.0912,
      "step": 46170
    },
    {
      "epoch": 2.586841441895639,
      "grad_norm": 0.7819557189941406,
      "learning_rate": 3.5353719378888956e-05,
      "loss": 0.1017,
      "step": 46180
    },
    {
      "epoch": 2.587401618911576,
      "grad_norm": 0.9310893416404724,
      "learning_rate": 3.533970514042267e-05,
      "loss": 0.1161,
      "step": 46190
    },
    {
      "epoch": 2.587961795927513,
      "grad_norm": 2.833484172821045,
      "learning_rate": 3.5325690901956385e-05,
      "loss": 0.1306,
      "step": 46200
    },
    {
      "epoch": 2.5885219729434503,
      "grad_norm": 4.572658538818359,
      "learning_rate": 3.531167666349011e-05,
      "loss": 0.092,
      "step": 46210
    },
    {
      "epoch": 2.589082149959387,
      "grad_norm": 1.4605803489685059,
      "learning_rate": 3.529766242502383e-05,
      "loss": 0.1201,
      "step": 46220
    },
    {
      "epoch": 2.589642326975324,
      "grad_norm": 1.4373306035995483,
      "learning_rate": 3.528364818655754e-05,
      "loss": 0.1477,
      "step": 46230
    },
    {
      "epoch": 2.5902025039912613,
      "grad_norm": 2.122253656387329,
      "learning_rate": 3.5269633948091265e-05,
      "loss": 0.1426,
      "step": 46240
    },
    {
      "epoch": 2.5907626810071984,
      "grad_norm": 6.537317752838135,
      "learning_rate": 3.525561970962498e-05,
      "loss": 0.1451,
      "step": 46250
    },
    {
      "epoch": 2.591322858023135,
      "grad_norm": 2.558015823364258,
      "learning_rate": 3.52416054711587e-05,
      "loss": 0.1049,
      "step": 46260
    },
    {
      "epoch": 2.5918830350390722,
      "grad_norm": 2.2731106281280518,
      "learning_rate": 3.522759123269242e-05,
      "loss": 0.1613,
      "step": 46270
    },
    {
      "epoch": 2.5924432120550094,
      "grad_norm": 1.9742907285690308,
      "learning_rate": 3.521357699422613e-05,
      "loss": 0.0967,
      "step": 46280
    },
    {
      "epoch": 2.5930033890709465,
      "grad_norm": 6.643243789672852,
      "learning_rate": 3.519956275575985e-05,
      "loss": 0.1432,
      "step": 46290
    },
    {
      "epoch": 2.5935635660868837,
      "grad_norm": 5.962028980255127,
      "learning_rate": 3.518554851729357e-05,
      "loss": 0.1733,
      "step": 46300
    },
    {
      "epoch": 2.5941237431028203,
      "grad_norm": 0.8975551724433899,
      "learning_rate": 3.517153427882729e-05,
      "loss": 0.145,
      "step": 46310
    },
    {
      "epoch": 2.5946839201187575,
      "grad_norm": 1.3159071207046509,
      "learning_rate": 3.515752004036101e-05,
      "loss": 0.2009,
      "step": 46320
    },
    {
      "epoch": 2.5952440971346946,
      "grad_norm": 2.594169855117798,
      "learning_rate": 3.5143505801894724e-05,
      "loss": 0.1167,
      "step": 46330
    },
    {
      "epoch": 2.5958042741506318,
      "grad_norm": 2.335536003112793,
      "learning_rate": 3.5129491563428446e-05,
      "loss": 0.116,
      "step": 46340
    },
    {
      "epoch": 2.5963644511665684,
      "grad_norm": 2.3897440433502197,
      "learning_rate": 3.511547732496217e-05,
      "loss": 0.1532,
      "step": 46350
    },
    {
      "epoch": 2.5969246281825056,
      "grad_norm": 3.4005651473999023,
      "learning_rate": 3.510146308649588e-05,
      "loss": 0.1431,
      "step": 46360
    },
    {
      "epoch": 2.5974848051984427,
      "grad_norm": 2.309554100036621,
      "learning_rate": 3.50874488480296e-05,
      "loss": 0.1287,
      "step": 46370
    },
    {
      "epoch": 2.59804498221438,
      "grad_norm": 1.7422070503234863,
      "learning_rate": 3.507343460956332e-05,
      "loss": 0.1249,
      "step": 46380
    },
    {
      "epoch": 2.598605159230317,
      "grad_norm": 1.6788811683654785,
      "learning_rate": 3.505942037109703e-05,
      "loss": 0.192,
      "step": 46390
    },
    {
      "epoch": 2.5991653362462537,
      "grad_norm": 2.4755444526672363,
      "learning_rate": 3.5045406132630755e-05,
      "loss": 0.1888,
      "step": 46400
    },
    {
      "epoch": 2.599725513262191,
      "grad_norm": 3.9484710693359375,
      "learning_rate": 3.503139189416447e-05,
      "loss": 0.143,
      "step": 46410
    },
    {
      "epoch": 2.600285690278128,
      "grad_norm": 2.2661550045013428,
      "learning_rate": 3.501737765569819e-05,
      "loss": 0.1623,
      "step": 46420
    },
    {
      "epoch": 2.600845867294065,
      "grad_norm": 3.676358222961426,
      "learning_rate": 3.500336341723191e-05,
      "loss": 0.096,
      "step": 46430
    },
    {
      "epoch": 2.6014060443100018,
      "grad_norm": 2.633054494857788,
      "learning_rate": 3.498934917876563e-05,
      "loss": 0.1133,
      "step": 46440
    },
    {
      "epoch": 2.601966221325939,
      "grad_norm": 0.9083089828491211,
      "learning_rate": 3.497533494029935e-05,
      "loss": 0.1628,
      "step": 46450
    },
    {
      "epoch": 2.602526398341876,
      "grad_norm": 1.7928180694580078,
      "learning_rate": 3.4961320701833063e-05,
      "loss": 0.1097,
      "step": 46460
    },
    {
      "epoch": 2.603086575357813,
      "grad_norm": 7.005558013916016,
      "learning_rate": 3.494730646336678e-05,
      "loss": 0.145,
      "step": 46470
    },
    {
      "epoch": 2.6036467523737503,
      "grad_norm": 2.934781074523926,
      "learning_rate": 3.49332922249005e-05,
      "loss": 0.174,
      "step": 46480
    },
    {
      "epoch": 2.604206929389687,
      "grad_norm": 1.5172783136367798,
      "learning_rate": 3.491927798643422e-05,
      "loss": 0.1822,
      "step": 46490
    },
    {
      "epoch": 2.604767106405624,
      "grad_norm": 2.0332231521606445,
      "learning_rate": 3.4905263747967936e-05,
      "loss": 0.1662,
      "step": 46500
    },
    {
      "epoch": 2.6053272834215613,
      "grad_norm": 4.405036449432373,
      "learning_rate": 3.489124950950166e-05,
      "loss": 0.0957,
      "step": 46510
    },
    {
      "epoch": 2.6058874604374984,
      "grad_norm": 2.508338212966919,
      "learning_rate": 3.487723527103537e-05,
      "loss": 0.1123,
      "step": 46520
    },
    {
      "epoch": 2.606447637453435,
      "grad_norm": 4.4731245040893555,
      "learning_rate": 3.4863221032569094e-05,
      "loss": 0.1329,
      "step": 46530
    },
    {
      "epoch": 2.6070078144693722,
      "grad_norm": 1.8680723905563354,
      "learning_rate": 3.4849206794102815e-05,
      "loss": 0.1095,
      "step": 46540
    },
    {
      "epoch": 2.6075679914853094,
      "grad_norm": 2.317955493927002,
      "learning_rate": 3.483519255563653e-05,
      "loss": 0.1201,
      "step": 46550
    },
    {
      "epoch": 2.6081281685012465,
      "grad_norm": 2.5662841796875,
      "learning_rate": 3.4821178317170245e-05,
      "loss": 0.1609,
      "step": 46560
    },
    {
      "epoch": 2.6086883455171836,
      "grad_norm": 2.935229539871216,
      "learning_rate": 3.4807164078703966e-05,
      "loss": 0.177,
      "step": 46570
    },
    {
      "epoch": 2.6092485225331203,
      "grad_norm": 1.783661127090454,
      "learning_rate": 3.479314984023768e-05,
      "loss": 0.1161,
      "step": 46580
    },
    {
      "epoch": 2.6098086995490575,
      "grad_norm": 2.136829376220703,
      "learning_rate": 3.47791356017714e-05,
      "loss": 0.08,
      "step": 46590
    },
    {
      "epoch": 2.6103688765649946,
      "grad_norm": 2.524766683578491,
      "learning_rate": 3.476512136330512e-05,
      "loss": 0.1729,
      "step": 46600
    },
    {
      "epoch": 2.6109290535809313,
      "grad_norm": 5.2982892990112305,
      "learning_rate": 3.475110712483884e-05,
      "loss": 0.1661,
      "step": 46610
    },
    {
      "epoch": 2.6114892305968684,
      "grad_norm": 4.700950622558594,
      "learning_rate": 3.473709288637256e-05,
      "loss": 0.1235,
      "step": 46620
    },
    {
      "epoch": 2.6120494076128056,
      "grad_norm": 1.105272889137268,
      "learning_rate": 3.4723078647906275e-05,
      "loss": 0.0789,
      "step": 46630
    },
    {
      "epoch": 2.6126095846287427,
      "grad_norm": 2.0104851722717285,
      "learning_rate": 3.4709064409439997e-05,
      "loss": 0.1282,
      "step": 46640
    },
    {
      "epoch": 2.61316976164468,
      "grad_norm": 3.7502858638763428,
      "learning_rate": 3.469505017097371e-05,
      "loss": 0.124,
      "step": 46650
    },
    {
      "epoch": 2.613729938660617,
      "grad_norm": 4.70515251159668,
      "learning_rate": 3.4681035932507426e-05,
      "loss": 0.1641,
      "step": 46660
    },
    {
      "epoch": 2.6142901156765537,
      "grad_norm": 1.4365355968475342,
      "learning_rate": 3.466702169404115e-05,
      "loss": 0.0919,
      "step": 46670
    },
    {
      "epoch": 2.614850292692491,
      "grad_norm": 2.372018814086914,
      "learning_rate": 3.465300745557486e-05,
      "loss": 0.1885,
      "step": 46680
    },
    {
      "epoch": 2.615410469708428,
      "grad_norm": 3.9437215328216553,
      "learning_rate": 3.4638993217108584e-05,
      "loss": 0.1141,
      "step": 46690
    },
    {
      "epoch": 2.6159706467243646,
      "grad_norm": 4.5544843673706055,
      "learning_rate": 3.4624978978642305e-05,
      "loss": 0.219,
      "step": 46700
    },
    {
      "epoch": 2.6165308237403018,
      "grad_norm": 2.7942521572113037,
      "learning_rate": 3.461096474017602e-05,
      "loss": 0.1689,
      "step": 46710
    },
    {
      "epoch": 2.617091000756239,
      "grad_norm": 2.789889335632324,
      "learning_rate": 3.459695050170974e-05,
      "loss": 0.1578,
      "step": 46720
    },
    {
      "epoch": 2.617651177772176,
      "grad_norm": 4.373779773712158,
      "learning_rate": 3.4582936263243456e-05,
      "loss": 0.1351,
      "step": 46730
    },
    {
      "epoch": 2.618211354788113,
      "grad_norm": 2.802133321762085,
      "learning_rate": 3.456892202477717e-05,
      "loss": 0.1396,
      "step": 46740
    },
    {
      "epoch": 2.6187715318040503,
      "grad_norm": 2.8942618370056152,
      "learning_rate": 3.455490778631089e-05,
      "loss": 0.1295,
      "step": 46750
    },
    {
      "epoch": 2.619331708819987,
      "grad_norm": 1.488459587097168,
      "learning_rate": 3.454089354784461e-05,
      "loss": 0.1097,
      "step": 46760
    },
    {
      "epoch": 2.619891885835924,
      "grad_norm": 2.4277617931365967,
      "learning_rate": 3.452687930937833e-05,
      "loss": 0.1348,
      "step": 46770
    },
    {
      "epoch": 2.6204520628518613,
      "grad_norm": 2.3804125785827637,
      "learning_rate": 3.451286507091205e-05,
      "loss": 0.1613,
      "step": 46780
    },
    {
      "epoch": 2.621012239867798,
      "grad_norm": 1.1462405920028687,
      "learning_rate": 3.4498850832445765e-05,
      "loss": 0.1409,
      "step": 46790
    },
    {
      "epoch": 2.621572416883735,
      "grad_norm": 1.181850552558899,
      "learning_rate": 3.448483659397949e-05,
      "loss": 0.1747,
      "step": 46800
    },
    {
      "epoch": 2.6221325938996722,
      "grad_norm": 1.6833713054656982,
      "learning_rate": 3.447082235551321e-05,
      "loss": 0.1096,
      "step": 46810
    },
    {
      "epoch": 2.6226927709156094,
      "grad_norm": 3.1290597915649414,
      "learning_rate": 3.445680811704692e-05,
      "loss": 0.0965,
      "step": 46820
    },
    {
      "epoch": 2.6232529479315465,
      "grad_norm": 1.2572141885757446,
      "learning_rate": 3.444279387858064e-05,
      "loss": 0.2193,
      "step": 46830
    },
    {
      "epoch": 2.6238131249474836,
      "grad_norm": 6.62049674987793,
      "learning_rate": 3.442877964011436e-05,
      "loss": 0.1238,
      "step": 46840
    },
    {
      "epoch": 2.6243733019634203,
      "grad_norm": 1.6827856302261353,
      "learning_rate": 3.4414765401648074e-05,
      "loss": 0.0709,
      "step": 46850
    },
    {
      "epoch": 2.6249334789793575,
      "grad_norm": 5.2471022605896,
      "learning_rate": 3.4400751163181795e-05,
      "loss": 0.16,
      "step": 46860
    },
    {
      "epoch": 2.6254936559952946,
      "grad_norm": 1.006977915763855,
      "learning_rate": 3.438673692471551e-05,
      "loss": 0.153,
      "step": 46870
    },
    {
      "epoch": 2.6260538330112313,
      "grad_norm": 2.0762269496917725,
      "learning_rate": 3.437272268624923e-05,
      "loss": 0.1146,
      "step": 46880
    },
    {
      "epoch": 2.6266140100271684,
      "grad_norm": 2.617305278778076,
      "learning_rate": 3.435870844778295e-05,
      "loss": 0.1254,
      "step": 46890
    },
    {
      "epoch": 2.6271741870431056,
      "grad_norm": 3.571841239929199,
      "learning_rate": 3.434469420931667e-05,
      "loss": 0.1405,
      "step": 46900
    },
    {
      "epoch": 2.6277343640590427,
      "grad_norm": 1.9278714656829834,
      "learning_rate": 3.433067997085039e-05,
      "loss": 0.118,
      "step": 46910
    },
    {
      "epoch": 2.62829454107498,
      "grad_norm": 1.5327059030532837,
      "learning_rate": 3.4316665732384104e-05,
      "loss": 0.1294,
      "step": 46920
    },
    {
      "epoch": 2.628854718090917,
      "grad_norm": 5.465883731842041,
      "learning_rate": 3.430265149391782e-05,
      "loss": 0.1283,
      "step": 46930
    },
    {
      "epoch": 2.6294148951068537,
      "grad_norm": 3.4046599864959717,
      "learning_rate": 3.428863725545154e-05,
      "loss": 0.1349,
      "step": 46940
    },
    {
      "epoch": 2.629975072122791,
      "grad_norm": 4.294295310974121,
      "learning_rate": 3.4274623016985255e-05,
      "loss": 0.1044,
      "step": 46950
    },
    {
      "epoch": 2.630535249138728,
      "grad_norm": 1.7433058023452759,
      "learning_rate": 3.426060877851898e-05,
      "loss": 0.1645,
      "step": 46960
    },
    {
      "epoch": 2.6310954261546646,
      "grad_norm": 1.8171027898788452,
      "learning_rate": 3.42465945400527e-05,
      "loss": 0.1079,
      "step": 46970
    },
    {
      "epoch": 2.6316556031706018,
      "grad_norm": 5.768804550170898,
      "learning_rate": 3.423258030158641e-05,
      "loss": 0.1146,
      "step": 46980
    },
    {
      "epoch": 2.632215780186539,
      "grad_norm": 4.835704803466797,
      "learning_rate": 3.4218566063120135e-05,
      "loss": 0.1483,
      "step": 46990
    },
    {
      "epoch": 2.632775957202476,
      "grad_norm": 2.308277130126953,
      "learning_rate": 3.420455182465385e-05,
      "loss": 0.1011,
      "step": 47000
    },
    {
      "epoch": 2.633336134218413,
      "grad_norm": 6.809445381164551,
      "learning_rate": 3.419053758618757e-05,
      "loss": 0.1343,
      "step": 47010
    },
    {
      "epoch": 2.6338963112343503,
      "grad_norm": 2.408846616744995,
      "learning_rate": 3.4176523347721286e-05,
      "loss": 0.146,
      "step": 47020
    },
    {
      "epoch": 2.634456488250287,
      "grad_norm": 4.213315010070801,
      "learning_rate": 3.4162509109255e-05,
      "loss": 0.1166,
      "step": 47030
    },
    {
      "epoch": 2.635016665266224,
      "grad_norm": 2.257690668106079,
      "learning_rate": 3.414849487078872e-05,
      "loss": 0.1157,
      "step": 47040
    },
    {
      "epoch": 2.6355768422821613,
      "grad_norm": 1.5606510639190674,
      "learning_rate": 3.413448063232244e-05,
      "loss": 0.1594,
      "step": 47050
    },
    {
      "epoch": 2.636137019298098,
      "grad_norm": 1.5594093799591064,
      "learning_rate": 3.412046639385616e-05,
      "loss": 0.1419,
      "step": 47060
    },
    {
      "epoch": 2.636697196314035,
      "grad_norm": 3.0680274963378906,
      "learning_rate": 3.410645215538988e-05,
      "loss": 0.167,
      "step": 47070
    },
    {
      "epoch": 2.6372573733299722,
      "grad_norm": 2.3279411792755127,
      "learning_rate": 3.4092437916923594e-05,
      "loss": 0.153,
      "step": 47080
    },
    {
      "epoch": 2.6378175503459094,
      "grad_norm": 4.74694299697876,
      "learning_rate": 3.4078423678457316e-05,
      "loss": 0.1984,
      "step": 47090
    },
    {
      "epoch": 2.6383777273618465,
      "grad_norm": 1.1188143491744995,
      "learning_rate": 3.406440943999104e-05,
      "loss": 0.2331,
      "step": 47100
    },
    {
      "epoch": 2.6389379043777836,
      "grad_norm": 4.276373863220215,
      "learning_rate": 3.4050395201524745e-05,
      "loss": 0.092,
      "step": 47110
    },
    {
      "epoch": 2.6394980813937203,
      "grad_norm": 2.6337108612060547,
      "learning_rate": 3.403638096305847e-05,
      "loss": 0.1459,
      "step": 47120
    },
    {
      "epoch": 2.6400582584096575,
      "grad_norm": 4.021043300628662,
      "learning_rate": 3.402236672459219e-05,
      "loss": 0.1023,
      "step": 47130
    },
    {
      "epoch": 2.6406184354255946,
      "grad_norm": 5.833593845367432,
      "learning_rate": 3.40083524861259e-05,
      "loss": 0.1359,
      "step": 47140
    },
    {
      "epoch": 2.6411786124415313,
      "grad_norm": 5.395263671875,
      "learning_rate": 3.3994338247659625e-05,
      "loss": 0.0998,
      "step": 47150
    },
    {
      "epoch": 2.6417387894574684,
      "grad_norm": 1.5516127347946167,
      "learning_rate": 3.3980324009193346e-05,
      "loss": 0.0955,
      "step": 47160
    },
    {
      "epoch": 2.6422989664734056,
      "grad_norm": 3.743269920349121,
      "learning_rate": 3.396630977072706e-05,
      "loss": 0.1166,
      "step": 47170
    },
    {
      "epoch": 2.6428591434893427,
      "grad_norm": 3.6473402976989746,
      "learning_rate": 3.395229553226078e-05,
      "loss": 0.1241,
      "step": 47180
    },
    {
      "epoch": 2.64341932050528,
      "grad_norm": 2.2229843139648438,
      "learning_rate": 3.39382812937945e-05,
      "loss": 0.1009,
      "step": 47190
    },
    {
      "epoch": 2.643979497521217,
      "grad_norm": 3.698180675506592,
      "learning_rate": 3.392426705532821e-05,
      "loss": 0.1772,
      "step": 47200
    },
    {
      "epoch": 2.6445396745371537,
      "grad_norm": 1.7131619453430176,
      "learning_rate": 3.3910252816861933e-05,
      "loss": 0.1265,
      "step": 47210
    },
    {
      "epoch": 2.645099851553091,
      "grad_norm": 1.5225071907043457,
      "learning_rate": 3.389623857839565e-05,
      "loss": 0.0869,
      "step": 47220
    },
    {
      "epoch": 2.645660028569028,
      "grad_norm": 0.799136757850647,
      "learning_rate": 3.388222433992937e-05,
      "loss": 0.1006,
      "step": 47230
    },
    {
      "epoch": 2.6462202055849646,
      "grad_norm": 5.985776424407959,
      "learning_rate": 3.386821010146309e-05,
      "loss": 0.123,
      "step": 47240
    },
    {
      "epoch": 2.6467803826009018,
      "grad_norm": 1.9268661737442017,
      "learning_rate": 3.3854195862996806e-05,
      "loss": 0.1536,
      "step": 47250
    },
    {
      "epoch": 2.647340559616839,
      "grad_norm": 10.132150650024414,
      "learning_rate": 3.384018162453053e-05,
      "loss": 0.1972,
      "step": 47260
    },
    {
      "epoch": 2.647900736632776,
      "grad_norm": 1.466062307357788,
      "learning_rate": 3.382616738606424e-05,
      "loss": 0.1195,
      "step": 47270
    },
    {
      "epoch": 2.648460913648713,
      "grad_norm": 2.2993128299713135,
      "learning_rate": 3.3812153147597964e-05,
      "loss": 0.1275,
      "step": 47280
    },
    {
      "epoch": 2.6490210906646503,
      "grad_norm": 1.90554940700531,
      "learning_rate": 3.379813890913168e-05,
      "loss": 0.1054,
      "step": 47290
    },
    {
      "epoch": 2.649581267680587,
      "grad_norm": 4.227004051208496,
      "learning_rate": 3.378412467066539e-05,
      "loss": 0.1645,
      "step": 47300
    },
    {
      "epoch": 2.650141444696524,
      "grad_norm": 1.250900149345398,
      "learning_rate": 3.3770110432199115e-05,
      "loss": 0.1059,
      "step": 47310
    },
    {
      "epoch": 2.6507016217124613,
      "grad_norm": 4.339997291564941,
      "learning_rate": 3.3756096193732836e-05,
      "loss": 0.1925,
      "step": 47320
    },
    {
      "epoch": 2.651261798728398,
      "grad_norm": 3.226925849914551,
      "learning_rate": 3.374208195526655e-05,
      "loss": 0.109,
      "step": 47330
    },
    {
      "epoch": 2.651821975744335,
      "grad_norm": 3.4384243488311768,
      "learning_rate": 3.372806771680027e-05,
      "loss": 0.141,
      "step": 47340
    },
    {
      "epoch": 2.6523821527602722,
      "grad_norm": 0.6516193747520447,
      "learning_rate": 3.371405347833399e-05,
      "loss": 0.0923,
      "step": 47350
    },
    {
      "epoch": 2.6529423297762094,
      "grad_norm": 0.5408161282539368,
      "learning_rate": 3.370003923986771e-05,
      "loss": 0.1219,
      "step": 47360
    },
    {
      "epoch": 2.6535025067921465,
      "grad_norm": 3.5379700660705566,
      "learning_rate": 3.368602500140143e-05,
      "loss": 0.177,
      "step": 47370
    },
    {
      "epoch": 2.654062683808083,
      "grad_norm": 2.4805169105529785,
      "learning_rate": 3.367201076293514e-05,
      "loss": 0.0971,
      "step": 47380
    },
    {
      "epoch": 2.6546228608240203,
      "grad_norm": 1.256556510925293,
      "learning_rate": 3.365799652446886e-05,
      "loss": 0.1399,
      "step": 47390
    },
    {
      "epoch": 2.6551830378399575,
      "grad_norm": 0.9353145956993103,
      "learning_rate": 3.364398228600258e-05,
      "loss": 0.1753,
      "step": 47400
    },
    {
      "epoch": 2.6557432148558946,
      "grad_norm": 3.7507245540618896,
      "learning_rate": 3.3629968047536296e-05,
      "loss": 0.2089,
      "step": 47410
    },
    {
      "epoch": 2.6563033918718313,
      "grad_norm": 1.4752960205078125,
      "learning_rate": 3.361595380907002e-05,
      "loss": 0.1037,
      "step": 47420
    },
    {
      "epoch": 2.6568635688877684,
      "grad_norm": 0.7850422859191895,
      "learning_rate": 3.360193957060373e-05,
      "loss": 0.1314,
      "step": 47430
    },
    {
      "epoch": 2.6574237459037056,
      "grad_norm": 4.720492839813232,
      "learning_rate": 3.3587925332137454e-05,
      "loss": 0.1006,
      "step": 47440
    },
    {
      "epoch": 2.6579839229196427,
      "grad_norm": 0.8145375847816467,
      "learning_rate": 3.3573911093671175e-05,
      "loss": 0.1333,
      "step": 47450
    },
    {
      "epoch": 2.65854409993558,
      "grad_norm": 0.7445312142372131,
      "learning_rate": 3.355989685520489e-05,
      "loss": 0.126,
      "step": 47460
    },
    {
      "epoch": 2.6591042769515165,
      "grad_norm": 2.72113037109375,
      "learning_rate": 3.3545882616738605e-05,
      "loss": 0.0953,
      "step": 47470
    },
    {
      "epoch": 2.6596644539674537,
      "grad_norm": 2.575371026992798,
      "learning_rate": 3.3531868378272326e-05,
      "loss": 0.1146,
      "step": 47480
    },
    {
      "epoch": 2.660224630983391,
      "grad_norm": 3.870177745819092,
      "learning_rate": 3.351785413980604e-05,
      "loss": 0.1807,
      "step": 47490
    },
    {
      "epoch": 2.660784807999328,
      "grad_norm": 3.9210331439971924,
      "learning_rate": 3.350383990133976e-05,
      "loss": 0.1674,
      "step": 47500
    },
    {
      "epoch": 2.6613449850152646,
      "grad_norm": 2.5833280086517334,
      "learning_rate": 3.3489825662873484e-05,
      "loss": 0.1066,
      "step": 47510
    },
    {
      "epoch": 2.6619051620312018,
      "grad_norm": 3.1941730976104736,
      "learning_rate": 3.34758114244072e-05,
      "loss": 0.131,
      "step": 47520
    },
    {
      "epoch": 2.662465339047139,
      "grad_norm": 1.2585182189941406,
      "learning_rate": 3.346179718594092e-05,
      "loss": 0.1072,
      "step": 47530
    },
    {
      "epoch": 2.663025516063076,
      "grad_norm": 1.5044499635696411,
      "learning_rate": 3.3447782947474635e-05,
      "loss": 0.1685,
      "step": 47540
    },
    {
      "epoch": 2.663585693079013,
      "grad_norm": 0.8730422854423523,
      "learning_rate": 3.343376870900836e-05,
      "loss": 0.1059,
      "step": 47550
    },
    {
      "epoch": 2.66414587009495,
      "grad_norm": 0.7851812839508057,
      "learning_rate": 3.341975447054208e-05,
      "loss": 0.1839,
      "step": 47560
    },
    {
      "epoch": 2.664706047110887,
      "grad_norm": 1.351895809173584,
      "learning_rate": 3.3405740232075786e-05,
      "loss": 0.1716,
      "step": 47570
    },
    {
      "epoch": 2.665266224126824,
      "grad_norm": 3.650789499282837,
      "learning_rate": 3.339172599360951e-05,
      "loss": 0.1325,
      "step": 47580
    },
    {
      "epoch": 2.6658264011427613,
      "grad_norm": 1.341874122619629,
      "learning_rate": 3.337771175514323e-05,
      "loss": 0.1409,
      "step": 47590
    },
    {
      "epoch": 2.666386578158698,
      "grad_norm": 1.8286595344543457,
      "learning_rate": 3.3363697516676944e-05,
      "loss": 0.1398,
      "step": 47600
    },
    {
      "epoch": 2.666946755174635,
      "grad_norm": 1.0401626825332642,
      "learning_rate": 3.3349683278210665e-05,
      "loss": 0.1492,
      "step": 47610
    },
    {
      "epoch": 2.667506932190572,
      "grad_norm": 4.7743048667907715,
      "learning_rate": 3.333566903974438e-05,
      "loss": 0.2326,
      "step": 47620
    },
    {
      "epoch": 2.6680671092065094,
      "grad_norm": 1.4832338094711304,
      "learning_rate": 3.33216548012781e-05,
      "loss": 0.1266,
      "step": 47630
    },
    {
      "epoch": 2.6686272862224465,
      "grad_norm": 3.528118848800659,
      "learning_rate": 3.330764056281182e-05,
      "loss": 0.1165,
      "step": 47640
    },
    {
      "epoch": 2.669187463238383,
      "grad_norm": 4.528268337249756,
      "learning_rate": 3.329362632434554e-05,
      "loss": 0.1255,
      "step": 47650
    },
    {
      "epoch": 2.6697476402543203,
      "grad_norm": 1.9278203248977661,
      "learning_rate": 3.327961208587925e-05,
      "loss": 0.0837,
      "step": 47660
    },
    {
      "epoch": 2.6703078172702575,
      "grad_norm": 6.20679235458374,
      "learning_rate": 3.3265597847412974e-05,
      "loss": 0.241,
      "step": 47670
    },
    {
      "epoch": 2.6708679942861946,
      "grad_norm": 4.5252275466918945,
      "learning_rate": 3.325158360894669e-05,
      "loss": 0.1977,
      "step": 47680
    },
    {
      "epoch": 2.6714281713021313,
      "grad_norm": 1.2784541845321655,
      "learning_rate": 3.323756937048041e-05,
      "loss": 0.1347,
      "step": 47690
    },
    {
      "epoch": 2.6719883483180684,
      "grad_norm": 0.6700648665428162,
      "learning_rate": 3.3223555132014125e-05,
      "loss": 0.1689,
      "step": 47700
    },
    {
      "epoch": 2.6725485253340056,
      "grad_norm": 3.0004918575286865,
      "learning_rate": 3.320954089354785e-05,
      "loss": 0.1524,
      "step": 47710
    },
    {
      "epoch": 2.6731087023499427,
      "grad_norm": 4.212092399597168,
      "learning_rate": 3.319552665508157e-05,
      "loss": 0.1645,
      "step": 47720
    },
    {
      "epoch": 2.67366887936588,
      "grad_norm": 0.8222927451133728,
      "learning_rate": 3.318151241661528e-05,
      "loss": 0.1123,
      "step": 47730
    },
    {
      "epoch": 2.6742290563818165,
      "grad_norm": 0.822484016418457,
      "learning_rate": 3.3167498178149005e-05,
      "loss": 0.0933,
      "step": 47740
    },
    {
      "epoch": 2.6747892333977537,
      "grad_norm": 3.359346389770508,
      "learning_rate": 3.315348393968272e-05,
      "loss": 0.1447,
      "step": 47750
    },
    {
      "epoch": 2.675349410413691,
      "grad_norm": 2.224033832550049,
      "learning_rate": 3.3139469701216434e-05,
      "loss": 0.1479,
      "step": 47760
    },
    {
      "epoch": 2.675909587429628,
      "grad_norm": 1.8283545970916748,
      "learning_rate": 3.3125455462750156e-05,
      "loss": 0.12,
      "step": 47770
    },
    {
      "epoch": 2.6764697644455646,
      "grad_norm": 2.138331174850464,
      "learning_rate": 3.311144122428387e-05,
      "loss": 0.1059,
      "step": 47780
    },
    {
      "epoch": 2.6770299414615017,
      "grad_norm": 1.2110904455184937,
      "learning_rate": 3.309742698581759e-05,
      "loss": 0.0869,
      "step": 47790
    },
    {
      "epoch": 2.677590118477439,
      "grad_norm": 2.8221616744995117,
      "learning_rate": 3.308341274735131e-05,
      "loss": 0.0797,
      "step": 47800
    },
    {
      "epoch": 2.678150295493376,
      "grad_norm": 2.1291074752807617,
      "learning_rate": 3.306939850888503e-05,
      "loss": 0.1122,
      "step": 47810
    },
    {
      "epoch": 2.678710472509313,
      "grad_norm": 6.4377570152282715,
      "learning_rate": 3.305538427041875e-05,
      "loss": 0.1808,
      "step": 47820
    },
    {
      "epoch": 2.67927064952525,
      "grad_norm": 2.3862462043762207,
      "learning_rate": 3.3041370031952464e-05,
      "loss": 0.1157,
      "step": 47830
    },
    {
      "epoch": 2.679830826541187,
      "grad_norm": 2.481874465942383,
      "learning_rate": 3.302735579348618e-05,
      "loss": 0.1274,
      "step": 47840
    },
    {
      "epoch": 2.680391003557124,
      "grad_norm": 2.867034912109375,
      "learning_rate": 3.30133415550199e-05,
      "loss": 0.1119,
      "step": 47850
    },
    {
      "epoch": 2.680951180573061,
      "grad_norm": 4.093911647796631,
      "learning_rate": 3.299932731655362e-05,
      "loss": 0.1337,
      "step": 47860
    },
    {
      "epoch": 2.681511357588998,
      "grad_norm": 3.2807388305664062,
      "learning_rate": 3.298531307808734e-05,
      "loss": 0.1436,
      "step": 47870
    },
    {
      "epoch": 2.682071534604935,
      "grad_norm": 2.553823232650757,
      "learning_rate": 3.297129883962106e-05,
      "loss": 0.0934,
      "step": 47880
    },
    {
      "epoch": 2.682631711620872,
      "grad_norm": 4.320738315582275,
      "learning_rate": 3.295728460115477e-05,
      "loss": 0.1024,
      "step": 47890
    },
    {
      "epoch": 2.6831918886368094,
      "grad_norm": 1.5758260488510132,
      "learning_rate": 3.2943270362688495e-05,
      "loss": 0.1069,
      "step": 47900
    },
    {
      "epoch": 2.6837520656527465,
      "grad_norm": 1.2292201519012451,
      "learning_rate": 3.2929256124222216e-05,
      "loss": 0.1405,
      "step": 47910
    },
    {
      "epoch": 2.684312242668683,
      "grad_norm": 1.7594999074935913,
      "learning_rate": 3.291524188575593e-05,
      "loss": 0.1137,
      "step": 47920
    },
    {
      "epoch": 2.6848724196846203,
      "grad_norm": 1.949757695198059,
      "learning_rate": 3.2901227647289646e-05,
      "loss": 0.1576,
      "step": 47930
    },
    {
      "epoch": 2.6854325967005575,
      "grad_norm": 5.371619701385498,
      "learning_rate": 3.288721340882337e-05,
      "loss": 0.1213,
      "step": 47940
    },
    {
      "epoch": 2.685992773716494,
      "grad_norm": 4.131110191345215,
      "learning_rate": 3.287319917035708e-05,
      "loss": 0.1176,
      "step": 47950
    },
    {
      "epoch": 2.6865529507324313,
      "grad_norm": 1.3932198286056519,
      "learning_rate": 3.2859184931890803e-05,
      "loss": 0.156,
      "step": 47960
    },
    {
      "epoch": 2.6871131277483684,
      "grad_norm": 1.3231022357940674,
      "learning_rate": 3.284517069342452e-05,
      "loss": 0.1444,
      "step": 47970
    },
    {
      "epoch": 2.6876733047643055,
      "grad_norm": 1.7744685411453247,
      "learning_rate": 3.283115645495824e-05,
      "loss": 0.1323,
      "step": 47980
    },
    {
      "epoch": 2.6882334817802427,
      "grad_norm": 1.5741618871688843,
      "learning_rate": 3.281714221649196e-05,
      "loss": 0.1254,
      "step": 47990
    },
    {
      "epoch": 2.68879365879618,
      "grad_norm": 4.924562931060791,
      "learning_rate": 3.2803127978025676e-05,
      "loss": 0.0942,
      "step": 48000
    },
    {
      "epoch": 2.6893538358121165,
      "grad_norm": 3.035217523574829,
      "learning_rate": 3.27891137395594e-05,
      "loss": 0.1317,
      "step": 48010
    },
    {
      "epoch": 2.6899140128280536,
      "grad_norm": 1.5391663312911987,
      "learning_rate": 3.277509950109311e-05,
      "loss": 0.1091,
      "step": 48020
    },
    {
      "epoch": 2.690474189843991,
      "grad_norm": 0.8459135293960571,
      "learning_rate": 3.276108526262683e-05,
      "loss": 0.1082,
      "step": 48030
    },
    {
      "epoch": 2.6910343668599275,
      "grad_norm": 2.9086315631866455,
      "learning_rate": 3.274707102416055e-05,
      "loss": 0.0805,
      "step": 48040
    },
    {
      "epoch": 2.6915945438758646,
      "grad_norm": 1.3049606084823608,
      "learning_rate": 3.273305678569426e-05,
      "loss": 0.1376,
      "step": 48050
    },
    {
      "epoch": 2.6921547208918017,
      "grad_norm": 3.8005435466766357,
      "learning_rate": 3.2719042547227985e-05,
      "loss": 0.1646,
      "step": 48060
    },
    {
      "epoch": 2.692714897907739,
      "grad_norm": 1.1618578433990479,
      "learning_rate": 3.2705028308761706e-05,
      "loss": 0.145,
      "step": 48070
    },
    {
      "epoch": 2.693275074923676,
      "grad_norm": 0.754623293876648,
      "learning_rate": 3.269101407029542e-05,
      "loss": 0.1055,
      "step": 48080
    },
    {
      "epoch": 2.693835251939613,
      "grad_norm": 3.431734561920166,
      "learning_rate": 3.267699983182914e-05,
      "loss": 0.1474,
      "step": 48090
    },
    {
      "epoch": 2.69439542895555,
      "grad_norm": 2.995086193084717,
      "learning_rate": 3.266298559336286e-05,
      "loss": 0.1474,
      "step": 48100
    },
    {
      "epoch": 2.694955605971487,
      "grad_norm": 2.5219602584838867,
      "learning_rate": 3.264897135489658e-05,
      "loss": 0.1764,
      "step": 48110
    },
    {
      "epoch": 2.695515782987424,
      "grad_norm": 1.617118000984192,
      "learning_rate": 3.2634957116430294e-05,
      "loss": 0.2462,
      "step": 48120
    },
    {
      "epoch": 2.696075960003361,
      "grad_norm": 2.373363971710205,
      "learning_rate": 3.262094287796401e-05,
      "loss": 0.0853,
      "step": 48130
    },
    {
      "epoch": 2.696636137019298,
      "grad_norm": 2.480005979537964,
      "learning_rate": 3.260692863949773e-05,
      "loss": 0.1486,
      "step": 48140
    },
    {
      "epoch": 2.697196314035235,
      "grad_norm": 0.9134802222251892,
      "learning_rate": 3.259291440103145e-05,
      "loss": 0.1043,
      "step": 48150
    },
    {
      "epoch": 2.697756491051172,
      "grad_norm": 4.197010517120361,
      "learning_rate": 3.2578900162565166e-05,
      "loss": 0.1293,
      "step": 48160
    },
    {
      "epoch": 2.6983166680671093,
      "grad_norm": 3.904468297958374,
      "learning_rate": 3.256488592409889e-05,
      "loss": 0.1484,
      "step": 48170
    },
    {
      "epoch": 2.6988768450830465,
      "grad_norm": 1.0607116222381592,
      "learning_rate": 3.25508716856326e-05,
      "loss": 0.2036,
      "step": 48180
    },
    {
      "epoch": 2.699437022098983,
      "grad_norm": 1.24898362159729,
      "learning_rate": 3.2536857447166324e-05,
      "loss": 0.0962,
      "step": 48190
    },
    {
      "epoch": 2.6999971991149203,
      "grad_norm": 1.2182998657226562,
      "learning_rate": 3.2522843208700045e-05,
      "loss": 0.1553,
      "step": 48200
    },
    {
      "epoch": 2.7005573761308574,
      "grad_norm": 6.764806270599365,
      "learning_rate": 3.250882897023376e-05,
      "loss": 0.2309,
      "step": 48210
    },
    {
      "epoch": 2.701117553146794,
      "grad_norm": 3.370239496231079,
      "learning_rate": 3.2494814731767475e-05,
      "loss": 0.1224,
      "step": 48220
    },
    {
      "epoch": 2.7016777301627313,
      "grad_norm": 2.043865442276001,
      "learning_rate": 3.2480800493301196e-05,
      "loss": 0.0974,
      "step": 48230
    },
    {
      "epoch": 2.7022379071786684,
      "grad_norm": 4.909175872802734,
      "learning_rate": 3.246678625483491e-05,
      "loss": 0.1495,
      "step": 48240
    },
    {
      "epoch": 2.7027980841946055,
      "grad_norm": 3.269087314605713,
      "learning_rate": 3.245277201636863e-05,
      "loss": 0.138,
      "step": 48250
    },
    {
      "epoch": 2.7033582612105427,
      "grad_norm": 2.745760917663574,
      "learning_rate": 3.2438757777902354e-05,
      "loss": 0.2008,
      "step": 48260
    },
    {
      "epoch": 2.70391843822648,
      "grad_norm": 1.14822518825531,
      "learning_rate": 3.242474353943607e-05,
      "loss": 0.0781,
      "step": 48270
    },
    {
      "epoch": 2.7044786152424165,
      "grad_norm": 3.5502378940582275,
      "learning_rate": 3.241072930096979e-05,
      "loss": 0.1378,
      "step": 48280
    },
    {
      "epoch": 2.7050387922583536,
      "grad_norm": 7.590257167816162,
      "learning_rate": 3.2396715062503505e-05,
      "loss": 0.1685,
      "step": 48290
    },
    {
      "epoch": 2.7055989692742908,
      "grad_norm": 6.778963565826416,
      "learning_rate": 3.238270082403722e-05,
      "loss": 0.1103,
      "step": 48300
    },
    {
      "epoch": 2.7061591462902275,
      "grad_norm": 4.637806415557861,
      "learning_rate": 3.236868658557094e-05,
      "loss": 0.121,
      "step": 48310
    },
    {
      "epoch": 2.7067193233061646,
      "grad_norm": 4.8759846687316895,
      "learning_rate": 3.2354672347104656e-05,
      "loss": 0.1809,
      "step": 48320
    },
    {
      "epoch": 2.7072795003221017,
      "grad_norm": 0.7023828625679016,
      "learning_rate": 3.234065810863838e-05,
      "loss": 0.1384,
      "step": 48330
    },
    {
      "epoch": 2.707839677338039,
      "grad_norm": 1.1811656951904297,
      "learning_rate": 3.23266438701721e-05,
      "loss": 0.1322,
      "step": 48340
    },
    {
      "epoch": 2.708399854353976,
      "grad_norm": 1.7181719541549683,
      "learning_rate": 3.2312629631705814e-05,
      "loss": 0.1447,
      "step": 48350
    },
    {
      "epoch": 2.708960031369913,
      "grad_norm": 2.4546010494232178,
      "learning_rate": 3.2298615393239535e-05,
      "loss": 0.0929,
      "step": 48360
    },
    {
      "epoch": 2.70952020838585,
      "grad_norm": 6.286445140838623,
      "learning_rate": 3.228460115477325e-05,
      "loss": 0.1101,
      "step": 48370
    },
    {
      "epoch": 2.710080385401787,
      "grad_norm": 1.305082082748413,
      "learning_rate": 3.227058691630697e-05,
      "loss": 0.2031,
      "step": 48380
    },
    {
      "epoch": 2.710640562417724,
      "grad_norm": 2.4654393196105957,
      "learning_rate": 3.2256572677840686e-05,
      "loss": 0.1023,
      "step": 48390
    },
    {
      "epoch": 2.711200739433661,
      "grad_norm": 1.7431570291519165,
      "learning_rate": 3.22425584393744e-05,
      "loss": 0.1533,
      "step": 48400
    },
    {
      "epoch": 2.711760916449598,
      "grad_norm": 5.459946632385254,
      "learning_rate": 3.222854420090812e-05,
      "loss": 0.1789,
      "step": 48410
    },
    {
      "epoch": 2.712321093465535,
      "grad_norm": 1.1302106380462646,
      "learning_rate": 3.2214529962441844e-05,
      "loss": 0.13,
      "step": 48420
    },
    {
      "epoch": 2.712881270481472,
      "grad_norm": 5.622837543487549,
      "learning_rate": 3.220051572397556e-05,
      "loss": 0.2091,
      "step": 48430
    },
    {
      "epoch": 2.7134414474974093,
      "grad_norm": 0.7880290746688843,
      "learning_rate": 3.218650148550928e-05,
      "loss": 0.1609,
      "step": 48440
    },
    {
      "epoch": 2.7140016245133465,
      "grad_norm": 3.793243169784546,
      "learning_rate": 3.2172487247042995e-05,
      "loss": 0.237,
      "step": 48450
    },
    {
      "epoch": 2.714561801529283,
      "grad_norm": 4.216675281524658,
      "learning_rate": 3.215847300857672e-05,
      "loss": 0.148,
      "step": 48460
    },
    {
      "epoch": 2.7151219785452203,
      "grad_norm": 5.6741461753845215,
      "learning_rate": 3.214445877011044e-05,
      "loss": 0.1486,
      "step": 48470
    },
    {
      "epoch": 2.7156821555611574,
      "grad_norm": 1.5922040939331055,
      "learning_rate": 3.2130444531644146e-05,
      "loss": 0.2325,
      "step": 48480
    },
    {
      "epoch": 2.716242332577094,
      "grad_norm": 3.946601152420044,
      "learning_rate": 3.211643029317787e-05,
      "loss": 0.1834,
      "step": 48490
    },
    {
      "epoch": 2.7168025095930313,
      "grad_norm": 3.9080123901367188,
      "learning_rate": 3.210241605471159e-05,
      "loss": 0.178,
      "step": 48500
    },
    {
      "epoch": 2.7173626866089684,
      "grad_norm": 1.3942718505859375,
      "learning_rate": 3.2088401816245304e-05,
      "loss": 0.1459,
      "step": 48510
    },
    {
      "epoch": 2.7179228636249055,
      "grad_norm": 1.7801650762557983,
      "learning_rate": 3.2074387577779026e-05,
      "loss": 0.1077,
      "step": 48520
    },
    {
      "epoch": 2.7184830406408427,
      "grad_norm": 2.4520387649536133,
      "learning_rate": 3.206037333931274e-05,
      "loss": 0.1023,
      "step": 48530
    },
    {
      "epoch": 2.71904321765678,
      "grad_norm": 0.6801546216011047,
      "learning_rate": 3.204635910084646e-05,
      "loss": 0.1545,
      "step": 48540
    },
    {
      "epoch": 2.7196033946727165,
      "grad_norm": 1.8791176080703735,
      "learning_rate": 3.203234486238018e-05,
      "loss": 0.1448,
      "step": 48550
    },
    {
      "epoch": 2.7201635716886536,
      "grad_norm": 3.7733707427978516,
      "learning_rate": 3.20183306239139e-05,
      "loss": 0.126,
      "step": 48560
    },
    {
      "epoch": 2.7207237487045908,
      "grad_norm": 3.9345555305480957,
      "learning_rate": 3.200431638544761e-05,
      "loss": 0.1685,
      "step": 48570
    },
    {
      "epoch": 2.7212839257205275,
      "grad_norm": 1.0404402017593384,
      "learning_rate": 3.1990302146981334e-05,
      "loss": 0.1689,
      "step": 48580
    },
    {
      "epoch": 2.7218441027364646,
      "grad_norm": 3.70127534866333,
      "learning_rate": 3.197628790851505e-05,
      "loss": 0.1467,
      "step": 48590
    },
    {
      "epoch": 2.7224042797524017,
      "grad_norm": 2.1324872970581055,
      "learning_rate": 3.196227367004877e-05,
      "loss": 0.1398,
      "step": 48600
    },
    {
      "epoch": 2.722964456768339,
      "grad_norm": 1.832026481628418,
      "learning_rate": 3.194825943158249e-05,
      "loss": 0.112,
      "step": 48610
    },
    {
      "epoch": 2.723524633784276,
      "grad_norm": 4.0372772216796875,
      "learning_rate": 3.193424519311621e-05,
      "loss": 0.1211,
      "step": 48620
    },
    {
      "epoch": 2.724084810800213,
      "grad_norm": 0.8306264281272888,
      "learning_rate": 3.192023095464993e-05,
      "loss": 0.1262,
      "step": 48630
    },
    {
      "epoch": 2.72464498781615,
      "grad_norm": 2.5648765563964844,
      "learning_rate": 3.190621671618364e-05,
      "loss": 0.0915,
      "step": 48640
    },
    {
      "epoch": 2.725205164832087,
      "grad_norm": 4.0210700035095215,
      "learning_rate": 3.1892202477717365e-05,
      "loss": 0.1752,
      "step": 48650
    },
    {
      "epoch": 2.725765341848024,
      "grad_norm": 1.5231053829193115,
      "learning_rate": 3.1878188239251086e-05,
      "loss": 0.1542,
      "step": 48660
    },
    {
      "epoch": 2.726325518863961,
      "grad_norm": 5.233156204223633,
      "learning_rate": 3.1864174000784794e-05,
      "loss": 0.2043,
      "step": 48670
    },
    {
      "epoch": 2.726885695879898,
      "grad_norm": 2.7585113048553467,
      "learning_rate": 3.1850159762318516e-05,
      "loss": 0.1109,
      "step": 48680
    },
    {
      "epoch": 2.727445872895835,
      "grad_norm": 3.0219922065734863,
      "learning_rate": 3.183614552385224e-05,
      "loss": 0.1243,
      "step": 48690
    },
    {
      "epoch": 2.728006049911772,
      "grad_norm": 3.8747265338897705,
      "learning_rate": 3.182213128538595e-05,
      "loss": 0.1112,
      "step": 48700
    },
    {
      "epoch": 2.7285662269277093,
      "grad_norm": 2.8047468662261963,
      "learning_rate": 3.1808117046919673e-05,
      "loss": 0.1509,
      "step": 48710
    },
    {
      "epoch": 2.729126403943646,
      "grad_norm": 1.1029738187789917,
      "learning_rate": 3.179410280845339e-05,
      "loss": 0.1701,
      "step": 48720
    },
    {
      "epoch": 2.729686580959583,
      "grad_norm": 1.2041893005371094,
      "learning_rate": 3.178008856998711e-05,
      "loss": 0.1037,
      "step": 48730
    },
    {
      "epoch": 2.7302467579755203,
      "grad_norm": 4.856557369232178,
      "learning_rate": 3.176607433152083e-05,
      "loss": 0.1737,
      "step": 48740
    },
    {
      "epoch": 2.7308069349914574,
      "grad_norm": 6.451129913330078,
      "learning_rate": 3.1752060093054546e-05,
      "loss": 0.211,
      "step": 48750
    },
    {
      "epoch": 2.731367112007394,
      "grad_norm": 3.788865089416504,
      "learning_rate": 3.173804585458826e-05,
      "loss": 0.0969,
      "step": 48760
    },
    {
      "epoch": 2.7319272890233313,
      "grad_norm": 4.522748947143555,
      "learning_rate": 3.172403161612198e-05,
      "loss": 0.1084,
      "step": 48770
    },
    {
      "epoch": 2.7324874660392684,
      "grad_norm": 3.170830726623535,
      "learning_rate": 3.17100173776557e-05,
      "loss": 0.1014,
      "step": 48780
    },
    {
      "epoch": 2.7330476430552055,
      "grad_norm": 2.284163236618042,
      "learning_rate": 3.169600313918942e-05,
      "loss": 0.1124,
      "step": 48790
    },
    {
      "epoch": 2.7336078200711427,
      "grad_norm": 1.2070136070251465,
      "learning_rate": 3.168198890072313e-05,
      "loss": 0.0925,
      "step": 48800
    },
    {
      "epoch": 2.7341679970870794,
      "grad_norm": 1.612526297569275,
      "learning_rate": 3.1667974662256855e-05,
      "loss": 0.1221,
      "step": 48810
    },
    {
      "epoch": 2.7347281741030165,
      "grad_norm": 3.850083827972412,
      "learning_rate": 3.1653960423790576e-05,
      "loss": 0.1299,
      "step": 48820
    },
    {
      "epoch": 2.7352883511189536,
      "grad_norm": 0.6536218523979187,
      "learning_rate": 3.163994618532429e-05,
      "loss": 0.126,
      "step": 48830
    },
    {
      "epoch": 2.7358485281348908,
      "grad_norm": 3.4608302116394043,
      "learning_rate": 3.162593194685801e-05,
      "loss": 0.1176,
      "step": 48840
    },
    {
      "epoch": 2.7364087051508275,
      "grad_norm": 3.114509105682373,
      "learning_rate": 3.161191770839173e-05,
      "loss": 0.1449,
      "step": 48850
    },
    {
      "epoch": 2.7369688821667646,
      "grad_norm": 5.173256874084473,
      "learning_rate": 3.159790346992544e-05,
      "loss": 0.0961,
      "step": 48860
    },
    {
      "epoch": 2.7375290591827017,
      "grad_norm": 1.7455532550811768,
      "learning_rate": 3.1583889231459164e-05,
      "loss": 0.0808,
      "step": 48870
    },
    {
      "epoch": 2.738089236198639,
      "grad_norm": 3.291684150695801,
      "learning_rate": 3.156987499299288e-05,
      "loss": 0.1324,
      "step": 48880
    },
    {
      "epoch": 2.738649413214576,
      "grad_norm": 2.8457155227661133,
      "learning_rate": 3.15558607545266e-05,
      "loss": 0.112,
      "step": 48890
    },
    {
      "epoch": 2.7392095902305127,
      "grad_norm": 3.9804439544677734,
      "learning_rate": 3.154184651606032e-05,
      "loss": 0.1654,
      "step": 48900
    },
    {
      "epoch": 2.73976976724645,
      "grad_norm": 2.704355478286743,
      "learning_rate": 3.1527832277594036e-05,
      "loss": 0.2041,
      "step": 48910
    },
    {
      "epoch": 2.740329944262387,
      "grad_norm": 1.8736073970794678,
      "learning_rate": 3.151381803912776e-05,
      "loss": 0.097,
      "step": 48920
    },
    {
      "epoch": 2.740890121278324,
      "grad_norm": 5.725702285766602,
      "learning_rate": 3.149980380066148e-05,
      "loss": 0.1408,
      "step": 48930
    },
    {
      "epoch": 2.741450298294261,
      "grad_norm": 3.490835189819336,
      "learning_rate": 3.148578956219519e-05,
      "loss": 0.1887,
      "step": 48940
    },
    {
      "epoch": 2.742010475310198,
      "grad_norm": 2.6947309970855713,
      "learning_rate": 3.147177532372891e-05,
      "loss": 0.1086,
      "step": 48950
    },
    {
      "epoch": 2.742570652326135,
      "grad_norm": 2.2753381729125977,
      "learning_rate": 3.145776108526263e-05,
      "loss": 0.1948,
      "step": 48960
    },
    {
      "epoch": 2.743130829342072,
      "grad_norm": 3.1241586208343506,
      "learning_rate": 3.1443746846796345e-05,
      "loss": 0.1223,
      "step": 48970
    },
    {
      "epoch": 2.7436910063580093,
      "grad_norm": 1.0168095827102661,
      "learning_rate": 3.1429732608330066e-05,
      "loss": 0.1196,
      "step": 48980
    },
    {
      "epoch": 2.744251183373946,
      "grad_norm": 2.9569239616394043,
      "learning_rate": 3.141571836986378e-05,
      "loss": 0.1667,
      "step": 48990
    },
    {
      "epoch": 2.744811360389883,
      "grad_norm": 1.1829711198806763,
      "learning_rate": 3.14017041313975e-05,
      "loss": 0.1041,
      "step": 49000
    },
    {
      "epoch": 2.7453715374058203,
      "grad_norm": 2.6413886547088623,
      "learning_rate": 3.1387689892931224e-05,
      "loss": 0.1441,
      "step": 49010
    },
    {
      "epoch": 2.7459317144217574,
      "grad_norm": 1.6925489902496338,
      "learning_rate": 3.137367565446494e-05,
      "loss": 0.1415,
      "step": 49020
    },
    {
      "epoch": 2.746491891437694,
      "grad_norm": 3.1279444694519043,
      "learning_rate": 3.1359661415998654e-05,
      "loss": 0.1387,
      "step": 49030
    },
    {
      "epoch": 2.7470520684536313,
      "grad_norm": 1.0610567331314087,
      "learning_rate": 3.1345647177532375e-05,
      "loss": 0.1188,
      "step": 49040
    },
    {
      "epoch": 2.7476122454695684,
      "grad_norm": 3.5334324836730957,
      "learning_rate": 3.133163293906609e-05,
      "loss": 0.167,
      "step": 49050
    },
    {
      "epoch": 2.7481724224855055,
      "grad_norm": 0.816001296043396,
      "learning_rate": 3.131761870059981e-05,
      "loss": 0.1,
      "step": 49060
    },
    {
      "epoch": 2.7487325995014427,
      "grad_norm": 1.466310977935791,
      "learning_rate": 3.1303604462133526e-05,
      "loss": 0.0899,
      "step": 49070
    },
    {
      "epoch": 2.7492927765173794,
      "grad_norm": 2.6811840534210205,
      "learning_rate": 3.128959022366725e-05,
      "loss": 0.1056,
      "step": 49080
    },
    {
      "epoch": 2.7498529535333165,
      "grad_norm": 4.241961479187012,
      "learning_rate": 3.127557598520097e-05,
      "loss": 0.1561,
      "step": 49090
    },
    {
      "epoch": 2.7504131305492536,
      "grad_norm": 5.59625768661499,
      "learning_rate": 3.1261561746734684e-05,
      "loss": 0.1213,
      "step": 49100
    },
    {
      "epoch": 2.7509733075651908,
      "grad_norm": 2.64155650138855,
      "learning_rate": 3.1247547508268405e-05,
      "loss": 0.129,
      "step": 49110
    },
    {
      "epoch": 2.7515334845811275,
      "grad_norm": 2.8266351222991943,
      "learning_rate": 3.123353326980212e-05,
      "loss": 0.1124,
      "step": 49120
    },
    {
      "epoch": 2.7520936615970646,
      "grad_norm": 3.497581720352173,
      "learning_rate": 3.1219519031335835e-05,
      "loss": 0.12,
      "step": 49130
    },
    {
      "epoch": 2.7526538386130017,
      "grad_norm": 1.4299372434616089,
      "learning_rate": 3.1205504792869556e-05,
      "loss": 0.1018,
      "step": 49140
    },
    {
      "epoch": 2.753214015628939,
      "grad_norm": 3.9250106811523438,
      "learning_rate": 3.119149055440327e-05,
      "loss": 0.129,
      "step": 49150
    },
    {
      "epoch": 2.753774192644876,
      "grad_norm": 0.5555265545845032,
      "learning_rate": 3.117747631593699e-05,
      "loss": 0.1429,
      "step": 49160
    },
    {
      "epoch": 2.7543343696608127,
      "grad_norm": 4.884567737579346,
      "learning_rate": 3.1163462077470714e-05,
      "loss": 0.1508,
      "step": 49170
    },
    {
      "epoch": 2.75489454667675,
      "grad_norm": 1.6759940385818481,
      "learning_rate": 3.114944783900443e-05,
      "loss": 0.1113,
      "step": 49180
    },
    {
      "epoch": 2.755454723692687,
      "grad_norm": 4.541722297668457,
      "learning_rate": 3.113543360053815e-05,
      "loss": 0.163,
      "step": 49190
    },
    {
      "epoch": 2.7560149007086236,
      "grad_norm": 1.272708535194397,
      "learning_rate": 3.1121419362071865e-05,
      "loss": 0.0896,
      "step": 49200
    },
    {
      "epoch": 2.756575077724561,
      "grad_norm": 2.597831964492798,
      "learning_rate": 3.110740512360559e-05,
      "loss": 0.1302,
      "step": 49210
    },
    {
      "epoch": 2.757135254740498,
      "grad_norm": 3.417872190475464,
      "learning_rate": 3.10933908851393e-05,
      "loss": 0.1158,
      "step": 49220
    },
    {
      "epoch": 2.757695431756435,
      "grad_norm": 5.276506423950195,
      "learning_rate": 3.1079376646673016e-05,
      "loss": 0.1318,
      "step": 49230
    },
    {
      "epoch": 2.758255608772372,
      "grad_norm": 1.6310794353485107,
      "learning_rate": 3.106536240820674e-05,
      "loss": 0.1393,
      "step": 49240
    },
    {
      "epoch": 2.7588157857883093,
      "grad_norm": 5.725958347320557,
      "learning_rate": 3.105134816974046e-05,
      "loss": 0.1484,
      "step": 49250
    },
    {
      "epoch": 2.759375962804246,
      "grad_norm": 2.054429531097412,
      "learning_rate": 3.1037333931274174e-05,
      "loss": 0.1328,
      "step": 49260
    },
    {
      "epoch": 2.759936139820183,
      "grad_norm": 4.537265300750732,
      "learning_rate": 3.1023319692807896e-05,
      "loss": 0.1354,
      "step": 49270
    },
    {
      "epoch": 2.7604963168361203,
      "grad_norm": 1.4067037105560303,
      "learning_rate": 3.100930545434162e-05,
      "loss": 0.1032,
      "step": 49280
    },
    {
      "epoch": 2.761056493852057,
      "grad_norm": 2.270841121673584,
      "learning_rate": 3.099529121587533e-05,
      "loss": 0.1421,
      "step": 49290
    },
    {
      "epoch": 2.761616670867994,
      "grad_norm": 1.692452073097229,
      "learning_rate": 3.098127697740905e-05,
      "loss": 0.1362,
      "step": 49300
    },
    {
      "epoch": 2.7621768478839313,
      "grad_norm": 1.2207233905792236,
      "learning_rate": 3.096726273894277e-05,
      "loss": 0.1311,
      "step": 49310
    },
    {
      "epoch": 2.7627370248998684,
      "grad_norm": 6.079824447631836,
      "learning_rate": 3.095324850047648e-05,
      "loss": 0.1351,
      "step": 49320
    },
    {
      "epoch": 2.7632972019158055,
      "grad_norm": 3.196686267852783,
      "learning_rate": 3.0939234262010204e-05,
      "loss": 0.0942,
      "step": 49330
    },
    {
      "epoch": 2.7638573789317427,
      "grad_norm": 6.238956451416016,
      "learning_rate": 3.092522002354392e-05,
      "loss": 0.1686,
      "step": 49340
    },
    {
      "epoch": 2.7644175559476794,
      "grad_norm": 2.003912925720215,
      "learning_rate": 3.091120578507764e-05,
      "loss": 0.1551,
      "step": 49350
    },
    {
      "epoch": 2.7649777329636165,
      "grad_norm": 4.721385955810547,
      "learning_rate": 3.089719154661136e-05,
      "loss": 0.1171,
      "step": 49360
    },
    {
      "epoch": 2.7655379099795536,
      "grad_norm": 0.5449547171592712,
      "learning_rate": 3.088317730814508e-05,
      "loss": 0.0976,
      "step": 49370
    },
    {
      "epoch": 2.7660980869954903,
      "grad_norm": 2.504943609237671,
      "learning_rate": 3.08691630696788e-05,
      "loss": 0.157,
      "step": 49380
    },
    {
      "epoch": 2.7666582640114274,
      "grad_norm": 5.836136341094971,
      "learning_rate": 3.085514883121251e-05,
      "loss": 0.0982,
      "step": 49390
    },
    {
      "epoch": 2.7672184410273646,
      "grad_norm": 3.281545639038086,
      "learning_rate": 3.084113459274623e-05,
      "loss": 0.1066,
      "step": 49400
    },
    {
      "epoch": 2.7677786180433017,
      "grad_norm": 1.2155195474624634,
      "learning_rate": 3.082712035427995e-05,
      "loss": 0.1151,
      "step": 49410
    },
    {
      "epoch": 2.768338795059239,
      "grad_norm": 1.9554885625839233,
      "learning_rate": 3.0813106115813664e-05,
      "loss": 0.0896,
      "step": 49420
    },
    {
      "epoch": 2.768898972075176,
      "grad_norm": 0.8759850263595581,
      "learning_rate": 3.0799091877347386e-05,
      "loss": 0.1006,
      "step": 49430
    },
    {
      "epoch": 2.7694591490911127,
      "grad_norm": 4.474609375,
      "learning_rate": 3.078507763888111e-05,
      "loss": 0.1143,
      "step": 49440
    },
    {
      "epoch": 2.77001932610705,
      "grad_norm": 1.566178321838379,
      "learning_rate": 3.077106340041482e-05,
      "loss": 0.1326,
      "step": 49450
    },
    {
      "epoch": 2.770579503122987,
      "grad_norm": 0.9366359114646912,
      "learning_rate": 3.0757049161948543e-05,
      "loss": 0.0989,
      "step": 49460
    },
    {
      "epoch": 2.7711396801389236,
      "grad_norm": 1.1608206033706665,
      "learning_rate": 3.074303492348226e-05,
      "loss": 0.1399,
      "step": 49470
    },
    {
      "epoch": 2.771699857154861,
      "grad_norm": 0.6596733927726746,
      "learning_rate": 3.072902068501598e-05,
      "loss": 0.1141,
      "step": 49480
    },
    {
      "epoch": 2.772260034170798,
      "grad_norm": 2.956554889678955,
      "learning_rate": 3.0715006446549694e-05,
      "loss": 0.0939,
      "step": 49490
    },
    {
      "epoch": 2.772820211186735,
      "grad_norm": 2.979487180709839,
      "learning_rate": 3.070099220808341e-05,
      "loss": 0.271,
      "step": 49500
    },
    {
      "epoch": 2.773380388202672,
      "grad_norm": 0.6834974884986877,
      "learning_rate": 3.068697796961713e-05,
      "loss": 0.1561,
      "step": 49510
    },
    {
      "epoch": 2.7739405652186093,
      "grad_norm": 5.99161958694458,
      "learning_rate": 3.067296373115085e-05,
      "loss": 0.1713,
      "step": 49520
    },
    {
      "epoch": 2.774500742234546,
      "grad_norm": 3.105255365371704,
      "learning_rate": 3.065894949268457e-05,
      "loss": 0.0955,
      "step": 49530
    },
    {
      "epoch": 2.775060919250483,
      "grad_norm": 3.953887701034546,
      "learning_rate": 3.064493525421829e-05,
      "loss": 0.0909,
      "step": 49540
    },
    {
      "epoch": 2.7756210962664203,
      "grad_norm": 1.557263731956482,
      "learning_rate": 3.0630921015752e-05,
      "loss": 0.0945,
      "step": 49550
    },
    {
      "epoch": 2.776181273282357,
      "grad_norm": 3.814988136291504,
      "learning_rate": 3.0616906777285725e-05,
      "loss": 0.0947,
      "step": 49560
    },
    {
      "epoch": 2.776741450298294,
      "grad_norm": 2.131495237350464,
      "learning_rate": 3.0602892538819446e-05,
      "loss": 0.1235,
      "step": 49570
    },
    {
      "epoch": 2.7773016273142312,
      "grad_norm": 2.628798007965088,
      "learning_rate": 3.0588878300353154e-05,
      "loss": 0.1192,
      "step": 49580
    },
    {
      "epoch": 2.7778618043301684,
      "grad_norm": 2.780644655227661,
      "learning_rate": 3.0574864061886876e-05,
      "loss": 0.0979,
      "step": 49590
    },
    {
      "epoch": 2.7784219813461055,
      "grad_norm": 1.2553942203521729,
      "learning_rate": 3.05608498234206e-05,
      "loss": 0.1402,
      "step": 49600
    },
    {
      "epoch": 2.7789821583620427,
      "grad_norm": 6.151702404022217,
      "learning_rate": 3.054683558495431e-05,
      "loss": 0.1336,
      "step": 49610
    },
    {
      "epoch": 2.7795423353779793,
      "grad_norm": 2.082526683807373,
      "learning_rate": 3.0532821346488034e-05,
      "loss": 0.1163,
      "step": 49620
    },
    {
      "epoch": 2.7801025123939165,
      "grad_norm": 1.9148727655410767,
      "learning_rate": 3.0518807108021755e-05,
      "loss": 0.0798,
      "step": 49630
    },
    {
      "epoch": 2.7806626894098536,
      "grad_norm": 4.223135471343994,
      "learning_rate": 3.050479286955547e-05,
      "loss": 0.0812,
      "step": 49640
    },
    {
      "epoch": 2.7812228664257903,
      "grad_norm": 4.960570335388184,
      "learning_rate": 3.049077863108919e-05,
      "loss": 0.0996,
      "step": 49650
    },
    {
      "epoch": 2.7817830434417274,
      "grad_norm": 1.813061237335205,
      "learning_rate": 3.047676439262291e-05,
      "loss": 0.0837,
      "step": 49660
    },
    {
      "epoch": 2.7823432204576646,
      "grad_norm": 1.6679351329803467,
      "learning_rate": 3.0462750154156628e-05,
      "loss": 0.0844,
      "step": 49670
    },
    {
      "epoch": 2.7829033974736017,
      "grad_norm": 0.9820757508277893,
      "learning_rate": 3.0448735915690342e-05,
      "loss": 0.0895,
      "step": 49680
    },
    {
      "epoch": 2.783463574489539,
      "grad_norm": 5.2641496658325195,
      "learning_rate": 3.043472167722406e-05,
      "loss": 0.1611,
      "step": 49690
    },
    {
      "epoch": 2.784023751505476,
      "grad_norm": 2.521275520324707,
      "learning_rate": 3.042070743875778e-05,
      "loss": 0.0941,
      "step": 49700
    },
    {
      "epoch": 2.7845839285214127,
      "grad_norm": 1.1299103498458862,
      "learning_rate": 3.0406693200291497e-05,
      "loss": 0.0967,
      "step": 49710
    },
    {
      "epoch": 2.78514410553735,
      "grad_norm": 2.079425096511841,
      "learning_rate": 3.0392678961825215e-05,
      "loss": 0.0973,
      "step": 49720
    },
    {
      "epoch": 2.785704282553287,
      "grad_norm": 4.410033226013184,
      "learning_rate": 3.0378664723358936e-05,
      "loss": 0.1865,
      "step": 49730
    },
    {
      "epoch": 2.7862644595692236,
      "grad_norm": 2.6041347980499268,
      "learning_rate": 3.0364650484892655e-05,
      "loss": 0.1501,
      "step": 49740
    },
    {
      "epoch": 2.7868246365851608,
      "grad_norm": 1.819197177886963,
      "learning_rate": 3.0350636246426373e-05,
      "loss": 0.1442,
      "step": 49750
    },
    {
      "epoch": 2.787384813601098,
      "grad_norm": 4.5040364265441895,
      "learning_rate": 3.033662200796009e-05,
      "loss": 0.1398,
      "step": 49760
    },
    {
      "epoch": 2.787944990617035,
      "grad_norm": 0.9352864623069763,
      "learning_rate": 3.0322607769493806e-05,
      "loss": 0.0837,
      "step": 49770
    },
    {
      "epoch": 2.788505167632972,
      "grad_norm": 1.5493603944778442,
      "learning_rate": 3.0308593531027524e-05,
      "loss": 0.1032,
      "step": 49780
    },
    {
      "epoch": 2.7890653446489093,
      "grad_norm": 2.176077365875244,
      "learning_rate": 3.0294579292561242e-05,
      "loss": 0.1102,
      "step": 49790
    },
    {
      "epoch": 2.789625521664846,
      "grad_norm": 2.448781728744507,
      "learning_rate": 3.028056505409496e-05,
      "loss": 0.1222,
      "step": 49800
    },
    {
      "epoch": 2.790185698680783,
      "grad_norm": 1.4469565153121948,
      "learning_rate": 3.026655081562868e-05,
      "loss": 0.1703,
      "step": 49810
    },
    {
      "epoch": 2.7907458756967203,
      "grad_norm": 4.577982425689697,
      "learning_rate": 3.02525365771624e-05,
      "loss": 0.1886,
      "step": 49820
    },
    {
      "epoch": 2.791306052712657,
      "grad_norm": 2.083728790283203,
      "learning_rate": 3.0238522338696118e-05,
      "loss": 0.167,
      "step": 49830
    },
    {
      "epoch": 2.791866229728594,
      "grad_norm": 4.543215751647949,
      "learning_rate": 3.0224508100229836e-05,
      "loss": 0.1286,
      "step": 49840
    },
    {
      "epoch": 2.7924264067445312,
      "grad_norm": 0.5558257699012756,
      "learning_rate": 3.0210493861763557e-05,
      "loss": 0.1075,
      "step": 49850
    },
    {
      "epoch": 2.7929865837604684,
      "grad_norm": 0.6795917749404907,
      "learning_rate": 3.019647962329727e-05,
      "loss": 0.0822,
      "step": 49860
    },
    {
      "epoch": 2.7935467607764055,
      "grad_norm": 2.318568229675293,
      "learning_rate": 3.0182465384830987e-05,
      "loss": 0.0774,
      "step": 49870
    },
    {
      "epoch": 2.7941069377923426,
      "grad_norm": 1.8189260959625244,
      "learning_rate": 3.016845114636471e-05,
      "loss": 0.1404,
      "step": 49880
    },
    {
      "epoch": 2.7946671148082793,
      "grad_norm": 5.232472896575928,
      "learning_rate": 3.0154436907898426e-05,
      "loss": 0.0834,
      "step": 49890
    },
    {
      "epoch": 2.7952272918242165,
      "grad_norm": 2.73124361038208,
      "learning_rate": 3.0140422669432145e-05,
      "loss": 0.1495,
      "step": 49900
    },
    {
      "epoch": 2.7957874688401536,
      "grad_norm": 1.9056448936462402,
      "learning_rate": 3.0126408430965863e-05,
      "loss": 0.1212,
      "step": 49910
    },
    {
      "epoch": 2.7963476458560903,
      "grad_norm": 1.3288019895553589,
      "learning_rate": 3.011239419249958e-05,
      "loss": 0.1941,
      "step": 49920
    },
    {
      "epoch": 2.7969078228720274,
      "grad_norm": 1.9383625984191895,
      "learning_rate": 3.0098379954033302e-05,
      "loss": 0.1006,
      "step": 49930
    },
    {
      "epoch": 2.7974679998879646,
      "grad_norm": 2.0644941329956055,
      "learning_rate": 3.008436571556702e-05,
      "loss": 0.1957,
      "step": 49940
    },
    {
      "epoch": 2.7980281769039017,
      "grad_norm": 1.9844154119491577,
      "learning_rate": 3.0070351477100732e-05,
      "loss": 0.1068,
      "step": 49950
    },
    {
      "epoch": 2.798588353919839,
      "grad_norm": 2.745354413986206,
      "learning_rate": 3.0056337238634453e-05,
      "loss": 0.1089,
      "step": 49960
    },
    {
      "epoch": 2.799148530935776,
      "grad_norm": 2.6564831733703613,
      "learning_rate": 3.004232300016817e-05,
      "loss": 0.1352,
      "step": 49970
    },
    {
      "epoch": 2.7997087079517127,
      "grad_norm": 3.2583577632904053,
      "learning_rate": 3.002830876170189e-05,
      "loss": 0.1155,
      "step": 49980
    },
    {
      "epoch": 2.80026888496765,
      "grad_norm": 4.271341800689697,
      "learning_rate": 3.0014294523235608e-05,
      "loss": 0.1521,
      "step": 49990
    },
    {
      "epoch": 2.800829061983587,
      "grad_norm": 1.1591674089431763,
      "learning_rate": 3.0000280284769326e-05,
      "loss": 0.0963,
      "step": 50000
    },
    {
      "epoch": 2.8013892389995236,
      "grad_norm": 3.4881765842437744,
      "learning_rate": 2.9986266046303047e-05,
      "loss": 0.1275,
      "step": 50010
    },
    {
      "epoch": 2.8019494160154608,
      "grad_norm": 0.8234696984291077,
      "learning_rate": 2.9972251807836766e-05,
      "loss": 0.1376,
      "step": 50020
    },
    {
      "epoch": 2.802509593031398,
      "grad_norm": 3.262765645980835,
      "learning_rate": 2.9958237569370484e-05,
      "loss": 0.1155,
      "step": 50030
    },
    {
      "epoch": 2.803069770047335,
      "grad_norm": 2.235792875289917,
      "learning_rate": 2.99442233309042e-05,
      "loss": 0.1683,
      "step": 50040
    },
    {
      "epoch": 2.803629947063272,
      "grad_norm": 1.4747565984725952,
      "learning_rate": 2.9930209092437917e-05,
      "loss": 0.1109,
      "step": 50050
    },
    {
      "epoch": 2.804190124079209,
      "grad_norm": 0.8338738679885864,
      "learning_rate": 2.9916194853971635e-05,
      "loss": 0.178,
      "step": 50060
    },
    {
      "epoch": 2.804750301095146,
      "grad_norm": 4.027144908905029,
      "learning_rate": 2.9902180615505353e-05,
      "loss": 0.1,
      "step": 50070
    },
    {
      "epoch": 2.805310478111083,
      "grad_norm": 5.156270980834961,
      "learning_rate": 2.9888166377039074e-05,
      "loss": 0.1288,
      "step": 50080
    },
    {
      "epoch": 2.8058706551270203,
      "grad_norm": 3.5106160640716553,
      "learning_rate": 2.9874152138572792e-05,
      "loss": 0.2176,
      "step": 50090
    },
    {
      "epoch": 2.806430832142957,
      "grad_norm": 3.9508137702941895,
      "learning_rate": 2.986013790010651e-05,
      "loss": 0.1582,
      "step": 50100
    },
    {
      "epoch": 2.806991009158894,
      "grad_norm": 1.8662660121917725,
      "learning_rate": 2.984612366164023e-05,
      "loss": 0.0893,
      "step": 50110
    },
    {
      "epoch": 2.8075511861748312,
      "grad_norm": 5.889981269836426,
      "learning_rate": 2.9832109423173947e-05,
      "loss": 0.2403,
      "step": 50120
    },
    {
      "epoch": 2.8081113631907684,
      "grad_norm": 2.1007425785064697,
      "learning_rate": 2.981809518470766e-05,
      "loss": 0.2138,
      "step": 50130
    },
    {
      "epoch": 2.8086715402067055,
      "grad_norm": 0.6189966201782227,
      "learning_rate": 2.980408094624138e-05,
      "loss": 0.11,
      "step": 50140
    },
    {
      "epoch": 2.809231717222642,
      "grad_norm": 1.829606056213379,
      "learning_rate": 2.9790066707775098e-05,
      "loss": 0.0969,
      "step": 50150
    },
    {
      "epoch": 2.8097918942385793,
      "grad_norm": 4.180613040924072,
      "learning_rate": 2.977605246930882e-05,
      "loss": 0.1344,
      "step": 50160
    },
    {
      "epoch": 2.8103520712545165,
      "grad_norm": 3.252612352371216,
      "learning_rate": 2.9762038230842538e-05,
      "loss": 0.1464,
      "step": 50170
    },
    {
      "epoch": 2.8109122482704536,
      "grad_norm": 2.316829204559326,
      "learning_rate": 2.9748023992376256e-05,
      "loss": 0.1254,
      "step": 50180
    },
    {
      "epoch": 2.8114724252863903,
      "grad_norm": 3.3302786350250244,
      "learning_rate": 2.9734009753909974e-05,
      "loss": 0.0876,
      "step": 50190
    },
    {
      "epoch": 2.8120326023023274,
      "grad_norm": 7.4188361167907715,
      "learning_rate": 2.9719995515443695e-05,
      "loss": 0.2137,
      "step": 50200
    },
    {
      "epoch": 2.8125927793182646,
      "grad_norm": 2.7492525577545166,
      "learning_rate": 2.9705981276977413e-05,
      "loss": 0.1099,
      "step": 50210
    },
    {
      "epoch": 2.8131529563342017,
      "grad_norm": 3.3035926818847656,
      "learning_rate": 2.969196703851113e-05,
      "loss": 0.1286,
      "step": 50220
    },
    {
      "epoch": 2.813713133350139,
      "grad_norm": 2.9207210540771484,
      "learning_rate": 2.9677952800044846e-05,
      "loss": 0.1414,
      "step": 50230
    },
    {
      "epoch": 2.8142733103660755,
      "grad_norm": 4.14912748336792,
      "learning_rate": 2.9663938561578564e-05,
      "loss": 0.248,
      "step": 50240
    },
    {
      "epoch": 2.8148334873820127,
      "grad_norm": 1.1515634059906006,
      "learning_rate": 2.9649924323112283e-05,
      "loss": 0.1013,
      "step": 50250
    },
    {
      "epoch": 2.81539366439795,
      "grad_norm": 3.3934919834136963,
      "learning_rate": 2.9635910084646e-05,
      "loss": 0.1253,
      "step": 50260
    },
    {
      "epoch": 2.815953841413887,
      "grad_norm": 3.7477006912231445,
      "learning_rate": 2.962189584617972e-05,
      "loss": 0.0901,
      "step": 50270
    },
    {
      "epoch": 2.8165140184298236,
      "grad_norm": 1.4021013975143433,
      "learning_rate": 2.960788160771344e-05,
      "loss": 0.1081,
      "step": 50280
    },
    {
      "epoch": 2.8170741954457608,
      "grad_norm": 1.4118015766143799,
      "learning_rate": 2.959386736924716e-05,
      "loss": 0.0991,
      "step": 50290
    },
    {
      "epoch": 2.817634372461698,
      "grad_norm": 0.9135287404060364,
      "learning_rate": 2.9579853130780877e-05,
      "loss": 0.0811,
      "step": 50300
    },
    {
      "epoch": 2.818194549477635,
      "grad_norm": 0.6305592656135559,
      "learning_rate": 2.9565838892314595e-05,
      "loss": 0.1426,
      "step": 50310
    },
    {
      "epoch": 2.818754726493572,
      "grad_norm": 0.689325749874115,
      "learning_rate": 2.955182465384831e-05,
      "loss": 0.1266,
      "step": 50320
    },
    {
      "epoch": 2.819314903509509,
      "grad_norm": 1.4382961988449097,
      "learning_rate": 2.9537810415382028e-05,
      "loss": 0.1067,
      "step": 50330
    },
    {
      "epoch": 2.819875080525446,
      "grad_norm": 4.186975479125977,
      "learning_rate": 2.9523796176915746e-05,
      "loss": 0.1219,
      "step": 50340
    },
    {
      "epoch": 2.820435257541383,
      "grad_norm": 5.447384834289551,
      "learning_rate": 2.9509781938449464e-05,
      "loss": 0.1224,
      "step": 50350
    },
    {
      "epoch": 2.8209954345573203,
      "grad_norm": 2.4768331050872803,
      "learning_rate": 2.9495767699983185e-05,
      "loss": 0.1566,
      "step": 50360
    },
    {
      "epoch": 2.821555611573257,
      "grad_norm": 2.7743654251098633,
      "learning_rate": 2.9481753461516904e-05,
      "loss": 0.1156,
      "step": 50370
    },
    {
      "epoch": 2.822115788589194,
      "grad_norm": 5.296530723571777,
      "learning_rate": 2.946773922305062e-05,
      "loss": 0.1971,
      "step": 50380
    },
    {
      "epoch": 2.8226759656051312,
      "grad_norm": 1.1986089944839478,
      "learning_rate": 2.945372498458434e-05,
      "loss": 0.0884,
      "step": 50390
    },
    {
      "epoch": 2.8232361426210684,
      "grad_norm": 1.8399066925048828,
      "learning_rate": 2.943971074611806e-05,
      "loss": 0.1365,
      "step": 50400
    },
    {
      "epoch": 2.8237963196370055,
      "grad_norm": 3.104740619659424,
      "learning_rate": 2.9425696507651773e-05,
      "loss": 0.1083,
      "step": 50410
    },
    {
      "epoch": 2.824356496652942,
      "grad_norm": 3.1193172931671143,
      "learning_rate": 2.941168226918549e-05,
      "loss": 0.1368,
      "step": 50420
    },
    {
      "epoch": 2.8249166736688793,
      "grad_norm": 1.3533270359039307,
      "learning_rate": 2.9397668030719212e-05,
      "loss": 0.0885,
      "step": 50430
    },
    {
      "epoch": 2.8254768506848165,
      "grad_norm": 5.501711845397949,
      "learning_rate": 2.938365379225293e-05,
      "loss": 0.1771,
      "step": 50440
    },
    {
      "epoch": 2.8260370277007536,
      "grad_norm": 0.6017729043960571,
      "learning_rate": 2.936963955378665e-05,
      "loss": 0.1364,
      "step": 50450
    },
    {
      "epoch": 2.8265972047166903,
      "grad_norm": 2.3201866149902344,
      "learning_rate": 2.9355625315320367e-05,
      "loss": 0.0831,
      "step": 50460
    },
    {
      "epoch": 2.8271573817326274,
      "grad_norm": 3.6831600666046143,
      "learning_rate": 2.9341611076854085e-05,
      "loss": 0.1135,
      "step": 50470
    },
    {
      "epoch": 2.8277175587485646,
      "grad_norm": 2.153416395187378,
      "learning_rate": 2.9327596838387806e-05,
      "loss": 0.0947,
      "step": 50480
    },
    {
      "epoch": 2.8282777357645017,
      "grad_norm": 2.9432899951934814,
      "learning_rate": 2.9313582599921525e-05,
      "loss": 0.1577,
      "step": 50490
    },
    {
      "epoch": 2.828837912780439,
      "grad_norm": 1.3208547830581665,
      "learning_rate": 2.9299568361455236e-05,
      "loss": 0.1243,
      "step": 50500
    },
    {
      "epoch": 2.8293980897963755,
      "grad_norm": 1.89109206199646,
      "learning_rate": 2.9285554122988957e-05,
      "loss": 0.1392,
      "step": 50510
    },
    {
      "epoch": 2.8299582668123127,
      "grad_norm": 1.154451847076416,
      "learning_rate": 2.9271539884522676e-05,
      "loss": 0.1125,
      "step": 50520
    },
    {
      "epoch": 2.83051844382825,
      "grad_norm": 1.4882047176361084,
      "learning_rate": 2.9257525646056394e-05,
      "loss": 0.0896,
      "step": 50530
    },
    {
      "epoch": 2.8310786208441865,
      "grad_norm": 1.0910366773605347,
      "learning_rate": 2.9243511407590112e-05,
      "loss": 0.1385,
      "step": 50540
    },
    {
      "epoch": 2.8316387978601236,
      "grad_norm": 5.754363536834717,
      "learning_rate": 2.9229497169123833e-05,
      "loss": 0.2091,
      "step": 50550
    },
    {
      "epoch": 2.8321989748760608,
      "grad_norm": 3.428663969039917,
      "learning_rate": 2.921548293065755e-05,
      "loss": 0.1326,
      "step": 50560
    },
    {
      "epoch": 2.832759151891998,
      "grad_norm": 1.5738826990127563,
      "learning_rate": 2.920146869219127e-05,
      "loss": 0.1266,
      "step": 50570
    },
    {
      "epoch": 2.833319328907935,
      "grad_norm": 4.50692892074585,
      "learning_rate": 2.9187454453724988e-05,
      "loss": 0.2092,
      "step": 50580
    },
    {
      "epoch": 2.833879505923872,
      "grad_norm": 2.8369650840759277,
      "learning_rate": 2.9173440215258702e-05,
      "loss": 0.1031,
      "step": 50590
    },
    {
      "epoch": 2.834439682939809,
      "grad_norm": 1.11048424243927,
      "learning_rate": 2.915942597679242e-05,
      "loss": 0.2032,
      "step": 50600
    },
    {
      "epoch": 2.834999859955746,
      "grad_norm": 2.1130237579345703,
      "learning_rate": 2.914541173832614e-05,
      "loss": 0.1344,
      "step": 50610
    },
    {
      "epoch": 2.835560036971683,
      "grad_norm": 1.3274718523025513,
      "learning_rate": 2.9131397499859857e-05,
      "loss": 0.0975,
      "step": 50620
    },
    {
      "epoch": 2.83612021398762,
      "grad_norm": 2.75315523147583,
      "learning_rate": 2.911738326139358e-05,
      "loss": 0.1052,
      "step": 50630
    },
    {
      "epoch": 2.836680391003557,
      "grad_norm": 4.169613838195801,
      "learning_rate": 2.9103369022927296e-05,
      "loss": 0.1105,
      "step": 50640
    },
    {
      "epoch": 2.837240568019494,
      "grad_norm": 2.6143369674682617,
      "learning_rate": 2.9089354784461015e-05,
      "loss": 0.1041,
      "step": 50650
    },
    {
      "epoch": 2.8378007450354312,
      "grad_norm": 0.5781051516532898,
      "learning_rate": 2.9075340545994733e-05,
      "loss": 0.1375,
      "step": 50660
    },
    {
      "epoch": 2.8383609220513684,
      "grad_norm": 3.267016649246216,
      "learning_rate": 2.906132630752845e-05,
      "loss": 0.1342,
      "step": 50670
    },
    {
      "epoch": 2.8389210990673055,
      "grad_norm": 1.0015472173690796,
      "learning_rate": 2.9047312069062166e-05,
      "loss": 0.1522,
      "step": 50680
    },
    {
      "epoch": 2.839481276083242,
      "grad_norm": 1.9618768692016602,
      "learning_rate": 2.9033297830595884e-05,
      "loss": 0.1307,
      "step": 50690
    },
    {
      "epoch": 2.8400414530991793,
      "grad_norm": 4.508795738220215,
      "learning_rate": 2.9019283592129602e-05,
      "loss": 0.3107,
      "step": 50700
    },
    {
      "epoch": 2.8406016301151165,
      "grad_norm": 1.003406286239624,
      "learning_rate": 2.9005269353663323e-05,
      "loss": 0.1235,
      "step": 50710
    },
    {
      "epoch": 2.841161807131053,
      "grad_norm": 1.4478354454040527,
      "learning_rate": 2.899125511519704e-05,
      "loss": 0.2063,
      "step": 50720
    },
    {
      "epoch": 2.8417219841469903,
      "grad_norm": 2.3665578365325928,
      "learning_rate": 2.897724087673076e-05,
      "loss": 0.1041,
      "step": 50730
    },
    {
      "epoch": 2.8422821611629274,
      "grad_norm": 3.7916271686553955,
      "learning_rate": 2.8963226638264478e-05,
      "loss": 0.1509,
      "step": 50740
    },
    {
      "epoch": 2.8428423381788646,
      "grad_norm": 1.5114115476608276,
      "learning_rate": 2.89492123997982e-05,
      "loss": 0.1474,
      "step": 50750
    },
    {
      "epoch": 2.8434025151948017,
      "grad_norm": 5.6718058586120605,
      "learning_rate": 2.8935198161331917e-05,
      "loss": 0.1316,
      "step": 50760
    },
    {
      "epoch": 2.843962692210739,
      "grad_norm": 4.340043544769287,
      "learning_rate": 2.8921183922865636e-05,
      "loss": 0.1226,
      "step": 50770
    },
    {
      "epoch": 2.8445228692266755,
      "grad_norm": 2.6356539726257324,
      "learning_rate": 2.890716968439935e-05,
      "loss": 0.1047,
      "step": 50780
    },
    {
      "epoch": 2.8450830462426127,
      "grad_norm": 3.7993321418762207,
      "learning_rate": 2.889315544593307e-05,
      "loss": 0.1815,
      "step": 50790
    },
    {
      "epoch": 2.84564322325855,
      "grad_norm": 3.152865171432495,
      "learning_rate": 2.8879141207466787e-05,
      "loss": 0.1058,
      "step": 50800
    },
    {
      "epoch": 2.8462034002744865,
      "grad_norm": 1.6583324670791626,
      "learning_rate": 2.8865126969000505e-05,
      "loss": 0.0901,
      "step": 50810
    },
    {
      "epoch": 2.8467635772904236,
      "grad_norm": 3.9761252403259277,
      "learning_rate": 2.8851112730534223e-05,
      "loss": 0.1015,
      "step": 50820
    },
    {
      "epoch": 2.8473237543063608,
      "grad_norm": 0.5917930006980896,
      "learning_rate": 2.8837098492067944e-05,
      "loss": 0.073,
      "step": 50830
    },
    {
      "epoch": 2.847883931322298,
      "grad_norm": 3.168903112411499,
      "learning_rate": 2.8823084253601662e-05,
      "loss": 0.1354,
      "step": 50840
    },
    {
      "epoch": 2.848444108338235,
      "grad_norm": 1.9192454814910889,
      "learning_rate": 2.880907001513538e-05,
      "loss": 0.0848,
      "step": 50850
    },
    {
      "epoch": 2.849004285354172,
      "grad_norm": 4.56900691986084,
      "learning_rate": 2.87950557766691e-05,
      "loss": 0.1129,
      "step": 50860
    },
    {
      "epoch": 2.849564462370109,
      "grad_norm": 1.896356225013733,
      "learning_rate": 2.8781041538202813e-05,
      "loss": 0.2099,
      "step": 50870
    },
    {
      "epoch": 2.850124639386046,
      "grad_norm": 2.2066903114318848,
      "learning_rate": 2.876702729973653e-05,
      "loss": 0.1142,
      "step": 50880
    },
    {
      "epoch": 2.850684816401983,
      "grad_norm": 3.0153653621673584,
      "learning_rate": 2.875301306127025e-05,
      "loss": 0.106,
      "step": 50890
    },
    {
      "epoch": 2.85124499341792,
      "grad_norm": 1.2514749765396118,
      "learning_rate": 2.873899882280397e-05,
      "loss": 0.1222,
      "step": 50900
    },
    {
      "epoch": 2.851805170433857,
      "grad_norm": 1.3158400058746338,
      "learning_rate": 2.872498458433769e-05,
      "loss": 0.1541,
      "step": 50910
    },
    {
      "epoch": 2.852365347449794,
      "grad_norm": 4.530951499938965,
      "learning_rate": 2.8710970345871408e-05,
      "loss": 0.098,
      "step": 50920
    },
    {
      "epoch": 2.852925524465731,
      "grad_norm": 3.558361291885376,
      "learning_rate": 2.8696956107405126e-05,
      "loss": 0.073,
      "step": 50930
    },
    {
      "epoch": 2.8534857014816684,
      "grad_norm": 1.7092219591140747,
      "learning_rate": 2.8682941868938844e-05,
      "loss": 0.1299,
      "step": 50940
    },
    {
      "epoch": 2.8540458784976055,
      "grad_norm": 1.2981077432632446,
      "learning_rate": 2.8668927630472565e-05,
      "loss": 0.0888,
      "step": 50950
    },
    {
      "epoch": 2.854606055513542,
      "grad_norm": 1.0902070999145508,
      "learning_rate": 2.8654913392006277e-05,
      "loss": 0.0993,
      "step": 50960
    },
    {
      "epoch": 2.8551662325294793,
      "grad_norm": 4.178929805755615,
      "learning_rate": 2.8640899153539995e-05,
      "loss": 0.1367,
      "step": 50970
    },
    {
      "epoch": 2.8557264095454165,
      "grad_norm": 0.7201100587844849,
      "learning_rate": 2.8626884915073716e-05,
      "loss": 0.0959,
      "step": 50980
    },
    {
      "epoch": 2.856286586561353,
      "grad_norm": 1.4017951488494873,
      "learning_rate": 2.8612870676607434e-05,
      "loss": 0.0961,
      "step": 50990
    },
    {
      "epoch": 2.8568467635772903,
      "grad_norm": 1.7195132970809937,
      "learning_rate": 2.8598856438141153e-05,
      "loss": 0.0908,
      "step": 51000
    },
    {
      "epoch": 2.8574069405932274,
      "grad_norm": 5.403498649597168,
      "learning_rate": 2.858484219967487e-05,
      "loss": 0.1272,
      "step": 51010
    },
    {
      "epoch": 2.8579671176091646,
      "grad_norm": 4.126651763916016,
      "learning_rate": 2.857082796120859e-05,
      "loss": 0.1086,
      "step": 51020
    },
    {
      "epoch": 2.8585272946251017,
      "grad_norm": 1.5919846296310425,
      "learning_rate": 2.855681372274231e-05,
      "loss": 0.1435,
      "step": 51030
    },
    {
      "epoch": 2.859087471641039,
      "grad_norm": 1.7077876329421997,
      "learning_rate": 2.854279948427603e-05,
      "loss": 0.1496,
      "step": 51040
    },
    {
      "epoch": 2.8596476486569755,
      "grad_norm": 1.0875662565231323,
      "learning_rate": 2.852878524580974e-05,
      "loss": 0.1388,
      "step": 51050
    },
    {
      "epoch": 2.8602078256729127,
      "grad_norm": 1.5974974632263184,
      "learning_rate": 2.851477100734346e-05,
      "loss": 0.0914,
      "step": 51060
    },
    {
      "epoch": 2.86076800268885,
      "grad_norm": 1.5115538835525513,
      "learning_rate": 2.850075676887718e-05,
      "loss": 0.1015,
      "step": 51070
    },
    {
      "epoch": 2.8613281797047865,
      "grad_norm": 0.7720876932144165,
      "learning_rate": 2.8486742530410898e-05,
      "loss": 0.1126,
      "step": 51080
    },
    {
      "epoch": 2.8618883567207236,
      "grad_norm": 1.6867457628250122,
      "learning_rate": 2.8472728291944616e-05,
      "loss": 0.1192,
      "step": 51090
    },
    {
      "epoch": 2.8624485337366607,
      "grad_norm": 2.7258174419403076,
      "learning_rate": 2.8458714053478337e-05,
      "loss": 0.1434,
      "step": 51100
    },
    {
      "epoch": 2.863008710752598,
      "grad_norm": 2.929089069366455,
      "learning_rate": 2.8444699815012055e-05,
      "loss": 0.1623,
      "step": 51110
    },
    {
      "epoch": 2.863568887768535,
      "grad_norm": 2.7898128032684326,
      "learning_rate": 2.8430685576545774e-05,
      "loss": 0.1489,
      "step": 51120
    },
    {
      "epoch": 2.864129064784472,
      "grad_norm": 1.0031462907791138,
      "learning_rate": 2.841667133807949e-05,
      "loss": 0.1788,
      "step": 51130
    },
    {
      "epoch": 2.864689241800409,
      "grad_norm": 2.1124281883239746,
      "learning_rate": 2.8402657099613206e-05,
      "loss": 0.0879,
      "step": 51140
    },
    {
      "epoch": 2.865249418816346,
      "grad_norm": 4.227268695831299,
      "learning_rate": 2.8388642861146925e-05,
      "loss": 0.1192,
      "step": 51150
    },
    {
      "epoch": 2.865809595832283,
      "grad_norm": 2.012397527694702,
      "learning_rate": 2.8374628622680643e-05,
      "loss": 0.1154,
      "step": 51160
    },
    {
      "epoch": 2.86636977284822,
      "grad_norm": 0.8132691979408264,
      "learning_rate": 2.836061438421436e-05,
      "loss": 0.1146,
      "step": 51170
    },
    {
      "epoch": 2.866929949864157,
      "grad_norm": 1.6539493799209595,
      "learning_rate": 2.8346600145748082e-05,
      "loss": 0.1329,
      "step": 51180
    },
    {
      "epoch": 2.867490126880094,
      "grad_norm": 5.0402374267578125,
      "learning_rate": 2.83325859072818e-05,
      "loss": 0.1358,
      "step": 51190
    },
    {
      "epoch": 2.868050303896031,
      "grad_norm": 2.6872074604034424,
      "learning_rate": 2.831857166881552e-05,
      "loss": 0.1131,
      "step": 51200
    },
    {
      "epoch": 2.8686104809119684,
      "grad_norm": 1.5000230073928833,
      "learning_rate": 2.8304557430349237e-05,
      "loss": 0.1068,
      "step": 51210
    },
    {
      "epoch": 2.8691706579279055,
      "grad_norm": 1.0888737440109253,
      "learning_rate": 2.8290543191882958e-05,
      "loss": 0.0913,
      "step": 51220
    },
    {
      "epoch": 2.869730834943842,
      "grad_norm": 2.160494565963745,
      "learning_rate": 2.827652895341667e-05,
      "loss": 0.1543,
      "step": 51230
    },
    {
      "epoch": 2.8702910119597793,
      "grad_norm": 1.0699552297592163,
      "learning_rate": 2.8262514714950388e-05,
      "loss": 0.0851,
      "step": 51240
    },
    {
      "epoch": 2.8708511889757165,
      "grad_norm": 2.236679792404175,
      "learning_rate": 2.824850047648411e-05,
      "loss": 0.1702,
      "step": 51250
    },
    {
      "epoch": 2.871411365991653,
      "grad_norm": 7.474810600280762,
      "learning_rate": 2.8234486238017827e-05,
      "loss": 0.1909,
      "step": 51260
    },
    {
      "epoch": 2.8719715430075903,
      "grad_norm": 0.8723602890968323,
      "learning_rate": 2.8220471999551546e-05,
      "loss": 0.1522,
      "step": 51270
    },
    {
      "epoch": 2.8725317200235274,
      "grad_norm": 1.4812209606170654,
      "learning_rate": 2.8206457761085264e-05,
      "loss": 0.1,
      "step": 51280
    },
    {
      "epoch": 2.8730918970394645,
      "grad_norm": 1.7393094301223755,
      "learning_rate": 2.8192443522618982e-05,
      "loss": 0.1043,
      "step": 51290
    },
    {
      "epoch": 2.8736520740554017,
      "grad_norm": 2.6054792404174805,
      "learning_rate": 2.8178429284152703e-05,
      "loss": 0.0995,
      "step": 51300
    },
    {
      "epoch": 2.874212251071339,
      "grad_norm": 0.7117700576782227,
      "learning_rate": 2.816441504568642e-05,
      "loss": 0.1177,
      "step": 51310
    },
    {
      "epoch": 2.8747724280872755,
      "grad_norm": 3.427586555480957,
      "learning_rate": 2.815040080722014e-05,
      "loss": 0.1447,
      "step": 51320
    },
    {
      "epoch": 2.8753326051032126,
      "grad_norm": 2.6609725952148438,
      "learning_rate": 2.8136386568753854e-05,
      "loss": 0.1767,
      "step": 51330
    },
    {
      "epoch": 2.87589278211915,
      "grad_norm": 3.8198020458221436,
      "learning_rate": 2.8122372330287572e-05,
      "loss": 0.1053,
      "step": 51340
    },
    {
      "epoch": 2.8764529591350865,
      "grad_norm": 1.5595775842666626,
      "learning_rate": 2.810835809182129e-05,
      "loss": 0.1613,
      "step": 51350
    },
    {
      "epoch": 2.8770131361510236,
      "grad_norm": 3.753614664077759,
      "learning_rate": 2.809434385335501e-05,
      "loss": 0.1118,
      "step": 51360
    },
    {
      "epoch": 2.8775733131669607,
      "grad_norm": 1.7347644567489624,
      "learning_rate": 2.8080329614888727e-05,
      "loss": 0.1345,
      "step": 51370
    },
    {
      "epoch": 2.878133490182898,
      "grad_norm": 2.6815104484558105,
      "learning_rate": 2.806631537642245e-05,
      "loss": 0.1316,
      "step": 51380
    },
    {
      "epoch": 2.878693667198835,
      "grad_norm": 4.235754013061523,
      "learning_rate": 2.8052301137956166e-05,
      "loss": 0.1349,
      "step": 51390
    },
    {
      "epoch": 2.8792538442147717,
      "grad_norm": 4.56463623046875,
      "learning_rate": 2.8038286899489885e-05,
      "loss": 0.123,
      "step": 51400
    },
    {
      "epoch": 2.879814021230709,
      "grad_norm": 3.812342643737793,
      "learning_rate": 2.8024272661023603e-05,
      "loss": 0.1132,
      "step": 51410
    },
    {
      "epoch": 2.880374198246646,
      "grad_norm": 2.634561538696289,
      "learning_rate": 2.8010258422557317e-05,
      "loss": 0.1281,
      "step": 51420
    },
    {
      "epoch": 2.880934375262583,
      "grad_norm": 5.032334327697754,
      "learning_rate": 2.7996244184091036e-05,
      "loss": 0.1572,
      "step": 51430
    },
    {
      "epoch": 2.88149455227852,
      "grad_norm": 3.741472005844116,
      "learning_rate": 2.7982229945624754e-05,
      "loss": 0.1219,
      "step": 51440
    },
    {
      "epoch": 2.882054729294457,
      "grad_norm": 8.261931419372559,
      "learning_rate": 2.7968215707158475e-05,
      "loss": 0.2787,
      "step": 51450
    },
    {
      "epoch": 2.882614906310394,
      "grad_norm": 1.7288005352020264,
      "learning_rate": 2.7954201468692193e-05,
      "loss": 0.1899,
      "step": 51460
    },
    {
      "epoch": 2.883175083326331,
      "grad_norm": 2.0038726329803467,
      "learning_rate": 2.794018723022591e-05,
      "loss": 0.1219,
      "step": 51470
    },
    {
      "epoch": 2.8837352603422683,
      "grad_norm": 2.71628737449646,
      "learning_rate": 2.792617299175963e-05,
      "loss": 0.14,
      "step": 51480
    },
    {
      "epoch": 2.884295437358205,
      "grad_norm": 3.8901314735412598,
      "learning_rate": 2.7912158753293348e-05,
      "loss": 0.182,
      "step": 51490
    },
    {
      "epoch": 2.884855614374142,
      "grad_norm": 2.638607978820801,
      "learning_rate": 2.789814451482707e-05,
      "loss": 0.1591,
      "step": 51500
    },
    {
      "epoch": 2.8854157913900793,
      "grad_norm": 2.353179693222046,
      "learning_rate": 2.788413027636078e-05,
      "loss": 0.1324,
      "step": 51510
    },
    {
      "epoch": 2.8859759684060164,
      "grad_norm": 2.4559412002563477,
      "learning_rate": 2.78701160378945e-05,
      "loss": 0.1231,
      "step": 51520
    },
    {
      "epoch": 2.886536145421953,
      "grad_norm": 2.1882381439208984,
      "learning_rate": 2.785610179942822e-05,
      "loss": 0.105,
      "step": 51530
    },
    {
      "epoch": 2.8870963224378903,
      "grad_norm": 3.433225393295288,
      "learning_rate": 2.784208756096194e-05,
      "loss": 0.1063,
      "step": 51540
    },
    {
      "epoch": 2.8876564994538274,
      "grad_norm": 1.662630558013916,
      "learning_rate": 2.7828073322495657e-05,
      "loss": 0.162,
      "step": 51550
    },
    {
      "epoch": 2.8882166764697645,
      "grad_norm": 1.0210676193237305,
      "learning_rate": 2.7814059084029375e-05,
      "loss": 0.1049,
      "step": 51560
    },
    {
      "epoch": 2.8887768534857017,
      "grad_norm": 2.4160263538360596,
      "learning_rate": 2.7800044845563096e-05,
      "loss": 0.1101,
      "step": 51570
    },
    {
      "epoch": 2.8893370305016384,
      "grad_norm": 1.161218285560608,
      "learning_rate": 2.7786030607096814e-05,
      "loss": 0.1281,
      "step": 51580
    },
    {
      "epoch": 2.8898972075175755,
      "grad_norm": 1.4871455430984497,
      "learning_rate": 2.7772016368630532e-05,
      "loss": 0.1344,
      "step": 51590
    },
    {
      "epoch": 2.8904573845335126,
      "grad_norm": 4.405919075012207,
      "learning_rate": 2.7758002130164247e-05,
      "loss": 0.1214,
      "step": 51600
    },
    {
      "epoch": 2.8910175615494498,
      "grad_norm": 2.1446754932403564,
      "learning_rate": 2.7743987891697965e-05,
      "loss": 0.1704,
      "step": 51610
    },
    {
      "epoch": 2.8915777385653865,
      "grad_norm": 2.4816765785217285,
      "learning_rate": 2.7729973653231683e-05,
      "loss": 0.1197,
      "step": 51620
    },
    {
      "epoch": 2.8921379155813236,
      "grad_norm": 2.7608814239501953,
      "learning_rate": 2.77159594147654e-05,
      "loss": 0.1056,
      "step": 51630
    },
    {
      "epoch": 2.8926980925972607,
      "grad_norm": 1.4818660020828247,
      "learning_rate": 2.770194517629912e-05,
      "loss": 0.2103,
      "step": 51640
    },
    {
      "epoch": 2.893258269613198,
      "grad_norm": 2.3686728477478027,
      "learning_rate": 2.768793093783284e-05,
      "loss": 0.1215,
      "step": 51650
    },
    {
      "epoch": 2.893818446629135,
      "grad_norm": 1.2087292671203613,
      "learning_rate": 2.767391669936656e-05,
      "loss": 0.0799,
      "step": 51660
    },
    {
      "epoch": 2.8943786236450717,
      "grad_norm": 1.4439979791641235,
      "learning_rate": 2.7659902460900278e-05,
      "loss": 0.0985,
      "step": 51670
    },
    {
      "epoch": 2.894938800661009,
      "grad_norm": 2.762613296508789,
      "learning_rate": 2.7645888222433996e-05,
      "loss": 0.1184,
      "step": 51680
    },
    {
      "epoch": 2.895498977676946,
      "grad_norm": 1.3757543563842773,
      "learning_rate": 2.763187398396771e-05,
      "loss": 0.1176,
      "step": 51690
    },
    {
      "epoch": 2.896059154692883,
      "grad_norm": 1.6952639818191528,
      "learning_rate": 2.761785974550143e-05,
      "loss": 0.1286,
      "step": 51700
    },
    {
      "epoch": 2.89661933170882,
      "grad_norm": 1.1056067943572998,
      "learning_rate": 2.7603845507035147e-05,
      "loss": 0.1907,
      "step": 51710
    },
    {
      "epoch": 2.897179508724757,
      "grad_norm": 1.4708282947540283,
      "learning_rate": 2.7589831268568865e-05,
      "loss": 0.1434,
      "step": 51720
    },
    {
      "epoch": 2.897739685740694,
      "grad_norm": 1.0281248092651367,
      "learning_rate": 2.7575817030102586e-05,
      "loss": 0.0847,
      "step": 51730
    },
    {
      "epoch": 2.898299862756631,
      "grad_norm": 1.3930487632751465,
      "learning_rate": 2.7561802791636304e-05,
      "loss": 0.0921,
      "step": 51740
    },
    {
      "epoch": 2.8988600397725683,
      "grad_norm": 4.51886510848999,
      "learning_rate": 2.7547788553170023e-05,
      "loss": 0.1086,
      "step": 51750
    },
    {
      "epoch": 2.899420216788505,
      "grad_norm": 2.6215972900390625,
      "learning_rate": 2.753377431470374e-05,
      "loss": 0.1342,
      "step": 51760
    },
    {
      "epoch": 2.899980393804442,
      "grad_norm": 3.2192394733428955,
      "learning_rate": 2.7519760076237462e-05,
      "loss": 0.1885,
      "step": 51770
    },
    {
      "epoch": 2.9005405708203793,
      "grad_norm": 1.8013781309127808,
      "learning_rate": 2.7505745837771174e-05,
      "loss": 0.13,
      "step": 51780
    },
    {
      "epoch": 2.9011007478363164,
      "grad_norm": 1.4884659051895142,
      "learning_rate": 2.7491731599304892e-05,
      "loss": 0.0916,
      "step": 51790
    },
    {
      "epoch": 2.901660924852253,
      "grad_norm": 1.866734504699707,
      "learning_rate": 2.7477717360838613e-05,
      "loss": 0.088,
      "step": 51800
    },
    {
      "epoch": 2.9022211018681903,
      "grad_norm": 4.400974273681641,
      "learning_rate": 2.746370312237233e-05,
      "loss": 0.1108,
      "step": 51810
    },
    {
      "epoch": 2.9027812788841274,
      "grad_norm": 0.7635570168495178,
      "learning_rate": 2.744968888390605e-05,
      "loss": 0.2071,
      "step": 51820
    },
    {
      "epoch": 2.9033414559000645,
      "grad_norm": 3.0921590328216553,
      "learning_rate": 2.7435674645439768e-05,
      "loss": 0.1406,
      "step": 51830
    },
    {
      "epoch": 2.9039016329160017,
      "grad_norm": 6.086826324462891,
      "learning_rate": 2.7421660406973486e-05,
      "loss": 0.1582,
      "step": 51840
    },
    {
      "epoch": 2.9044618099319384,
      "grad_norm": 0.5869067311286926,
      "learning_rate": 2.7407646168507207e-05,
      "loss": 0.1849,
      "step": 51850
    },
    {
      "epoch": 2.9050219869478755,
      "grad_norm": 4.129146575927734,
      "learning_rate": 2.7393631930040925e-05,
      "loss": 0.1147,
      "step": 51860
    },
    {
      "epoch": 2.9055821639638126,
      "grad_norm": 0.7003001570701599,
      "learning_rate": 2.7379617691574644e-05,
      "loss": 0.1339,
      "step": 51870
    },
    {
      "epoch": 2.9061423409797493,
      "grad_norm": 1.9584834575653076,
      "learning_rate": 2.7365603453108358e-05,
      "loss": 0.1309,
      "step": 51880
    },
    {
      "epoch": 2.9067025179956865,
      "grad_norm": 0.8631673455238342,
      "learning_rate": 2.7351589214642076e-05,
      "loss": 0.1018,
      "step": 51890
    },
    {
      "epoch": 2.9072626950116236,
      "grad_norm": 1.2303054332733154,
      "learning_rate": 2.7337574976175795e-05,
      "loss": 0.1657,
      "step": 51900
    },
    {
      "epoch": 2.9078228720275607,
      "grad_norm": 0.6343650817871094,
      "learning_rate": 2.7323560737709513e-05,
      "loss": 0.1241,
      "step": 51910
    },
    {
      "epoch": 2.908383049043498,
      "grad_norm": 3.582379102706909,
      "learning_rate": 2.7309546499243234e-05,
      "loss": 0.1398,
      "step": 51920
    },
    {
      "epoch": 2.908943226059435,
      "grad_norm": 2.653726816177368,
      "learning_rate": 2.7295532260776952e-05,
      "loss": 0.0955,
      "step": 51930
    },
    {
      "epoch": 2.9095034030753717,
      "grad_norm": 4.882052898406982,
      "learning_rate": 2.728151802231067e-05,
      "loss": 0.0986,
      "step": 51940
    },
    {
      "epoch": 2.910063580091309,
      "grad_norm": 3.950385570526123,
      "learning_rate": 2.726750378384439e-05,
      "loss": 0.1374,
      "step": 51950
    },
    {
      "epoch": 2.910623757107246,
      "grad_norm": 2.18387508392334,
      "learning_rate": 2.7253489545378107e-05,
      "loss": 0.1044,
      "step": 51960
    },
    {
      "epoch": 2.9111839341231827,
      "grad_norm": 3.4354984760284424,
      "learning_rate": 2.723947530691182e-05,
      "loss": 0.2013,
      "step": 51970
    },
    {
      "epoch": 2.91174411113912,
      "grad_norm": 2.4607083797454834,
      "learning_rate": 2.722546106844554e-05,
      "loss": 0.1946,
      "step": 51980
    },
    {
      "epoch": 2.912304288155057,
      "grad_norm": 2.993844985961914,
      "learning_rate": 2.7211446829979258e-05,
      "loss": 0.0769,
      "step": 51990
    },
    {
      "epoch": 2.912864465170994,
      "grad_norm": 11.595376014709473,
      "learning_rate": 2.719743259151298e-05,
      "loss": 0.1227,
      "step": 52000
    },
    {
      "epoch": 2.913424642186931,
      "grad_norm": 3.0525684356689453,
      "learning_rate": 2.7183418353046697e-05,
      "loss": 0.1138,
      "step": 52010
    },
    {
      "epoch": 2.9139848192028683,
      "grad_norm": 5.3578715324401855,
      "learning_rate": 2.7169404114580416e-05,
      "loss": 0.1391,
      "step": 52020
    },
    {
      "epoch": 2.914544996218805,
      "grad_norm": 4.384127616882324,
      "learning_rate": 2.7155389876114134e-05,
      "loss": 0.1974,
      "step": 52030
    },
    {
      "epoch": 2.915105173234742,
      "grad_norm": 3.433870315551758,
      "learning_rate": 2.7141375637647852e-05,
      "loss": 0.2078,
      "step": 52040
    },
    {
      "epoch": 2.9156653502506793,
      "grad_norm": 1.6048420667648315,
      "learning_rate": 2.7127361399181573e-05,
      "loss": 0.1287,
      "step": 52050
    },
    {
      "epoch": 2.916225527266616,
      "grad_norm": 2.8292412757873535,
      "learning_rate": 2.7113347160715285e-05,
      "loss": 0.1026,
      "step": 52060
    },
    {
      "epoch": 2.916785704282553,
      "grad_norm": 3.7681844234466553,
      "learning_rate": 2.7099332922249003e-05,
      "loss": 0.1292,
      "step": 52070
    },
    {
      "epoch": 2.9173458812984903,
      "grad_norm": 2.7077224254608154,
      "learning_rate": 2.7085318683782724e-05,
      "loss": 0.1595,
      "step": 52080
    },
    {
      "epoch": 2.9179060583144274,
      "grad_norm": 1.0261939764022827,
      "learning_rate": 2.7071304445316442e-05,
      "loss": 0.123,
      "step": 52090
    },
    {
      "epoch": 2.9184662353303645,
      "grad_norm": 2.946192979812622,
      "learning_rate": 2.705729020685016e-05,
      "loss": 0.1501,
      "step": 52100
    },
    {
      "epoch": 2.9190264123463017,
      "grad_norm": 0.9717217087745667,
      "learning_rate": 2.704327596838388e-05,
      "loss": 0.1551,
      "step": 52110
    },
    {
      "epoch": 2.9195865893622384,
      "grad_norm": 2.8032963275909424,
      "learning_rate": 2.70292617299176e-05,
      "loss": 0.1174,
      "step": 52120
    },
    {
      "epoch": 2.9201467663781755,
      "grad_norm": 4.786061763763428,
      "learning_rate": 2.701524749145132e-05,
      "loss": 0.1258,
      "step": 52130
    },
    {
      "epoch": 2.9207069433941126,
      "grad_norm": 4.296040058135986,
      "learning_rate": 2.7001233252985036e-05,
      "loss": 0.1257,
      "step": 52140
    },
    {
      "epoch": 2.9212671204100493,
      "grad_norm": 0.7419175505638123,
      "learning_rate": 2.698721901451875e-05,
      "loss": 0.0989,
      "step": 52150
    },
    {
      "epoch": 2.9218272974259865,
      "grad_norm": 2.534698724746704,
      "learning_rate": 2.697320477605247e-05,
      "loss": 0.0806,
      "step": 52160
    },
    {
      "epoch": 2.9223874744419236,
      "grad_norm": 6.736429214477539,
      "learning_rate": 2.6959190537586187e-05,
      "loss": 0.2619,
      "step": 52170
    },
    {
      "epoch": 2.9229476514578607,
      "grad_norm": 7.67273473739624,
      "learning_rate": 2.6945176299119906e-05,
      "loss": 0.1835,
      "step": 52180
    },
    {
      "epoch": 2.923507828473798,
      "grad_norm": 0.6942657232284546,
      "learning_rate": 2.6931162060653624e-05,
      "loss": 0.111,
      "step": 52190
    },
    {
      "epoch": 2.924068005489735,
      "grad_norm": 2.8050711154937744,
      "learning_rate": 2.6917147822187345e-05,
      "loss": 0.1574,
      "step": 52200
    },
    {
      "epoch": 2.9246281825056717,
      "grad_norm": 2.1261260509490967,
      "learning_rate": 2.6903133583721063e-05,
      "loss": 0.1229,
      "step": 52210
    },
    {
      "epoch": 2.925188359521609,
      "grad_norm": 2.9913408756256104,
      "learning_rate": 2.688911934525478e-05,
      "loss": 0.1278,
      "step": 52220
    },
    {
      "epoch": 2.925748536537546,
      "grad_norm": 0.7422308325767517,
      "learning_rate": 2.68751051067885e-05,
      "loss": 0.0722,
      "step": 52230
    },
    {
      "epoch": 2.9263087135534827,
      "grad_norm": 2.383207082748413,
      "learning_rate": 2.6861090868322214e-05,
      "loss": 0.1442,
      "step": 52240
    },
    {
      "epoch": 2.92686889056942,
      "grad_norm": 3.7051777839660645,
      "learning_rate": 2.6847076629855933e-05,
      "loss": 0.178,
      "step": 52250
    },
    {
      "epoch": 2.927429067585357,
      "grad_norm": 4.203698635101318,
      "learning_rate": 2.683306239138965e-05,
      "loss": 0.1741,
      "step": 52260
    },
    {
      "epoch": 2.927989244601294,
      "grad_norm": 2.018707275390625,
      "learning_rate": 2.6819048152923372e-05,
      "loss": 0.1659,
      "step": 52270
    },
    {
      "epoch": 2.928549421617231,
      "grad_norm": 2.2208786010742188,
      "learning_rate": 2.680503391445709e-05,
      "loss": 0.1325,
      "step": 52280
    },
    {
      "epoch": 2.9291095986331683,
      "grad_norm": 4.978847980499268,
      "learning_rate": 2.679101967599081e-05,
      "loss": 0.1282,
      "step": 52290
    },
    {
      "epoch": 2.929669775649105,
      "grad_norm": 2.870818614959717,
      "learning_rate": 2.6777005437524527e-05,
      "loss": 0.1211,
      "step": 52300
    },
    {
      "epoch": 2.930229952665042,
      "grad_norm": 2.266103744506836,
      "learning_rate": 2.6762991199058245e-05,
      "loss": 0.1458,
      "step": 52310
    },
    {
      "epoch": 2.9307901296809793,
      "grad_norm": 0.7055703997612,
      "learning_rate": 2.6748976960591966e-05,
      "loss": 0.0799,
      "step": 52320
    },
    {
      "epoch": 2.931350306696916,
      "grad_norm": 2.4666686058044434,
      "learning_rate": 2.6734962722125678e-05,
      "loss": 0.1128,
      "step": 52330
    },
    {
      "epoch": 2.931910483712853,
      "grad_norm": 2.8275296688079834,
      "learning_rate": 2.6720948483659396e-05,
      "loss": 0.1249,
      "step": 52340
    },
    {
      "epoch": 2.9324706607287903,
      "grad_norm": 1.5977953672409058,
      "learning_rate": 2.6706934245193117e-05,
      "loss": 0.1045,
      "step": 52350
    },
    {
      "epoch": 2.9330308377447274,
      "grad_norm": 4.652602672576904,
      "learning_rate": 2.6692920006726835e-05,
      "loss": 0.2844,
      "step": 52360
    },
    {
      "epoch": 2.9335910147606645,
      "grad_norm": 4.040063858032227,
      "learning_rate": 2.6678905768260554e-05,
      "loss": 0.1304,
      "step": 52370
    },
    {
      "epoch": 2.9341511917766017,
      "grad_norm": 5.893504619598389,
      "learning_rate": 2.666489152979427e-05,
      "loss": 0.1425,
      "step": 52380
    },
    {
      "epoch": 2.9347113687925384,
      "grad_norm": 2.265599250793457,
      "learning_rate": 2.665087729132799e-05,
      "loss": 0.1196,
      "step": 52390
    },
    {
      "epoch": 2.9352715458084755,
      "grad_norm": 3.2534472942352295,
      "learning_rate": 2.663686305286171e-05,
      "loss": 0.1316,
      "step": 52400
    },
    {
      "epoch": 2.9358317228244126,
      "grad_norm": 5.309103965759277,
      "learning_rate": 2.662284881439543e-05,
      "loss": 0.2078,
      "step": 52410
    },
    {
      "epoch": 2.9363918998403493,
      "grad_norm": 0.6043572425842285,
      "learning_rate": 2.6608834575929148e-05,
      "loss": 0.0808,
      "step": 52420
    },
    {
      "epoch": 2.9369520768562865,
      "grad_norm": 1.775364875793457,
      "learning_rate": 2.6594820337462862e-05,
      "loss": 0.1042,
      "step": 52430
    },
    {
      "epoch": 2.9375122538722236,
      "grad_norm": 1.1540659666061401,
      "learning_rate": 2.658080609899658e-05,
      "loss": 0.0954,
      "step": 52440
    },
    {
      "epoch": 2.9380724308881607,
      "grad_norm": 2.5656216144561768,
      "learning_rate": 2.65667918605303e-05,
      "loss": 0.1397,
      "step": 52450
    },
    {
      "epoch": 2.938632607904098,
      "grad_norm": 1.340100884437561,
      "learning_rate": 2.6552777622064017e-05,
      "loss": 0.1448,
      "step": 52460
    },
    {
      "epoch": 2.939192784920035,
      "grad_norm": 1.3461741209030151,
      "learning_rate": 2.6538763383597738e-05,
      "loss": 0.171,
      "step": 52470
    },
    {
      "epoch": 2.9397529619359717,
      "grad_norm": 1.9928251504898071,
      "learning_rate": 2.6524749145131456e-05,
      "loss": 0.1446,
      "step": 52480
    },
    {
      "epoch": 2.940313138951909,
      "grad_norm": 0.6761856079101562,
      "learning_rate": 2.6510734906665174e-05,
      "loss": 0.1164,
      "step": 52490
    },
    {
      "epoch": 2.940873315967846,
      "grad_norm": 3.107042074203491,
      "learning_rate": 2.6496720668198893e-05,
      "loss": 0.1242,
      "step": 52500
    },
    {
      "epoch": 2.9414334929837826,
      "grad_norm": 1.2776250839233398,
      "learning_rate": 2.648270642973261e-05,
      "loss": 0.081,
      "step": 52510
    },
    {
      "epoch": 2.94199366999972,
      "grad_norm": 0.43117964267730713,
      "learning_rate": 2.6468692191266325e-05,
      "loss": 0.0853,
      "step": 52520
    },
    {
      "epoch": 2.942553847015657,
      "grad_norm": 4.882801055908203,
      "learning_rate": 2.6454677952800044e-05,
      "loss": 0.1814,
      "step": 52530
    },
    {
      "epoch": 2.943114024031594,
      "grad_norm": 3.316669464111328,
      "learning_rate": 2.6440663714333762e-05,
      "loss": 0.1015,
      "step": 52540
    },
    {
      "epoch": 2.943674201047531,
      "grad_norm": 2.0005226135253906,
      "learning_rate": 2.6426649475867483e-05,
      "loss": 0.0816,
      "step": 52550
    },
    {
      "epoch": 2.9442343780634683,
      "grad_norm": 0.8932665586471558,
      "learning_rate": 2.64126352374012e-05,
      "loss": 0.0985,
      "step": 52560
    },
    {
      "epoch": 2.944794555079405,
      "grad_norm": 2.375214099884033,
      "learning_rate": 2.639862099893492e-05,
      "loss": 0.0927,
      "step": 52570
    },
    {
      "epoch": 2.945354732095342,
      "grad_norm": 1.6184844970703125,
      "learning_rate": 2.6384606760468638e-05,
      "loss": 0.1227,
      "step": 52580
    },
    {
      "epoch": 2.9459149091112793,
      "grad_norm": 1.0525438785552979,
      "learning_rate": 2.637059252200236e-05,
      "loss": 0.0884,
      "step": 52590
    },
    {
      "epoch": 2.946475086127216,
      "grad_norm": 1.0384516716003418,
      "learning_rate": 2.6356578283536077e-05,
      "loss": 0.1001,
      "step": 52600
    },
    {
      "epoch": 2.947035263143153,
      "grad_norm": 2.9337456226348877,
      "learning_rate": 2.634256404506979e-05,
      "loss": 0.1583,
      "step": 52610
    },
    {
      "epoch": 2.9475954401590903,
      "grad_norm": 3.5739831924438477,
      "learning_rate": 2.632854980660351e-05,
      "loss": 0.0945,
      "step": 52620
    },
    {
      "epoch": 2.9481556171750274,
      "grad_norm": 1.8429404497146606,
      "learning_rate": 2.6314535568137228e-05,
      "loss": 0.0858,
      "step": 52630
    },
    {
      "epoch": 2.9487157941909645,
      "grad_norm": 3.9913394451141357,
      "learning_rate": 2.6300521329670946e-05,
      "loss": 0.1762,
      "step": 52640
    },
    {
      "epoch": 2.949275971206901,
      "grad_norm": 1.7913689613342285,
      "learning_rate": 2.6286507091204665e-05,
      "loss": 0.1195,
      "step": 52650
    },
    {
      "epoch": 2.9498361482228384,
      "grad_norm": 2.770353078842163,
      "learning_rate": 2.6272492852738383e-05,
      "loss": 0.1161,
      "step": 52660
    },
    {
      "epoch": 2.9503963252387755,
      "grad_norm": 2.929436445236206,
      "learning_rate": 2.6258478614272104e-05,
      "loss": 0.1262,
      "step": 52670
    },
    {
      "epoch": 2.9509565022547126,
      "grad_norm": 7.2177734375,
      "learning_rate": 2.6244464375805822e-05,
      "loss": 0.1956,
      "step": 52680
    },
    {
      "epoch": 2.9515166792706493,
      "grad_norm": 0.8239361643791199,
      "learning_rate": 2.623045013733954e-05,
      "loss": 0.1528,
      "step": 52690
    },
    {
      "epoch": 2.9520768562865864,
      "grad_norm": 1.9387266635894775,
      "learning_rate": 2.6216435898873255e-05,
      "loss": 0.1217,
      "step": 52700
    },
    {
      "epoch": 2.9526370333025236,
      "grad_norm": 2.2159860134124756,
      "learning_rate": 2.6202421660406973e-05,
      "loss": 0.1836,
      "step": 52710
    },
    {
      "epoch": 2.9531972103184607,
      "grad_norm": 4.047492027282715,
      "learning_rate": 2.618840742194069e-05,
      "loss": 0.1971,
      "step": 52720
    },
    {
      "epoch": 2.953757387334398,
      "grad_norm": 2.6261560916900635,
      "learning_rate": 2.617439318347441e-05,
      "loss": 0.1204,
      "step": 52730
    },
    {
      "epoch": 2.9543175643503345,
      "grad_norm": 3.659548759460449,
      "learning_rate": 2.6160378945008128e-05,
      "loss": 0.1359,
      "step": 52740
    },
    {
      "epoch": 2.9548777413662717,
      "grad_norm": 2.1104767322540283,
      "learning_rate": 2.614636470654185e-05,
      "loss": 0.1067,
      "step": 52750
    },
    {
      "epoch": 2.955437918382209,
      "grad_norm": 0.992748498916626,
      "learning_rate": 2.6132350468075567e-05,
      "loss": 0.1193,
      "step": 52760
    },
    {
      "epoch": 2.955998095398146,
      "grad_norm": 3.9019668102264404,
      "learning_rate": 2.6118336229609286e-05,
      "loss": 0.1311,
      "step": 52770
    },
    {
      "epoch": 2.9565582724140826,
      "grad_norm": 2.072565793991089,
      "learning_rate": 2.6104321991143004e-05,
      "loss": 0.1013,
      "step": 52780
    },
    {
      "epoch": 2.95711844943002,
      "grad_norm": 1.0993280410766602,
      "learning_rate": 2.609030775267672e-05,
      "loss": 0.0947,
      "step": 52790
    },
    {
      "epoch": 2.957678626445957,
      "grad_norm": 2.8385837078094482,
      "learning_rate": 2.6076293514210437e-05,
      "loss": 0.0973,
      "step": 52800
    },
    {
      "epoch": 2.958238803461894,
      "grad_norm": 0.8525103330612183,
      "learning_rate": 2.6062279275744155e-05,
      "loss": 0.1101,
      "step": 52810
    },
    {
      "epoch": 2.958798980477831,
      "grad_norm": 2.267629384994507,
      "learning_rate": 2.6048265037277876e-05,
      "loss": 0.1159,
      "step": 52820
    },
    {
      "epoch": 2.959359157493768,
      "grad_norm": 1.6384539604187012,
      "learning_rate": 2.6034250798811594e-05,
      "loss": 0.1338,
      "step": 52830
    },
    {
      "epoch": 2.959919334509705,
      "grad_norm": 0.761372447013855,
      "learning_rate": 2.6020236560345312e-05,
      "loss": 0.1312,
      "step": 52840
    },
    {
      "epoch": 2.960479511525642,
      "grad_norm": 3.980046510696411,
      "learning_rate": 2.600622232187903e-05,
      "loss": 0.1364,
      "step": 52850
    },
    {
      "epoch": 2.9610396885415793,
      "grad_norm": 1.2797343730926514,
      "learning_rate": 2.599220808341275e-05,
      "loss": 0.1746,
      "step": 52860
    },
    {
      "epoch": 2.961599865557516,
      "grad_norm": 6.0506181716918945,
      "learning_rate": 2.597819384494647e-05,
      "loss": 0.1546,
      "step": 52870
    },
    {
      "epoch": 2.962160042573453,
      "grad_norm": 1.8425099849700928,
      "learning_rate": 2.596417960648019e-05,
      "loss": 0.1701,
      "step": 52880
    },
    {
      "epoch": 2.9627202195893902,
      "grad_norm": 2.208956718444824,
      "learning_rate": 2.59501653680139e-05,
      "loss": 0.0926,
      "step": 52890
    },
    {
      "epoch": 2.9632803966053274,
      "grad_norm": 2.6380529403686523,
      "learning_rate": 2.593615112954762e-05,
      "loss": 0.1015,
      "step": 52900
    },
    {
      "epoch": 2.9638405736212645,
      "grad_norm": 2.3140532970428467,
      "learning_rate": 2.592213689108134e-05,
      "loss": 0.1036,
      "step": 52910
    },
    {
      "epoch": 2.964400750637201,
      "grad_norm": 6.079852104187012,
      "learning_rate": 2.5908122652615057e-05,
      "loss": 0.1068,
      "step": 52920
    },
    {
      "epoch": 2.9649609276531383,
      "grad_norm": 1.8776050806045532,
      "learning_rate": 2.5894108414148776e-05,
      "loss": 0.1661,
      "step": 52930
    },
    {
      "epoch": 2.9655211046690755,
      "grad_norm": 2.104400873184204,
      "learning_rate": 2.5880094175682497e-05,
      "loss": 0.0984,
      "step": 52940
    },
    {
      "epoch": 2.9660812816850126,
      "grad_norm": 1.642454743385315,
      "learning_rate": 2.5866079937216215e-05,
      "loss": 0.0915,
      "step": 52950
    },
    {
      "epoch": 2.9666414587009493,
      "grad_norm": 1.5627660751342773,
      "learning_rate": 2.5852065698749933e-05,
      "loss": 0.205,
      "step": 52960
    },
    {
      "epoch": 2.9672016357168864,
      "grad_norm": 4.796762466430664,
      "learning_rate": 2.583805146028365e-05,
      "loss": 0.0801,
      "step": 52970
    },
    {
      "epoch": 2.9677618127328236,
      "grad_norm": 5.529174327850342,
      "learning_rate": 2.5824037221817366e-05,
      "loss": 0.1246,
      "step": 52980
    },
    {
      "epoch": 2.9683219897487607,
      "grad_norm": 1.7217319011688232,
      "learning_rate": 2.5810022983351084e-05,
      "loss": 0.1424,
      "step": 52990
    },
    {
      "epoch": 2.968882166764698,
      "grad_norm": 1.7998346090316772,
      "learning_rate": 2.5796008744884803e-05,
      "loss": 0.1371,
      "step": 53000
    },
    {
      "epoch": 2.9694423437806345,
      "grad_norm": 0.8542033433914185,
      "learning_rate": 2.578199450641852e-05,
      "loss": 0.1716,
      "step": 53010
    },
    {
      "epoch": 2.9700025207965717,
      "grad_norm": 1.3040117025375366,
      "learning_rate": 2.5767980267952242e-05,
      "loss": 0.1389,
      "step": 53020
    },
    {
      "epoch": 2.970562697812509,
      "grad_norm": 0.8407422304153442,
      "learning_rate": 2.575396602948596e-05,
      "loss": 0.185,
      "step": 53030
    },
    {
      "epoch": 2.971122874828446,
      "grad_norm": 1.6766682863235474,
      "learning_rate": 2.573995179101968e-05,
      "loss": 0.1334,
      "step": 53040
    },
    {
      "epoch": 2.9716830518443826,
      "grad_norm": 2.841409683227539,
      "learning_rate": 2.5725937552553397e-05,
      "loss": 0.132,
      "step": 53050
    },
    {
      "epoch": 2.9722432288603198,
      "grad_norm": 1.1015440225601196,
      "learning_rate": 2.5711923314087115e-05,
      "loss": 0.1464,
      "step": 53060
    },
    {
      "epoch": 2.972803405876257,
      "grad_norm": 3.431633234024048,
      "learning_rate": 2.569790907562083e-05,
      "loss": 0.0756,
      "step": 53070
    },
    {
      "epoch": 2.973363582892194,
      "grad_norm": 3.314351797103882,
      "learning_rate": 2.5683894837154548e-05,
      "loss": 0.1065,
      "step": 53080
    },
    {
      "epoch": 2.973923759908131,
      "grad_norm": 4.510712623596191,
      "learning_rate": 2.5669880598688266e-05,
      "loss": 0.0945,
      "step": 53090
    },
    {
      "epoch": 2.974483936924068,
      "grad_norm": 7.261544227600098,
      "learning_rate": 2.5655866360221987e-05,
      "loss": 0.1228,
      "step": 53100
    },
    {
      "epoch": 2.975044113940005,
      "grad_norm": 1.3209103345870972,
      "learning_rate": 2.5641852121755705e-05,
      "loss": 0.0993,
      "step": 53110
    },
    {
      "epoch": 2.975604290955942,
      "grad_norm": 4.464733600616455,
      "learning_rate": 2.5627837883289424e-05,
      "loss": 0.1382,
      "step": 53120
    },
    {
      "epoch": 2.9761644679718793,
      "grad_norm": 2.9462687969207764,
      "learning_rate": 2.561382364482314e-05,
      "loss": 0.1804,
      "step": 53130
    },
    {
      "epoch": 2.976724644987816,
      "grad_norm": 1.881850242614746,
      "learning_rate": 2.5599809406356863e-05,
      "loss": 0.1201,
      "step": 53140
    },
    {
      "epoch": 2.977284822003753,
      "grad_norm": 3.530014991760254,
      "learning_rate": 2.558579516789058e-05,
      "loss": 0.1393,
      "step": 53150
    },
    {
      "epoch": 2.9778449990196902,
      "grad_norm": 4.776022911071777,
      "learning_rate": 2.5571780929424293e-05,
      "loss": 0.1219,
      "step": 53160
    },
    {
      "epoch": 2.9784051760356274,
      "grad_norm": 0.9366949200630188,
      "learning_rate": 2.5557766690958014e-05,
      "loss": 0.1715,
      "step": 53170
    },
    {
      "epoch": 2.9789653530515645,
      "grad_norm": 1.0952515602111816,
      "learning_rate": 2.5543752452491732e-05,
      "loss": 0.1025,
      "step": 53180
    },
    {
      "epoch": 2.979525530067501,
      "grad_norm": 2.2491672039031982,
      "learning_rate": 2.552973821402545e-05,
      "loss": 0.2048,
      "step": 53190
    },
    {
      "epoch": 2.9800857070834383,
      "grad_norm": 0.9563465714454651,
      "learning_rate": 2.551572397555917e-05,
      "loss": 0.084,
      "step": 53200
    },
    {
      "epoch": 2.9806458840993755,
      "grad_norm": 2.311110496520996,
      "learning_rate": 2.5501709737092887e-05,
      "loss": 0.1051,
      "step": 53210
    },
    {
      "epoch": 2.981206061115312,
      "grad_norm": 1.350879430770874,
      "learning_rate": 2.5487695498626608e-05,
      "loss": 0.1319,
      "step": 53220
    },
    {
      "epoch": 2.9817662381312493,
      "grad_norm": 1.490761160850525,
      "learning_rate": 2.5473681260160326e-05,
      "loss": 0.1275,
      "step": 53230
    },
    {
      "epoch": 2.9823264151471864,
      "grad_norm": 1.2468229532241821,
      "learning_rate": 2.5459667021694044e-05,
      "loss": 0.1045,
      "step": 53240
    },
    {
      "epoch": 2.9828865921631236,
      "grad_norm": 2.1774797439575195,
      "learning_rate": 2.544565278322776e-05,
      "loss": 0.1936,
      "step": 53250
    },
    {
      "epoch": 2.9834467691790607,
      "grad_norm": 0.8535531759262085,
      "learning_rate": 2.5431638544761477e-05,
      "loss": 0.1179,
      "step": 53260
    },
    {
      "epoch": 2.984006946194998,
      "grad_norm": 1.4701498746871948,
      "learning_rate": 2.5417624306295195e-05,
      "loss": 0.1269,
      "step": 53270
    },
    {
      "epoch": 2.9845671232109345,
      "grad_norm": 3.1574900150299072,
      "learning_rate": 2.5403610067828914e-05,
      "loss": 0.0995,
      "step": 53280
    },
    {
      "epoch": 2.9851273002268717,
      "grad_norm": 2.910651445388794,
      "learning_rate": 2.5389595829362635e-05,
      "loss": 0.165,
      "step": 53290
    },
    {
      "epoch": 2.985687477242809,
      "grad_norm": 3.9294557571411133,
      "learning_rate": 2.5375581590896353e-05,
      "loss": 0.1455,
      "step": 53300
    },
    {
      "epoch": 2.9862476542587455,
      "grad_norm": 1.9684947729110718,
      "learning_rate": 2.536156735243007e-05,
      "loss": 0.1323,
      "step": 53310
    },
    {
      "epoch": 2.9868078312746826,
      "grad_norm": 0.8751424551010132,
      "learning_rate": 2.534755311396379e-05,
      "loss": 0.1213,
      "step": 53320
    },
    {
      "epoch": 2.9873680082906198,
      "grad_norm": 2.9481489658355713,
      "learning_rate": 2.5333538875497508e-05,
      "loss": 0.1542,
      "step": 53330
    },
    {
      "epoch": 2.987928185306557,
      "grad_norm": 3.6141088008880615,
      "learning_rate": 2.5319524637031222e-05,
      "loss": 0.1148,
      "step": 53340
    },
    {
      "epoch": 2.988488362322494,
      "grad_norm": 1.44818115234375,
      "learning_rate": 2.530551039856494e-05,
      "loss": 0.1635,
      "step": 53350
    },
    {
      "epoch": 2.989048539338431,
      "grad_norm": 2.8802270889282227,
      "learning_rate": 2.529149616009866e-05,
      "loss": 0.1333,
      "step": 53360
    },
    {
      "epoch": 2.989608716354368,
      "grad_norm": 3.5044922828674316,
      "learning_rate": 2.527748192163238e-05,
      "loss": 0.136,
      "step": 53370
    },
    {
      "epoch": 2.990168893370305,
      "grad_norm": 2.8039865493774414,
      "learning_rate": 2.5263467683166098e-05,
      "loss": 0.106,
      "step": 53380
    },
    {
      "epoch": 2.990729070386242,
      "grad_norm": 0.9877342581748962,
      "learning_rate": 2.5249453444699816e-05,
      "loss": 0.1256,
      "step": 53390
    },
    {
      "epoch": 2.991289247402179,
      "grad_norm": 2.184114456176758,
      "learning_rate": 2.5235439206233535e-05,
      "loss": 0.129,
      "step": 53400
    },
    {
      "epoch": 2.991849424418116,
      "grad_norm": 2.4546074867248535,
      "learning_rate": 2.5221424967767253e-05,
      "loss": 0.1224,
      "step": 53410
    },
    {
      "epoch": 2.992409601434053,
      "grad_norm": 4.447304725646973,
      "learning_rate": 2.5207410729300974e-05,
      "loss": 0.1248,
      "step": 53420
    },
    {
      "epoch": 2.9929697784499902,
      "grad_norm": 2.6998066902160645,
      "learning_rate": 2.5193396490834692e-05,
      "loss": 0.1436,
      "step": 53430
    },
    {
      "epoch": 2.9935299554659274,
      "grad_norm": 2.803121328353882,
      "learning_rate": 2.5179382252368404e-05,
      "loss": 0.18,
      "step": 53440
    },
    {
      "epoch": 2.9940901324818645,
      "grad_norm": 2.2318387031555176,
      "learning_rate": 2.5165368013902125e-05,
      "loss": 0.1328,
      "step": 53450
    },
    {
      "epoch": 2.994650309497801,
      "grad_norm": 5.820152759552002,
      "learning_rate": 2.5151353775435843e-05,
      "loss": 0.108,
      "step": 53460
    },
    {
      "epoch": 2.9952104865137383,
      "grad_norm": 3.5309464931488037,
      "learning_rate": 2.513733953696956e-05,
      "loss": 0.1302,
      "step": 53470
    },
    {
      "epoch": 2.9957706635296755,
      "grad_norm": 2.732248544692993,
      "learning_rate": 2.512332529850328e-05,
      "loss": 0.1242,
      "step": 53480
    },
    {
      "epoch": 2.996330840545612,
      "grad_norm": 3.3845672607421875,
      "learning_rate": 2.5109311060037e-05,
      "loss": 0.1132,
      "step": 53490
    },
    {
      "epoch": 2.9968910175615493,
      "grad_norm": 2.0914456844329834,
      "learning_rate": 2.509529682157072e-05,
      "loss": 0.152,
      "step": 53500
    },
    {
      "epoch": 2.9974511945774864,
      "grad_norm": 5.868215560913086,
      "learning_rate": 2.5081282583104437e-05,
      "loss": 0.1752,
      "step": 53510
    },
    {
      "epoch": 2.9980113715934236,
      "grad_norm": 3.0461766719818115,
      "learning_rate": 2.5067268344638156e-05,
      "loss": 0.1049,
      "step": 53520
    },
    {
      "epoch": 2.9985715486093607,
      "grad_norm": 0.9160864353179932,
      "learning_rate": 2.505325410617187e-05,
      "loss": 0.123,
      "step": 53530
    },
    {
      "epoch": 2.999131725625298,
      "grad_norm": 2.683793306350708,
      "learning_rate": 2.503923986770559e-05,
      "loss": 0.1286,
      "step": 53540
    },
    {
      "epoch": 2.9996919026412345,
      "grad_norm": 0.7345919609069824,
      "learning_rate": 2.5025225629239307e-05,
      "loss": 0.0996,
      "step": 53550
    },
    {
      "epoch": 3.0002240708063748,
      "grad_norm": 4.109445095062256,
      "learning_rate": 2.5011211390773025e-05,
      "loss": 0.0912,
      "step": 53560
    },
    {
      "epoch": 3.000784247822312,
      "grad_norm": 4.216263771057129,
      "learning_rate": 2.4997197152306746e-05,
      "loss": 0.1295,
      "step": 53570
    },
    {
      "epoch": 3.001344424838249,
      "grad_norm": 6.195122718811035,
      "learning_rate": 2.4983182913840464e-05,
      "loss": 0.177,
      "step": 53580
    },
    {
      "epoch": 3.0019046018541857,
      "grad_norm": 3.657646656036377,
      "learning_rate": 2.4969168675374182e-05,
      "loss": 0.1783,
      "step": 53590
    },
    {
      "epoch": 3.002464778870123,
      "grad_norm": 2.342923641204834,
      "learning_rate": 2.4955154436907897e-05,
      "loss": 0.1211,
      "step": 53600
    },
    {
      "epoch": 3.00302495588606,
      "grad_norm": 1.3977926969528198,
      "learning_rate": 2.494114019844162e-05,
      "loss": 0.094,
      "step": 53610
    },
    {
      "epoch": 3.003585132901997,
      "grad_norm": 5.871997833251953,
      "learning_rate": 2.4927125959975337e-05,
      "loss": 0.1995,
      "step": 53620
    },
    {
      "epoch": 3.0041453099179343,
      "grad_norm": 3.616708517074585,
      "learning_rate": 2.4913111721509055e-05,
      "loss": 0.1849,
      "step": 53630
    },
    {
      "epoch": 3.004705486933871,
      "grad_norm": 4.743052959442139,
      "learning_rate": 2.4899097483042773e-05,
      "loss": 0.103,
      "step": 53640
    },
    {
      "epoch": 3.005265663949808,
      "grad_norm": 4.247009754180908,
      "learning_rate": 2.488508324457649e-05,
      "loss": 0.0906,
      "step": 53650
    },
    {
      "epoch": 3.0058258409657452,
      "grad_norm": 3.7021567821502686,
      "learning_rate": 2.487106900611021e-05,
      "loss": 0.1582,
      "step": 53660
    },
    {
      "epoch": 3.0063860179816824,
      "grad_norm": 1.2032060623168945,
      "learning_rate": 2.4857054767643927e-05,
      "loss": 0.126,
      "step": 53670
    },
    {
      "epoch": 3.006946194997619,
      "grad_norm": 1.829627513885498,
      "learning_rate": 2.4843040529177646e-05,
      "loss": 0.0988,
      "step": 53680
    },
    {
      "epoch": 3.007506372013556,
      "grad_norm": 0.8734883666038513,
      "learning_rate": 2.4829026290711364e-05,
      "loss": 0.15,
      "step": 53690
    },
    {
      "epoch": 3.0080665490294933,
      "grad_norm": 0.5645432472229004,
      "learning_rate": 2.4815012052245082e-05,
      "loss": 0.0841,
      "step": 53700
    },
    {
      "epoch": 3.0086267260454305,
      "grad_norm": 2.5564498901367188,
      "learning_rate": 2.48009978137788e-05,
      "loss": 0.1144,
      "step": 53710
    },
    {
      "epoch": 3.0091869030613676,
      "grad_norm": 5.254829406738281,
      "learning_rate": 2.4786983575312518e-05,
      "loss": 0.1298,
      "step": 53720
    },
    {
      "epoch": 3.0097470800773043,
      "grad_norm": 1.9618579149246216,
      "learning_rate": 2.4772969336846236e-05,
      "loss": 0.1044,
      "step": 53730
    },
    {
      "epoch": 3.0103072570932414,
      "grad_norm": 1.8325738906860352,
      "learning_rate": 2.4758955098379954e-05,
      "loss": 0.1273,
      "step": 53740
    },
    {
      "epoch": 3.0108674341091786,
      "grad_norm": 0.7739994525909424,
      "learning_rate": 2.4744940859913673e-05,
      "loss": 0.0798,
      "step": 53750
    },
    {
      "epoch": 3.0114276111251157,
      "grad_norm": 0.9140270948410034,
      "learning_rate": 2.473092662144739e-05,
      "loss": 0.0876,
      "step": 53760
    },
    {
      "epoch": 3.0119877881410524,
      "grad_norm": 2.50925874710083,
      "learning_rate": 2.4716912382981112e-05,
      "loss": 0.1399,
      "step": 53770
    },
    {
      "epoch": 3.0125479651569895,
      "grad_norm": 2.237044095993042,
      "learning_rate": 2.4702898144514827e-05,
      "loss": 0.1528,
      "step": 53780
    },
    {
      "epoch": 3.0131081421729267,
      "grad_norm": 0.8641389012336731,
      "learning_rate": 2.4688883906048545e-05,
      "loss": 0.1459,
      "step": 53790
    },
    {
      "epoch": 3.013668319188864,
      "grad_norm": 1.7418301105499268,
      "learning_rate": 2.4674869667582267e-05,
      "loss": 0.0871,
      "step": 53800
    },
    {
      "epoch": 3.014228496204801,
      "grad_norm": 3.5434441566467285,
      "learning_rate": 2.4660855429115985e-05,
      "loss": 0.1805,
      "step": 53810
    },
    {
      "epoch": 3.0147886732207376,
      "grad_norm": 2.3221545219421387,
      "learning_rate": 2.4646841190649703e-05,
      "loss": 0.0895,
      "step": 53820
    },
    {
      "epoch": 3.0153488502366748,
      "grad_norm": 1.353182077407837,
      "learning_rate": 2.4632826952183418e-05,
      "loss": 0.1072,
      "step": 53830
    },
    {
      "epoch": 3.015909027252612,
      "grad_norm": 3.004236936569214,
      "learning_rate": 2.461881271371714e-05,
      "loss": 0.1138,
      "step": 53840
    },
    {
      "epoch": 3.016469204268549,
      "grad_norm": 5.274458408355713,
      "learning_rate": 2.4604798475250857e-05,
      "loss": 0.1272,
      "step": 53850
    },
    {
      "epoch": 3.0170293812844857,
      "grad_norm": 2.515289545059204,
      "learning_rate": 2.4590784236784575e-05,
      "loss": 0.1077,
      "step": 53860
    },
    {
      "epoch": 3.017589558300423,
      "grad_norm": 2.2982540130615234,
      "learning_rate": 2.457676999831829e-05,
      "loss": 0.1414,
      "step": 53870
    },
    {
      "epoch": 3.01814973531636,
      "grad_norm": 3.758471727371216,
      "learning_rate": 2.456275575985201e-05,
      "loss": 0.1452,
      "step": 53880
    },
    {
      "epoch": 3.018709912332297,
      "grad_norm": 1.3096946477890015,
      "learning_rate": 2.454874152138573e-05,
      "loss": 0.1654,
      "step": 53890
    },
    {
      "epoch": 3.0192700893482343,
      "grad_norm": 3.2504966259002686,
      "learning_rate": 2.4534727282919448e-05,
      "loss": 0.0786,
      "step": 53900
    },
    {
      "epoch": 3.019830266364171,
      "grad_norm": 0.8001413941383362,
      "learning_rate": 2.4520713044453166e-05,
      "loss": 0.1162,
      "step": 53910
    },
    {
      "epoch": 3.020390443380108,
      "grad_norm": 5.017122745513916,
      "learning_rate": 2.4506698805986884e-05,
      "loss": 0.1112,
      "step": 53920
    },
    {
      "epoch": 3.0209506203960452,
      "grad_norm": 1.3495012521743774,
      "learning_rate": 2.4492684567520602e-05,
      "loss": 0.1237,
      "step": 53930
    },
    {
      "epoch": 3.0215107974119824,
      "grad_norm": 5.381363391876221,
      "learning_rate": 2.447867032905432e-05,
      "loss": 0.2051,
      "step": 53940
    },
    {
      "epoch": 3.022070974427919,
      "grad_norm": 1.4072505235671997,
      "learning_rate": 2.446465609058804e-05,
      "loss": 0.1599,
      "step": 53950
    },
    {
      "epoch": 3.022631151443856,
      "grad_norm": 2.0464911460876465,
      "learning_rate": 2.4450641852121757e-05,
      "loss": 0.0942,
      "step": 53960
    },
    {
      "epoch": 3.0231913284597933,
      "grad_norm": 1.2667251825332642,
      "learning_rate": 2.4436627613655475e-05,
      "loss": 0.118,
      "step": 53970
    },
    {
      "epoch": 3.0237515054757305,
      "grad_norm": 2.4918954372406006,
      "learning_rate": 2.4422613375189193e-05,
      "loss": 0.1577,
      "step": 53980
    },
    {
      "epoch": 3.024311682491667,
      "grad_norm": 1.7829809188842773,
      "learning_rate": 2.440859913672291e-05,
      "loss": 0.0839,
      "step": 53990
    },
    {
      "epoch": 3.0248718595076043,
      "grad_norm": 4.222012519836426,
      "learning_rate": 2.4394584898256633e-05,
      "loss": 0.1259,
      "step": 54000
    },
    {
      "epoch": 3.0254320365235414,
      "grad_norm": 3.549957513809204,
      "learning_rate": 2.4380570659790347e-05,
      "loss": 0.1162,
      "step": 54010
    },
    {
      "epoch": 3.0259922135394786,
      "grad_norm": 1.5443722009658813,
      "learning_rate": 2.4366556421324065e-05,
      "loss": 0.126,
      "step": 54020
    },
    {
      "epoch": 3.0265523905554157,
      "grad_norm": 5.253130912780762,
      "learning_rate": 2.4352542182857784e-05,
      "loss": 0.1305,
      "step": 54030
    },
    {
      "epoch": 3.0271125675713524,
      "grad_norm": 4.093721389770508,
      "learning_rate": 2.4338527944391505e-05,
      "loss": 0.1233,
      "step": 54040
    },
    {
      "epoch": 3.0276727445872895,
      "grad_norm": 6.092530250549316,
      "learning_rate": 2.432451370592522e-05,
      "loss": 0.1555,
      "step": 54050
    },
    {
      "epoch": 3.0282329216032267,
      "grad_norm": 1.0521169900894165,
      "learning_rate": 2.4310499467458938e-05,
      "loss": 0.1305,
      "step": 54060
    },
    {
      "epoch": 3.028793098619164,
      "grad_norm": 0.922304093837738,
      "learning_rate": 2.4296485228992656e-05,
      "loss": 0.154,
      "step": 54070
    },
    {
      "epoch": 3.0293532756351005,
      "grad_norm": 1.0272902250289917,
      "learning_rate": 2.4282470990526378e-05,
      "loss": 0.0906,
      "step": 54080
    },
    {
      "epoch": 3.0299134526510376,
      "grad_norm": 3.4000585079193115,
      "learning_rate": 2.4268456752060096e-05,
      "loss": 0.1492,
      "step": 54090
    },
    {
      "epoch": 3.0304736296669748,
      "grad_norm": 4.790282726287842,
      "learning_rate": 2.425444251359381e-05,
      "loss": 0.1912,
      "step": 54100
    },
    {
      "epoch": 3.031033806682912,
      "grad_norm": 1.4050931930541992,
      "learning_rate": 2.424042827512753e-05,
      "loss": 0.1147,
      "step": 54110
    },
    {
      "epoch": 3.031593983698849,
      "grad_norm": 3.4570043087005615,
      "learning_rate": 2.422641403666125e-05,
      "loss": 0.0968,
      "step": 54120
    },
    {
      "epoch": 3.0321541607147857,
      "grad_norm": 1.619696855545044,
      "learning_rate": 2.4212399798194968e-05,
      "loss": 0.1279,
      "step": 54130
    },
    {
      "epoch": 3.032714337730723,
      "grad_norm": 3.1771371364593506,
      "learning_rate": 2.4198385559728686e-05,
      "loss": 0.1122,
      "step": 54140
    },
    {
      "epoch": 3.03327451474666,
      "grad_norm": 0.7742523550987244,
      "learning_rate": 2.4184371321262405e-05,
      "loss": 0.1232,
      "step": 54150
    },
    {
      "epoch": 3.033834691762597,
      "grad_norm": 0.8851040005683899,
      "learning_rate": 2.4170357082796123e-05,
      "loss": 0.1552,
      "step": 54160
    },
    {
      "epoch": 3.034394868778534,
      "grad_norm": 1.668055534362793,
      "learning_rate": 2.415634284432984e-05,
      "loss": 0.0916,
      "step": 54170
    },
    {
      "epoch": 3.034955045794471,
      "grad_norm": 4.649234771728516,
      "learning_rate": 2.414232860586356e-05,
      "loss": 0.213,
      "step": 54180
    },
    {
      "epoch": 3.035515222810408,
      "grad_norm": 1.1703866720199585,
      "learning_rate": 2.4128314367397277e-05,
      "loss": 0.0886,
      "step": 54190
    },
    {
      "epoch": 3.0360753998263452,
      "grad_norm": 1.7622101306915283,
      "learning_rate": 2.4114300128930995e-05,
      "loss": 0.0988,
      "step": 54200
    },
    {
      "epoch": 3.0366355768422824,
      "grad_norm": 2.717449903488159,
      "learning_rate": 2.4100285890464713e-05,
      "loss": 0.112,
      "step": 54210
    },
    {
      "epoch": 3.037195753858219,
      "grad_norm": 3.8560755252838135,
      "learning_rate": 2.408627165199843e-05,
      "loss": 0.1263,
      "step": 54220
    },
    {
      "epoch": 3.037755930874156,
      "grad_norm": 3.0656676292419434,
      "learning_rate": 2.407225741353215e-05,
      "loss": 0.1014,
      "step": 54230
    },
    {
      "epoch": 3.0383161078900933,
      "grad_norm": 1.5970051288604736,
      "learning_rate": 2.4058243175065868e-05,
      "loss": 0.1363,
      "step": 54240
    },
    {
      "epoch": 3.0388762849060305,
      "grad_norm": 2.71791934967041,
      "learning_rate": 2.4044228936599586e-05,
      "loss": 0.1335,
      "step": 54250
    },
    {
      "epoch": 3.039436461921967,
      "grad_norm": 1.2181421518325806,
      "learning_rate": 2.4030214698133304e-05,
      "loss": 0.1052,
      "step": 54260
    },
    {
      "epoch": 3.0399966389379043,
      "grad_norm": 2.623666763305664,
      "learning_rate": 2.4016200459667022e-05,
      "loss": 0.1017,
      "step": 54270
    },
    {
      "epoch": 3.0405568159538414,
      "grad_norm": 3.9882147312164307,
      "learning_rate": 2.400218622120074e-05,
      "loss": 0.133,
      "step": 54280
    },
    {
      "epoch": 3.0411169929697786,
      "grad_norm": 2.6134421825408936,
      "learning_rate": 2.398817198273446e-05,
      "loss": 0.1443,
      "step": 54290
    },
    {
      "epoch": 3.0416771699857157,
      "grad_norm": 1.989517092704773,
      "learning_rate": 2.3974157744268177e-05,
      "loss": 0.16,
      "step": 54300
    },
    {
      "epoch": 3.0422373470016524,
      "grad_norm": 1.879837989807129,
      "learning_rate": 2.3960143505801898e-05,
      "loss": 0.1316,
      "step": 54310
    },
    {
      "epoch": 3.0427975240175895,
      "grad_norm": 0.6147890090942383,
      "learning_rate": 2.3946129267335616e-05,
      "loss": 0.0836,
      "step": 54320
    },
    {
      "epoch": 3.0433577010335267,
      "grad_norm": 4.537795066833496,
      "learning_rate": 2.393211502886933e-05,
      "loss": 0.1203,
      "step": 54330
    },
    {
      "epoch": 3.043917878049464,
      "grad_norm": 3.1102659702301025,
      "learning_rate": 2.391810079040305e-05,
      "loss": 0.0948,
      "step": 54340
    },
    {
      "epoch": 3.0444780550654005,
      "grad_norm": 3.211921215057373,
      "learning_rate": 2.390408655193677e-05,
      "loss": 0.0983,
      "step": 54350
    },
    {
      "epoch": 3.0450382320813376,
      "grad_norm": 3.427422285079956,
      "learning_rate": 2.389007231347049e-05,
      "loss": 0.0973,
      "step": 54360
    },
    {
      "epoch": 3.0455984090972748,
      "grad_norm": 0.6965519189834595,
      "learning_rate": 2.3876058075004207e-05,
      "loss": 0.1314,
      "step": 54370
    },
    {
      "epoch": 3.046158586113212,
      "grad_norm": 1.9770938158035278,
      "learning_rate": 2.386204383653792e-05,
      "loss": 0.0932,
      "step": 54380
    },
    {
      "epoch": 3.046718763129149,
      "grad_norm": 0.6617208123207092,
      "learning_rate": 2.3848029598071643e-05,
      "loss": 0.1511,
      "step": 54390
    },
    {
      "epoch": 3.0472789401450857,
      "grad_norm": 1.5311946868896484,
      "learning_rate": 2.383401535960536e-05,
      "loss": 0.1252,
      "step": 54400
    },
    {
      "epoch": 3.047839117161023,
      "grad_norm": 0.48279911279678345,
      "learning_rate": 2.382000112113908e-05,
      "loss": 0.1309,
      "step": 54410
    },
    {
      "epoch": 3.04839929417696,
      "grad_norm": 4.182973861694336,
      "learning_rate": 2.3805986882672794e-05,
      "loss": 0.1926,
      "step": 54420
    },
    {
      "epoch": 3.048959471192897,
      "grad_norm": 1.3409037590026855,
      "learning_rate": 2.3791972644206516e-05,
      "loss": 0.1172,
      "step": 54430
    },
    {
      "epoch": 3.049519648208834,
      "grad_norm": 1.6911393404006958,
      "learning_rate": 2.3777958405740234e-05,
      "loss": 0.0958,
      "step": 54440
    },
    {
      "epoch": 3.050079825224771,
      "grad_norm": 1.2886006832122803,
      "learning_rate": 2.3763944167273952e-05,
      "loss": 0.1091,
      "step": 54450
    },
    {
      "epoch": 3.050640002240708,
      "grad_norm": 8.021417617797852,
      "learning_rate": 2.374992992880767e-05,
      "loss": 0.1235,
      "step": 54460
    },
    {
      "epoch": 3.051200179256645,
      "grad_norm": 3.2630205154418945,
      "learning_rate": 2.3735915690341388e-05,
      "loss": 0.1069,
      "step": 54470
    },
    {
      "epoch": 3.0517603562725824,
      "grad_norm": 4.275548458099365,
      "learning_rate": 2.3721901451875106e-05,
      "loss": 0.1039,
      "step": 54480
    },
    {
      "epoch": 3.052320533288519,
      "grad_norm": 1.4042481184005737,
      "learning_rate": 2.3707887213408824e-05,
      "loss": 0.0836,
      "step": 54490
    },
    {
      "epoch": 3.052880710304456,
      "grad_norm": 1.157631754875183,
      "learning_rate": 2.3693872974942543e-05,
      "loss": 0.1469,
      "step": 54500
    },
    {
      "epoch": 3.0534408873203933,
      "grad_norm": 0.7053259015083313,
      "learning_rate": 2.367985873647626e-05,
      "loss": 0.163,
      "step": 54510
    },
    {
      "epoch": 3.0540010643363305,
      "grad_norm": 4.212541580200195,
      "learning_rate": 2.366584449800998e-05,
      "loss": 0.0966,
      "step": 54520
    },
    {
      "epoch": 3.054561241352267,
      "grad_norm": 1.7524189949035645,
      "learning_rate": 2.3651830259543697e-05,
      "loss": 0.1189,
      "step": 54530
    },
    {
      "epoch": 3.0551214183682043,
      "grad_norm": 4.581737041473389,
      "learning_rate": 2.3637816021077415e-05,
      "loss": 0.1196,
      "step": 54540
    },
    {
      "epoch": 3.0556815953841414,
      "grad_norm": 2.2530219554901123,
      "learning_rate": 2.3623801782611137e-05,
      "loss": 0.1073,
      "step": 54550
    },
    {
      "epoch": 3.0562417724000785,
      "grad_norm": 2.3263745307922363,
      "learning_rate": 2.360978754414485e-05,
      "loss": 0.1118,
      "step": 54560
    },
    {
      "epoch": 3.0568019494160152,
      "grad_norm": 5.9837727546691895,
      "learning_rate": 2.359577330567857e-05,
      "loss": 0.1112,
      "step": 54570
    },
    {
      "epoch": 3.0573621264319524,
      "grad_norm": 1.2784432172775269,
      "learning_rate": 2.3581759067212288e-05,
      "loss": 0.0764,
      "step": 54580
    },
    {
      "epoch": 3.0579223034478895,
      "grad_norm": 2.7105438709259033,
      "learning_rate": 2.356774482874601e-05,
      "loss": 0.1097,
      "step": 54590
    },
    {
      "epoch": 3.0584824804638266,
      "grad_norm": 2.5915884971618652,
      "learning_rate": 2.3553730590279724e-05,
      "loss": 0.1573,
      "step": 54600
    },
    {
      "epoch": 3.059042657479764,
      "grad_norm": 1.8976025581359863,
      "learning_rate": 2.3539716351813442e-05,
      "loss": 0.1034,
      "step": 54610
    },
    {
      "epoch": 3.0596028344957005,
      "grad_norm": 4.524542331695557,
      "learning_rate": 2.352570211334716e-05,
      "loss": 0.0883,
      "step": 54620
    },
    {
      "epoch": 3.0601630115116376,
      "grad_norm": 1.3166927099227905,
      "learning_rate": 2.351168787488088e-05,
      "loss": 0.1303,
      "step": 54630
    },
    {
      "epoch": 3.0607231885275747,
      "grad_norm": 0.9125118255615234,
      "learning_rate": 2.34976736364146e-05,
      "loss": 0.2356,
      "step": 54640
    },
    {
      "epoch": 3.061283365543512,
      "grad_norm": 5.233238220214844,
      "learning_rate": 2.3483659397948315e-05,
      "loss": 0.1129,
      "step": 54650
    },
    {
      "epoch": 3.0618435425594486,
      "grad_norm": 6.625055313110352,
      "learning_rate": 2.3469645159482036e-05,
      "loss": 0.1001,
      "step": 54660
    },
    {
      "epoch": 3.0624037195753857,
      "grad_norm": 3.1902949810028076,
      "learning_rate": 2.3455630921015754e-05,
      "loss": 0.1025,
      "step": 54670
    },
    {
      "epoch": 3.062963896591323,
      "grad_norm": 6.081116199493408,
      "learning_rate": 2.3441616682549472e-05,
      "loss": 0.1487,
      "step": 54680
    },
    {
      "epoch": 3.06352407360726,
      "grad_norm": 0.9575780630111694,
      "learning_rate": 2.342760244408319e-05,
      "loss": 0.0845,
      "step": 54690
    },
    {
      "epoch": 3.064084250623197,
      "grad_norm": 3.8113505840301514,
      "learning_rate": 2.341358820561691e-05,
      "loss": 0.1046,
      "step": 54700
    },
    {
      "epoch": 3.064644427639134,
      "grad_norm": 1.5720696449279785,
      "learning_rate": 2.3399573967150627e-05,
      "loss": 0.0902,
      "step": 54710
    },
    {
      "epoch": 3.065204604655071,
      "grad_norm": 0.8319259881973267,
      "learning_rate": 2.3385559728684345e-05,
      "loss": 0.0937,
      "step": 54720
    },
    {
      "epoch": 3.065764781671008,
      "grad_norm": 1.363315224647522,
      "learning_rate": 2.3371545490218063e-05,
      "loss": 0.0841,
      "step": 54730
    },
    {
      "epoch": 3.066324958686945,
      "grad_norm": 3.580889940261841,
      "learning_rate": 2.335753125175178e-05,
      "loss": 0.107,
      "step": 54740
    },
    {
      "epoch": 3.066885135702882,
      "grad_norm": 3.4486677646636963,
      "learning_rate": 2.33435170132855e-05,
      "loss": 0.1011,
      "step": 54750
    },
    {
      "epoch": 3.067445312718819,
      "grad_norm": 5.243958950042725,
      "learning_rate": 2.3329502774819217e-05,
      "loss": 0.0952,
      "step": 54760
    },
    {
      "epoch": 3.068005489734756,
      "grad_norm": 2.8273351192474365,
      "learning_rate": 2.3315488536352935e-05,
      "loss": 0.0839,
      "step": 54770
    },
    {
      "epoch": 3.0685656667506933,
      "grad_norm": 2.2422890663146973,
      "learning_rate": 2.3301474297886654e-05,
      "loss": 0.1178,
      "step": 54780
    },
    {
      "epoch": 3.0691258437666304,
      "grad_norm": 3.1781883239746094,
      "learning_rate": 2.3287460059420372e-05,
      "loss": 0.12,
      "step": 54790
    },
    {
      "epoch": 3.069686020782567,
      "grad_norm": 2.4764020442962646,
      "learning_rate": 2.327344582095409e-05,
      "loss": 0.1191,
      "step": 54800
    },
    {
      "epoch": 3.0702461977985043,
      "grad_norm": 3.2737622261047363,
      "learning_rate": 2.3259431582487808e-05,
      "loss": 0.1521,
      "step": 54810
    },
    {
      "epoch": 3.0708063748144414,
      "grad_norm": 0.8177244663238525,
      "learning_rate": 2.3245417344021526e-05,
      "loss": 0.1491,
      "step": 54820
    },
    {
      "epoch": 3.0713665518303785,
      "grad_norm": 2.038532257080078,
      "learning_rate": 2.3231403105555244e-05,
      "loss": 0.1563,
      "step": 54830
    },
    {
      "epoch": 3.0719267288463152,
      "grad_norm": 2.314429998397827,
      "learning_rate": 2.3217388867088962e-05,
      "loss": 0.0825,
      "step": 54840
    },
    {
      "epoch": 3.0724869058622524,
      "grad_norm": 2.944373369216919,
      "learning_rate": 2.320337462862268e-05,
      "loss": 0.1852,
      "step": 54850
    },
    {
      "epoch": 3.0730470828781895,
      "grad_norm": 1.0323607921600342,
      "learning_rate": 2.3189360390156402e-05,
      "loss": 0.0813,
      "step": 54860
    },
    {
      "epoch": 3.0736072598941266,
      "grad_norm": 1.1726969480514526,
      "learning_rate": 2.317534615169012e-05,
      "loss": 0.1018,
      "step": 54870
    },
    {
      "epoch": 3.0741674369100638,
      "grad_norm": 5.068790435791016,
      "learning_rate": 2.3161331913223835e-05,
      "loss": 0.1179,
      "step": 54880
    },
    {
      "epoch": 3.0747276139260005,
      "grad_norm": 3.5182855129241943,
      "learning_rate": 2.3147317674757553e-05,
      "loss": 0.1329,
      "step": 54890
    },
    {
      "epoch": 3.0752877909419376,
      "grad_norm": 2.8615612983703613,
      "learning_rate": 2.3133303436291275e-05,
      "loss": 0.114,
      "step": 54900
    },
    {
      "epoch": 3.0758479679578747,
      "grad_norm": 2.7827882766723633,
      "learning_rate": 2.3119289197824993e-05,
      "loss": 0.1213,
      "step": 54910
    },
    {
      "epoch": 3.076408144973812,
      "grad_norm": 4.016505241394043,
      "learning_rate": 2.310527495935871e-05,
      "loss": 0.1382,
      "step": 54920
    },
    {
      "epoch": 3.0769683219897486,
      "grad_norm": 3.1352221965789795,
      "learning_rate": 2.3091260720892426e-05,
      "loss": 0.144,
      "step": 54930
    },
    {
      "epoch": 3.0775284990056857,
      "grad_norm": 2.529103994369507,
      "learning_rate": 2.3077246482426147e-05,
      "loss": 0.1384,
      "step": 54940
    },
    {
      "epoch": 3.078088676021623,
      "grad_norm": 4.861976146697998,
      "learning_rate": 2.3063232243959865e-05,
      "loss": 0.109,
      "step": 54950
    },
    {
      "epoch": 3.07864885303756,
      "grad_norm": 1.5076935291290283,
      "learning_rate": 2.3049218005493583e-05,
      "loss": 0.1191,
      "step": 54960
    },
    {
      "epoch": 3.079209030053497,
      "grad_norm": 3.2023611068725586,
      "learning_rate": 2.3035203767027298e-05,
      "loss": 0.1162,
      "step": 54970
    },
    {
      "epoch": 3.079769207069434,
      "grad_norm": 2.440152883529663,
      "learning_rate": 2.302118952856102e-05,
      "loss": 0.0823,
      "step": 54980
    },
    {
      "epoch": 3.080329384085371,
      "grad_norm": 3.9189417362213135,
      "learning_rate": 2.3007175290094738e-05,
      "loss": 0.0844,
      "step": 54990
    },
    {
      "epoch": 3.080889561101308,
      "grad_norm": 2.0253162384033203,
      "learning_rate": 2.2993161051628456e-05,
      "loss": 0.1283,
      "step": 55000
    },
    {
      "epoch": 3.081449738117245,
      "grad_norm": 2.7008328437805176,
      "learning_rate": 2.2979146813162174e-05,
      "loss": 0.1089,
      "step": 55010
    },
    {
      "epoch": 3.082009915133182,
      "grad_norm": 2.033079147338867,
      "learning_rate": 2.2965132574695892e-05,
      "loss": 0.1329,
      "step": 55020
    },
    {
      "epoch": 3.082570092149119,
      "grad_norm": 5.183932781219482,
      "learning_rate": 2.295111833622961e-05,
      "loss": 0.1259,
      "step": 55030
    },
    {
      "epoch": 3.083130269165056,
      "grad_norm": 3.911198139190674,
      "learning_rate": 2.293710409776333e-05,
      "loss": 0.1498,
      "step": 55040
    },
    {
      "epoch": 3.0836904461809933,
      "grad_norm": 1.6355847120285034,
      "learning_rate": 2.2923089859297047e-05,
      "loss": 0.1027,
      "step": 55050
    },
    {
      "epoch": 3.0842506231969304,
      "grad_norm": 0.9047352075576782,
      "learning_rate": 2.2909075620830765e-05,
      "loss": 0.0921,
      "step": 55060
    },
    {
      "epoch": 3.084810800212867,
      "grad_norm": 1.6550672054290771,
      "learning_rate": 2.2895061382364483e-05,
      "loss": 0.1289,
      "step": 55070
    },
    {
      "epoch": 3.0853709772288043,
      "grad_norm": 3.586916446685791,
      "learning_rate": 2.28810471438982e-05,
      "loss": 0.1396,
      "step": 55080
    },
    {
      "epoch": 3.0859311542447414,
      "grad_norm": 2.965926170349121,
      "learning_rate": 2.286703290543192e-05,
      "loss": 0.0998,
      "step": 55090
    },
    {
      "epoch": 3.0864913312606785,
      "grad_norm": 0.9056788086891174,
      "learning_rate": 2.285301866696564e-05,
      "loss": 0.1136,
      "step": 55100
    },
    {
      "epoch": 3.0870515082766152,
      "grad_norm": 2.503602981567383,
      "learning_rate": 2.2839004428499355e-05,
      "loss": 0.1839,
      "step": 55110
    },
    {
      "epoch": 3.0876116852925524,
      "grad_norm": 3.5502231121063232,
      "learning_rate": 2.2824990190033073e-05,
      "loss": 0.1421,
      "step": 55120
    },
    {
      "epoch": 3.0881718623084895,
      "grad_norm": 1.17445707321167,
      "learning_rate": 2.281097595156679e-05,
      "loss": 0.1629,
      "step": 55130
    },
    {
      "epoch": 3.0887320393244266,
      "grad_norm": 4.947197437286377,
      "learning_rate": 2.2796961713100513e-05,
      "loss": 0.1241,
      "step": 55140
    },
    {
      "epoch": 3.0892922163403638,
      "grad_norm": 3.057756185531616,
      "learning_rate": 2.278294747463423e-05,
      "loss": 0.1235,
      "step": 55150
    },
    {
      "epoch": 3.0898523933563005,
      "grad_norm": 4.609675407409668,
      "learning_rate": 2.2768933236167946e-05,
      "loss": 0.1354,
      "step": 55160
    },
    {
      "epoch": 3.0904125703722376,
      "grad_norm": 3.3490049839019775,
      "learning_rate": 2.2754918997701664e-05,
      "loss": 0.1415,
      "step": 55170
    },
    {
      "epoch": 3.0909727473881747,
      "grad_norm": 1.942665457725525,
      "learning_rate": 2.2740904759235386e-05,
      "loss": 0.127,
      "step": 55180
    },
    {
      "epoch": 3.091532924404112,
      "grad_norm": 0.9513453245162964,
      "learning_rate": 2.2726890520769104e-05,
      "loss": 0.0822,
      "step": 55190
    },
    {
      "epoch": 3.0920931014200486,
      "grad_norm": 2.819061279296875,
      "learning_rate": 2.271287628230282e-05,
      "loss": 0.0958,
      "step": 55200
    },
    {
      "epoch": 3.0926532784359857,
      "grad_norm": 4.37202787399292,
      "learning_rate": 2.269886204383654e-05,
      "loss": 0.1202,
      "step": 55210
    },
    {
      "epoch": 3.093213455451923,
      "grad_norm": 3.57766056060791,
      "learning_rate": 2.2684847805370258e-05,
      "loss": 0.0986,
      "step": 55220
    },
    {
      "epoch": 3.09377363246786,
      "grad_norm": 7.368700981140137,
      "learning_rate": 2.2670833566903976e-05,
      "loss": 0.1593,
      "step": 55230
    },
    {
      "epoch": 3.094333809483797,
      "grad_norm": 1.0435802936553955,
      "learning_rate": 2.2656819328437694e-05,
      "loss": 0.2252,
      "step": 55240
    },
    {
      "epoch": 3.094893986499734,
      "grad_norm": 1.0109609365463257,
      "learning_rate": 2.2642805089971413e-05,
      "loss": 0.094,
      "step": 55250
    },
    {
      "epoch": 3.095454163515671,
      "grad_norm": 3.809436559677124,
      "learning_rate": 2.262879085150513e-05,
      "loss": 0.0894,
      "step": 55260
    },
    {
      "epoch": 3.096014340531608,
      "grad_norm": 5.098217964172363,
      "learning_rate": 2.261477661303885e-05,
      "loss": 0.1208,
      "step": 55270
    },
    {
      "epoch": 3.096574517547545,
      "grad_norm": 2.193373203277588,
      "learning_rate": 2.2600762374572567e-05,
      "loss": 0.1119,
      "step": 55280
    },
    {
      "epoch": 3.097134694563482,
      "grad_norm": 4.3491740226745605,
      "learning_rate": 2.2586748136106285e-05,
      "loss": 0.1279,
      "step": 55290
    },
    {
      "epoch": 3.097694871579419,
      "grad_norm": 3.0670166015625,
      "learning_rate": 2.2572733897640003e-05,
      "loss": 0.1235,
      "step": 55300
    },
    {
      "epoch": 3.098255048595356,
      "grad_norm": 4.59352970123291,
      "learning_rate": 2.255871965917372e-05,
      "loss": 0.1405,
      "step": 55310
    },
    {
      "epoch": 3.0988152256112933,
      "grad_norm": 3.237725019454956,
      "learning_rate": 2.254470542070744e-05,
      "loss": 0.0991,
      "step": 55320
    },
    {
      "epoch": 3.09937540262723,
      "grad_norm": 1.1116524934768677,
      "learning_rate": 2.2530691182241158e-05,
      "loss": 0.1017,
      "step": 55330
    },
    {
      "epoch": 3.099935579643167,
      "grad_norm": 3.0166006088256836,
      "learning_rate": 2.2516676943774876e-05,
      "loss": 0.0829,
      "step": 55340
    },
    {
      "epoch": 3.1004957566591043,
      "grad_norm": 5.192481517791748,
      "learning_rate": 2.2502662705308594e-05,
      "loss": 0.1142,
      "step": 55350
    },
    {
      "epoch": 3.1010559336750414,
      "grad_norm": 2.062300682067871,
      "learning_rate": 2.2488648466842312e-05,
      "loss": 0.0959,
      "step": 55360
    },
    {
      "epoch": 3.1016161106909785,
      "grad_norm": 2.5990302562713623,
      "learning_rate": 2.2474634228376034e-05,
      "loss": 0.1096,
      "step": 55370
    },
    {
      "epoch": 3.1021762877069152,
      "grad_norm": 2.680227279663086,
      "learning_rate": 2.2460619989909748e-05,
      "loss": 0.1052,
      "step": 55380
    },
    {
      "epoch": 3.1027364647228524,
      "grad_norm": 1.0319006443023682,
      "learning_rate": 2.2446605751443466e-05,
      "loss": 0.1111,
      "step": 55390
    },
    {
      "epoch": 3.1032966417387895,
      "grad_norm": 0.7382081151008606,
      "learning_rate": 2.2432591512977185e-05,
      "loss": 0.1027,
      "step": 55400
    },
    {
      "epoch": 3.1038568187547266,
      "grad_norm": 2.9500036239624023,
      "learning_rate": 2.2418577274510906e-05,
      "loss": 0.1173,
      "step": 55410
    },
    {
      "epoch": 3.1044169957706633,
      "grad_norm": 3.5321481227874756,
      "learning_rate": 2.2404563036044624e-05,
      "loss": 0.1366,
      "step": 55420
    },
    {
      "epoch": 3.1049771727866005,
      "grad_norm": 4.439854621887207,
      "learning_rate": 2.239054879757834e-05,
      "loss": 0.0922,
      "step": 55430
    },
    {
      "epoch": 3.1055373498025376,
      "grad_norm": 2.2569732666015625,
      "learning_rate": 2.2376534559112057e-05,
      "loss": 0.1151,
      "step": 55440
    },
    {
      "epoch": 3.1060975268184747,
      "grad_norm": 3.814770221710205,
      "learning_rate": 2.236252032064578e-05,
      "loss": 0.1213,
      "step": 55450
    },
    {
      "epoch": 3.106657703834412,
      "grad_norm": 2.672595262527466,
      "learning_rate": 2.2348506082179497e-05,
      "loss": 0.1071,
      "step": 55460
    },
    {
      "epoch": 3.1072178808503486,
      "grad_norm": 4.261481761932373,
      "learning_rate": 2.2334491843713215e-05,
      "loss": 0.1136,
      "step": 55470
    },
    {
      "epoch": 3.1077780578662857,
      "grad_norm": 1.1819673776626587,
      "learning_rate": 2.232047760524693e-05,
      "loss": 0.0828,
      "step": 55480
    },
    {
      "epoch": 3.108338234882223,
      "grad_norm": 2.0513970851898193,
      "learning_rate": 2.230646336678065e-05,
      "loss": 0.1075,
      "step": 55490
    },
    {
      "epoch": 3.10889841189816,
      "grad_norm": 0.4974457323551178,
      "learning_rate": 2.229244912831437e-05,
      "loss": 0.1063,
      "step": 55500
    },
    {
      "epoch": 3.1094585889140967,
      "grad_norm": 5.183100700378418,
      "learning_rate": 2.2278434889848087e-05,
      "loss": 0.1315,
      "step": 55510
    },
    {
      "epoch": 3.110018765930034,
      "grad_norm": 1.232892394065857,
      "learning_rate": 2.2264420651381802e-05,
      "loss": 0.1293,
      "step": 55520
    },
    {
      "epoch": 3.110578942945971,
      "grad_norm": 1.1586551666259766,
      "learning_rate": 2.2250406412915524e-05,
      "loss": 0.1522,
      "step": 55530
    },
    {
      "epoch": 3.111139119961908,
      "grad_norm": 2.438718318939209,
      "learning_rate": 2.2236392174449242e-05,
      "loss": 0.1328,
      "step": 55540
    },
    {
      "epoch": 3.111699296977845,
      "grad_norm": 1.8851256370544434,
      "learning_rate": 2.222237793598296e-05,
      "loss": 0.1979,
      "step": 55550
    },
    {
      "epoch": 3.112259473993782,
      "grad_norm": 2.548067331314087,
      "learning_rate": 2.2208363697516678e-05,
      "loss": 0.1226,
      "step": 55560
    },
    {
      "epoch": 3.112819651009719,
      "grad_norm": 3.2368199825286865,
      "learning_rate": 2.2194349459050396e-05,
      "loss": 0.0963,
      "step": 55570
    },
    {
      "epoch": 3.113379828025656,
      "grad_norm": 2.4484503269195557,
      "learning_rate": 2.2180335220584114e-05,
      "loss": 0.1714,
      "step": 55580
    },
    {
      "epoch": 3.1139400050415933,
      "grad_norm": 0.8202876448631287,
      "learning_rate": 2.2166320982117832e-05,
      "loss": 0.1049,
      "step": 55590
    },
    {
      "epoch": 3.11450018205753,
      "grad_norm": 2.5321333408355713,
      "learning_rate": 2.215230674365155e-05,
      "loss": 0.0838,
      "step": 55600
    },
    {
      "epoch": 3.115060359073467,
      "grad_norm": 6.743050575256348,
      "learning_rate": 2.213829250518527e-05,
      "loss": 0.1058,
      "step": 55610
    },
    {
      "epoch": 3.1156205360894043,
      "grad_norm": 2.9986369609832764,
      "learning_rate": 2.2124278266718987e-05,
      "loss": 0.1049,
      "step": 55620
    },
    {
      "epoch": 3.1161807131053414,
      "grad_norm": 1.9297388792037964,
      "learning_rate": 2.2110264028252705e-05,
      "loss": 0.1034,
      "step": 55630
    },
    {
      "epoch": 3.1167408901212785,
      "grad_norm": 3.0622787475585938,
      "learning_rate": 2.2096249789786423e-05,
      "loss": 0.1226,
      "step": 55640
    },
    {
      "epoch": 3.117301067137215,
      "grad_norm": 2.9473166465759277,
      "learning_rate": 2.2082235551320145e-05,
      "loss": 0.1347,
      "step": 55650
    },
    {
      "epoch": 3.1178612441531524,
      "grad_norm": 2.59700608253479,
      "learning_rate": 2.206822131285386e-05,
      "loss": 0.1126,
      "step": 55660
    },
    {
      "epoch": 3.1184214211690895,
      "grad_norm": 1.0166456699371338,
      "learning_rate": 2.2054207074387577e-05,
      "loss": 0.081,
      "step": 55670
    },
    {
      "epoch": 3.1189815981850266,
      "grad_norm": 1.5733484029769897,
      "learning_rate": 2.2040192835921296e-05,
      "loss": 0.1137,
      "step": 55680
    },
    {
      "epoch": 3.1195417752009633,
      "grad_norm": 4.875693321228027,
      "learning_rate": 2.2026178597455017e-05,
      "loss": 0.0936,
      "step": 55690
    },
    {
      "epoch": 3.1201019522169005,
      "grad_norm": 3.2061920166015625,
      "learning_rate": 2.2012164358988735e-05,
      "loss": 0.1097,
      "step": 55700
    },
    {
      "epoch": 3.1206621292328376,
      "grad_norm": 2.212390899658203,
      "learning_rate": 2.199815012052245e-05,
      "loss": 0.1653,
      "step": 55710
    },
    {
      "epoch": 3.1212223062487747,
      "grad_norm": 1.4606473445892334,
      "learning_rate": 2.198413588205617e-05,
      "loss": 0.0979,
      "step": 55720
    },
    {
      "epoch": 3.121782483264712,
      "grad_norm": 1.762686848640442,
      "learning_rate": 2.197012164358989e-05,
      "loss": 0.1169,
      "step": 55730
    },
    {
      "epoch": 3.1223426602806486,
      "grad_norm": 1.6055787801742554,
      "learning_rate": 2.1956107405123608e-05,
      "loss": 0.1103,
      "step": 55740
    },
    {
      "epoch": 3.1229028372965857,
      "grad_norm": 3.3398501873016357,
      "learning_rate": 2.1942093166657322e-05,
      "loss": 0.1909,
      "step": 55750
    },
    {
      "epoch": 3.123463014312523,
      "grad_norm": 4.4095001220703125,
      "learning_rate": 2.1928078928191044e-05,
      "loss": 0.1381,
      "step": 55760
    },
    {
      "epoch": 3.12402319132846,
      "grad_norm": 2.304872989654541,
      "learning_rate": 2.1914064689724762e-05,
      "loss": 0.1179,
      "step": 55770
    },
    {
      "epoch": 3.1245833683443966,
      "grad_norm": 3.7668068408966064,
      "learning_rate": 2.190005045125848e-05,
      "loss": 0.1044,
      "step": 55780
    },
    {
      "epoch": 3.125143545360334,
      "grad_norm": 3.6457135677337646,
      "learning_rate": 2.18860362127922e-05,
      "loss": 0.141,
      "step": 55790
    },
    {
      "epoch": 3.125703722376271,
      "grad_norm": 0.5945393443107605,
      "learning_rate": 2.1872021974325917e-05,
      "loss": 0.1504,
      "step": 55800
    },
    {
      "epoch": 3.126263899392208,
      "grad_norm": 1.6678683757781982,
      "learning_rate": 2.1858007735859635e-05,
      "loss": 0.1238,
      "step": 55810
    },
    {
      "epoch": 3.1268240764081447,
      "grad_norm": 1.814181923866272,
      "learning_rate": 2.1843993497393353e-05,
      "loss": 0.1505,
      "step": 55820
    },
    {
      "epoch": 3.127384253424082,
      "grad_norm": 2.248438596725464,
      "learning_rate": 2.182997925892707e-05,
      "loss": 0.1116,
      "step": 55830
    },
    {
      "epoch": 3.127944430440019,
      "grad_norm": 1.3581005334854126,
      "learning_rate": 2.181596502046079e-05,
      "loss": 0.0837,
      "step": 55840
    },
    {
      "epoch": 3.128504607455956,
      "grad_norm": 2.7011661529541016,
      "learning_rate": 2.1801950781994507e-05,
      "loss": 0.0863,
      "step": 55850
    },
    {
      "epoch": 3.1290647844718933,
      "grad_norm": 1.2985190153121948,
      "learning_rate": 2.1787936543528225e-05,
      "loss": 0.1039,
      "step": 55860
    },
    {
      "epoch": 3.12962496148783,
      "grad_norm": 1.282371997833252,
      "learning_rate": 2.1773922305061943e-05,
      "loss": 0.1025,
      "step": 55870
    },
    {
      "epoch": 3.130185138503767,
      "grad_norm": 1.2559629678726196,
      "learning_rate": 2.1759908066595665e-05,
      "loss": 0.0949,
      "step": 55880
    },
    {
      "epoch": 3.1307453155197043,
      "grad_norm": 3.5958282947540283,
      "learning_rate": 2.174589382812938e-05,
      "loss": 0.0998,
      "step": 55890
    },
    {
      "epoch": 3.1313054925356414,
      "grad_norm": 2.7159855365753174,
      "learning_rate": 2.1731879589663098e-05,
      "loss": 0.1329,
      "step": 55900
    },
    {
      "epoch": 3.131865669551578,
      "grad_norm": 1.9449199438095093,
      "learning_rate": 2.1717865351196816e-05,
      "loss": 0.0932,
      "step": 55910
    },
    {
      "epoch": 3.132425846567515,
      "grad_norm": 0.8969306349754333,
      "learning_rate": 2.1703851112730537e-05,
      "loss": 0.1941,
      "step": 55920
    },
    {
      "epoch": 3.1329860235834524,
      "grad_norm": 1.7504048347473145,
      "learning_rate": 2.1689836874264252e-05,
      "loss": 0.0758,
      "step": 55930
    },
    {
      "epoch": 3.1335462005993895,
      "grad_norm": 1.2992026805877686,
      "learning_rate": 2.167582263579797e-05,
      "loss": 0.1576,
      "step": 55940
    },
    {
      "epoch": 3.1341063776153266,
      "grad_norm": 1.84380042552948,
      "learning_rate": 2.166180839733169e-05,
      "loss": 0.1932,
      "step": 55950
    },
    {
      "epoch": 3.1346665546312633,
      "grad_norm": 1.0933716297149658,
      "learning_rate": 2.164779415886541e-05,
      "loss": 0.1181,
      "step": 55960
    },
    {
      "epoch": 3.1352267316472004,
      "grad_norm": 1.3032786846160889,
      "learning_rate": 2.1633779920399128e-05,
      "loss": 0.0956,
      "step": 55970
    },
    {
      "epoch": 3.1357869086631376,
      "grad_norm": 3.0369884967803955,
      "learning_rate": 2.1619765681932843e-05,
      "loss": 0.1387,
      "step": 55980
    },
    {
      "epoch": 3.1363470856790747,
      "grad_norm": 1.6067464351654053,
      "learning_rate": 2.160575144346656e-05,
      "loss": 0.1148,
      "step": 55990
    },
    {
      "epoch": 3.1369072626950114,
      "grad_norm": 1.59515380859375,
      "learning_rate": 2.1591737205000283e-05,
      "loss": 0.1327,
      "step": 56000
    },
    {
      "epoch": 3.1374674397109485,
      "grad_norm": 2.311535596847534,
      "learning_rate": 2.1577722966534e-05,
      "loss": 0.1503,
      "step": 56010
    },
    {
      "epoch": 3.1380276167268857,
      "grad_norm": 1.020022988319397,
      "learning_rate": 2.156370872806772e-05,
      "loss": 0.0714,
      "step": 56020
    },
    {
      "epoch": 3.138587793742823,
      "grad_norm": 5.004942893981934,
      "learning_rate": 2.1549694489601434e-05,
      "loss": 0.0976,
      "step": 56030
    },
    {
      "epoch": 3.13914797075876,
      "grad_norm": 2.115391969680786,
      "learning_rate": 2.1535680251135155e-05,
      "loss": 0.0743,
      "step": 56040
    },
    {
      "epoch": 3.1397081477746966,
      "grad_norm": 0.928164541721344,
      "learning_rate": 2.1521666012668873e-05,
      "loss": 0.1214,
      "step": 56050
    },
    {
      "epoch": 3.140268324790634,
      "grad_norm": 4.470887184143066,
      "learning_rate": 2.150765177420259e-05,
      "loss": 0.0964,
      "step": 56060
    },
    {
      "epoch": 3.140828501806571,
      "grad_norm": 1.384392499923706,
      "learning_rate": 2.149363753573631e-05,
      "loss": 0.1268,
      "step": 56070
    },
    {
      "epoch": 3.141388678822508,
      "grad_norm": 4.233158588409424,
      "learning_rate": 2.1479623297270028e-05,
      "loss": 0.2028,
      "step": 56080
    },
    {
      "epoch": 3.1419488558384447,
      "grad_norm": 0.48807117342948914,
      "learning_rate": 2.1465609058803746e-05,
      "loss": 0.1342,
      "step": 56090
    },
    {
      "epoch": 3.142509032854382,
      "grad_norm": 4.476644992828369,
      "learning_rate": 2.1451594820337464e-05,
      "loss": 0.1209,
      "step": 56100
    },
    {
      "epoch": 3.143069209870319,
      "grad_norm": 1.2659268379211426,
      "learning_rate": 2.1437580581871182e-05,
      "loss": 0.1282,
      "step": 56110
    },
    {
      "epoch": 3.143629386886256,
      "grad_norm": 2.915066957473755,
      "learning_rate": 2.14235663434049e-05,
      "loss": 0.1208,
      "step": 56120
    },
    {
      "epoch": 3.1441895639021933,
      "grad_norm": 0.6817322373390198,
      "learning_rate": 2.1409552104938618e-05,
      "loss": 0.0703,
      "step": 56130
    },
    {
      "epoch": 3.14474974091813,
      "grad_norm": 1.623593807220459,
      "learning_rate": 2.1395537866472336e-05,
      "loss": 0.2517,
      "step": 56140
    },
    {
      "epoch": 3.145309917934067,
      "grad_norm": 2.234550952911377,
      "learning_rate": 2.1381523628006055e-05,
      "loss": 0.1532,
      "step": 56150
    },
    {
      "epoch": 3.1458700949500042,
      "grad_norm": 1.7298516035079956,
      "learning_rate": 2.1367509389539773e-05,
      "loss": 0.0979,
      "step": 56160
    },
    {
      "epoch": 3.1464302719659414,
      "grad_norm": 0.7793868780136108,
      "learning_rate": 2.135349515107349e-05,
      "loss": 0.0927,
      "step": 56170
    },
    {
      "epoch": 3.146990448981878,
      "grad_norm": 1.931880235671997,
      "learning_rate": 2.133948091260721e-05,
      "loss": 0.1587,
      "step": 56180
    },
    {
      "epoch": 3.147550625997815,
      "grad_norm": 1.5113427639007568,
      "learning_rate": 2.1325466674140927e-05,
      "loss": 0.1025,
      "step": 56190
    },
    {
      "epoch": 3.1481108030137523,
      "grad_norm": 1.8222951889038086,
      "learning_rate": 2.131145243567465e-05,
      "loss": 0.118,
      "step": 56200
    },
    {
      "epoch": 3.1486709800296895,
      "grad_norm": 3.0264031887054443,
      "learning_rate": 2.1297438197208363e-05,
      "loss": 0.1072,
      "step": 56210
    },
    {
      "epoch": 3.1492311570456266,
      "grad_norm": 1.1168534755706787,
      "learning_rate": 2.128342395874208e-05,
      "loss": 0.1046,
      "step": 56220
    },
    {
      "epoch": 3.1497913340615633,
      "grad_norm": 2.180495023727417,
      "learning_rate": 2.1269409720275803e-05,
      "loss": 0.1164,
      "step": 56230
    },
    {
      "epoch": 3.1503515110775004,
      "grad_norm": 2.040621519088745,
      "learning_rate": 2.125539548180952e-05,
      "loss": 0.1523,
      "step": 56240
    },
    {
      "epoch": 3.1509116880934376,
      "grad_norm": 3.5749971866607666,
      "learning_rate": 2.124138124334324e-05,
      "loss": 0.137,
      "step": 56250
    },
    {
      "epoch": 3.1514718651093747,
      "grad_norm": 1.5404596328735352,
      "learning_rate": 2.1227367004876954e-05,
      "loss": 0.0904,
      "step": 56260
    },
    {
      "epoch": 3.1520320421253114,
      "grad_norm": 1.294776439666748,
      "learning_rate": 2.1213352766410675e-05,
      "loss": 0.1359,
      "step": 56270
    },
    {
      "epoch": 3.1525922191412485,
      "grad_norm": 3.54451584815979,
      "learning_rate": 2.1199338527944394e-05,
      "loss": 0.0894,
      "step": 56280
    },
    {
      "epoch": 3.1531523961571857,
      "grad_norm": 2.968557119369507,
      "learning_rate": 2.1185324289478112e-05,
      "loss": 0.1335,
      "step": 56290
    },
    {
      "epoch": 3.153712573173123,
      "grad_norm": 0.6859267950057983,
      "learning_rate": 2.1171310051011826e-05,
      "loss": 0.1089,
      "step": 56300
    },
    {
      "epoch": 3.15427275018906,
      "grad_norm": 3.623361587524414,
      "learning_rate": 2.1157295812545548e-05,
      "loss": 0.1834,
      "step": 56310
    },
    {
      "epoch": 3.1548329272049966,
      "grad_norm": 1.9898438453674316,
      "learning_rate": 2.1143281574079266e-05,
      "loss": 0.2008,
      "step": 56320
    },
    {
      "epoch": 3.1553931042209338,
      "grad_norm": 1.4460452795028687,
      "learning_rate": 2.1129267335612984e-05,
      "loss": 0.1186,
      "step": 56330
    },
    {
      "epoch": 3.155953281236871,
      "grad_norm": 4.817111492156982,
      "learning_rate": 2.1115253097146702e-05,
      "loss": 0.1303,
      "step": 56340
    },
    {
      "epoch": 3.156513458252808,
      "grad_norm": 0.8014888763427734,
      "learning_rate": 2.110123885868042e-05,
      "loss": 0.1061,
      "step": 56350
    },
    {
      "epoch": 3.1570736352687447,
      "grad_norm": 2.003835439682007,
      "learning_rate": 2.108722462021414e-05,
      "loss": 0.0859,
      "step": 56360
    },
    {
      "epoch": 3.157633812284682,
      "grad_norm": 1.01546311378479,
      "learning_rate": 2.1073210381747857e-05,
      "loss": 0.0951,
      "step": 56370
    },
    {
      "epoch": 3.158193989300619,
      "grad_norm": 6.791284561157227,
      "learning_rate": 2.1059196143281575e-05,
      "loss": 0.1702,
      "step": 56380
    },
    {
      "epoch": 3.158754166316556,
      "grad_norm": 2.313201427459717,
      "learning_rate": 2.1045181904815293e-05,
      "loss": 0.0921,
      "step": 56390
    },
    {
      "epoch": 3.1593143433324933,
      "grad_norm": 4.510307788848877,
      "learning_rate": 2.103116766634901e-05,
      "loss": 0.0885,
      "step": 56400
    },
    {
      "epoch": 3.15987452034843,
      "grad_norm": 2.987748861312866,
      "learning_rate": 2.101715342788273e-05,
      "loss": 0.1376,
      "step": 56410
    },
    {
      "epoch": 3.160434697364367,
      "grad_norm": 2.2546279430389404,
      "learning_rate": 2.1003139189416447e-05,
      "loss": 0.0876,
      "step": 56420
    },
    {
      "epoch": 3.1609948743803042,
      "grad_norm": 3.838392972946167,
      "learning_rate": 2.098912495095017e-05,
      "loss": 0.1011,
      "step": 56430
    },
    {
      "epoch": 3.1615550513962414,
      "grad_norm": 1.7265052795410156,
      "learning_rate": 2.0975110712483884e-05,
      "loss": 0.1791,
      "step": 56440
    },
    {
      "epoch": 3.162115228412178,
      "grad_norm": 2.463095188140869,
      "learning_rate": 2.0961096474017602e-05,
      "loss": 0.1165,
      "step": 56450
    },
    {
      "epoch": 3.162675405428115,
      "grad_norm": 4.383352279663086,
      "learning_rate": 2.094708223555132e-05,
      "loss": 0.1254,
      "step": 56460
    },
    {
      "epoch": 3.1632355824440523,
      "grad_norm": 6.182142734527588,
      "learning_rate": 2.093306799708504e-05,
      "loss": 0.1072,
      "step": 56470
    },
    {
      "epoch": 3.1637957594599895,
      "grad_norm": 3.2343735694885254,
      "learning_rate": 2.091905375861876e-05,
      "loss": 0.1,
      "step": 56480
    },
    {
      "epoch": 3.1643559364759266,
      "grad_norm": 1.505041480064392,
      "learning_rate": 2.0905039520152474e-05,
      "loss": 0.126,
      "step": 56490
    },
    {
      "epoch": 3.1649161134918633,
      "grad_norm": 2.2336580753326416,
      "learning_rate": 2.0891025281686192e-05,
      "loss": 0.1105,
      "step": 56500
    },
    {
      "epoch": 3.1654762905078004,
      "grad_norm": 1.6716870069503784,
      "learning_rate": 2.0877011043219914e-05,
      "loss": 0.1349,
      "step": 56510
    },
    {
      "epoch": 3.1660364675237376,
      "grad_norm": 4.3295722007751465,
      "learning_rate": 2.0862996804753632e-05,
      "loss": 0.1263,
      "step": 56520
    },
    {
      "epoch": 3.1665966445396747,
      "grad_norm": 2.129791259765625,
      "learning_rate": 2.0848982566287347e-05,
      "loss": 0.0852,
      "step": 56530
    },
    {
      "epoch": 3.1671568215556114,
      "grad_norm": 3.853132486343384,
      "learning_rate": 2.0834968327821065e-05,
      "loss": 0.0797,
      "step": 56540
    },
    {
      "epoch": 3.1677169985715485,
      "grad_norm": 3.1384434700012207,
      "learning_rate": 2.0820954089354787e-05,
      "loss": 0.1558,
      "step": 56550
    },
    {
      "epoch": 3.1682771755874857,
      "grad_norm": 2.6740739345550537,
      "learning_rate": 2.0806939850888505e-05,
      "loss": 0.1003,
      "step": 56560
    },
    {
      "epoch": 3.168837352603423,
      "grad_norm": 2.199265718460083,
      "learning_rate": 2.0792925612422223e-05,
      "loss": 0.1007,
      "step": 56570
    },
    {
      "epoch": 3.16939752961936,
      "grad_norm": 4.231700897216797,
      "learning_rate": 2.077891137395594e-05,
      "loss": 0.1293,
      "step": 56580
    },
    {
      "epoch": 3.1699577066352966,
      "grad_norm": 1.08293616771698,
      "learning_rate": 2.076489713548966e-05,
      "loss": 0.0698,
      "step": 56590
    },
    {
      "epoch": 3.1705178836512338,
      "grad_norm": 1.2993459701538086,
      "learning_rate": 2.0750882897023377e-05,
      "loss": 0.2759,
      "step": 56600
    },
    {
      "epoch": 3.171078060667171,
      "grad_norm": 4.21776008605957,
      "learning_rate": 2.0736868658557095e-05,
      "loss": 0.1441,
      "step": 56610
    },
    {
      "epoch": 3.171638237683108,
      "grad_norm": 1.807826042175293,
      "learning_rate": 2.0722854420090813e-05,
      "loss": 0.0976,
      "step": 56620
    },
    {
      "epoch": 3.1721984146990447,
      "grad_norm": 2.518108367919922,
      "learning_rate": 2.070884018162453e-05,
      "loss": 0.0879,
      "step": 56630
    },
    {
      "epoch": 3.172758591714982,
      "grad_norm": 1.501465916633606,
      "learning_rate": 2.069482594315825e-05,
      "loss": 0.1007,
      "step": 56640
    },
    {
      "epoch": 3.173318768730919,
      "grad_norm": 1.7637486457824707,
      "learning_rate": 2.0680811704691968e-05,
      "loss": 0.0959,
      "step": 56650
    },
    {
      "epoch": 3.173878945746856,
      "grad_norm": 1.6547306776046753,
      "learning_rate": 2.0666797466225686e-05,
      "loss": 0.0818,
      "step": 56660
    },
    {
      "epoch": 3.1744391227627933,
      "grad_norm": 3.707839012145996,
      "learning_rate": 2.0652783227759404e-05,
      "loss": 0.1305,
      "step": 56670
    },
    {
      "epoch": 3.17499929977873,
      "grad_norm": 2.20678973197937,
      "learning_rate": 2.0638768989293122e-05,
      "loss": 0.0918,
      "step": 56680
    },
    {
      "epoch": 3.175559476794667,
      "grad_norm": 3.690878391265869,
      "learning_rate": 2.062475475082684e-05,
      "loss": 0.1173,
      "step": 56690
    },
    {
      "epoch": 3.1761196538106042,
      "grad_norm": 4.017864227294922,
      "learning_rate": 2.061074051236056e-05,
      "loss": 0.1437,
      "step": 56700
    },
    {
      "epoch": 3.1766798308265414,
      "grad_norm": 6.044910430908203,
      "learning_rate": 2.0596726273894277e-05,
      "loss": 0.1282,
      "step": 56710
    },
    {
      "epoch": 3.177240007842478,
      "grad_norm": 3.5115606784820557,
      "learning_rate": 2.0582712035427995e-05,
      "loss": 0.1014,
      "step": 56720
    },
    {
      "epoch": 3.177800184858415,
      "grad_norm": 3.2961678504943848,
      "learning_rate": 2.0568697796961713e-05,
      "loss": 0.1426,
      "step": 56730
    },
    {
      "epoch": 3.1783603618743523,
      "grad_norm": 1.7053030729293823,
      "learning_rate": 2.0554683558495434e-05,
      "loss": 0.1193,
      "step": 56740
    },
    {
      "epoch": 3.1789205388902895,
      "grad_norm": 3.759735584259033,
      "learning_rate": 2.0540669320029153e-05,
      "loss": 0.1108,
      "step": 56750
    },
    {
      "epoch": 3.179480715906226,
      "grad_norm": 1.2838134765625,
      "learning_rate": 2.0526655081562867e-05,
      "loss": 0.142,
      "step": 56760
    },
    {
      "epoch": 3.1800408929221633,
      "grad_norm": 2.653351306915283,
      "learning_rate": 2.0512640843096585e-05,
      "loss": 0.1154,
      "step": 56770
    },
    {
      "epoch": 3.1806010699381004,
      "grad_norm": 1.5448834896087646,
      "learning_rate": 2.0498626604630307e-05,
      "loss": 0.1814,
      "step": 56780
    },
    {
      "epoch": 3.1811612469540376,
      "grad_norm": 0.7874035239219666,
      "learning_rate": 2.0484612366164025e-05,
      "loss": 0.0975,
      "step": 56790
    },
    {
      "epoch": 3.1817214239699747,
      "grad_norm": 1.2776601314544678,
      "learning_rate": 2.0470598127697743e-05,
      "loss": 0.0716,
      "step": 56800
    },
    {
      "epoch": 3.1822816009859114,
      "grad_norm": 0.5238962769508362,
      "learning_rate": 2.0456583889231458e-05,
      "loss": 0.0782,
      "step": 56810
    },
    {
      "epoch": 3.1828417780018485,
      "grad_norm": 1.2713968753814697,
      "learning_rate": 2.044256965076518e-05,
      "loss": 0.1069,
      "step": 56820
    },
    {
      "epoch": 3.1834019550177857,
      "grad_norm": 6.199702739715576,
      "learning_rate": 2.0428555412298898e-05,
      "loss": 0.1191,
      "step": 56830
    },
    {
      "epoch": 3.183962132033723,
      "grad_norm": 0.8428279161453247,
      "learning_rate": 2.0414541173832616e-05,
      "loss": 0.0944,
      "step": 56840
    },
    {
      "epoch": 3.1845223090496595,
      "grad_norm": 5.755338191986084,
      "learning_rate": 2.040052693536633e-05,
      "loss": 0.1096,
      "step": 56850
    },
    {
      "epoch": 3.1850824860655966,
      "grad_norm": 0.9583078622817993,
      "learning_rate": 2.0386512696900052e-05,
      "loss": 0.1305,
      "step": 56860
    },
    {
      "epoch": 3.1856426630815338,
      "grad_norm": 0.9987177848815918,
      "learning_rate": 2.037249845843377e-05,
      "loss": 0.1489,
      "step": 56870
    },
    {
      "epoch": 3.186202840097471,
      "grad_norm": 1.4616833925247192,
      "learning_rate": 2.0358484219967488e-05,
      "loss": 0.1375,
      "step": 56880
    },
    {
      "epoch": 3.186763017113408,
      "grad_norm": 5.1997246742248535,
      "learning_rate": 2.0344469981501206e-05,
      "loss": 0.1259,
      "step": 56890
    },
    {
      "epoch": 3.1873231941293447,
      "grad_norm": 3.0532050132751465,
      "learning_rate": 2.0330455743034925e-05,
      "loss": 0.1257,
      "step": 56900
    },
    {
      "epoch": 3.187883371145282,
      "grad_norm": 2.8008339405059814,
      "learning_rate": 2.0316441504568643e-05,
      "loss": 0.1183,
      "step": 56910
    },
    {
      "epoch": 3.188443548161219,
      "grad_norm": 0.6475566029548645,
      "learning_rate": 2.030242726610236e-05,
      "loss": 0.1101,
      "step": 56920
    },
    {
      "epoch": 3.189003725177156,
      "grad_norm": 1.2658259868621826,
      "learning_rate": 2.028841302763608e-05,
      "loss": 0.0883,
      "step": 56930
    },
    {
      "epoch": 3.189563902193093,
      "grad_norm": 3.574582576751709,
      "learning_rate": 2.0274398789169797e-05,
      "loss": 0.1064,
      "step": 56940
    },
    {
      "epoch": 3.19012407920903,
      "grad_norm": 1.588620901107788,
      "learning_rate": 2.0260384550703515e-05,
      "loss": 0.1009,
      "step": 56950
    },
    {
      "epoch": 3.190684256224967,
      "grad_norm": 0.7522919178009033,
      "learning_rate": 2.0246370312237233e-05,
      "loss": 0.1004,
      "step": 56960
    },
    {
      "epoch": 3.1912444332409042,
      "grad_norm": 3.1406469345092773,
      "learning_rate": 2.023235607377095e-05,
      "loss": 0.0934,
      "step": 56970
    },
    {
      "epoch": 3.1918046102568414,
      "grad_norm": 0.4717213809490204,
      "learning_rate": 2.0218341835304673e-05,
      "loss": 0.0873,
      "step": 56980
    },
    {
      "epoch": 3.192364787272778,
      "grad_norm": 1.7149766683578491,
      "learning_rate": 2.0204327596838388e-05,
      "loss": 0.0756,
      "step": 56990
    },
    {
      "epoch": 3.192924964288715,
      "grad_norm": 5.539967060089111,
      "learning_rate": 2.0190313358372106e-05,
      "loss": 0.2024,
      "step": 57000
    },
    {
      "epoch": 3.1934851413046523,
      "grad_norm": 4.3766279220581055,
      "learning_rate": 2.0176299119905824e-05,
      "loss": 0.1491,
      "step": 57010
    },
    {
      "epoch": 3.1940453183205895,
      "grad_norm": 5.269170761108398,
      "learning_rate": 2.0162284881439545e-05,
      "loss": 0.1878,
      "step": 57020
    },
    {
      "epoch": 3.194605495336526,
      "grad_norm": 1.9942564964294434,
      "learning_rate": 2.0148270642973264e-05,
      "loss": 0.0958,
      "step": 57030
    },
    {
      "epoch": 3.1951656723524633,
      "grad_norm": 6.051104545593262,
      "learning_rate": 2.013425640450698e-05,
      "loss": 0.1231,
      "step": 57040
    },
    {
      "epoch": 3.1957258493684004,
      "grad_norm": 2.080980062484741,
      "learning_rate": 2.0120242166040696e-05,
      "loss": 0.1735,
      "step": 57050
    },
    {
      "epoch": 3.1962860263843376,
      "grad_norm": 4.633241653442383,
      "learning_rate": 2.0106227927574418e-05,
      "loss": 0.1182,
      "step": 57060
    },
    {
      "epoch": 3.1968462034002743,
      "grad_norm": 3.1485800743103027,
      "learning_rate": 2.0092213689108136e-05,
      "loss": 0.1119,
      "step": 57070
    },
    {
      "epoch": 3.1974063804162114,
      "grad_norm": 0.6135791540145874,
      "learning_rate": 2.007819945064185e-05,
      "loss": 0.0815,
      "step": 57080
    },
    {
      "epoch": 3.1979665574321485,
      "grad_norm": 1.2046597003936768,
      "learning_rate": 2.0064185212175572e-05,
      "loss": 0.0883,
      "step": 57090
    },
    {
      "epoch": 3.1985267344480857,
      "grad_norm": 2.012950897216797,
      "learning_rate": 2.005017097370929e-05,
      "loss": 0.1255,
      "step": 57100
    },
    {
      "epoch": 3.199086911464023,
      "grad_norm": 2.7052161693573,
      "learning_rate": 2.003615673524301e-05,
      "loss": 0.1224,
      "step": 57110
    },
    {
      "epoch": 3.1996470884799595,
      "grad_norm": 0.7735146880149841,
      "learning_rate": 2.0022142496776727e-05,
      "loss": 0.0935,
      "step": 57120
    },
    {
      "epoch": 3.2002072654958966,
      "grad_norm": 1.403960943222046,
      "learning_rate": 2.0008128258310445e-05,
      "loss": 0.1246,
      "step": 57130
    },
    {
      "epoch": 3.2007674425118338,
      "grad_norm": 3.7089691162109375,
      "learning_rate": 1.9994114019844163e-05,
      "loss": 0.1572,
      "step": 57140
    },
    {
      "epoch": 3.201327619527771,
      "grad_norm": 3.392843008041382,
      "learning_rate": 1.998009978137788e-05,
      "loss": 0.1249,
      "step": 57150
    },
    {
      "epoch": 3.2018877965437076,
      "grad_norm": 0.8395329713821411,
      "learning_rate": 1.99660855429116e-05,
      "loss": 0.11,
      "step": 57160
    },
    {
      "epoch": 3.2024479735596447,
      "grad_norm": 0.6017102599143982,
      "learning_rate": 1.9952071304445317e-05,
      "loss": 0.0943,
      "step": 57170
    },
    {
      "epoch": 3.203008150575582,
      "grad_norm": 2.0394163131713867,
      "learning_rate": 1.9938057065979036e-05,
      "loss": 0.1044,
      "step": 57180
    },
    {
      "epoch": 3.203568327591519,
      "grad_norm": 2.3429245948791504,
      "learning_rate": 1.9924042827512754e-05,
      "loss": 0.0985,
      "step": 57190
    },
    {
      "epoch": 3.204128504607456,
      "grad_norm": 5.498767852783203,
      "learning_rate": 1.9910028589046472e-05,
      "loss": 0.1698,
      "step": 57200
    },
    {
      "epoch": 3.204688681623393,
      "grad_norm": 4.788024425506592,
      "learning_rate": 1.989601435058019e-05,
      "loss": 0.1272,
      "step": 57210
    },
    {
      "epoch": 3.20524885863933,
      "grad_norm": 1.4227138757705688,
      "learning_rate": 1.9882000112113908e-05,
      "loss": 0.11,
      "step": 57220
    },
    {
      "epoch": 3.205809035655267,
      "grad_norm": 1.0721608400344849,
      "learning_rate": 1.9867985873647626e-05,
      "loss": 0.1203,
      "step": 57230
    },
    {
      "epoch": 3.2063692126712042,
      "grad_norm": 1.5211467742919922,
      "learning_rate": 1.9853971635181344e-05,
      "loss": 0.1038,
      "step": 57240
    },
    {
      "epoch": 3.206929389687141,
      "grad_norm": 2.7143144607543945,
      "learning_rate": 1.9839957396715066e-05,
      "loss": 0.1101,
      "step": 57250
    },
    {
      "epoch": 3.207489566703078,
      "grad_norm": 3.818119525909424,
      "learning_rate": 1.982594315824878e-05,
      "loss": 0.1174,
      "step": 57260
    },
    {
      "epoch": 3.208049743719015,
      "grad_norm": 3.5776641368865967,
      "learning_rate": 1.98119289197825e-05,
      "loss": 0.1303,
      "step": 57270
    },
    {
      "epoch": 3.2086099207349523,
      "grad_norm": 2.5036559104919434,
      "learning_rate": 1.9797914681316217e-05,
      "loss": 0.1251,
      "step": 57280
    },
    {
      "epoch": 3.2091700977508895,
      "grad_norm": 1.6996515989303589,
      "learning_rate": 1.978390044284994e-05,
      "loss": 0.1072,
      "step": 57290
    },
    {
      "epoch": 3.209730274766826,
      "grad_norm": 2.4256467819213867,
      "learning_rate": 1.9769886204383657e-05,
      "loss": 0.1038,
      "step": 57300
    },
    {
      "epoch": 3.2102904517827633,
      "grad_norm": 2.980445384979248,
      "learning_rate": 1.975587196591737e-05,
      "loss": 0.1021,
      "step": 57310
    },
    {
      "epoch": 3.2108506287987004,
      "grad_norm": 0.6131042242050171,
      "learning_rate": 1.974185772745109e-05,
      "loss": 0.0846,
      "step": 57320
    },
    {
      "epoch": 3.2114108058146376,
      "grad_norm": 0.8555768728256226,
      "learning_rate": 1.972784348898481e-05,
      "loss": 0.1186,
      "step": 57330
    },
    {
      "epoch": 3.2119709828305743,
      "grad_norm": 1.4951324462890625,
      "learning_rate": 1.971382925051853e-05,
      "loss": 0.1516,
      "step": 57340
    },
    {
      "epoch": 3.2125311598465114,
      "grad_norm": 3.5373501777648926,
      "learning_rate": 1.9699815012052247e-05,
      "loss": 0.1006,
      "step": 57350
    },
    {
      "epoch": 3.2130913368624485,
      "grad_norm": 3.163435697555542,
      "learning_rate": 1.9685800773585962e-05,
      "loss": 0.1351,
      "step": 57360
    },
    {
      "epoch": 3.2136515138783857,
      "grad_norm": 1.5600733757019043,
      "learning_rate": 1.9671786535119683e-05,
      "loss": 0.1042,
      "step": 57370
    },
    {
      "epoch": 3.214211690894323,
      "grad_norm": 2.373420476913452,
      "learning_rate": 1.96577722966534e-05,
      "loss": 0.0913,
      "step": 57380
    },
    {
      "epoch": 3.2147718679102595,
      "grad_norm": 2.7789697647094727,
      "learning_rate": 1.964375805818712e-05,
      "loss": 0.1907,
      "step": 57390
    },
    {
      "epoch": 3.2153320449261966,
      "grad_norm": 0.9542554020881653,
      "learning_rate": 1.9629743819720834e-05,
      "loss": 0.1212,
      "step": 57400
    },
    {
      "epoch": 3.2158922219421338,
      "grad_norm": 5.258404731750488,
      "learning_rate": 1.9615729581254556e-05,
      "loss": 0.1508,
      "step": 57410
    },
    {
      "epoch": 3.216452398958071,
      "grad_norm": 3.0893747806549072,
      "learning_rate": 1.9601715342788274e-05,
      "loss": 0.0923,
      "step": 57420
    },
    {
      "epoch": 3.2170125759740076,
      "grad_norm": 1.020280361175537,
      "learning_rate": 1.9587701104321992e-05,
      "loss": 0.07,
      "step": 57430
    },
    {
      "epoch": 3.2175727529899447,
      "grad_norm": 2.5637569427490234,
      "learning_rate": 1.957368686585571e-05,
      "loss": 0.0873,
      "step": 57440
    },
    {
      "epoch": 3.218132930005882,
      "grad_norm": 2.4230148792266846,
      "learning_rate": 1.955967262738943e-05,
      "loss": 0.1098,
      "step": 57450
    },
    {
      "epoch": 3.218693107021819,
      "grad_norm": 3.2488956451416016,
      "learning_rate": 1.9545658388923147e-05,
      "loss": 0.0867,
      "step": 57460
    },
    {
      "epoch": 3.219253284037756,
      "grad_norm": 0.8252742290496826,
      "learning_rate": 1.9531644150456865e-05,
      "loss": 0.0999,
      "step": 57470
    },
    {
      "epoch": 3.219813461053693,
      "grad_norm": 5.442871570587158,
      "learning_rate": 1.9517629911990583e-05,
      "loss": 0.1412,
      "step": 57480
    },
    {
      "epoch": 3.22037363806963,
      "grad_norm": 1.545723557472229,
      "learning_rate": 1.95036156735243e-05,
      "loss": 0.1286,
      "step": 57490
    },
    {
      "epoch": 3.220933815085567,
      "grad_norm": 2.274867534637451,
      "learning_rate": 1.948960143505802e-05,
      "loss": 0.0913,
      "step": 57500
    },
    {
      "epoch": 3.2214939921015042,
      "grad_norm": 1.3319010734558105,
      "learning_rate": 1.9475587196591737e-05,
      "loss": 0.1079,
      "step": 57510
    },
    {
      "epoch": 3.222054169117441,
      "grad_norm": 2.694662094116211,
      "learning_rate": 1.9461572958125455e-05,
      "loss": 0.1418,
      "step": 57520
    },
    {
      "epoch": 3.222614346133378,
      "grad_norm": 2.500061273574829,
      "learning_rate": 1.9447558719659177e-05,
      "loss": 0.1081,
      "step": 57530
    },
    {
      "epoch": 3.223174523149315,
      "grad_norm": 2.24963641166687,
      "learning_rate": 1.943354448119289e-05,
      "loss": 0.0804,
      "step": 57540
    },
    {
      "epoch": 3.2237347001652523,
      "grad_norm": 1.1280931234359741,
      "learning_rate": 1.941953024272661e-05,
      "loss": 0.0832,
      "step": 57550
    },
    {
      "epoch": 3.2242948771811895,
      "grad_norm": 1.7651658058166504,
      "learning_rate": 1.9405516004260328e-05,
      "loss": 0.0764,
      "step": 57560
    },
    {
      "epoch": 3.224855054197126,
      "grad_norm": 4.673501491546631,
      "learning_rate": 1.939150176579405e-05,
      "loss": 0.1152,
      "step": 57570
    },
    {
      "epoch": 3.2254152312130633,
      "grad_norm": 1.155806541442871,
      "learning_rate": 1.9377487527327768e-05,
      "loss": 0.1291,
      "step": 57580
    },
    {
      "epoch": 3.2259754082290004,
      "grad_norm": 2.3726956844329834,
      "learning_rate": 1.9363473288861482e-05,
      "loss": 0.1195,
      "step": 57590
    },
    {
      "epoch": 3.2265355852449376,
      "grad_norm": 4.327273845672607,
      "learning_rate": 1.9349459050395204e-05,
      "loss": 0.1451,
      "step": 57600
    },
    {
      "epoch": 3.2270957622608742,
      "grad_norm": 1.213322401046753,
      "learning_rate": 1.9335444811928922e-05,
      "loss": 0.1392,
      "step": 57610
    },
    {
      "epoch": 3.2276559392768114,
      "grad_norm": 4.398966312408447,
      "learning_rate": 1.932143057346264e-05,
      "loss": 0.105,
      "step": 57620
    },
    {
      "epoch": 3.2282161162927485,
      "grad_norm": 0.6339094042778015,
      "learning_rate": 1.9307416334996355e-05,
      "loss": 0.0899,
      "step": 57630
    },
    {
      "epoch": 3.2287762933086857,
      "grad_norm": 1.2774832248687744,
      "learning_rate": 1.9293402096530076e-05,
      "loss": 0.1299,
      "step": 57640
    },
    {
      "epoch": 3.229336470324623,
      "grad_norm": 0.8583582639694214,
      "learning_rate": 1.9279387858063795e-05,
      "loss": 0.1499,
      "step": 57650
    },
    {
      "epoch": 3.2298966473405595,
      "grad_norm": 2.066460132598877,
      "learning_rate": 1.9265373619597513e-05,
      "loss": 0.1431,
      "step": 57660
    },
    {
      "epoch": 3.2304568243564966,
      "grad_norm": 5.8278374671936035,
      "learning_rate": 1.925135938113123e-05,
      "loss": 0.1314,
      "step": 57670
    },
    {
      "epoch": 3.2310170013724338,
      "grad_norm": 0.8940076231956482,
      "learning_rate": 1.923734514266495e-05,
      "loss": 0.1078,
      "step": 57680
    },
    {
      "epoch": 3.231577178388371,
      "grad_norm": 0.6404283046722412,
      "learning_rate": 1.9223330904198667e-05,
      "loss": 0.1141,
      "step": 57690
    },
    {
      "epoch": 3.2321373554043076,
      "grad_norm": 3.4275527000427246,
      "learning_rate": 1.9209316665732385e-05,
      "loss": 0.1512,
      "step": 57700
    },
    {
      "epoch": 3.2326975324202447,
      "grad_norm": 1.7325636148452759,
      "learning_rate": 1.9195302427266103e-05,
      "loss": 0.0907,
      "step": 57710
    },
    {
      "epoch": 3.233257709436182,
      "grad_norm": 2.15413236618042,
      "learning_rate": 1.918128818879982e-05,
      "loss": 0.111,
      "step": 57720
    },
    {
      "epoch": 3.233817886452119,
      "grad_norm": 4.1660895347595215,
      "learning_rate": 1.916727395033354e-05,
      "loss": 0.1874,
      "step": 57730
    },
    {
      "epoch": 3.234378063468056,
      "grad_norm": 2.8320515155792236,
      "learning_rate": 1.9153259711867258e-05,
      "loss": 0.1008,
      "step": 57740
    },
    {
      "epoch": 3.234938240483993,
      "grad_norm": 1.8681552410125732,
      "learning_rate": 1.9139245473400976e-05,
      "loss": 0.0956,
      "step": 57750
    },
    {
      "epoch": 3.23549841749993,
      "grad_norm": 2.185208320617676,
      "learning_rate": 1.9125231234934697e-05,
      "loss": 0.1257,
      "step": 57760
    },
    {
      "epoch": 3.236058594515867,
      "grad_norm": 1.9032068252563477,
      "learning_rate": 1.9111216996468412e-05,
      "loss": 0.1312,
      "step": 57770
    },
    {
      "epoch": 3.236618771531804,
      "grad_norm": 2.3923940658569336,
      "learning_rate": 1.909720275800213e-05,
      "loss": 0.0859,
      "step": 57780
    },
    {
      "epoch": 3.237178948547741,
      "grad_norm": 1.4128249883651733,
      "learning_rate": 1.908318851953585e-05,
      "loss": 0.1133,
      "step": 57790
    },
    {
      "epoch": 3.237739125563678,
      "grad_norm": 2.0338611602783203,
      "learning_rate": 1.906917428106957e-05,
      "loss": 0.0927,
      "step": 57800
    },
    {
      "epoch": 3.238299302579615,
      "grad_norm": 3.608721971511841,
      "learning_rate": 1.9055160042603285e-05,
      "loss": 0.1292,
      "step": 57810
    },
    {
      "epoch": 3.2388594795955523,
      "grad_norm": 4.061764240264893,
      "learning_rate": 1.9041145804137003e-05,
      "loss": 0.1092,
      "step": 57820
    },
    {
      "epoch": 3.2394196566114895,
      "grad_norm": 1.4675674438476562,
      "learning_rate": 1.902713156567072e-05,
      "loss": 0.1212,
      "step": 57830
    },
    {
      "epoch": 3.239979833627426,
      "grad_norm": 4.497636318206787,
      "learning_rate": 1.9013117327204442e-05,
      "loss": 0.1226,
      "step": 57840
    },
    {
      "epoch": 3.2405400106433633,
      "grad_norm": 1.366965651512146,
      "learning_rate": 1.899910308873816e-05,
      "loss": 0.0852,
      "step": 57850
    },
    {
      "epoch": 3.2411001876593004,
      "grad_norm": 0.9367450475692749,
      "learning_rate": 1.8985088850271875e-05,
      "loss": 0.1523,
      "step": 57860
    },
    {
      "epoch": 3.2416603646752375,
      "grad_norm": 1.981401801109314,
      "learning_rate": 1.8971074611805593e-05,
      "loss": 0.1097,
      "step": 57870
    },
    {
      "epoch": 3.2422205416911742,
      "grad_norm": 2.9157955646514893,
      "learning_rate": 1.8957060373339315e-05,
      "loss": 0.0788,
      "step": 57880
    },
    {
      "epoch": 3.2427807187071114,
      "grad_norm": 1.2254236936569214,
      "learning_rate": 1.8943046134873033e-05,
      "loss": 0.1381,
      "step": 57890
    },
    {
      "epoch": 3.2433408957230485,
      "grad_norm": 0.9289955496788025,
      "learning_rate": 1.892903189640675e-05,
      "loss": 0.1462,
      "step": 57900
    },
    {
      "epoch": 3.2439010727389856,
      "grad_norm": 0.9922747611999512,
      "learning_rate": 1.8915017657940466e-05,
      "loss": 0.1187,
      "step": 57910
    },
    {
      "epoch": 3.244461249754923,
      "grad_norm": 1.4133738279342651,
      "learning_rate": 1.8901003419474187e-05,
      "loss": 0.1303,
      "step": 57920
    },
    {
      "epoch": 3.2450214267708595,
      "grad_norm": 2.5460851192474365,
      "learning_rate": 1.8886989181007906e-05,
      "loss": 0.1158,
      "step": 57930
    },
    {
      "epoch": 3.2455816037867966,
      "grad_norm": 4.148451805114746,
      "learning_rate": 1.8872974942541624e-05,
      "loss": 0.134,
      "step": 57940
    },
    {
      "epoch": 3.2461417808027337,
      "grad_norm": 2.297074556350708,
      "learning_rate": 1.8858960704075342e-05,
      "loss": 0.087,
      "step": 57950
    },
    {
      "epoch": 3.246701957818671,
      "grad_norm": 1.4128458499908447,
      "learning_rate": 1.884494646560906e-05,
      "loss": 0.1514,
      "step": 57960
    },
    {
      "epoch": 3.2472621348346076,
      "grad_norm": 3.2649002075195312,
      "learning_rate": 1.8830932227142778e-05,
      "loss": 0.1457,
      "step": 57970
    },
    {
      "epoch": 3.2478223118505447,
      "grad_norm": 2.0088706016540527,
      "learning_rate": 1.8816917988676496e-05,
      "loss": 0.0968,
      "step": 57980
    },
    {
      "epoch": 3.248382488866482,
      "grad_norm": 1.7977067232131958,
      "learning_rate": 1.8802903750210214e-05,
      "loss": 0.186,
      "step": 57990
    },
    {
      "epoch": 3.248942665882419,
      "grad_norm": 1.6685264110565186,
      "learning_rate": 1.8788889511743932e-05,
      "loss": 0.0872,
      "step": 58000
    },
    {
      "epoch": 3.249502842898356,
      "grad_norm": 3.3564505577087402,
      "learning_rate": 1.877487527327765e-05,
      "loss": 0.098,
      "step": 58010
    },
    {
      "epoch": 3.250063019914293,
      "grad_norm": 3.9859020709991455,
      "learning_rate": 1.876086103481137e-05,
      "loss": 0.0913,
      "step": 58020
    },
    {
      "epoch": 3.25062319693023,
      "grad_norm": 3.596677780151367,
      "learning_rate": 1.8746846796345087e-05,
      "loss": 0.1398,
      "step": 58030
    },
    {
      "epoch": 3.251183373946167,
      "grad_norm": 1.6204015016555786,
      "learning_rate": 1.8732832557878805e-05,
      "loss": 0.0815,
      "step": 58040
    },
    {
      "epoch": 3.251743550962104,
      "grad_norm": 1.4613760709762573,
      "learning_rate": 1.8718818319412523e-05,
      "loss": 0.0917,
      "step": 58050
    },
    {
      "epoch": 3.252303727978041,
      "grad_norm": 6.649281024932861,
      "learning_rate": 1.870480408094624e-05,
      "loss": 0.127,
      "step": 58060
    },
    {
      "epoch": 3.252863904993978,
      "grad_norm": 4.861042499542236,
      "learning_rate": 1.869078984247996e-05,
      "loss": 0.1396,
      "step": 58070
    },
    {
      "epoch": 3.253424082009915,
      "grad_norm": 0.9985156655311584,
      "learning_rate": 1.867677560401368e-05,
      "loss": 0.1034,
      "step": 58080
    },
    {
      "epoch": 3.2539842590258523,
      "grad_norm": 4.379677772521973,
      "learning_rate": 1.8662761365547396e-05,
      "loss": 0.1292,
      "step": 58090
    },
    {
      "epoch": 3.2545444360417894,
      "grad_norm": 2.3691794872283936,
      "learning_rate": 1.8648747127081114e-05,
      "loss": 0.1131,
      "step": 58100
    },
    {
      "epoch": 3.255104613057726,
      "grad_norm": 2.014233112335205,
      "learning_rate": 1.8634732888614835e-05,
      "loss": 0.0942,
      "step": 58110
    },
    {
      "epoch": 3.2556647900736633,
      "grad_norm": 3.1470820903778076,
      "learning_rate": 1.8620718650148553e-05,
      "loss": 0.0849,
      "step": 58120
    },
    {
      "epoch": 3.2562249670896004,
      "grad_norm": 0.7016656398773193,
      "learning_rate": 1.860670441168227e-05,
      "loss": 0.0921,
      "step": 58130
    },
    {
      "epoch": 3.2567851441055375,
      "grad_norm": 0.5214571356773376,
      "learning_rate": 1.8592690173215986e-05,
      "loss": 0.0897,
      "step": 58140
    },
    {
      "epoch": 3.2573453211214742,
      "grad_norm": 3.057999610900879,
      "learning_rate": 1.8578675934749708e-05,
      "loss": 0.128,
      "step": 58150
    },
    {
      "epoch": 3.2579054981374114,
      "grad_norm": 1.6787629127502441,
      "learning_rate": 1.8564661696283426e-05,
      "loss": 0.1161,
      "step": 58160
    },
    {
      "epoch": 3.2584656751533485,
      "grad_norm": 5.862451553344727,
      "learning_rate": 1.8550647457817144e-05,
      "loss": 0.1483,
      "step": 58170
    },
    {
      "epoch": 3.2590258521692856,
      "grad_norm": 2.471395969390869,
      "learning_rate": 1.853663321935086e-05,
      "loss": 0.1263,
      "step": 58180
    },
    {
      "epoch": 3.2595860291852228,
      "grad_norm": 2.459012746810913,
      "learning_rate": 1.852261898088458e-05,
      "loss": 0.0989,
      "step": 58190
    },
    {
      "epoch": 3.2601462062011595,
      "grad_norm": 1.8262302875518799,
      "learning_rate": 1.85086047424183e-05,
      "loss": 0.0865,
      "step": 58200
    },
    {
      "epoch": 3.2607063832170966,
      "grad_norm": 2.9381303787231445,
      "learning_rate": 1.8494590503952017e-05,
      "loss": 0.0993,
      "step": 58210
    },
    {
      "epoch": 3.2612665602330337,
      "grad_norm": 3.8161885738372803,
      "learning_rate": 1.8480576265485735e-05,
      "loss": 0.1598,
      "step": 58220
    },
    {
      "epoch": 3.2618267372489704,
      "grad_norm": 1.9556550979614258,
      "learning_rate": 1.8466562027019453e-05,
      "loss": 0.0964,
      "step": 58230
    },
    {
      "epoch": 3.2623869142649076,
      "grad_norm": 2.4187467098236084,
      "learning_rate": 1.845254778855317e-05,
      "loss": 0.1415,
      "step": 58240
    },
    {
      "epoch": 3.2629470912808447,
      "grad_norm": 1.8403297662734985,
      "learning_rate": 1.843853355008689e-05,
      "loss": 0.1083,
      "step": 58250
    },
    {
      "epoch": 3.263507268296782,
      "grad_norm": 0.9997745156288147,
      "learning_rate": 1.8424519311620607e-05,
      "loss": 0.0864,
      "step": 58260
    },
    {
      "epoch": 3.264067445312719,
      "grad_norm": 2.022895574569702,
      "learning_rate": 1.8410505073154325e-05,
      "loss": 0.094,
      "step": 58270
    },
    {
      "epoch": 3.2646276223286557,
      "grad_norm": 2.75158953666687,
      "learning_rate": 1.8396490834688044e-05,
      "loss": 0.0968,
      "step": 58280
    },
    {
      "epoch": 3.265187799344593,
      "grad_norm": 4.138613700866699,
      "learning_rate": 1.838247659622176e-05,
      "loss": 0.108,
      "step": 58290
    },
    {
      "epoch": 3.26574797636053,
      "grad_norm": 4.303863525390625,
      "learning_rate": 1.836846235775548e-05,
      "loss": 0.1054,
      "step": 58300
    },
    {
      "epoch": 3.266308153376467,
      "grad_norm": 1.31719970703125,
      "learning_rate": 1.83544481192892e-05,
      "loss": 0.1039,
      "step": 58310
    },
    {
      "epoch": 3.2668683303924038,
      "grad_norm": 0.9477536082267761,
      "learning_rate": 1.8340433880822916e-05,
      "loss": 0.0838,
      "step": 58320
    },
    {
      "epoch": 3.267428507408341,
      "grad_norm": 5.450206756591797,
      "learning_rate": 1.8326419642356634e-05,
      "loss": 0.1065,
      "step": 58330
    },
    {
      "epoch": 3.267988684424278,
      "grad_norm": 0.496651291847229,
      "learning_rate": 1.8312405403890352e-05,
      "loss": 0.1015,
      "step": 58340
    },
    {
      "epoch": 3.268548861440215,
      "grad_norm": 2.570594310760498,
      "learning_rate": 1.8298391165424074e-05,
      "loss": 0.1275,
      "step": 58350
    },
    {
      "epoch": 3.2691090384561523,
      "grad_norm": 2.4866464138031006,
      "learning_rate": 1.8284376926957792e-05,
      "loss": 0.0971,
      "step": 58360
    },
    {
      "epoch": 3.269669215472089,
      "grad_norm": 1.81529700756073,
      "learning_rate": 1.8270362688491507e-05,
      "loss": 0.0789,
      "step": 58370
    },
    {
      "epoch": 3.270229392488026,
      "grad_norm": 3.2373406887054443,
      "learning_rate": 1.8256348450025225e-05,
      "loss": 0.0892,
      "step": 58380
    },
    {
      "epoch": 3.2707895695039633,
      "grad_norm": 5.289764881134033,
      "learning_rate": 1.8242334211558946e-05,
      "loss": 0.1052,
      "step": 58390
    },
    {
      "epoch": 3.2713497465199004,
      "grad_norm": 0.8506767749786377,
      "learning_rate": 1.8228319973092665e-05,
      "loss": 0.0686,
      "step": 58400
    },
    {
      "epoch": 3.271909923535837,
      "grad_norm": 1.6389588117599487,
      "learning_rate": 1.821430573462638e-05,
      "loss": 0.2222,
      "step": 58410
    },
    {
      "epoch": 3.2724701005517742,
      "grad_norm": 4.695733547210693,
      "learning_rate": 1.8200291496160097e-05,
      "loss": 0.1329,
      "step": 58420
    },
    {
      "epoch": 3.2730302775677114,
      "grad_norm": 3.8682775497436523,
      "learning_rate": 1.818627725769382e-05,
      "loss": 0.1078,
      "step": 58430
    },
    {
      "epoch": 3.2735904545836485,
      "grad_norm": 2.783665657043457,
      "learning_rate": 1.8172263019227537e-05,
      "loss": 0.1072,
      "step": 58440
    },
    {
      "epoch": 3.2741506315995856,
      "grad_norm": 0.7467087507247925,
      "learning_rate": 1.8158248780761255e-05,
      "loss": 0.1174,
      "step": 58450
    },
    {
      "epoch": 3.2747108086155223,
      "grad_norm": 0.6352440714836121,
      "learning_rate": 1.8144234542294973e-05,
      "loss": 0.0894,
      "step": 58460
    },
    {
      "epoch": 3.2752709856314595,
      "grad_norm": 3.605808734893799,
      "learning_rate": 1.813022030382869e-05,
      "loss": 0.0727,
      "step": 58470
    },
    {
      "epoch": 3.2758311626473966,
      "grad_norm": 1.2559038400650024,
      "learning_rate": 1.811620606536241e-05,
      "loss": 0.1026,
      "step": 58480
    },
    {
      "epoch": 3.2763913396633337,
      "grad_norm": 1.6072269678115845,
      "learning_rate": 1.8102191826896128e-05,
      "loss": 0.1045,
      "step": 58490
    },
    {
      "epoch": 3.2769515166792704,
      "grad_norm": 1.9934979677200317,
      "learning_rate": 1.8088177588429846e-05,
      "loss": 0.1116,
      "step": 58500
    },
    {
      "epoch": 3.2775116936952076,
      "grad_norm": 4.344790935516357,
      "learning_rate": 1.8074163349963564e-05,
      "loss": 0.0897,
      "step": 58510
    },
    {
      "epoch": 3.2780718707111447,
      "grad_norm": 8.986572265625,
      "learning_rate": 1.8060149111497282e-05,
      "loss": 0.1618,
      "step": 58520
    },
    {
      "epoch": 3.278632047727082,
      "grad_norm": 1.4329349994659424,
      "learning_rate": 1.8046134873031e-05,
      "loss": 0.1077,
      "step": 58530
    },
    {
      "epoch": 3.279192224743019,
      "grad_norm": 1.7577729225158691,
      "learning_rate": 1.803212063456472e-05,
      "loss": 0.0859,
      "step": 58540
    },
    {
      "epoch": 3.2797524017589557,
      "grad_norm": 4.175530433654785,
      "learning_rate": 1.8018106396098436e-05,
      "loss": 0.2429,
      "step": 58550
    },
    {
      "epoch": 3.280312578774893,
      "grad_norm": 2.9294867515563965,
      "learning_rate": 1.8004092157632155e-05,
      "loss": 0.1206,
      "step": 58560
    },
    {
      "epoch": 3.28087275579083,
      "grad_norm": 0.8508327007293701,
      "learning_rate": 1.7990077919165873e-05,
      "loss": 0.1026,
      "step": 58570
    },
    {
      "epoch": 3.281432932806767,
      "grad_norm": 1.253462314605713,
      "learning_rate": 1.797606368069959e-05,
      "loss": 0.0795,
      "step": 58580
    },
    {
      "epoch": 3.2819931098227038,
      "grad_norm": 4.593430519104004,
      "learning_rate": 1.796204944223331e-05,
      "loss": 0.1307,
      "step": 58590
    },
    {
      "epoch": 3.282553286838641,
      "grad_norm": 2.4066152572631836,
      "learning_rate": 1.7948035203767027e-05,
      "loss": 0.1408,
      "step": 58600
    },
    {
      "epoch": 3.283113463854578,
      "grad_norm": 1.2353209257125854,
      "learning_rate": 1.7934020965300745e-05,
      "loss": 0.098,
      "step": 58610
    },
    {
      "epoch": 3.283673640870515,
      "grad_norm": 2.092472553253174,
      "learning_rate": 1.7920006726834467e-05,
      "loss": 0.0943,
      "step": 58620
    },
    {
      "epoch": 3.2842338178864523,
      "grad_norm": 1.0066832304000854,
      "learning_rate": 1.7905992488368185e-05,
      "loss": 0.1121,
      "step": 58630
    },
    {
      "epoch": 3.284793994902389,
      "grad_norm": 2.241516351699829,
      "learning_rate": 1.78919782499019e-05,
      "loss": 0.1052,
      "step": 58640
    },
    {
      "epoch": 3.285354171918326,
      "grad_norm": 4.4708123207092285,
      "learning_rate": 1.7877964011435618e-05,
      "loss": 0.1139,
      "step": 58650
    },
    {
      "epoch": 3.2859143489342633,
      "grad_norm": 1.6140923500061035,
      "learning_rate": 1.786394977296934e-05,
      "loss": 0.1191,
      "step": 58660
    },
    {
      "epoch": 3.2864745259502004,
      "grad_norm": 2.0218849182128906,
      "learning_rate": 1.7849935534503057e-05,
      "loss": 0.1146,
      "step": 58670
    },
    {
      "epoch": 3.287034702966137,
      "grad_norm": 3.0179121494293213,
      "learning_rate": 1.7835921296036776e-05,
      "loss": 0.103,
      "step": 58680
    },
    {
      "epoch": 3.2875948799820742,
      "grad_norm": 1.110421895980835,
      "learning_rate": 1.782190705757049e-05,
      "loss": 0.1185,
      "step": 58690
    },
    {
      "epoch": 3.2881550569980114,
      "grad_norm": 2.677366018295288,
      "learning_rate": 1.7807892819104212e-05,
      "loss": 0.1488,
      "step": 58700
    },
    {
      "epoch": 3.2887152340139485,
      "grad_norm": 2.331480026245117,
      "learning_rate": 1.779387858063793e-05,
      "loss": 0.1029,
      "step": 58710
    },
    {
      "epoch": 3.2892754110298856,
      "grad_norm": 3.895322799682617,
      "learning_rate": 1.7779864342171648e-05,
      "loss": 0.1254,
      "step": 58720
    },
    {
      "epoch": 3.2898355880458223,
      "grad_norm": 0.5787818431854248,
      "learning_rate": 1.7765850103705363e-05,
      "loss": 0.1293,
      "step": 58730
    },
    {
      "epoch": 3.2903957650617595,
      "grad_norm": 1.9645185470581055,
      "learning_rate": 1.7751835865239084e-05,
      "loss": 0.1205,
      "step": 58740
    },
    {
      "epoch": 3.2909559420776966,
      "grad_norm": 2.5600674152374268,
      "learning_rate": 1.7737821626772802e-05,
      "loss": 0.1045,
      "step": 58750
    },
    {
      "epoch": 3.2915161190936337,
      "grad_norm": 4.667322635650635,
      "learning_rate": 1.772380738830652e-05,
      "loss": 0.1187,
      "step": 58760
    },
    {
      "epoch": 3.2920762961095704,
      "grad_norm": 1.3692166805267334,
      "learning_rate": 1.770979314984024e-05,
      "loss": 0.0845,
      "step": 58770
    },
    {
      "epoch": 3.2926364731255076,
      "grad_norm": 3.1612110137939453,
      "learning_rate": 1.7695778911373957e-05,
      "loss": 0.1629,
      "step": 58780
    },
    {
      "epoch": 3.2931966501414447,
      "grad_norm": 0.6305524706840515,
      "learning_rate": 1.7681764672907675e-05,
      "loss": 0.1076,
      "step": 58790
    },
    {
      "epoch": 3.293756827157382,
      "grad_norm": 3.4697060585021973,
      "learning_rate": 1.7667750434441393e-05,
      "loss": 0.1252,
      "step": 58800
    },
    {
      "epoch": 3.294317004173319,
      "grad_norm": 3.980731248855591,
      "learning_rate": 1.765373619597511e-05,
      "loss": 0.153,
      "step": 58810
    },
    {
      "epoch": 3.2948771811892557,
      "grad_norm": 1.8678243160247803,
      "learning_rate": 1.763972195750883e-05,
      "loss": 0.1347,
      "step": 58820
    },
    {
      "epoch": 3.295437358205193,
      "grad_norm": 1.674116849899292,
      "learning_rate": 1.7625707719042548e-05,
      "loss": 0.0934,
      "step": 58830
    },
    {
      "epoch": 3.29599753522113,
      "grad_norm": 4.495565414428711,
      "learning_rate": 1.7611693480576266e-05,
      "loss": 0.1243,
      "step": 58840
    },
    {
      "epoch": 3.296557712237067,
      "grad_norm": 4.412524700164795,
      "learning_rate": 1.7597679242109984e-05,
      "loss": 0.0855,
      "step": 58850
    },
    {
      "epoch": 3.2971178892530038,
      "grad_norm": 3.5643651485443115,
      "learning_rate": 1.7583665003643705e-05,
      "loss": 0.0913,
      "step": 58860
    },
    {
      "epoch": 3.297678066268941,
      "grad_norm": 0.6143871545791626,
      "learning_rate": 1.756965076517742e-05,
      "loss": 0.1307,
      "step": 58870
    },
    {
      "epoch": 3.298238243284878,
      "grad_norm": 2.7877938747406006,
      "learning_rate": 1.7555636526711138e-05,
      "loss": 0.1517,
      "step": 58880
    },
    {
      "epoch": 3.298798420300815,
      "grad_norm": 0.8620725274085999,
      "learning_rate": 1.7541622288244856e-05,
      "loss": 0.1114,
      "step": 58890
    },
    {
      "epoch": 3.2993585973167523,
      "grad_norm": 4.5042595863342285,
      "learning_rate": 1.7527608049778578e-05,
      "loss": 0.1043,
      "step": 58900
    },
    {
      "epoch": 3.299918774332689,
      "grad_norm": 2.1815664768218994,
      "learning_rate": 1.7513593811312296e-05,
      "loss": 0.122,
      "step": 58910
    },
    {
      "epoch": 3.300478951348626,
      "grad_norm": 3.374967336654663,
      "learning_rate": 1.749957957284601e-05,
      "loss": 0.093,
      "step": 58920
    },
    {
      "epoch": 3.3010391283645633,
      "grad_norm": 2.0246706008911133,
      "learning_rate": 1.748556533437973e-05,
      "loss": 0.1465,
      "step": 58930
    },
    {
      "epoch": 3.3015993053805004,
      "grad_norm": 2.5712411403656006,
      "learning_rate": 1.747155109591345e-05,
      "loss": 0.0869,
      "step": 58940
    },
    {
      "epoch": 3.302159482396437,
      "grad_norm": 1.2075200080871582,
      "learning_rate": 1.745753685744717e-05,
      "loss": 0.0891,
      "step": 58950
    },
    {
      "epoch": 3.302719659412374,
      "grad_norm": 2.5289793014526367,
      "learning_rate": 1.7443522618980883e-05,
      "loss": 0.1486,
      "step": 58960
    },
    {
      "epoch": 3.3032798364283114,
      "grad_norm": 2.3785581588745117,
      "learning_rate": 1.7429508380514605e-05,
      "loss": 0.0846,
      "step": 58970
    },
    {
      "epoch": 3.3038400134442485,
      "grad_norm": 5.063289642333984,
      "learning_rate": 1.7415494142048323e-05,
      "loss": 0.1097,
      "step": 58980
    },
    {
      "epoch": 3.3044001904601856,
      "grad_norm": 1.2604174613952637,
      "learning_rate": 1.740147990358204e-05,
      "loss": 0.0956,
      "step": 58990
    },
    {
      "epoch": 3.3049603674761223,
      "grad_norm": 4.8049516677856445,
      "learning_rate": 1.738746566511576e-05,
      "loss": 0.0792,
      "step": 59000
    },
    {
      "epoch": 3.3055205444920595,
      "grad_norm": 4.406918048858643,
      "learning_rate": 1.7373451426649477e-05,
      "loss": 0.1058,
      "step": 59010
    },
    {
      "epoch": 3.3060807215079966,
      "grad_norm": 0.8964340686798096,
      "learning_rate": 1.7359437188183195e-05,
      "loss": 0.0868,
      "step": 59020
    },
    {
      "epoch": 3.3066408985239337,
      "grad_norm": 4.741512298583984,
      "learning_rate": 1.7345422949716914e-05,
      "loss": 0.1129,
      "step": 59030
    },
    {
      "epoch": 3.3072010755398704,
      "grad_norm": 2.017106771469116,
      "learning_rate": 1.733140871125063e-05,
      "loss": 0.1158,
      "step": 59040
    },
    {
      "epoch": 3.3077612525558076,
      "grad_norm": 0.9161184430122375,
      "learning_rate": 1.731739447278435e-05,
      "loss": 0.0759,
      "step": 59050
    },
    {
      "epoch": 3.3083214295717447,
      "grad_norm": 1.9369730949401855,
      "learning_rate": 1.7303380234318068e-05,
      "loss": 0.0772,
      "step": 59060
    },
    {
      "epoch": 3.308881606587682,
      "grad_norm": 2.391676902770996,
      "learning_rate": 1.7289365995851786e-05,
      "loss": 0.1274,
      "step": 59070
    },
    {
      "epoch": 3.309441783603619,
      "grad_norm": 1.1182711124420166,
      "learning_rate": 1.7275351757385504e-05,
      "loss": 0.0798,
      "step": 59080
    },
    {
      "epoch": 3.3100019606195556,
      "grad_norm": 5.470183849334717,
      "learning_rate": 1.7261337518919222e-05,
      "loss": 0.1822,
      "step": 59090
    },
    {
      "epoch": 3.310562137635493,
      "grad_norm": 3.594815492630005,
      "learning_rate": 1.724732328045294e-05,
      "loss": 0.0814,
      "step": 59100
    },
    {
      "epoch": 3.31112231465143,
      "grad_norm": 2.625882387161255,
      "learning_rate": 1.723330904198666e-05,
      "loss": 0.1021,
      "step": 59110
    },
    {
      "epoch": 3.311682491667367,
      "grad_norm": 1.5417672395706177,
      "learning_rate": 1.7219294803520377e-05,
      "loss": 0.1121,
      "step": 59120
    },
    {
      "epoch": 3.3122426686833037,
      "grad_norm": 3.4396355152130127,
      "learning_rate": 1.7205280565054098e-05,
      "loss": 0.1171,
      "step": 59130
    },
    {
      "epoch": 3.312802845699241,
      "grad_norm": 3.0347115993499756,
      "learning_rate": 1.7191266326587813e-05,
      "loss": 0.1039,
      "step": 59140
    },
    {
      "epoch": 3.313363022715178,
      "grad_norm": 0.7351311445236206,
      "learning_rate": 1.717725208812153e-05,
      "loss": 0.1083,
      "step": 59150
    },
    {
      "epoch": 3.313923199731115,
      "grad_norm": 2.5382277965545654,
      "learning_rate": 1.716323784965525e-05,
      "loss": 0.105,
      "step": 59160
    },
    {
      "epoch": 3.3144833767470523,
      "grad_norm": 3.314012050628662,
      "learning_rate": 1.714922361118897e-05,
      "loss": 0.1356,
      "step": 59170
    },
    {
      "epoch": 3.315043553762989,
      "grad_norm": 1.8191547393798828,
      "learning_rate": 1.713520937272269e-05,
      "loss": 0.086,
      "step": 59180
    },
    {
      "epoch": 3.315603730778926,
      "grad_norm": 0.6285895705223083,
      "learning_rate": 1.7121195134256404e-05,
      "loss": 0.0969,
      "step": 59190
    },
    {
      "epoch": 3.3161639077948633,
      "grad_norm": 2.489811420440674,
      "learning_rate": 1.7107180895790122e-05,
      "loss": 0.134,
      "step": 59200
    },
    {
      "epoch": 3.3167240848108004,
      "grad_norm": 1.919304370880127,
      "learning_rate": 1.7093166657323843e-05,
      "loss": 0.0838,
      "step": 59210
    },
    {
      "epoch": 3.317284261826737,
      "grad_norm": 1.7050281763076782,
      "learning_rate": 1.707915241885756e-05,
      "loss": 0.1191,
      "step": 59220
    },
    {
      "epoch": 3.317844438842674,
      "grad_norm": 3.6159908771514893,
      "learning_rate": 1.706513818039128e-05,
      "loss": 0.1193,
      "step": 59230
    },
    {
      "epoch": 3.3184046158586114,
      "grad_norm": 2.8178539276123047,
      "learning_rate": 1.7051123941924994e-05,
      "loss": 0.1271,
      "step": 59240
    },
    {
      "epoch": 3.3189647928745485,
      "grad_norm": 1.2209550142288208,
      "learning_rate": 1.7037109703458716e-05,
      "loss": 0.0919,
      "step": 59250
    },
    {
      "epoch": 3.3195249698904856,
      "grad_norm": 1.7711015939712524,
      "learning_rate": 1.7023095464992434e-05,
      "loss": 0.1077,
      "step": 59260
    },
    {
      "epoch": 3.3200851469064223,
      "grad_norm": 0.9318242073059082,
      "learning_rate": 1.7009081226526152e-05,
      "loss": 0.0872,
      "step": 59270
    },
    {
      "epoch": 3.3206453239223594,
      "grad_norm": 1.7866570949554443,
      "learning_rate": 1.6995066988059867e-05,
      "loss": 0.1185,
      "step": 59280
    },
    {
      "epoch": 3.3212055009382966,
      "grad_norm": 1.1521387100219727,
      "learning_rate": 1.698105274959359e-05,
      "loss": 0.0912,
      "step": 59290
    },
    {
      "epoch": 3.3217656779542337,
      "grad_norm": 1.6865407228469849,
      "learning_rate": 1.6967038511127306e-05,
      "loss": 0.0724,
      "step": 59300
    },
    {
      "epoch": 3.3223258549701704,
      "grad_norm": 1.4172170162200928,
      "learning_rate": 1.6953024272661025e-05,
      "loss": 0.0798,
      "step": 59310
    },
    {
      "epoch": 3.3228860319861075,
      "grad_norm": 1.7307777404785156,
      "learning_rate": 1.6939010034194743e-05,
      "loss": 0.0847,
      "step": 59320
    },
    {
      "epoch": 3.3234462090020447,
      "grad_norm": 0.5554687976837158,
      "learning_rate": 1.692499579572846e-05,
      "loss": 0.1078,
      "step": 59330
    },
    {
      "epoch": 3.324006386017982,
      "grad_norm": 2.942211389541626,
      "learning_rate": 1.691098155726218e-05,
      "loss": 0.107,
      "step": 59340
    },
    {
      "epoch": 3.324566563033919,
      "grad_norm": 3.2550599575042725,
      "learning_rate": 1.6896967318795897e-05,
      "loss": 0.119,
      "step": 59350
    },
    {
      "epoch": 3.3251267400498556,
      "grad_norm": 2.6601710319519043,
      "learning_rate": 1.6882953080329615e-05,
      "loss": 0.0781,
      "step": 59360
    },
    {
      "epoch": 3.325686917065793,
      "grad_norm": 2.4986507892608643,
      "learning_rate": 1.6868938841863333e-05,
      "loss": 0.1194,
      "step": 59370
    },
    {
      "epoch": 3.32624709408173,
      "grad_norm": 5.282649517059326,
      "learning_rate": 1.685492460339705e-05,
      "loss": 0.0966,
      "step": 59380
    },
    {
      "epoch": 3.326807271097667,
      "grad_norm": 6.290078163146973,
      "learning_rate": 1.684091036493077e-05,
      "loss": 0.1979,
      "step": 59390
    },
    {
      "epoch": 3.3273674481136037,
      "grad_norm": 1.3410879373550415,
      "learning_rate": 1.6826896126464488e-05,
      "loss": 0.1769,
      "step": 59400
    },
    {
      "epoch": 3.327927625129541,
      "grad_norm": 2.594388961791992,
      "learning_rate": 1.681288188799821e-05,
      "loss": 0.0959,
      "step": 59410
    },
    {
      "epoch": 3.328487802145478,
      "grad_norm": 1.5628278255462646,
      "learning_rate": 1.6798867649531924e-05,
      "loss": 0.1257,
      "step": 59420
    },
    {
      "epoch": 3.329047979161415,
      "grad_norm": 3.057126045227051,
      "learning_rate": 1.6784853411065642e-05,
      "loss": 0.0864,
      "step": 59430
    },
    {
      "epoch": 3.3296081561773523,
      "grad_norm": 2.3937971591949463,
      "learning_rate": 1.677083917259936e-05,
      "loss": 0.1072,
      "step": 59440
    },
    {
      "epoch": 3.330168333193289,
      "grad_norm": 3.1720876693725586,
      "learning_rate": 1.6756824934133082e-05,
      "loss": 0.1914,
      "step": 59450
    },
    {
      "epoch": 3.330728510209226,
      "grad_norm": 3.5522093772888184,
      "learning_rate": 1.67428106956668e-05,
      "loss": 0.0889,
      "step": 59460
    },
    {
      "epoch": 3.3312886872251632,
      "grad_norm": 3.526176691055298,
      "learning_rate": 1.6728796457200515e-05,
      "loss": 0.1334,
      "step": 59470
    },
    {
      "epoch": 3.3318488642411,
      "grad_norm": 4.949300289154053,
      "learning_rate": 1.6714782218734236e-05,
      "loss": 0.0852,
      "step": 59480
    },
    {
      "epoch": 3.332409041257037,
      "grad_norm": 0.5569319725036621,
      "learning_rate": 1.6700767980267954e-05,
      "loss": 0.1203,
      "step": 59490
    },
    {
      "epoch": 3.332969218272974,
      "grad_norm": 0.8216263055801392,
      "learning_rate": 1.6686753741801672e-05,
      "loss": 0.069,
      "step": 59500
    },
    {
      "epoch": 3.3335293952889113,
      "grad_norm": 2.329329490661621,
      "learning_rate": 1.6672739503335387e-05,
      "loss": 0.1399,
      "step": 59510
    },
    {
      "epoch": 3.3340895723048485,
      "grad_norm": 1.5651649236679077,
      "learning_rate": 1.665872526486911e-05,
      "loss": 0.124,
      "step": 59520
    },
    {
      "epoch": 3.3346497493207856,
      "grad_norm": 0.7848420739173889,
      "learning_rate": 1.6644711026402827e-05,
      "loss": 0.1503,
      "step": 59530
    },
    {
      "epoch": 3.3352099263367223,
      "grad_norm": 3.6124277114868164,
      "learning_rate": 1.6630696787936545e-05,
      "loss": 0.1111,
      "step": 59540
    },
    {
      "epoch": 3.3357701033526594,
      "grad_norm": 4.242270469665527,
      "learning_rate": 1.6616682549470263e-05,
      "loss": 0.1016,
      "step": 59550
    },
    {
      "epoch": 3.3363302803685966,
      "grad_norm": 2.023672342300415,
      "learning_rate": 1.660266831100398e-05,
      "loss": 0.1307,
      "step": 59560
    },
    {
      "epoch": 3.3368904573845333,
      "grad_norm": 0.656696617603302,
      "learning_rate": 1.65886540725377e-05,
      "loss": 0.1192,
      "step": 59570
    },
    {
      "epoch": 3.3374506344004704,
      "grad_norm": 0.7837681770324707,
      "learning_rate": 1.6574639834071418e-05,
      "loss": 0.1209,
      "step": 59580
    },
    {
      "epoch": 3.3380108114164075,
      "grad_norm": 0.8501207232475281,
      "learning_rate": 1.6560625595605136e-05,
      "loss": 0.1233,
      "step": 59590
    },
    {
      "epoch": 3.3385709884323447,
      "grad_norm": 5.228994369506836,
      "learning_rate": 1.6546611357138854e-05,
      "loss": 0.1099,
      "step": 59600
    },
    {
      "epoch": 3.339131165448282,
      "grad_norm": 2.625699758529663,
      "learning_rate": 1.6532597118672572e-05,
      "loss": 0.1513,
      "step": 59610
    },
    {
      "epoch": 3.3396913424642185,
      "grad_norm": 1.7001079320907593,
      "learning_rate": 1.651858288020629e-05,
      "loss": 0.1666,
      "step": 59620
    },
    {
      "epoch": 3.3402515194801556,
      "grad_norm": 0.7684457898139954,
      "learning_rate": 1.6504568641740008e-05,
      "loss": 0.0797,
      "step": 59630
    },
    {
      "epoch": 3.3408116964960928,
      "grad_norm": 2.860549211502075,
      "learning_rate": 1.649055440327373e-05,
      "loss": 0.1039,
      "step": 59640
    },
    {
      "epoch": 3.34137187351203,
      "grad_norm": 1.4388507604599,
      "learning_rate": 1.6476540164807444e-05,
      "loss": 0.1124,
      "step": 59650
    },
    {
      "epoch": 3.3419320505279666,
      "grad_norm": 0.6504759192466736,
      "learning_rate": 1.6462525926341163e-05,
      "loss": 0.114,
      "step": 59660
    },
    {
      "epoch": 3.3424922275439037,
      "grad_norm": 1.1014612913131714,
      "learning_rate": 1.644851168787488e-05,
      "loss": 0.1269,
      "step": 59670
    },
    {
      "epoch": 3.343052404559841,
      "grad_norm": 6.481894493103027,
      "learning_rate": 1.6434497449408602e-05,
      "loss": 0.1258,
      "step": 59680
    },
    {
      "epoch": 3.343612581575778,
      "grad_norm": 4.106019020080566,
      "learning_rate": 1.642048321094232e-05,
      "loss": 0.0955,
      "step": 59690
    },
    {
      "epoch": 3.344172758591715,
      "grad_norm": 1.1859163045883179,
      "learning_rate": 1.6406468972476035e-05,
      "loss": 0.0865,
      "step": 59700
    },
    {
      "epoch": 3.344732935607652,
      "grad_norm": 3.3001809120178223,
      "learning_rate": 1.6392454734009753e-05,
      "loss": 0.1183,
      "step": 59710
    },
    {
      "epoch": 3.345293112623589,
      "grad_norm": 1.8153752088546753,
      "learning_rate": 1.6378440495543475e-05,
      "loss": 0.0923,
      "step": 59720
    },
    {
      "epoch": 3.345853289639526,
      "grad_norm": 3.3476672172546387,
      "learning_rate": 1.6364426257077193e-05,
      "loss": 0.1008,
      "step": 59730
    },
    {
      "epoch": 3.3464134666554632,
      "grad_norm": 2.3298792839050293,
      "learning_rate": 1.6350412018610908e-05,
      "loss": 0.1014,
      "step": 59740
    },
    {
      "epoch": 3.3469736436714,
      "grad_norm": 3.225773811340332,
      "learning_rate": 1.6336397780144626e-05,
      "loss": 0.121,
      "step": 59750
    },
    {
      "epoch": 3.347533820687337,
      "grad_norm": 0.7844073176383972,
      "learning_rate": 1.6322383541678347e-05,
      "loss": 0.1284,
      "step": 59760
    },
    {
      "epoch": 3.348093997703274,
      "grad_norm": 4.493311405181885,
      "learning_rate": 1.6308369303212065e-05,
      "loss": 0.108,
      "step": 59770
    },
    {
      "epoch": 3.3486541747192113,
      "grad_norm": 3.226059675216675,
      "learning_rate": 1.6294355064745784e-05,
      "loss": 0.0945,
      "step": 59780
    },
    {
      "epoch": 3.3492143517351485,
      "grad_norm": 1.3135709762573242,
      "learning_rate": 1.6280340826279498e-05,
      "loss": 0.136,
      "step": 59790
    },
    {
      "epoch": 3.349774528751085,
      "grad_norm": 2.7861979007720947,
      "learning_rate": 1.626632658781322e-05,
      "loss": 0.0883,
      "step": 59800
    },
    {
      "epoch": 3.3503347057670223,
      "grad_norm": 3.5644702911376953,
      "learning_rate": 1.6252312349346938e-05,
      "loss": 0.1206,
      "step": 59810
    },
    {
      "epoch": 3.3508948827829594,
      "grad_norm": 3.426682233810425,
      "learning_rate": 1.6238298110880656e-05,
      "loss": 0.111,
      "step": 59820
    },
    {
      "epoch": 3.3514550597988966,
      "grad_norm": 1.4409061670303345,
      "learning_rate": 1.6224283872414374e-05,
      "loss": 0.0915,
      "step": 59830
    },
    {
      "epoch": 3.3520152368148333,
      "grad_norm": 1.653001070022583,
      "learning_rate": 1.6210269633948092e-05,
      "loss": 0.1008,
      "step": 59840
    },
    {
      "epoch": 3.3525754138307704,
      "grad_norm": 1.3265135288238525,
      "learning_rate": 1.619625539548181e-05,
      "loss": 0.0964,
      "step": 59850
    },
    {
      "epoch": 3.3531355908467075,
      "grad_norm": 1.5444607734680176,
      "learning_rate": 1.618224115701553e-05,
      "loss": 0.1243,
      "step": 59860
    },
    {
      "epoch": 3.3536957678626447,
      "grad_norm": 3.7066755294799805,
      "learning_rate": 1.6168226918549247e-05,
      "loss": 0.093,
      "step": 59870
    },
    {
      "epoch": 3.354255944878582,
      "grad_norm": 3.171293258666992,
      "learning_rate": 1.6154212680082965e-05,
      "loss": 0.098,
      "step": 59880
    },
    {
      "epoch": 3.3548161218945185,
      "grad_norm": 6.224576473236084,
      "learning_rate": 1.6140198441616683e-05,
      "loss": 0.1596,
      "step": 59890
    },
    {
      "epoch": 3.3553762989104556,
      "grad_norm": 4.166423320770264,
      "learning_rate": 1.61261842031504e-05,
      "loss": 0.1306,
      "step": 59900
    },
    {
      "epoch": 3.3559364759263928,
      "grad_norm": 2.796764612197876,
      "learning_rate": 1.611216996468412e-05,
      "loss": 0.0987,
      "step": 59910
    },
    {
      "epoch": 3.35649665294233,
      "grad_norm": 1.9258426427841187,
      "learning_rate": 1.6098155726217837e-05,
      "loss": 0.1189,
      "step": 59920
    },
    {
      "epoch": 3.3570568299582666,
      "grad_norm": 3.6043570041656494,
      "learning_rate": 1.6084141487751556e-05,
      "loss": 0.1606,
      "step": 59930
    },
    {
      "epoch": 3.3576170069742037,
      "grad_norm": 1.2152905464172363,
      "learning_rate": 1.6070127249285274e-05,
      "loss": 0.1277,
      "step": 59940
    },
    {
      "epoch": 3.358177183990141,
      "grad_norm": 1.500773549079895,
      "learning_rate": 1.6056113010818992e-05,
      "loss": 0.1733,
      "step": 59950
    },
    {
      "epoch": 3.358737361006078,
      "grad_norm": 1.3425991535186768,
      "learning_rate": 1.6042098772352713e-05,
      "loss": 0.0963,
      "step": 59960
    },
    {
      "epoch": 3.359297538022015,
      "grad_norm": 7.202592849731445,
      "learning_rate": 1.6028084533886428e-05,
      "loss": 0.1291,
      "step": 59970
    },
    {
      "epoch": 3.359857715037952,
      "grad_norm": 1.2838025093078613,
      "learning_rate": 1.6014070295420146e-05,
      "loss": 0.1762,
      "step": 59980
    },
    {
      "epoch": 3.360417892053889,
      "grad_norm": 5.107777118682861,
      "learning_rate": 1.6000056056953864e-05,
      "loss": 0.1275,
      "step": 59990
    },
    {
      "epoch": 3.360978069069826,
      "grad_norm": 4.352349758148193,
      "learning_rate": 1.5986041818487586e-05,
      "loss": 0.1532,
      "step": 60000
    },
    {
      "epoch": 3.3615382460857632,
      "grad_norm": 1.863373041152954,
      "learning_rate": 1.5972027580021304e-05,
      "loss": 0.0835,
      "step": 60010
    },
    {
      "epoch": 3.3620984231017,
      "grad_norm": 1.3963546752929688,
      "learning_rate": 1.595801334155502e-05,
      "loss": 0.1051,
      "step": 60020
    },
    {
      "epoch": 3.362658600117637,
      "grad_norm": 0.8946248888969421,
      "learning_rate": 1.594399910308874e-05,
      "loss": 0.085,
      "step": 60030
    },
    {
      "epoch": 3.363218777133574,
      "grad_norm": 2.252467632293701,
      "learning_rate": 1.592998486462246e-05,
      "loss": 0.122,
      "step": 60040
    },
    {
      "epoch": 3.3637789541495113,
      "grad_norm": 2.4260072708129883,
      "learning_rate": 1.5915970626156176e-05,
      "loss": 0.2675,
      "step": 60050
    },
    {
      "epoch": 3.3643391311654485,
      "grad_norm": 1.6051666736602783,
      "learning_rate": 1.590195638768989e-05,
      "loss": 0.091,
      "step": 60060
    },
    {
      "epoch": 3.364899308181385,
      "grad_norm": 3.3923373222351074,
      "learning_rate": 1.5887942149223613e-05,
      "loss": 0.1106,
      "step": 60070
    },
    {
      "epoch": 3.3654594851973223,
      "grad_norm": 1.5705066919326782,
      "learning_rate": 1.587392791075733e-05,
      "loss": 0.0937,
      "step": 60080
    },
    {
      "epoch": 3.3660196622132594,
      "grad_norm": 3.2512731552124023,
      "learning_rate": 1.585991367229105e-05,
      "loss": 0.0942,
      "step": 60090
    },
    {
      "epoch": 3.3665798392291966,
      "grad_norm": 1.0030328035354614,
      "learning_rate": 1.5845899433824767e-05,
      "loss": 0.1218,
      "step": 60100
    },
    {
      "epoch": 3.3671400162451333,
      "grad_norm": 0.9099421501159668,
      "learning_rate": 1.5831885195358485e-05,
      "loss": 0.1065,
      "step": 60110
    },
    {
      "epoch": 3.3677001932610704,
      "grad_norm": 2.818460464477539,
      "learning_rate": 1.5817870956892203e-05,
      "loss": 0.0935,
      "step": 60120
    },
    {
      "epoch": 3.3682603702770075,
      "grad_norm": 0.6688805818557739,
      "learning_rate": 1.580385671842592e-05,
      "loss": 0.1137,
      "step": 60130
    },
    {
      "epoch": 3.3688205472929447,
      "grad_norm": 3.023012638092041,
      "learning_rate": 1.578984247995964e-05,
      "loss": 0.1406,
      "step": 60140
    },
    {
      "epoch": 3.369380724308882,
      "grad_norm": 3.6943442821502686,
      "learning_rate": 1.5775828241493358e-05,
      "loss": 0.1008,
      "step": 60150
    },
    {
      "epoch": 3.3699409013248185,
      "grad_norm": 1.079852819442749,
      "learning_rate": 1.5761814003027076e-05,
      "loss": 0.0945,
      "step": 60160
    },
    {
      "epoch": 3.3705010783407556,
      "grad_norm": 3.236151933670044,
      "learning_rate": 1.5747799764560794e-05,
      "loss": 0.103,
      "step": 60170
    },
    {
      "epoch": 3.3710612553566928,
      "grad_norm": 2.5138208866119385,
      "learning_rate": 1.5733785526094512e-05,
      "loss": 0.1514,
      "step": 60180
    },
    {
      "epoch": 3.37162143237263,
      "grad_norm": 1.8821260929107666,
      "learning_rate": 1.5719771287628234e-05,
      "loss": 0.1178,
      "step": 60190
    },
    {
      "epoch": 3.3721816093885666,
      "grad_norm": 2.770084857940674,
      "learning_rate": 1.570575704916195e-05,
      "loss": 0.1405,
      "step": 60200
    },
    {
      "epoch": 3.3727417864045037,
      "grad_norm": 1.4923821687698364,
      "learning_rate": 1.5691742810695667e-05,
      "loss": 0.1574,
      "step": 60210
    },
    {
      "epoch": 3.373301963420441,
      "grad_norm": 1.7333098649978638,
      "learning_rate": 1.5677728572229385e-05,
      "loss": 0.1251,
      "step": 60220
    },
    {
      "epoch": 3.373862140436378,
      "grad_norm": 0.9392908215522766,
      "learning_rate": 1.5663714333763106e-05,
      "loss": 0.1519,
      "step": 60230
    },
    {
      "epoch": 3.374422317452315,
      "grad_norm": 2.375751256942749,
      "learning_rate": 1.5649700095296824e-05,
      "loss": 0.1008,
      "step": 60240
    },
    {
      "epoch": 3.374982494468252,
      "grad_norm": 4.016846179962158,
      "learning_rate": 1.563568585683054e-05,
      "loss": 0.0982,
      "step": 60250
    },
    {
      "epoch": 3.375542671484189,
      "grad_norm": 1.2513090372085571,
      "learning_rate": 1.5621671618364257e-05,
      "loss": 0.0794,
      "step": 60260
    },
    {
      "epoch": 3.376102848500126,
      "grad_norm": 1.7003406286239624,
      "learning_rate": 1.560765737989798e-05,
      "loss": 0.0789,
      "step": 60270
    },
    {
      "epoch": 3.3766630255160632,
      "grad_norm": 3.501616954803467,
      "learning_rate": 1.5593643141431697e-05,
      "loss": 0.1382,
      "step": 60280
    },
    {
      "epoch": 3.377223202532,
      "grad_norm": 1.8596999645233154,
      "learning_rate": 1.557962890296541e-05,
      "loss": 0.0893,
      "step": 60290
    },
    {
      "epoch": 3.377783379547937,
      "grad_norm": 0.5761237144470215,
      "learning_rate": 1.556561466449913e-05,
      "loss": 0.1604,
      "step": 60300
    },
    {
      "epoch": 3.378343556563874,
      "grad_norm": 2.5198566913604736,
      "learning_rate": 1.555160042603285e-05,
      "loss": 0.1041,
      "step": 60310
    },
    {
      "epoch": 3.3789037335798113,
      "grad_norm": 4.389797687530518,
      "learning_rate": 1.553758618756657e-05,
      "loss": 0.1211,
      "step": 60320
    },
    {
      "epoch": 3.3794639105957485,
      "grad_norm": 1.9509514570236206,
      "learning_rate": 1.5523571949100288e-05,
      "loss": 0.1189,
      "step": 60330
    },
    {
      "epoch": 3.380024087611685,
      "grad_norm": 2.290558099746704,
      "learning_rate": 1.5509557710634002e-05,
      "loss": 0.095,
      "step": 60340
    },
    {
      "epoch": 3.3805842646276223,
      "grad_norm": 1.6885337829589844,
      "learning_rate": 1.5495543472167724e-05,
      "loss": 0.1622,
      "step": 60350
    },
    {
      "epoch": 3.3811444416435594,
      "grad_norm": 1.4371525049209595,
      "learning_rate": 1.5481529233701442e-05,
      "loss": 0.107,
      "step": 60360
    },
    {
      "epoch": 3.3817046186594966,
      "grad_norm": 0.6791096329689026,
      "learning_rate": 1.546751499523516e-05,
      "loss": 0.1597,
      "step": 60370
    },
    {
      "epoch": 3.3822647956754333,
      "grad_norm": 2.6031017303466797,
      "learning_rate": 1.5453500756768878e-05,
      "loss": 0.1334,
      "step": 60380
    },
    {
      "epoch": 3.3828249726913704,
      "grad_norm": 0.5056564211845398,
      "learning_rate": 1.5439486518302596e-05,
      "loss": 0.1294,
      "step": 60390
    },
    {
      "epoch": 3.3833851497073075,
      "grad_norm": 1.3083242177963257,
      "learning_rate": 1.5425472279836314e-05,
      "loss": 0.1362,
      "step": 60400
    },
    {
      "epoch": 3.3839453267232447,
      "grad_norm": 1.4597983360290527,
      "learning_rate": 1.5411458041370033e-05,
      "loss": 0.1529,
      "step": 60410
    },
    {
      "epoch": 3.384505503739182,
      "grad_norm": 1.6700596809387207,
      "learning_rate": 1.539744380290375e-05,
      "loss": 0.0877,
      "step": 60420
    },
    {
      "epoch": 3.3850656807551185,
      "grad_norm": 1.0731515884399414,
      "learning_rate": 1.538342956443747e-05,
      "loss": 0.0924,
      "step": 60430
    },
    {
      "epoch": 3.3856258577710556,
      "grad_norm": 2.0462803840637207,
      "learning_rate": 1.5369415325971187e-05,
      "loss": 0.1312,
      "step": 60440
    },
    {
      "epoch": 3.3861860347869928,
      "grad_norm": 1.963684320449829,
      "learning_rate": 1.5355401087504905e-05,
      "loss": 0.1797,
      "step": 60450
    },
    {
      "epoch": 3.38674621180293,
      "grad_norm": 2.074434280395508,
      "learning_rate": 1.5341386849038623e-05,
      "loss": 0.0953,
      "step": 60460
    },
    {
      "epoch": 3.3873063888188666,
      "grad_norm": 2.3506290912628174,
      "learning_rate": 1.532737261057234e-05,
      "loss": 0.1476,
      "step": 60470
    },
    {
      "epoch": 3.3878665658348037,
      "grad_norm": 2.897664785385132,
      "learning_rate": 1.531335837210606e-05,
      "loss": 0.1136,
      "step": 60480
    },
    {
      "epoch": 3.388426742850741,
      "grad_norm": 2.1879618167877197,
      "learning_rate": 1.5299344133639778e-05,
      "loss": 0.0931,
      "step": 60490
    },
    {
      "epoch": 3.388986919866678,
      "grad_norm": 3.060183048248291,
      "learning_rate": 1.5285329895173496e-05,
      "loss": 0.1749,
      "step": 60500
    },
    {
      "epoch": 3.389547096882615,
      "grad_norm": 3.714512586593628,
      "learning_rate": 1.5271315656707217e-05,
      "loss": 0.108,
      "step": 60510
    },
    {
      "epoch": 3.390107273898552,
      "grad_norm": 0.6660033464431763,
      "learning_rate": 1.5257301418240932e-05,
      "loss": 0.086,
      "step": 60520
    },
    {
      "epoch": 3.390667450914489,
      "grad_norm": 2.1776070594787598,
      "learning_rate": 1.5243287179774652e-05,
      "loss": 0.0905,
      "step": 60530
    },
    {
      "epoch": 3.391227627930426,
      "grad_norm": 0.7257583141326904,
      "learning_rate": 1.522927294130837e-05,
      "loss": 0.0753,
      "step": 60540
    },
    {
      "epoch": 3.3917878049463632,
      "grad_norm": 0.6760731935501099,
      "learning_rate": 1.521525870284209e-05,
      "loss": 0.1253,
      "step": 60550
    },
    {
      "epoch": 3.3923479819623,
      "grad_norm": 0.8189485669136047,
      "learning_rate": 1.5201244464375808e-05,
      "loss": 0.0996,
      "step": 60560
    },
    {
      "epoch": 3.392908158978237,
      "grad_norm": 2.462726593017578,
      "learning_rate": 1.5187230225909524e-05,
      "loss": 0.1154,
      "step": 60570
    },
    {
      "epoch": 3.393468335994174,
      "grad_norm": 3.6088013648986816,
      "learning_rate": 1.5173215987443243e-05,
      "loss": 0.1503,
      "step": 60580
    },
    {
      "epoch": 3.3940285130101113,
      "grad_norm": 0.9488279223442078,
      "learning_rate": 1.5159201748976962e-05,
      "loss": 0.1274,
      "step": 60590
    },
    {
      "epoch": 3.3945886900260485,
      "grad_norm": 1.2011538743972778,
      "learning_rate": 1.514518751051068e-05,
      "loss": 0.1043,
      "step": 60600
    },
    {
      "epoch": 3.395148867041985,
      "grad_norm": 1.6683884859085083,
      "learning_rate": 1.5131173272044397e-05,
      "loss": 0.1251,
      "step": 60610
    },
    {
      "epoch": 3.3957090440579223,
      "grad_norm": 4.023934364318848,
      "learning_rate": 1.5117159033578115e-05,
      "loss": 0.1066,
      "step": 60620
    },
    {
      "epoch": 3.3962692210738594,
      "grad_norm": 3.1807045936584473,
      "learning_rate": 1.5103144795111835e-05,
      "loss": 0.0954,
      "step": 60630
    },
    {
      "epoch": 3.3968293980897966,
      "grad_norm": 1.4149504899978638,
      "learning_rate": 1.5089130556645553e-05,
      "loss": 0.089,
      "step": 60640
    },
    {
      "epoch": 3.3973895751057333,
      "grad_norm": 2.7538552284240723,
      "learning_rate": 1.5075116318179273e-05,
      "loss": 0.1057,
      "step": 60650
    },
    {
      "epoch": 3.3979497521216704,
      "grad_norm": 0.6228281855583191,
      "learning_rate": 1.5061102079712988e-05,
      "loss": 0.0878,
      "step": 60660
    },
    {
      "epoch": 3.3985099291376075,
      "grad_norm": 2.8272781372070312,
      "learning_rate": 1.5047087841246707e-05,
      "loss": 0.0939,
      "step": 60670
    },
    {
      "epoch": 3.3990701061535447,
      "grad_norm": 6.2494025230407715,
      "learning_rate": 1.5033073602780426e-05,
      "loss": 0.1619,
      "step": 60680
    },
    {
      "epoch": 3.399630283169482,
      "grad_norm": 2.4627745151519775,
      "learning_rate": 1.5019059364314145e-05,
      "loss": 0.094,
      "step": 60690
    },
    {
      "epoch": 3.4001904601854185,
      "grad_norm": 2.520231246948242,
      "learning_rate": 1.500504512584786e-05,
      "loss": 0.1125,
      "step": 60700
    },
    {
      "epoch": 3.4007506372013556,
      "grad_norm": 1.8033078908920288,
      "learning_rate": 1.499103088738158e-05,
      "loss": 0.1766,
      "step": 60710
    },
    {
      "epoch": 3.4013108142172928,
      "grad_norm": 1.9875717163085938,
      "learning_rate": 1.4977016648915298e-05,
      "loss": 0.0725,
      "step": 60720
    },
    {
      "epoch": 3.40187099123323,
      "grad_norm": 1.5220855474472046,
      "learning_rate": 1.4963002410449018e-05,
      "loss": 0.12,
      "step": 60730
    },
    {
      "epoch": 3.4024311682491666,
      "grad_norm": 4.053224086761475,
      "learning_rate": 1.4948988171982736e-05,
      "loss": 0.1814,
      "step": 60740
    },
    {
      "epoch": 3.4029913452651037,
      "grad_norm": 2.42901611328125,
      "learning_rate": 1.4934973933516452e-05,
      "loss": 0.1367,
      "step": 60750
    },
    {
      "epoch": 3.403551522281041,
      "grad_norm": 1.0870254039764404,
      "learning_rate": 1.492095969505017e-05,
      "loss": 0.1462,
      "step": 60760
    },
    {
      "epoch": 3.404111699296978,
      "grad_norm": 1.1038812398910522,
      "learning_rate": 1.490694545658389e-05,
      "loss": 0.1189,
      "step": 60770
    },
    {
      "epoch": 3.404671876312915,
      "grad_norm": 3.6005845069885254,
      "learning_rate": 1.4892931218117609e-05,
      "loss": 0.0834,
      "step": 60780
    },
    {
      "epoch": 3.405232053328852,
      "grad_norm": 5.166678428649902,
      "learning_rate": 1.4878916979651328e-05,
      "loss": 0.1648,
      "step": 60790
    },
    {
      "epoch": 3.405792230344789,
      "grad_norm": 0.7662099003791809,
      "learning_rate": 1.4864902741185043e-05,
      "loss": 0.1148,
      "step": 60800
    },
    {
      "epoch": 3.406352407360726,
      "grad_norm": 1.9272667169570923,
      "learning_rate": 1.4850888502718763e-05,
      "loss": 0.1193,
      "step": 60810
    },
    {
      "epoch": 3.406912584376663,
      "grad_norm": 4.180962562561035,
      "learning_rate": 1.4836874264252481e-05,
      "loss": 0.1309,
      "step": 60820
    },
    {
      "epoch": 3.4074727613926,
      "grad_norm": 1.0086480379104614,
      "learning_rate": 1.4822860025786201e-05,
      "loss": 0.0876,
      "step": 60830
    },
    {
      "epoch": 3.408032938408537,
      "grad_norm": 2.378054141998291,
      "learning_rate": 1.4808845787319917e-05,
      "loss": 0.099,
      "step": 60840
    },
    {
      "epoch": 3.408593115424474,
      "grad_norm": 1.7504026889801025,
      "learning_rate": 1.4794831548853635e-05,
      "loss": 0.1465,
      "step": 60850
    },
    {
      "epoch": 3.4091532924404113,
      "grad_norm": 3.5554325580596924,
      "learning_rate": 1.4780817310387354e-05,
      "loss": 0.0952,
      "step": 60860
    },
    {
      "epoch": 3.4097134694563485,
      "grad_norm": 1.0145814418792725,
      "learning_rate": 1.4766803071921073e-05,
      "loss": 0.1143,
      "step": 60870
    },
    {
      "epoch": 3.410273646472285,
      "grad_norm": 1.331672191619873,
      "learning_rate": 1.4752788833454792e-05,
      "loss": 0.0673,
      "step": 60880
    },
    {
      "epoch": 3.4108338234882223,
      "grad_norm": 1.577475666999817,
      "learning_rate": 1.4738774594988508e-05,
      "loss": 0.1028,
      "step": 60890
    },
    {
      "epoch": 3.4113940005041594,
      "grad_norm": 1.188585877418518,
      "learning_rate": 1.4724760356522228e-05,
      "loss": 0.0841,
      "step": 60900
    },
    {
      "epoch": 3.411954177520096,
      "grad_norm": 1.5737621784210205,
      "learning_rate": 1.4710746118055946e-05,
      "loss": 0.0844,
      "step": 60910
    },
    {
      "epoch": 3.4125143545360332,
      "grad_norm": 3.5824520587921143,
      "learning_rate": 1.4696731879589664e-05,
      "loss": 0.1208,
      "step": 60920
    },
    {
      "epoch": 3.4130745315519704,
      "grad_norm": 1.622202754020691,
      "learning_rate": 1.468271764112338e-05,
      "loss": 0.1131,
      "step": 60930
    },
    {
      "epoch": 3.4136347085679075,
      "grad_norm": 0.48827090859413147,
      "learning_rate": 1.46687034026571e-05,
      "loss": 0.146,
      "step": 60940
    },
    {
      "epoch": 3.4141948855838447,
      "grad_norm": 1.5895284414291382,
      "learning_rate": 1.4654689164190818e-05,
      "loss": 0.1121,
      "step": 60950
    },
    {
      "epoch": 3.4147550625997813,
      "grad_norm": 1.272706389427185,
      "learning_rate": 1.4640674925724537e-05,
      "loss": 0.0742,
      "step": 60960
    },
    {
      "epoch": 3.4153152396157185,
      "grad_norm": 3.997950315475464,
      "learning_rate": 1.4626660687258256e-05,
      "loss": 0.1023,
      "step": 60970
    },
    {
      "epoch": 3.4158754166316556,
      "grad_norm": 4.25130033493042,
      "learning_rate": 1.4612646448791973e-05,
      "loss": 0.1103,
      "step": 60980
    },
    {
      "epoch": 3.4164355936475927,
      "grad_norm": 3.8955013751983643,
      "learning_rate": 1.4598632210325691e-05,
      "loss": 0.1168,
      "step": 60990
    },
    {
      "epoch": 3.4169957706635294,
      "grad_norm": 2.917692184448242,
      "learning_rate": 1.458461797185941e-05,
      "loss": 0.1168,
      "step": 61000
    },
    {
      "epoch": 3.4175559476794666,
      "grad_norm": 1.2420904636383057,
      "learning_rate": 1.4570603733393129e-05,
      "loss": 0.1432,
      "step": 61010
    },
    {
      "epoch": 3.4181161246954037,
      "grad_norm": 2.79553484916687,
      "learning_rate": 1.4556589494926847e-05,
      "loss": 0.1359,
      "step": 61020
    },
    {
      "epoch": 3.418676301711341,
      "grad_norm": 0.87990403175354,
      "learning_rate": 1.4542575256460564e-05,
      "loss": 0.088,
      "step": 61030
    },
    {
      "epoch": 3.419236478727278,
      "grad_norm": 1.572776436805725,
      "learning_rate": 1.4528561017994283e-05,
      "loss": 0.1519,
      "step": 61040
    },
    {
      "epoch": 3.4197966557432147,
      "grad_norm": 3.888370990753174,
      "learning_rate": 1.4514546779528001e-05,
      "loss": 0.1204,
      "step": 61050
    },
    {
      "epoch": 3.420356832759152,
      "grad_norm": 1.3278496265411377,
      "learning_rate": 1.450053254106172e-05,
      "loss": 0.088,
      "step": 61060
    },
    {
      "epoch": 3.420917009775089,
      "grad_norm": 1.9128135442733765,
      "learning_rate": 1.4486518302595436e-05,
      "loss": 0.077,
      "step": 61070
    },
    {
      "epoch": 3.421477186791026,
      "grad_norm": 3.962245464324951,
      "learning_rate": 1.4472504064129156e-05,
      "loss": 0.1373,
      "step": 61080
    },
    {
      "epoch": 3.4220373638069628,
      "grad_norm": 0.8230745792388916,
      "learning_rate": 1.4458489825662874e-05,
      "loss": 0.1095,
      "step": 61090
    },
    {
      "epoch": 3.4225975408229,
      "grad_norm": 1.7329492568969727,
      "learning_rate": 1.4444475587196594e-05,
      "loss": 0.0861,
      "step": 61100
    },
    {
      "epoch": 3.423157717838837,
      "grad_norm": 5.473269939422607,
      "learning_rate": 1.4430461348730312e-05,
      "loss": 0.1276,
      "step": 61110
    },
    {
      "epoch": 3.423717894854774,
      "grad_norm": 2.216808319091797,
      "learning_rate": 1.4416447110264028e-05,
      "loss": 0.0901,
      "step": 61120
    },
    {
      "epoch": 3.4242780718707113,
      "grad_norm": 1.829246163368225,
      "learning_rate": 1.4402432871797747e-05,
      "loss": 0.0661,
      "step": 61130
    },
    {
      "epoch": 3.424838248886648,
      "grad_norm": 2.462416172027588,
      "learning_rate": 1.4388418633331466e-05,
      "loss": 0.1253,
      "step": 61140
    },
    {
      "epoch": 3.425398425902585,
      "grad_norm": 0.9345658421516418,
      "learning_rate": 1.4374404394865184e-05,
      "loss": 0.1383,
      "step": 61150
    },
    {
      "epoch": 3.4259586029185223,
      "grad_norm": 1.8014005422592163,
      "learning_rate": 1.4360390156398901e-05,
      "loss": 0.084,
      "step": 61160
    },
    {
      "epoch": 3.4265187799344594,
      "grad_norm": 4.506470203399658,
      "learning_rate": 1.4346375917932619e-05,
      "loss": 0.1032,
      "step": 61170
    },
    {
      "epoch": 3.427078956950396,
      "grad_norm": 5.520998954772949,
      "learning_rate": 1.4332361679466339e-05,
      "loss": 0.231,
      "step": 61180
    },
    {
      "epoch": 3.4276391339663332,
      "grad_norm": 0.9547963738441467,
      "learning_rate": 1.4318347441000057e-05,
      "loss": 0.1315,
      "step": 61190
    },
    {
      "epoch": 3.4281993109822704,
      "grad_norm": 4.521153450012207,
      "learning_rate": 1.4304333202533777e-05,
      "loss": 0.1479,
      "step": 61200
    },
    {
      "epoch": 3.4287594879982075,
      "grad_norm": 0.813167929649353,
      "learning_rate": 1.4290318964067492e-05,
      "loss": 0.0969,
      "step": 61210
    },
    {
      "epoch": 3.4293196650141446,
      "grad_norm": 1.577538013458252,
      "learning_rate": 1.4276304725601211e-05,
      "loss": 0.0831,
      "step": 61220
    },
    {
      "epoch": 3.4298798420300813,
      "grad_norm": 0.9493497610092163,
      "learning_rate": 1.426229048713493e-05,
      "loss": 0.0976,
      "step": 61230
    },
    {
      "epoch": 3.4304400190460185,
      "grad_norm": 2.647289752960205,
      "learning_rate": 1.424827624866865e-05,
      "loss": 0.1195,
      "step": 61240
    },
    {
      "epoch": 3.4310001960619556,
      "grad_norm": 0.8542068600654602,
      "learning_rate": 1.4234262010202366e-05,
      "loss": 0.121,
      "step": 61250
    },
    {
      "epoch": 3.4315603730778927,
      "grad_norm": 2.1242191791534424,
      "learning_rate": 1.4220247771736084e-05,
      "loss": 0.0744,
      "step": 61260
    },
    {
      "epoch": 3.4321205500938294,
      "grad_norm": 3.075502634048462,
      "learning_rate": 1.4206233533269802e-05,
      "loss": 0.0852,
      "step": 61270
    },
    {
      "epoch": 3.4326807271097666,
      "grad_norm": 2.1133811473846436,
      "learning_rate": 1.4192219294803522e-05,
      "loss": 0.1603,
      "step": 61280
    },
    {
      "epoch": 3.4332409041257037,
      "grad_norm": 2.610046863555908,
      "learning_rate": 1.417820505633724e-05,
      "loss": 0.0809,
      "step": 61290
    },
    {
      "epoch": 3.433801081141641,
      "grad_norm": 2.6562440395355225,
      "learning_rate": 1.4164190817870956e-05,
      "loss": 0.0879,
      "step": 61300
    },
    {
      "epoch": 3.434361258157578,
      "grad_norm": 4.463907241821289,
      "learning_rate": 1.4150176579404675e-05,
      "loss": 0.1845,
      "step": 61310
    },
    {
      "epoch": 3.4349214351735147,
      "grad_norm": 0.7538941502571106,
      "learning_rate": 1.4136162340938394e-05,
      "loss": 0.0949,
      "step": 61320
    },
    {
      "epoch": 3.435481612189452,
      "grad_norm": 1.2150354385375977,
      "learning_rate": 1.4122148102472113e-05,
      "loss": 0.1573,
      "step": 61330
    },
    {
      "epoch": 3.436041789205389,
      "grad_norm": 3.1242289543151855,
      "learning_rate": 1.4108133864005832e-05,
      "loss": 0.1318,
      "step": 61340
    },
    {
      "epoch": 3.436601966221326,
      "grad_norm": 3.026087760925293,
      "learning_rate": 1.4094119625539549e-05,
      "loss": 0.1332,
      "step": 61350
    },
    {
      "epoch": 3.4371621432372628,
      "grad_norm": 3.715533494949341,
      "learning_rate": 1.4080105387073267e-05,
      "loss": 0.1335,
      "step": 61360
    },
    {
      "epoch": 3.4377223202532,
      "grad_norm": 1.6670730113983154,
      "learning_rate": 1.4066091148606985e-05,
      "loss": 0.0959,
      "step": 61370
    },
    {
      "epoch": 3.438282497269137,
      "grad_norm": 2.209321975708008,
      "learning_rate": 1.4052076910140705e-05,
      "loss": 0.1724,
      "step": 61380
    },
    {
      "epoch": 3.438842674285074,
      "grad_norm": 1.0890973806381226,
      "learning_rate": 1.4038062671674421e-05,
      "loss": 0.1295,
      "step": 61390
    },
    {
      "epoch": 3.4394028513010113,
      "grad_norm": 2.4714322090148926,
      "learning_rate": 1.402404843320814e-05,
      "loss": 0.0882,
      "step": 61400
    },
    {
      "epoch": 3.439963028316948,
      "grad_norm": 1.4614728689193726,
      "learning_rate": 1.4010034194741858e-05,
      "loss": 0.1039,
      "step": 61410
    },
    {
      "epoch": 3.440523205332885,
      "grad_norm": 2.571359872817993,
      "learning_rate": 1.3996019956275577e-05,
      "loss": 0.1363,
      "step": 61420
    },
    {
      "epoch": 3.4410833823488223,
      "grad_norm": 4.667875289916992,
      "learning_rate": 1.3982005717809296e-05,
      "loss": 0.1449,
      "step": 61430
    },
    {
      "epoch": 3.4416435593647594,
      "grad_norm": 2.66519832611084,
      "learning_rate": 1.3967991479343012e-05,
      "loss": 0.1857,
      "step": 61440
    },
    {
      "epoch": 3.442203736380696,
      "grad_norm": 1.7109904289245605,
      "learning_rate": 1.3953977240876732e-05,
      "loss": 0.101,
      "step": 61450
    },
    {
      "epoch": 3.4427639133966332,
      "grad_norm": 0.5465870499610901,
      "learning_rate": 1.393996300241045e-05,
      "loss": 0.0949,
      "step": 61460
    },
    {
      "epoch": 3.4433240904125704,
      "grad_norm": 0.8484376668930054,
      "learning_rate": 1.3925948763944168e-05,
      "loss": 0.0844,
      "step": 61470
    },
    {
      "epoch": 3.4438842674285075,
      "grad_norm": 0.8310772776603699,
      "learning_rate": 1.3911934525477884e-05,
      "loss": 0.0746,
      "step": 61480
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 2.737311840057373,
      "learning_rate": 1.3897920287011604e-05,
      "loss": 0.0824,
      "step": 61490
    },
    {
      "epoch": 3.4450046214603813,
      "grad_norm": 3.3367273807525635,
      "learning_rate": 1.3883906048545322e-05,
      "loss": 0.0974,
      "step": 61500
    },
    {
      "epoch": 3.4455647984763185,
      "grad_norm": 0.6709444522857666,
      "learning_rate": 1.3869891810079042e-05,
      "loss": 0.1528,
      "step": 61510
    },
    {
      "epoch": 3.4461249754922556,
      "grad_norm": 5.13486909866333,
      "learning_rate": 1.385587757161276e-05,
      "loss": 0.1241,
      "step": 61520
    },
    {
      "epoch": 3.4466851525081927,
      "grad_norm": 4.505711555480957,
      "learning_rate": 1.3841863333146477e-05,
      "loss": 0.127,
      "step": 61530
    },
    {
      "epoch": 3.4472453295241294,
      "grad_norm": 1.783268690109253,
      "learning_rate": 1.3827849094680195e-05,
      "loss": 0.0981,
      "step": 61540
    },
    {
      "epoch": 3.4478055065400666,
      "grad_norm": 2.0389060974121094,
      "learning_rate": 1.3813834856213915e-05,
      "loss": 0.0826,
      "step": 61550
    },
    {
      "epoch": 3.4483656835560037,
      "grad_norm": 1.7210897207260132,
      "learning_rate": 1.3799820617747633e-05,
      "loss": 0.0745,
      "step": 61560
    },
    {
      "epoch": 3.448925860571941,
      "grad_norm": 0.9551751613616943,
      "learning_rate": 1.3785806379281351e-05,
      "loss": 0.0986,
      "step": 61570
    },
    {
      "epoch": 3.449486037587878,
      "grad_norm": 1.4602503776550293,
      "learning_rate": 1.3771792140815067e-05,
      "loss": 0.0961,
      "step": 61580
    },
    {
      "epoch": 3.4500462146038147,
      "grad_norm": 2.1385347843170166,
      "learning_rate": 1.3757777902348787e-05,
      "loss": 0.1151,
      "step": 61590
    },
    {
      "epoch": 3.450606391619752,
      "grad_norm": 2.2264180183410645,
      "learning_rate": 1.3743763663882505e-05,
      "loss": 0.1074,
      "step": 61600
    },
    {
      "epoch": 3.451166568635689,
      "grad_norm": 2.1882035732269287,
      "learning_rate": 1.3729749425416225e-05,
      "loss": 0.0907,
      "step": 61610
    },
    {
      "epoch": 3.451726745651626,
      "grad_norm": 0.7494767904281616,
      "learning_rate": 1.371573518694994e-05,
      "loss": 0.0716,
      "step": 61620
    },
    {
      "epoch": 3.4522869226675628,
      "grad_norm": 0.4660620391368866,
      "learning_rate": 1.370172094848366e-05,
      "loss": 0.0998,
      "step": 61630
    },
    {
      "epoch": 3.4528470996835,
      "grad_norm": 1.470279574394226,
      "learning_rate": 1.3687706710017378e-05,
      "loss": 0.1021,
      "step": 61640
    },
    {
      "epoch": 3.453407276699437,
      "grad_norm": 1.0922118425369263,
      "learning_rate": 1.3673692471551098e-05,
      "loss": 0.1716,
      "step": 61650
    },
    {
      "epoch": 3.453967453715374,
      "grad_norm": 3.747101068496704,
      "learning_rate": 1.3659678233084816e-05,
      "loss": 0.1196,
      "step": 61660
    },
    {
      "epoch": 3.4545276307313113,
      "grad_norm": 1.0136977434158325,
      "learning_rate": 1.3645663994618532e-05,
      "loss": 0.0969,
      "step": 61670
    },
    {
      "epoch": 3.455087807747248,
      "grad_norm": 1.629098653793335,
      "learning_rate": 1.363164975615225e-05,
      "loss": 0.1,
      "step": 61680
    },
    {
      "epoch": 3.455647984763185,
      "grad_norm": 2.353757381439209,
      "learning_rate": 1.361763551768597e-05,
      "loss": 0.1612,
      "step": 61690
    },
    {
      "epoch": 3.4562081617791223,
      "grad_norm": 4.085355758666992,
      "learning_rate": 1.3603621279219688e-05,
      "loss": 0.1632,
      "step": 61700
    },
    {
      "epoch": 3.4567683387950594,
      "grad_norm": 1.8453725576400757,
      "learning_rate": 1.3589607040753405e-05,
      "loss": 0.125,
      "step": 61710
    },
    {
      "epoch": 3.457328515810996,
      "grad_norm": 2.0525078773498535,
      "learning_rate": 1.3575592802287123e-05,
      "loss": 0.1041,
      "step": 61720
    },
    {
      "epoch": 3.4578886928269332,
      "grad_norm": 3.2767624855041504,
      "learning_rate": 1.3561578563820843e-05,
      "loss": 0.1412,
      "step": 61730
    },
    {
      "epoch": 3.4584488698428704,
      "grad_norm": 4.307493686676025,
      "learning_rate": 1.3547564325354561e-05,
      "loss": 0.1271,
      "step": 61740
    },
    {
      "epoch": 3.4590090468588075,
      "grad_norm": 2.2391324043273926,
      "learning_rate": 1.353355008688828e-05,
      "loss": 0.0948,
      "step": 61750
    },
    {
      "epoch": 3.4595692238747446,
      "grad_norm": 2.3884479999542236,
      "learning_rate": 1.3519535848421996e-05,
      "loss": 0.143,
      "step": 61760
    },
    {
      "epoch": 3.4601294008906813,
      "grad_norm": 2.068957567214966,
      "learning_rate": 1.3505521609955715e-05,
      "loss": 0.1215,
      "step": 61770
    },
    {
      "epoch": 3.4606895779066185,
      "grad_norm": 3.5219385623931885,
      "learning_rate": 1.3491507371489434e-05,
      "loss": 0.1066,
      "step": 61780
    },
    {
      "epoch": 3.4612497549225556,
      "grad_norm": 0.9618404507637024,
      "learning_rate": 1.3477493133023153e-05,
      "loss": 0.0871,
      "step": 61790
    },
    {
      "epoch": 3.4618099319384927,
      "grad_norm": 1.0436760187149048,
      "learning_rate": 1.346347889455687e-05,
      "loss": 0.145,
      "step": 61800
    },
    {
      "epoch": 3.4623701089544294,
      "grad_norm": 0.9257228970527649,
      "learning_rate": 1.3449464656090588e-05,
      "loss": 0.0805,
      "step": 61810
    },
    {
      "epoch": 3.4629302859703666,
      "grad_norm": 2.8426735401153564,
      "learning_rate": 1.3435450417624306e-05,
      "loss": 0.096,
      "step": 61820
    },
    {
      "epoch": 3.4634904629863037,
      "grad_norm": 2.5065126419067383,
      "learning_rate": 1.3421436179158026e-05,
      "loss": 0.0876,
      "step": 61830
    },
    {
      "epoch": 3.464050640002241,
      "grad_norm": 0.5091090202331543,
      "learning_rate": 1.3407421940691744e-05,
      "loss": 0.1289,
      "step": 61840
    },
    {
      "epoch": 3.464610817018178,
      "grad_norm": 1.6154483556747437,
      "learning_rate": 1.339340770222546e-05,
      "loss": 0.2063,
      "step": 61850
    },
    {
      "epoch": 3.4651709940341147,
      "grad_norm": 0.9220567345619202,
      "learning_rate": 1.337939346375918e-05,
      "loss": 0.0725,
      "step": 61860
    },
    {
      "epoch": 3.465731171050052,
      "grad_norm": 5.11158561706543,
      "learning_rate": 1.3365379225292898e-05,
      "loss": 0.1171,
      "step": 61870
    },
    {
      "epoch": 3.466291348065989,
      "grad_norm": 1.059287667274475,
      "learning_rate": 1.3351364986826617e-05,
      "loss": 0.0963,
      "step": 61880
    },
    {
      "epoch": 3.466851525081926,
      "grad_norm": 2.047666311264038,
      "learning_rate": 1.3337350748360336e-05,
      "loss": 0.0771,
      "step": 61890
    },
    {
      "epoch": 3.4674117020978628,
      "grad_norm": 4.585951328277588,
      "learning_rate": 1.3323336509894053e-05,
      "loss": 0.1031,
      "step": 61900
    },
    {
      "epoch": 3.4679718791138,
      "grad_norm": 3.327589750289917,
      "learning_rate": 1.3309322271427771e-05,
      "loss": 0.1092,
      "step": 61910
    },
    {
      "epoch": 3.468532056129737,
      "grad_norm": 2.857470989227295,
      "learning_rate": 1.3295308032961489e-05,
      "loss": 0.0935,
      "step": 61920
    },
    {
      "epoch": 3.469092233145674,
      "grad_norm": 1.239016056060791,
      "learning_rate": 1.3281293794495209e-05,
      "loss": 0.1251,
      "step": 61930
    },
    {
      "epoch": 3.4696524101616113,
      "grad_norm": 2.4428141117095947,
      "learning_rate": 1.3267279556028925e-05,
      "loss": 0.1338,
      "step": 61940
    },
    {
      "epoch": 3.470212587177548,
      "grad_norm": 1.3139584064483643,
      "learning_rate": 1.3253265317562643e-05,
      "loss": 0.0983,
      "step": 61950
    },
    {
      "epoch": 3.470772764193485,
      "grad_norm": 2.3880538940429688,
      "learning_rate": 1.3239251079096363e-05,
      "loss": 0.0893,
      "step": 61960
    },
    {
      "epoch": 3.4713329412094223,
      "grad_norm": 1.5067880153656006,
      "learning_rate": 1.3225236840630081e-05,
      "loss": 0.1162,
      "step": 61970
    },
    {
      "epoch": 3.4718931182253594,
      "grad_norm": 4.511417388916016,
      "learning_rate": 1.32112226021638e-05,
      "loss": 0.095,
      "step": 61980
    },
    {
      "epoch": 3.472453295241296,
      "grad_norm": 1.3008999824523926,
      "learning_rate": 1.3197208363697516e-05,
      "loss": 0.1205,
      "step": 61990
    },
    {
      "epoch": 3.4730134722572332,
      "grad_norm": 1.1891930103302002,
      "learning_rate": 1.3183194125231236e-05,
      "loss": 0.082,
      "step": 62000
    },
    {
      "epoch": 3.4735736492731704,
      "grad_norm": 1.0946685075759888,
      "learning_rate": 1.3169179886764954e-05,
      "loss": 0.1064,
      "step": 62010
    },
    {
      "epoch": 3.4741338262891075,
      "grad_norm": 5.7599968910217285,
      "learning_rate": 1.3155165648298674e-05,
      "loss": 0.1885,
      "step": 62020
    },
    {
      "epoch": 3.4746940033050446,
      "grad_norm": 1.1375765800476074,
      "learning_rate": 1.3141151409832388e-05,
      "loss": 0.1004,
      "step": 62030
    },
    {
      "epoch": 3.4752541803209813,
      "grad_norm": 1.3257200717926025,
      "learning_rate": 1.3127137171366108e-05,
      "loss": 0.1161,
      "step": 62040
    },
    {
      "epoch": 3.4758143573369185,
      "grad_norm": 3.37399959564209,
      "learning_rate": 1.3113122932899826e-05,
      "loss": 0.1017,
      "step": 62050
    },
    {
      "epoch": 3.4763745343528556,
      "grad_norm": 4.7531328201293945,
      "learning_rate": 1.3099108694433546e-05,
      "loss": 0.1053,
      "step": 62060
    },
    {
      "epoch": 3.4769347113687927,
      "grad_norm": 5.3843488693237305,
      "learning_rate": 1.3085094455967264e-05,
      "loss": 0.1304,
      "step": 62070
    },
    {
      "epoch": 3.4774948883847294,
      "grad_norm": 1.0615450143814087,
      "learning_rate": 1.307108021750098e-05,
      "loss": 0.0986,
      "step": 62080
    },
    {
      "epoch": 3.4780550654006666,
      "grad_norm": 1.4231175184249878,
      "learning_rate": 1.3057065979034699e-05,
      "loss": 0.1149,
      "step": 62090
    },
    {
      "epoch": 3.4786152424166037,
      "grad_norm": 2.3613908290863037,
      "learning_rate": 1.3043051740568419e-05,
      "loss": 0.1767,
      "step": 62100
    },
    {
      "epoch": 3.479175419432541,
      "grad_norm": 1.117081642150879,
      "learning_rate": 1.3029037502102137e-05,
      "loss": 0.1029,
      "step": 62110
    },
    {
      "epoch": 3.479735596448478,
      "grad_norm": 1.4636123180389404,
      "learning_rate": 1.3015023263635857e-05,
      "loss": 0.0922,
      "step": 62120
    },
    {
      "epoch": 3.4802957734644147,
      "grad_norm": 3.0038137435913086,
      "learning_rate": 1.3001009025169571e-05,
      "loss": 0.0984,
      "step": 62130
    },
    {
      "epoch": 3.480855950480352,
      "grad_norm": 1.7623764276504517,
      "learning_rate": 1.2986994786703291e-05,
      "loss": 0.1151,
      "step": 62140
    },
    {
      "epoch": 3.481416127496289,
      "grad_norm": 4.577624320983887,
      "learning_rate": 1.297298054823701e-05,
      "loss": 0.1414,
      "step": 62150
    },
    {
      "epoch": 3.4819763045122256,
      "grad_norm": 3.4462361335754395,
      "learning_rate": 1.295896630977073e-05,
      "loss": 0.092,
      "step": 62160
    },
    {
      "epoch": 3.4825364815281628,
      "grad_norm": 0.6337170004844666,
      "learning_rate": 1.2944952071304444e-05,
      "loss": 0.0699,
      "step": 62170
    },
    {
      "epoch": 3.4830966585441,
      "grad_norm": 0.6448855996131897,
      "learning_rate": 1.2930937832838164e-05,
      "loss": 0.0771,
      "step": 62180
    },
    {
      "epoch": 3.483656835560037,
      "grad_norm": 4.802000045776367,
      "learning_rate": 1.2916923594371882e-05,
      "loss": 0.135,
      "step": 62190
    },
    {
      "epoch": 3.484217012575974,
      "grad_norm": 3.7384235858917236,
      "learning_rate": 1.2902909355905602e-05,
      "loss": 0.108,
      "step": 62200
    },
    {
      "epoch": 3.484777189591911,
      "grad_norm": 2.170865535736084,
      "learning_rate": 1.288889511743932e-05,
      "loss": 0.0977,
      "step": 62210
    },
    {
      "epoch": 3.485337366607848,
      "grad_norm": 1.5430585145950317,
      "learning_rate": 1.2874880878973036e-05,
      "loss": 0.122,
      "step": 62220
    },
    {
      "epoch": 3.485897543623785,
      "grad_norm": 1.1579965353012085,
      "learning_rate": 1.2860866640506754e-05,
      "loss": 0.1087,
      "step": 62230
    },
    {
      "epoch": 3.4864577206397223,
      "grad_norm": 1.2883628606796265,
      "learning_rate": 1.2846852402040474e-05,
      "loss": 0.1256,
      "step": 62240
    },
    {
      "epoch": 3.487017897655659,
      "grad_norm": 3.3004322052001953,
      "learning_rate": 1.2832838163574192e-05,
      "loss": 0.1492,
      "step": 62250
    },
    {
      "epoch": 3.487578074671596,
      "grad_norm": 5.420833587646484,
      "learning_rate": 1.2818823925107909e-05,
      "loss": 0.1162,
      "step": 62260
    },
    {
      "epoch": 3.488138251687533,
      "grad_norm": 1.7393155097961426,
      "learning_rate": 1.2804809686641627e-05,
      "loss": 0.0799,
      "step": 62270
    },
    {
      "epoch": 3.4886984287034704,
      "grad_norm": 4.016873359680176,
      "learning_rate": 1.2790795448175347e-05,
      "loss": 0.133,
      "step": 62280
    },
    {
      "epoch": 3.4892586057194075,
      "grad_norm": 1.866208553314209,
      "learning_rate": 1.2776781209709065e-05,
      "loss": 0.1495,
      "step": 62290
    },
    {
      "epoch": 3.489818782735344,
      "grad_norm": 0.7961155772209167,
      "learning_rate": 1.2762766971242785e-05,
      "loss": 0.0844,
      "step": 62300
    },
    {
      "epoch": 3.4903789597512813,
      "grad_norm": 4.035044193267822,
      "learning_rate": 1.2748752732776501e-05,
      "loss": 0.1686,
      "step": 62310
    },
    {
      "epoch": 3.4909391367672185,
      "grad_norm": 2.206190586090088,
      "learning_rate": 1.273473849431022e-05,
      "loss": 0.1115,
      "step": 62320
    },
    {
      "epoch": 3.4914993137831556,
      "grad_norm": 1.2850396633148193,
      "learning_rate": 1.2720724255843937e-05,
      "loss": 0.1116,
      "step": 62330
    },
    {
      "epoch": 3.4920594907990923,
      "grad_norm": 0.8820683360099792,
      "learning_rate": 1.2706710017377657e-05,
      "loss": 0.1526,
      "step": 62340
    },
    {
      "epoch": 3.4926196678150294,
      "grad_norm": 1.8054287433624268,
      "learning_rate": 1.2692695778911374e-05,
      "loss": 0.0909,
      "step": 62350
    },
    {
      "epoch": 3.4931798448309666,
      "grad_norm": 0.5099029541015625,
      "learning_rate": 1.2678681540445092e-05,
      "loss": 0.0903,
      "step": 62360
    },
    {
      "epoch": 3.4937400218469037,
      "grad_norm": 4.1319661140441895,
      "learning_rate": 1.2664667301978812e-05,
      "loss": 0.1026,
      "step": 62370
    },
    {
      "epoch": 3.494300198862841,
      "grad_norm": 1.5088872909545898,
      "learning_rate": 1.265065306351253e-05,
      "loss": 0.1153,
      "step": 62380
    },
    {
      "epoch": 3.4948603758787775,
      "grad_norm": 1.6976487636566162,
      "learning_rate": 1.2636638825046248e-05,
      "loss": 0.0775,
      "step": 62390
    },
    {
      "epoch": 3.4954205528947146,
      "grad_norm": 1.942636489868164,
      "learning_rate": 1.2622624586579964e-05,
      "loss": 0.1115,
      "step": 62400
    },
    {
      "epoch": 3.495980729910652,
      "grad_norm": 3.3503637313842773,
      "learning_rate": 1.2608610348113684e-05,
      "loss": 0.1201,
      "step": 62410
    },
    {
      "epoch": 3.496540906926589,
      "grad_norm": 1.5719678401947021,
      "learning_rate": 1.2594596109647402e-05,
      "loss": 0.1143,
      "step": 62420
    },
    {
      "epoch": 3.4971010839425256,
      "grad_norm": 1.418522596359253,
      "learning_rate": 1.258058187118112e-05,
      "loss": 0.1512,
      "step": 62430
    },
    {
      "epoch": 3.4976612609584627,
      "grad_norm": 1.1111218929290771,
      "learning_rate": 1.256656763271484e-05,
      "loss": 0.116,
      "step": 62440
    },
    {
      "epoch": 3.4982214379744,
      "grad_norm": 1.9284240007400513,
      "learning_rate": 1.2552553394248557e-05,
      "loss": 0.0895,
      "step": 62450
    },
    {
      "epoch": 3.498781614990337,
      "grad_norm": 2.0993247032165527,
      "learning_rate": 1.2538539155782275e-05,
      "loss": 0.1953,
      "step": 62460
    },
    {
      "epoch": 3.499341792006274,
      "grad_norm": 1.2347325086593628,
      "learning_rate": 1.2524524917315995e-05,
      "loss": 0.1748,
      "step": 62470
    },
    {
      "epoch": 3.499901969022211,
      "grad_norm": 2.434931516647339,
      "learning_rate": 1.2510510678849713e-05,
      "loss": 0.0855,
      "step": 62480
    },
    {
      "epoch": 3.500462146038148,
      "grad_norm": 2.601576089859009,
      "learning_rate": 1.2496496440383431e-05,
      "loss": 0.0793,
      "step": 62490
    },
    {
      "epoch": 3.501022323054085,
      "grad_norm": 4.294299602508545,
      "learning_rate": 1.2482482201917147e-05,
      "loss": 0.094,
      "step": 62500
    },
    {
      "epoch": 3.5015825000700223,
      "grad_norm": 0.9295383095741272,
      "learning_rate": 1.2468467963450867e-05,
      "loss": 0.184,
      "step": 62510
    },
    {
      "epoch": 3.502142677085959,
      "grad_norm": 3.2794957160949707,
      "learning_rate": 1.2454453724984585e-05,
      "loss": 0.1357,
      "step": 62520
    },
    {
      "epoch": 3.502702854101896,
      "grad_norm": 1.4548732042312622,
      "learning_rate": 1.2440439486518304e-05,
      "loss": 0.0886,
      "step": 62530
    },
    {
      "epoch": 3.503263031117833,
      "grad_norm": 1.4373294115066528,
      "learning_rate": 1.2426425248052022e-05,
      "loss": 0.0842,
      "step": 62540
    },
    {
      "epoch": 3.5038232081337704,
      "grad_norm": 2.1153950691223145,
      "learning_rate": 1.241241100958574e-05,
      "loss": 0.0855,
      "step": 62550
    },
    {
      "epoch": 3.5043833851497075,
      "grad_norm": 3.702760934829712,
      "learning_rate": 1.2398396771119458e-05,
      "loss": 0.1016,
      "step": 62560
    },
    {
      "epoch": 3.504943562165644,
      "grad_norm": 1.019831657409668,
      "learning_rate": 1.2384382532653176e-05,
      "loss": 0.0887,
      "step": 62570
    },
    {
      "epoch": 3.5055037391815813,
      "grad_norm": 1.170276165008545,
      "learning_rate": 1.2370368294186894e-05,
      "loss": 0.1452,
      "step": 62580
    },
    {
      "epoch": 3.5060639161975184,
      "grad_norm": 2.2596447467803955,
      "learning_rate": 1.2356354055720614e-05,
      "loss": 0.1447,
      "step": 62590
    },
    {
      "epoch": 3.5066240932134556,
      "grad_norm": 4.647976398468018,
      "learning_rate": 1.234233981725433e-05,
      "loss": 0.0984,
      "step": 62600
    },
    {
      "epoch": 3.5071842702293923,
      "grad_norm": 4.113625526428223,
      "learning_rate": 1.232832557878805e-05,
      "loss": 0.1169,
      "step": 62610
    },
    {
      "epoch": 3.5077444472453294,
      "grad_norm": 1.0412564277648926,
      "learning_rate": 1.2314311340321767e-05,
      "loss": 0.0919,
      "step": 62620
    },
    {
      "epoch": 3.5083046242612665,
      "grad_norm": 4.002978324890137,
      "learning_rate": 1.2300297101855487e-05,
      "loss": 0.1085,
      "step": 62630
    },
    {
      "epoch": 3.5088648012772037,
      "grad_norm": 2.1888487339019775,
      "learning_rate": 1.2286282863389203e-05,
      "loss": 0.1245,
      "step": 62640
    },
    {
      "epoch": 3.509424978293141,
      "grad_norm": 2.502197265625,
      "learning_rate": 1.2272268624922923e-05,
      "loss": 0.0954,
      "step": 62650
    },
    {
      "epoch": 3.5099851553090775,
      "grad_norm": 0.7153126001358032,
      "learning_rate": 1.225825438645664e-05,
      "loss": 0.1493,
      "step": 62660
    },
    {
      "epoch": 3.5105453323250146,
      "grad_norm": 0.588692843914032,
      "learning_rate": 1.2244240147990359e-05,
      "loss": 0.0835,
      "step": 62670
    },
    {
      "epoch": 3.511105509340952,
      "grad_norm": 3.475285053253174,
      "learning_rate": 1.2230225909524077e-05,
      "loss": 0.0958,
      "step": 62680
    },
    {
      "epoch": 3.511665686356889,
      "grad_norm": 3.0418355464935303,
      "learning_rate": 1.2216211671057795e-05,
      "loss": 0.0926,
      "step": 62690
    },
    {
      "epoch": 3.5122258633728256,
      "grad_norm": 0.8281956911087036,
      "learning_rate": 1.2202197432591513e-05,
      "loss": 0.1101,
      "step": 62700
    },
    {
      "epoch": 3.5127860403887627,
      "grad_norm": 1.8619799613952637,
      "learning_rate": 1.2188183194125232e-05,
      "loss": 0.0993,
      "step": 62710
    },
    {
      "epoch": 3.5133462174047,
      "grad_norm": 1.4792710542678833,
      "learning_rate": 1.217416895565895e-05,
      "loss": 0.1085,
      "step": 62720
    },
    {
      "epoch": 3.513906394420637,
      "grad_norm": 1.0689144134521484,
      "learning_rate": 1.2160154717192668e-05,
      "loss": 0.1133,
      "step": 62730
    },
    {
      "epoch": 3.514466571436574,
      "grad_norm": 1.2869281768798828,
      "learning_rate": 1.2146140478726386e-05,
      "loss": 0.1496,
      "step": 62740
    },
    {
      "epoch": 3.515026748452511,
      "grad_norm": 0.615464448928833,
      "learning_rate": 1.2132126240260106e-05,
      "loss": 0.1435,
      "step": 62750
    },
    {
      "epoch": 3.515586925468448,
      "grad_norm": 0.6806870698928833,
      "learning_rate": 1.2118112001793822e-05,
      "loss": 0.1358,
      "step": 62760
    },
    {
      "epoch": 3.516147102484385,
      "grad_norm": 2.8185346126556396,
      "learning_rate": 1.2104097763327542e-05,
      "loss": 0.0878,
      "step": 62770
    },
    {
      "epoch": 3.5167072795003222,
      "grad_norm": 3.60921049118042,
      "learning_rate": 1.2090083524861258e-05,
      "loss": 0.1147,
      "step": 62780
    },
    {
      "epoch": 3.517267456516259,
      "grad_norm": 3.30016827583313,
      "learning_rate": 1.2076069286394978e-05,
      "loss": 0.1028,
      "step": 62790
    },
    {
      "epoch": 3.517827633532196,
      "grad_norm": 0.8123872876167297,
      "learning_rate": 1.2062055047928696e-05,
      "loss": 0.1405,
      "step": 62800
    },
    {
      "epoch": 3.518387810548133,
      "grad_norm": 1.9054529666900635,
      "learning_rate": 1.2048040809462415e-05,
      "loss": 0.089,
      "step": 62810
    },
    {
      "epoch": 3.5189479875640703,
      "grad_norm": 2.554541826248169,
      "learning_rate": 1.2034026570996133e-05,
      "loss": 0.1287,
      "step": 62820
    },
    {
      "epoch": 3.5195081645800075,
      "grad_norm": 1.1731387376785278,
      "learning_rate": 1.202001233252985e-05,
      "loss": 0.1237,
      "step": 62830
    },
    {
      "epoch": 3.520068341595944,
      "grad_norm": 2.0758743286132812,
      "learning_rate": 1.2005998094063569e-05,
      "loss": 0.0966,
      "step": 62840
    },
    {
      "epoch": 3.5206285186118813,
      "grad_norm": 1.2816646099090576,
      "learning_rate": 1.1991983855597287e-05,
      "loss": 0.1143,
      "step": 62850
    },
    {
      "epoch": 3.5211886956278184,
      "grad_norm": 3.138979911804199,
      "learning_rate": 1.1977969617131005e-05,
      "loss": 0.1341,
      "step": 62860
    },
    {
      "epoch": 3.5217488726437556,
      "grad_norm": 2.9611611366271973,
      "learning_rate": 1.1963955378664723e-05,
      "loss": 0.1513,
      "step": 62870
    },
    {
      "epoch": 3.5223090496596923,
      "grad_norm": 3.2019193172454834,
      "learning_rate": 1.1949941140198443e-05,
      "loss": 0.1273,
      "step": 62880
    },
    {
      "epoch": 3.5228692266756294,
      "grad_norm": 0.7478639483451843,
      "learning_rate": 1.193592690173216e-05,
      "loss": 0.1028,
      "step": 62890
    },
    {
      "epoch": 3.5234294036915665,
      "grad_norm": 1.9013209342956543,
      "learning_rate": 1.192191266326588e-05,
      "loss": 0.103,
      "step": 62900
    },
    {
      "epoch": 3.5239895807075037,
      "grad_norm": 1.7394025325775146,
      "learning_rate": 1.1907898424799598e-05,
      "loss": 0.0996,
      "step": 62910
    },
    {
      "epoch": 3.524549757723441,
      "grad_norm": 4.082112789154053,
      "learning_rate": 1.1893884186333316e-05,
      "loss": 0.1119,
      "step": 62920
    },
    {
      "epoch": 3.5251099347393775,
      "grad_norm": 3.171151876449585,
      "learning_rate": 1.1879869947867034e-05,
      "loss": 0.1294,
      "step": 62930
    },
    {
      "epoch": 3.5256701117553146,
      "grad_norm": 0.6886109113693237,
      "learning_rate": 1.1865855709400752e-05,
      "loss": 0.1267,
      "step": 62940
    },
    {
      "epoch": 3.5262302887712518,
      "grad_norm": 5.109856605529785,
      "learning_rate": 1.185184147093447e-05,
      "loss": 0.1987,
      "step": 62950
    },
    {
      "epoch": 3.526790465787189,
      "grad_norm": 0.9577568769454956,
      "learning_rate": 1.1837827232468188e-05,
      "loss": 0.1237,
      "step": 62960
    },
    {
      "epoch": 3.5273506428031256,
      "grad_norm": 3.488525867462158,
      "learning_rate": 1.1823812994001906e-05,
      "loss": 0.1033,
      "step": 62970
    },
    {
      "epoch": 3.5279108198190627,
      "grad_norm": 6.545567512512207,
      "learning_rate": 1.1809798755535626e-05,
      "loss": 0.163,
      "step": 62980
    },
    {
      "epoch": 3.528470996835,
      "grad_norm": 2.0684587955474854,
      "learning_rate": 1.1795784517069343e-05,
      "loss": 0.0919,
      "step": 62990
    },
    {
      "epoch": 3.529031173850937,
      "grad_norm": 2.5159411430358887,
      "learning_rate": 1.1781770278603062e-05,
      "loss": 0.1256,
      "step": 63000
    },
    {
      "epoch": 3.529591350866874,
      "grad_norm": 3.6825947761535645,
      "learning_rate": 1.1767756040136779e-05,
      "loss": 0.1179,
      "step": 63010
    },
    {
      "epoch": 3.530151527882811,
      "grad_norm": 1.0938539505004883,
      "learning_rate": 1.1753741801670499e-05,
      "loss": 0.0632,
      "step": 63020
    },
    {
      "epoch": 3.530711704898748,
      "grad_norm": 1.9935972690582275,
      "learning_rate": 1.1739727563204215e-05,
      "loss": 0.0965,
      "step": 63030
    },
    {
      "epoch": 3.531271881914685,
      "grad_norm": 4.258916854858398,
      "learning_rate": 1.1725713324737935e-05,
      "loss": 0.175,
      "step": 63040
    },
    {
      "epoch": 3.5318320589306222,
      "grad_norm": 1.9728708267211914,
      "learning_rate": 1.1711699086271651e-05,
      "loss": 0.091,
      "step": 63050
    },
    {
      "epoch": 3.532392235946559,
      "grad_norm": 3.7623722553253174,
      "learning_rate": 1.1697684847805371e-05,
      "loss": 0.1175,
      "step": 63060
    },
    {
      "epoch": 3.532952412962496,
      "grad_norm": 2.2246146202087402,
      "learning_rate": 1.168367060933909e-05,
      "loss": 0.0808,
      "step": 63070
    },
    {
      "epoch": 3.533512589978433,
      "grad_norm": 0.4954164922237396,
      "learning_rate": 1.1669656370872807e-05,
      "loss": 0.0943,
      "step": 63080
    },
    {
      "epoch": 3.5340727669943703,
      "grad_norm": 0.7357907295227051,
      "learning_rate": 1.1655642132406526e-05,
      "loss": 0.0784,
      "step": 63090
    },
    {
      "epoch": 3.5346329440103075,
      "grad_norm": 2.110028028488159,
      "learning_rate": 1.1641627893940244e-05,
      "loss": 0.0883,
      "step": 63100
    },
    {
      "epoch": 3.535193121026244,
      "grad_norm": 1.6089338064193726,
      "learning_rate": 1.1627613655473962e-05,
      "loss": 0.0842,
      "step": 63110
    },
    {
      "epoch": 3.5357532980421813,
      "grad_norm": 4.1807475090026855,
      "learning_rate": 1.161359941700768e-05,
      "loss": 0.1143,
      "step": 63120
    },
    {
      "epoch": 3.5363134750581184,
      "grad_norm": 4.161509990692139,
      "learning_rate": 1.1599585178541398e-05,
      "loss": 0.1893,
      "step": 63130
    },
    {
      "epoch": 3.536873652074055,
      "grad_norm": 0.5543531775474548,
      "learning_rate": 1.1585570940075118e-05,
      "loss": 0.1039,
      "step": 63140
    },
    {
      "epoch": 3.5374338290899923,
      "grad_norm": 0.5693126916885376,
      "learning_rate": 1.1571556701608834e-05,
      "loss": 0.0738,
      "step": 63150
    },
    {
      "epoch": 3.5379940061059294,
      "grad_norm": 0.8879414796829224,
      "learning_rate": 1.1557542463142554e-05,
      "loss": 0.0916,
      "step": 63160
    },
    {
      "epoch": 3.5385541831218665,
      "grad_norm": 2.65320086479187,
      "learning_rate": 1.154352822467627e-05,
      "loss": 0.1465,
      "step": 63170
    },
    {
      "epoch": 3.5391143601378037,
      "grad_norm": 1.0242151021957397,
      "learning_rate": 1.152951398620999e-05,
      "loss": 0.1245,
      "step": 63180
    },
    {
      "epoch": 3.539674537153741,
      "grad_norm": 1.1324974298477173,
      "learning_rate": 1.1515499747743707e-05,
      "loss": 0.0733,
      "step": 63190
    },
    {
      "epoch": 3.5402347141696775,
      "grad_norm": 1.5703516006469727,
      "learning_rate": 1.1501485509277427e-05,
      "loss": 0.0821,
      "step": 63200
    },
    {
      "epoch": 3.5407948911856146,
      "grad_norm": 3.7814249992370605,
      "learning_rate": 1.1487471270811143e-05,
      "loss": 0.1013,
      "step": 63210
    },
    {
      "epoch": 3.5413550682015518,
      "grad_norm": 5.082404613494873,
      "learning_rate": 1.1473457032344863e-05,
      "loss": 0.1475,
      "step": 63220
    },
    {
      "epoch": 3.5419152452174885,
      "grad_norm": 2.0499002933502197,
      "learning_rate": 1.1459442793878581e-05,
      "loss": 0.0946,
      "step": 63230
    },
    {
      "epoch": 3.5424754222334256,
      "grad_norm": 2.3278815746307373,
      "learning_rate": 1.14454285554123e-05,
      "loss": 0.0894,
      "step": 63240
    },
    {
      "epoch": 3.5430355992493627,
      "grad_norm": 3.2995333671569824,
      "learning_rate": 1.1431414316946017e-05,
      "loss": 0.1014,
      "step": 63250
    },
    {
      "epoch": 3.5435957762653,
      "grad_norm": 3.6944353580474854,
      "learning_rate": 1.1417400078479736e-05,
      "loss": 0.1053,
      "step": 63260
    },
    {
      "epoch": 3.544155953281237,
      "grad_norm": 2.0467402935028076,
      "learning_rate": 1.1403385840013454e-05,
      "loss": 0.0987,
      "step": 63270
    },
    {
      "epoch": 3.544716130297174,
      "grad_norm": 6.716034412384033,
      "learning_rate": 1.1389371601547172e-05,
      "loss": 0.134,
      "step": 63280
    },
    {
      "epoch": 3.545276307313111,
      "grad_norm": 2.8895089626312256,
      "learning_rate": 1.137535736308089e-05,
      "loss": 0.1348,
      "step": 63290
    },
    {
      "epoch": 3.545836484329048,
      "grad_norm": 3.9913792610168457,
      "learning_rate": 1.136134312461461e-05,
      "loss": 0.1204,
      "step": 63300
    },
    {
      "epoch": 3.546396661344985,
      "grad_norm": 0.7644689083099365,
      "learning_rate": 1.1347328886148328e-05,
      "loss": 0.1014,
      "step": 63310
    },
    {
      "epoch": 3.546956838360922,
      "grad_norm": 4.396152973175049,
      "learning_rate": 1.1333314647682046e-05,
      "loss": 0.1452,
      "step": 63320
    },
    {
      "epoch": 3.547517015376859,
      "grad_norm": 2.464052200317383,
      "learning_rate": 1.1319300409215764e-05,
      "loss": 0.1094,
      "step": 63330
    },
    {
      "epoch": 3.548077192392796,
      "grad_norm": 1.1018257141113281,
      "learning_rate": 1.1305286170749482e-05,
      "loss": 0.0871,
      "step": 63340
    },
    {
      "epoch": 3.548637369408733,
      "grad_norm": 4.387832164764404,
      "learning_rate": 1.12912719322832e-05,
      "loss": 0.1061,
      "step": 63350
    },
    {
      "epoch": 3.5491975464246703,
      "grad_norm": 6.2292633056640625,
      "learning_rate": 1.1277257693816919e-05,
      "loss": 0.146,
      "step": 63360
    },
    {
      "epoch": 3.5497577234406075,
      "grad_norm": 4.255006790161133,
      "learning_rate": 1.1263243455350637e-05,
      "loss": 0.0852,
      "step": 63370
    },
    {
      "epoch": 3.550317900456544,
      "grad_norm": 1.9584585428237915,
      "learning_rate": 1.1249229216884355e-05,
      "loss": 0.1246,
      "step": 63380
    },
    {
      "epoch": 3.5508780774724813,
      "grad_norm": 1.8659471273422241,
      "learning_rate": 1.1235214978418075e-05,
      "loss": 0.1108,
      "step": 63390
    },
    {
      "epoch": 3.5514382544884184,
      "grad_norm": 2.99211049079895,
      "learning_rate": 1.1221200739951791e-05,
      "loss": 0.0958,
      "step": 63400
    },
    {
      "epoch": 3.551998431504355,
      "grad_norm": 2.8459393978118896,
      "learning_rate": 1.1207186501485511e-05,
      "loss": 0.1751,
      "step": 63410
    },
    {
      "epoch": 3.5525586085202923,
      "grad_norm": 1.947617769241333,
      "learning_rate": 1.1193172263019227e-05,
      "loss": 0.0951,
      "step": 63420
    },
    {
      "epoch": 3.5531187855362294,
      "grad_norm": 3.542376756668091,
      "learning_rate": 1.1179158024552947e-05,
      "loss": 0.086,
      "step": 63430
    },
    {
      "epoch": 3.5536789625521665,
      "grad_norm": 7.391509532928467,
      "learning_rate": 1.1165143786086664e-05,
      "loss": 0.1156,
      "step": 63440
    },
    {
      "epoch": 3.5542391395681037,
      "grad_norm": 1.2364658117294312,
      "learning_rate": 1.1151129547620383e-05,
      "loss": 0.0795,
      "step": 63450
    },
    {
      "epoch": 3.554799316584041,
      "grad_norm": 0.7824456691741943,
      "learning_rate": 1.1137115309154102e-05,
      "loss": 0.1317,
      "step": 63460
    },
    {
      "epoch": 3.5553594935999775,
      "grad_norm": 0.7664210796356201,
      "learning_rate": 1.112310107068782e-05,
      "loss": 0.1492,
      "step": 63470
    },
    {
      "epoch": 3.5559196706159146,
      "grad_norm": 1.9713833332061768,
      "learning_rate": 1.1109086832221538e-05,
      "loss": 0.1039,
      "step": 63480
    },
    {
      "epoch": 3.5564798476318518,
      "grad_norm": 5.448173999786377,
      "learning_rate": 1.1095072593755256e-05,
      "loss": 0.1028,
      "step": 63490
    },
    {
      "epoch": 3.5570400246477885,
      "grad_norm": 3.9096648693084717,
      "learning_rate": 1.1081058355288974e-05,
      "loss": 0.0798,
      "step": 63500
    },
    {
      "epoch": 3.5576002016637256,
      "grad_norm": 1.135893702507019,
      "learning_rate": 1.1067044116822692e-05,
      "loss": 0.1325,
      "step": 63510
    },
    {
      "epoch": 3.5581603786796627,
      "grad_norm": 3.886683940887451,
      "learning_rate": 1.105302987835641e-05,
      "loss": 0.2073,
      "step": 63520
    },
    {
      "epoch": 3.5587205556956,
      "grad_norm": 2.8091800212860107,
      "learning_rate": 1.103901563989013e-05,
      "loss": 0.0932,
      "step": 63530
    },
    {
      "epoch": 3.559280732711537,
      "grad_norm": 1.31034255027771,
      "learning_rate": 1.1025001401423847e-05,
      "loss": 0.1471,
      "step": 63540
    },
    {
      "epoch": 3.559840909727474,
      "grad_norm": 1.293485403060913,
      "learning_rate": 1.1010987162957566e-05,
      "loss": 0.0687,
      "step": 63550
    },
    {
      "epoch": 3.560401086743411,
      "grad_norm": 6.203402996063232,
      "learning_rate": 1.0996972924491283e-05,
      "loss": 0.1081,
      "step": 63560
    },
    {
      "epoch": 3.560961263759348,
      "grad_norm": 0.4787296652793884,
      "learning_rate": 1.0982958686025003e-05,
      "loss": 0.0813,
      "step": 63570
    },
    {
      "epoch": 3.561521440775285,
      "grad_norm": 2.2831838130950928,
      "learning_rate": 1.0968944447558719e-05,
      "loss": 0.1168,
      "step": 63580
    },
    {
      "epoch": 3.562081617791222,
      "grad_norm": 0.7272865772247314,
      "learning_rate": 1.0954930209092439e-05,
      "loss": 0.104,
      "step": 63590
    },
    {
      "epoch": 3.562641794807159,
      "grad_norm": 2.9211933612823486,
      "learning_rate": 1.0940915970626155e-05,
      "loss": 0.0987,
      "step": 63600
    },
    {
      "epoch": 3.563201971823096,
      "grad_norm": 2.2235538959503174,
      "learning_rate": 1.0926901732159875e-05,
      "loss": 0.1042,
      "step": 63610
    },
    {
      "epoch": 3.563762148839033,
      "grad_norm": 5.475436687469482,
      "learning_rate": 1.0912887493693593e-05,
      "loss": 0.115,
      "step": 63620
    },
    {
      "epoch": 3.5643223258549703,
      "grad_norm": 1.1494452953338623,
      "learning_rate": 1.0898873255227311e-05,
      "loss": 0.1051,
      "step": 63630
    },
    {
      "epoch": 3.5648825028709075,
      "grad_norm": 6.810654640197754,
      "learning_rate": 1.088485901676103e-05,
      "loss": 0.1263,
      "step": 63640
    },
    {
      "epoch": 3.565442679886844,
      "grad_norm": 1.0240070819854736,
      "learning_rate": 1.0870844778294748e-05,
      "loss": 0.0887,
      "step": 63650
    },
    {
      "epoch": 3.5660028569027813,
      "grad_norm": 3.3818554878234863,
      "learning_rate": 1.0856830539828466e-05,
      "loss": 0.1321,
      "step": 63660
    },
    {
      "epoch": 3.5665630339187184,
      "grad_norm": 2.8726110458374023,
      "learning_rate": 1.0842816301362184e-05,
      "loss": 0.109,
      "step": 63670
    },
    {
      "epoch": 3.567123210934655,
      "grad_norm": 0.5860356092453003,
      "learning_rate": 1.0828802062895902e-05,
      "loss": 0.1361,
      "step": 63680
    },
    {
      "epoch": 3.5676833879505923,
      "grad_norm": 0.8246546387672424,
      "learning_rate": 1.0814787824429622e-05,
      "loss": 0.0916,
      "step": 63690
    },
    {
      "epoch": 3.5682435649665294,
      "grad_norm": 1.0745033025741577,
      "learning_rate": 1.0800773585963338e-05,
      "loss": 0.1312,
      "step": 63700
    },
    {
      "epoch": 3.5688037419824665,
      "grad_norm": 0.8610960245132446,
      "learning_rate": 1.0786759347497058e-05,
      "loss": 0.1045,
      "step": 63710
    },
    {
      "epoch": 3.5693639189984037,
      "grad_norm": 1.1159496307373047,
      "learning_rate": 1.0772745109030775e-05,
      "loss": 0.1208,
      "step": 63720
    },
    {
      "epoch": 3.569924096014341,
      "grad_norm": 0.6232032775878906,
      "learning_rate": 1.0758730870564494e-05,
      "loss": 0.0887,
      "step": 63730
    },
    {
      "epoch": 3.5704842730302775,
      "grad_norm": 7.8672404289245605,
      "learning_rate": 1.0744716632098213e-05,
      "loss": 0.1332,
      "step": 63740
    },
    {
      "epoch": 3.5710444500462146,
      "grad_norm": 4.113609790802002,
      "learning_rate": 1.073070239363193e-05,
      "loss": 0.1624,
      "step": 63750
    },
    {
      "epoch": 3.5716046270621518,
      "grad_norm": 2.877479076385498,
      "learning_rate": 1.0716688155165649e-05,
      "loss": 0.1031,
      "step": 63760
    },
    {
      "epoch": 3.5721648040780885,
      "grad_norm": 5.330853462219238,
      "learning_rate": 1.0702673916699367e-05,
      "loss": 0.1169,
      "step": 63770
    },
    {
      "epoch": 3.5727249810940256,
      "grad_norm": 2.983846664428711,
      "learning_rate": 1.0688659678233085e-05,
      "loss": 0.0981,
      "step": 63780
    },
    {
      "epoch": 3.5732851581099627,
      "grad_norm": 2.469705820083618,
      "learning_rate": 1.0674645439766803e-05,
      "loss": 0.1106,
      "step": 63790
    },
    {
      "epoch": 3.5738453351259,
      "grad_norm": 3.9673244953155518,
      "learning_rate": 1.0660631201300521e-05,
      "loss": 0.1006,
      "step": 63800
    },
    {
      "epoch": 3.574405512141837,
      "grad_norm": 1.2841728925704956,
      "learning_rate": 1.064661696283424e-05,
      "loss": 0.0878,
      "step": 63810
    },
    {
      "epoch": 3.574965689157774,
      "grad_norm": 1.4246505498886108,
      "learning_rate": 1.0632602724367958e-05,
      "loss": 0.1156,
      "step": 63820
    },
    {
      "epoch": 3.575525866173711,
      "grad_norm": 1.0069228410720825,
      "learning_rate": 1.0618588485901676e-05,
      "loss": 0.1019,
      "step": 63830
    },
    {
      "epoch": 3.576086043189648,
      "grad_norm": 1.9184249639511108,
      "learning_rate": 1.0604574247435396e-05,
      "loss": 0.0847,
      "step": 63840
    },
    {
      "epoch": 3.576646220205585,
      "grad_norm": 3.109907388687134,
      "learning_rate": 1.0590560008969114e-05,
      "loss": 0.139,
      "step": 63850
    },
    {
      "epoch": 3.577206397221522,
      "grad_norm": 0.8451648950576782,
      "learning_rate": 1.0576545770502832e-05,
      "loss": 0.1404,
      "step": 63860
    },
    {
      "epoch": 3.577766574237459,
      "grad_norm": 1.1680272817611694,
      "learning_rate": 1.056253153203655e-05,
      "loss": 0.0734,
      "step": 63870
    },
    {
      "epoch": 3.578326751253396,
      "grad_norm": 4.708595275878906,
      "learning_rate": 1.0548517293570268e-05,
      "loss": 0.0928,
      "step": 63880
    },
    {
      "epoch": 3.578886928269333,
      "grad_norm": 1.0481767654418945,
      "learning_rate": 1.0534503055103986e-05,
      "loss": 0.1878,
      "step": 63890
    },
    {
      "epoch": 3.5794471052852703,
      "grad_norm": 2.008768320083618,
      "learning_rate": 1.0520488816637704e-05,
      "loss": 0.0958,
      "step": 63900
    },
    {
      "epoch": 3.580007282301207,
      "grad_norm": 2.151778221130371,
      "learning_rate": 1.0506474578171423e-05,
      "loss": 0.0942,
      "step": 63910
    },
    {
      "epoch": 3.580567459317144,
      "grad_norm": 3.117260694503784,
      "learning_rate": 1.0492460339705142e-05,
      "loss": 0.1206,
      "step": 63920
    },
    {
      "epoch": 3.5811276363330813,
      "grad_norm": 1.401865839958191,
      "learning_rate": 1.0478446101238859e-05,
      "loss": 0.1056,
      "step": 63930
    },
    {
      "epoch": 3.5816878133490184,
      "grad_norm": 2.361574172973633,
      "learning_rate": 1.0464431862772579e-05,
      "loss": 0.0978,
      "step": 63940
    },
    {
      "epoch": 3.582247990364955,
      "grad_norm": 0.8033369779586792,
      "learning_rate": 1.0450417624306295e-05,
      "loss": 0.0702,
      "step": 63950
    },
    {
      "epoch": 3.5828081673808923,
      "grad_norm": 0.8619202971458435,
      "learning_rate": 1.0436403385840015e-05,
      "loss": 0.095,
      "step": 63960
    },
    {
      "epoch": 3.5833683443968294,
      "grad_norm": 1.434730887413025,
      "learning_rate": 1.0422389147373731e-05,
      "loss": 0.1157,
      "step": 63970
    },
    {
      "epoch": 3.5839285214127665,
      "grad_norm": 1.2042633295059204,
      "learning_rate": 1.0408374908907451e-05,
      "loss": 0.1217,
      "step": 63980
    },
    {
      "epoch": 3.5844886984287037,
      "grad_norm": 3.3035969734191895,
      "learning_rate": 1.0394360670441168e-05,
      "loss": 0.1654,
      "step": 63990
    },
    {
      "epoch": 3.5850488754446403,
      "grad_norm": 1.3540352582931519,
      "learning_rate": 1.0380346431974887e-05,
      "loss": 0.0657,
      "step": 64000
    },
    {
      "epoch": 3.5856090524605775,
      "grad_norm": 2.5776071548461914,
      "learning_rate": 1.0366332193508606e-05,
      "loss": 0.0942,
      "step": 64010
    },
    {
      "epoch": 3.5861692294765146,
      "grad_norm": 2.096503257751465,
      "learning_rate": 1.0352317955042324e-05,
      "loss": 0.1137,
      "step": 64020
    },
    {
      "epoch": 3.5867294064924518,
      "grad_norm": 1.624603271484375,
      "learning_rate": 1.0338303716576042e-05,
      "loss": 0.0751,
      "step": 64030
    },
    {
      "epoch": 3.5872895835083884,
      "grad_norm": 0.830292284488678,
      "learning_rate": 1.032428947810976e-05,
      "loss": 0.1423,
      "step": 64040
    },
    {
      "epoch": 3.5878497605243256,
      "grad_norm": 2.1672096252441406,
      "learning_rate": 1.0310275239643478e-05,
      "loss": 0.189,
      "step": 64050
    },
    {
      "epoch": 3.5884099375402627,
      "grad_norm": 2.755164384841919,
      "learning_rate": 1.0296261001177196e-05,
      "loss": 0.2022,
      "step": 64060
    },
    {
      "epoch": 3.5889701145562,
      "grad_norm": 4.537144660949707,
      "learning_rate": 1.0282246762710914e-05,
      "loss": 0.1266,
      "step": 64070
    },
    {
      "epoch": 3.589530291572137,
      "grad_norm": 0.8845065832138062,
      "learning_rate": 1.0268232524244634e-05,
      "loss": 0.0793,
      "step": 64080
    },
    {
      "epoch": 3.5900904685880737,
      "grad_norm": 0.9860884547233582,
      "learning_rate": 1.025421828577835e-05,
      "loss": 0.0844,
      "step": 64090
    },
    {
      "epoch": 3.590650645604011,
      "grad_norm": 4.326413154602051,
      "learning_rate": 1.024020404731207e-05,
      "loss": 0.1199,
      "step": 64100
    },
    {
      "epoch": 3.591210822619948,
      "grad_norm": 3.0771443843841553,
      "learning_rate": 1.0226189808845787e-05,
      "loss": 0.0964,
      "step": 64110
    },
    {
      "epoch": 3.591770999635885,
      "grad_norm": 4.012955665588379,
      "learning_rate": 1.0212175570379507e-05,
      "loss": 0.073,
      "step": 64120
    },
    {
      "epoch": 3.592331176651822,
      "grad_norm": 0.9084320664405823,
      "learning_rate": 1.0198161331913223e-05,
      "loss": 0.1011,
      "step": 64130
    },
    {
      "epoch": 3.592891353667759,
      "grad_norm": 2.559568405151367,
      "learning_rate": 1.0184147093446943e-05,
      "loss": 0.0999,
      "step": 64140
    },
    {
      "epoch": 3.593451530683696,
      "grad_norm": 3.870736837387085,
      "learning_rate": 1.017013285498066e-05,
      "loss": 0.0895,
      "step": 64150
    },
    {
      "epoch": 3.594011707699633,
      "grad_norm": 2.588513135910034,
      "learning_rate": 1.015611861651438e-05,
      "loss": 0.0898,
      "step": 64160
    },
    {
      "epoch": 3.5945718847155703,
      "grad_norm": 1.1596184968948364,
      "learning_rate": 1.0142104378048097e-05,
      "loss": 0.0873,
      "step": 64170
    },
    {
      "epoch": 3.595132061731507,
      "grad_norm": 1.3109782934188843,
      "learning_rate": 1.0128090139581815e-05,
      "loss": 0.0985,
      "step": 64180
    },
    {
      "epoch": 3.595692238747444,
      "grad_norm": 0.5783345103263855,
      "learning_rate": 1.0114075901115534e-05,
      "loss": 0.0919,
      "step": 64190
    },
    {
      "epoch": 3.5962524157633813,
      "grad_norm": 1.3259509801864624,
      "learning_rate": 1.0100061662649252e-05,
      "loss": 0.0808,
      "step": 64200
    },
    {
      "epoch": 3.5968125927793184,
      "grad_norm": 0.634121298789978,
      "learning_rate": 1.008604742418297e-05,
      "loss": 0.0763,
      "step": 64210
    },
    {
      "epoch": 3.597372769795255,
      "grad_norm": 1.102885127067566,
      "learning_rate": 1.0072033185716688e-05,
      "loss": 0.0821,
      "step": 64220
    },
    {
      "epoch": 3.5979329468111922,
      "grad_norm": 5.892719745635986,
      "learning_rate": 1.0058018947250406e-05,
      "loss": 0.1491,
      "step": 64230
    },
    {
      "epoch": 3.5984931238271294,
      "grad_norm": 1.0616883039474487,
      "learning_rate": 1.0044004708784126e-05,
      "loss": 0.1072,
      "step": 64240
    },
    {
      "epoch": 3.5990533008430665,
      "grad_norm": 0.6032638549804688,
      "learning_rate": 1.0029990470317842e-05,
      "loss": 0.0837,
      "step": 64250
    },
    {
      "epoch": 3.5996134778590037,
      "grad_norm": 1.3402326107025146,
      "learning_rate": 1.0015976231851562e-05,
      "loss": 0.1083,
      "step": 64260
    },
    {
      "epoch": 3.6001736548749403,
      "grad_norm": 1.6812736988067627,
      "learning_rate": 1.000196199338528e-05,
      "loss": 0.1514,
      "step": 64270
    },
    {
      "epoch": 3.6007338318908775,
      "grad_norm": 0.677218496799469,
      "learning_rate": 9.987947754918998e-06,
      "loss": 0.073,
      "step": 64280
    },
    {
      "epoch": 3.6012940089068146,
      "grad_norm": 5.959051132202148,
      "learning_rate": 9.973933516452717e-06,
      "loss": 0.1348,
      "step": 64290
    },
    {
      "epoch": 3.6018541859227517,
      "grad_norm": 0.6337058544158936,
      "learning_rate": 9.959919277986435e-06,
      "loss": 0.0864,
      "step": 64300
    },
    {
      "epoch": 3.6024143629386884,
      "grad_norm": 3.713974714279175,
      "learning_rate": 9.945905039520153e-06,
      "loss": 0.2016,
      "step": 64310
    },
    {
      "epoch": 3.6029745399546256,
      "grad_norm": 1.5323504209518433,
      "learning_rate": 9.931890801053871e-06,
      "loss": 0.1681,
      "step": 64320
    },
    {
      "epoch": 3.6035347169705627,
      "grad_norm": 0.8763026595115662,
      "learning_rate": 9.917876562587589e-06,
      "loss": 0.0804,
      "step": 64330
    },
    {
      "epoch": 3.6040948939865,
      "grad_norm": 1.4877485036849976,
      "learning_rate": 9.903862324121307e-06,
      "loss": 0.0865,
      "step": 64340
    },
    {
      "epoch": 3.604655071002437,
      "grad_norm": 1.4254392385482788,
      "learning_rate": 9.889848085655027e-06,
      "loss": 0.0754,
      "step": 64350
    },
    {
      "epoch": 3.6052152480183737,
      "grad_norm": 0.9525784254074097,
      "learning_rate": 9.875833847188744e-06,
      "loss": 0.0931,
      "step": 64360
    },
    {
      "epoch": 3.605775425034311,
      "grad_norm": 0.9251575469970703,
      "learning_rate": 9.861819608722463e-06,
      "loss": 0.1024,
      "step": 64370
    },
    {
      "epoch": 3.606335602050248,
      "grad_norm": 2.054896593093872,
      "learning_rate": 9.84780537025618e-06,
      "loss": 0.1297,
      "step": 64380
    },
    {
      "epoch": 3.6068957790661846,
      "grad_norm": 1.8042712211608887,
      "learning_rate": 9.8337911317899e-06,
      "loss": 0.1731,
      "step": 64390
    },
    {
      "epoch": 3.6074559560821218,
      "grad_norm": 2.182985544204712,
      "learning_rate": 9.819776893323618e-06,
      "loss": 0.1023,
      "step": 64400
    },
    {
      "epoch": 3.608016133098059,
      "grad_norm": 2.9307875633239746,
      "learning_rate": 9.805762654857336e-06,
      "loss": 0.1601,
      "step": 64410
    },
    {
      "epoch": 3.608576310113996,
      "grad_norm": 0.5124914050102234,
      "learning_rate": 9.791748416391054e-06,
      "loss": 0.1029,
      "step": 64420
    },
    {
      "epoch": 3.609136487129933,
      "grad_norm": 3.684636116027832,
      "learning_rate": 9.777734177924772e-06,
      "loss": 0.1098,
      "step": 64430
    },
    {
      "epoch": 3.6096966641458703,
      "grad_norm": 1.3561781644821167,
      "learning_rate": 9.76371993945849e-06,
      "loss": 0.0998,
      "step": 64440
    },
    {
      "epoch": 3.610256841161807,
      "grad_norm": 1.4521205425262451,
      "learning_rate": 9.749705700992208e-06,
      "loss": 0.0955,
      "step": 64450
    },
    {
      "epoch": 3.610817018177744,
      "grad_norm": 1.2880600690841675,
      "learning_rate": 9.735691462525927e-06,
      "loss": 0.1177,
      "step": 64460
    },
    {
      "epoch": 3.6113771951936813,
      "grad_norm": 2.6349446773529053,
      "learning_rate": 9.721677224059646e-06,
      "loss": 0.1103,
      "step": 64470
    },
    {
      "epoch": 3.611937372209618,
      "grad_norm": 1.7296786308288574,
      "learning_rate": 9.707662985593363e-06,
      "loss": 0.1452,
      "step": 64480
    },
    {
      "epoch": 3.612497549225555,
      "grad_norm": 3.214811325073242,
      "learning_rate": 9.693648747127083e-06,
      "loss": 0.1563,
      "step": 64490
    },
    {
      "epoch": 3.6130577262414922,
      "grad_norm": 0.748271107673645,
      "learning_rate": 9.679634508660799e-06,
      "loss": 0.2031,
      "step": 64500
    },
    {
      "epoch": 3.6136179032574294,
      "grad_norm": 1.6064413785934448,
      "learning_rate": 9.665620270194519e-06,
      "loss": 0.0891,
      "step": 64510
    },
    {
      "epoch": 3.6141780802733665,
      "grad_norm": 1.464464545249939,
      "learning_rate": 9.651606031728235e-06,
      "loss": 0.0963,
      "step": 64520
    },
    {
      "epoch": 3.6147382572893036,
      "grad_norm": 1.0976533889770508,
      "learning_rate": 9.637591793261955e-06,
      "loss": 0.1348,
      "step": 64530
    },
    {
      "epoch": 3.6152984343052403,
      "grad_norm": 2.871260404586792,
      "learning_rate": 9.623577554795672e-06,
      "loss": 0.1213,
      "step": 64540
    },
    {
      "epoch": 3.6158586113211775,
      "grad_norm": 1.348850965499878,
      "learning_rate": 9.609563316329391e-06,
      "loss": 0.089,
      "step": 64550
    },
    {
      "epoch": 3.6164187883371146,
      "grad_norm": 2.112569570541382,
      "learning_rate": 9.59554907786311e-06,
      "loss": 0.1149,
      "step": 64560
    },
    {
      "epoch": 3.6169789653530513,
      "grad_norm": 3.100844383239746,
      "learning_rate": 9.581534839396828e-06,
      "loss": 0.1235,
      "step": 64570
    },
    {
      "epoch": 3.6175391423689884,
      "grad_norm": 0.8247731328010559,
      "learning_rate": 9.567520600930546e-06,
      "loss": 0.1279,
      "step": 64580
    },
    {
      "epoch": 3.6180993193849256,
      "grad_norm": 0.656050443649292,
      "learning_rate": 9.553506362464264e-06,
      "loss": 0.0764,
      "step": 64590
    },
    {
      "epoch": 3.6186594964008627,
      "grad_norm": 1.130886435508728,
      "learning_rate": 9.539492123997982e-06,
      "loss": 0.1036,
      "step": 64600
    },
    {
      "epoch": 3.6192196734168,
      "grad_norm": 0.691569447517395,
      "learning_rate": 9.5254778855317e-06,
      "loss": 0.0833,
      "step": 64610
    },
    {
      "epoch": 3.619779850432737,
      "grad_norm": 3.7455027103424072,
      "learning_rate": 9.511463647065418e-06,
      "loss": 0.0806,
      "step": 64620
    },
    {
      "epoch": 3.6203400274486737,
      "grad_norm": 5.769714832305908,
      "learning_rate": 9.497449408599138e-06,
      "loss": 0.1188,
      "step": 64630
    },
    {
      "epoch": 3.620900204464611,
      "grad_norm": 0.7616424560546875,
      "learning_rate": 9.483435170132855e-06,
      "loss": 0.1196,
      "step": 64640
    },
    {
      "epoch": 3.621460381480548,
      "grad_norm": 1.326194405555725,
      "learning_rate": 9.469420931666574e-06,
      "loss": 0.0966,
      "step": 64650
    },
    {
      "epoch": 3.6220205584964846,
      "grad_norm": 5.684115409851074,
      "learning_rate": 9.455406693200291e-06,
      "loss": 0.12,
      "step": 64660
    },
    {
      "epoch": 3.6225807355124218,
      "grad_norm": 2.7470688819885254,
      "learning_rate": 9.44139245473401e-06,
      "loss": 0.0695,
      "step": 64670
    },
    {
      "epoch": 3.623140912528359,
      "grad_norm": 1.3823939561843872,
      "learning_rate": 9.427378216267727e-06,
      "loss": 0.1587,
      "step": 64680
    },
    {
      "epoch": 3.623701089544296,
      "grad_norm": 3.8727152347564697,
      "learning_rate": 9.413363977801447e-06,
      "loss": 0.0909,
      "step": 64690
    },
    {
      "epoch": 3.624261266560233,
      "grad_norm": 3.934303045272827,
      "learning_rate": 9.399349739335165e-06,
      "loss": 0.1099,
      "step": 64700
    },
    {
      "epoch": 3.6248214435761703,
      "grad_norm": 1.1690914630889893,
      "learning_rate": 9.385335500868883e-06,
      "loss": 0.1527,
      "step": 64710
    },
    {
      "epoch": 3.625381620592107,
      "grad_norm": 4.671351909637451,
      "learning_rate": 9.371321262402601e-06,
      "loss": 0.1387,
      "step": 64720
    },
    {
      "epoch": 3.625941797608044,
      "grad_norm": 1.532089352607727,
      "learning_rate": 9.35730702393632e-06,
      "loss": 0.0975,
      "step": 64730
    },
    {
      "epoch": 3.6265019746239813,
      "grad_norm": 4.785133361816406,
      "learning_rate": 9.343292785470038e-06,
      "loss": 0.1085,
      "step": 64740
    },
    {
      "epoch": 3.627062151639918,
      "grad_norm": 1.4467169046401978,
      "learning_rate": 9.329278547003756e-06,
      "loss": 0.0782,
      "step": 64750
    },
    {
      "epoch": 3.627622328655855,
      "grad_norm": 2.0539517402648926,
      "learning_rate": 9.315264308537474e-06,
      "loss": 0.1081,
      "step": 64760
    },
    {
      "epoch": 3.6281825056717922,
      "grad_norm": 3.0340378284454346,
      "learning_rate": 9.301250070071192e-06,
      "loss": 0.1087,
      "step": 64770
    },
    {
      "epoch": 3.6287426826877294,
      "grad_norm": 2.2157602310180664,
      "learning_rate": 9.287235831604912e-06,
      "loss": 0.0875,
      "step": 64780
    },
    {
      "epoch": 3.6293028597036665,
      "grad_norm": 1.4126027822494507,
      "learning_rate": 9.27322159313863e-06,
      "loss": 0.0924,
      "step": 64790
    },
    {
      "epoch": 3.6298630367196036,
      "grad_norm": 1.193681001663208,
      "learning_rate": 9.259207354672348e-06,
      "loss": 0.1475,
      "step": 64800
    },
    {
      "epoch": 3.6304232137355403,
      "grad_norm": 1.3806065320968628,
      "learning_rate": 9.245193116206066e-06,
      "loss": 0.1108,
      "step": 64810
    },
    {
      "epoch": 3.6309833907514775,
      "grad_norm": 1.6671173572540283,
      "learning_rate": 9.231178877739784e-06,
      "loss": 0.0971,
      "step": 64820
    },
    {
      "epoch": 3.6315435677674146,
      "grad_norm": 0.4978334903717041,
      "learning_rate": 9.217164639273502e-06,
      "loss": 0.087,
      "step": 64830
    },
    {
      "epoch": 3.6321037447833513,
      "grad_norm": 1.105202078819275,
      "learning_rate": 9.20315040080722e-06,
      "loss": 0.1501,
      "step": 64840
    },
    {
      "epoch": 3.6326639217992884,
      "grad_norm": 2.0769522190093994,
      "learning_rate": 9.189136162340939e-06,
      "loss": 0.0836,
      "step": 64850
    },
    {
      "epoch": 3.6332240988152256,
      "grad_norm": 1.6312947273254395,
      "learning_rate": 9.175121923874659e-06,
      "loss": 0.1218,
      "step": 64860
    },
    {
      "epoch": 3.6337842758311627,
      "grad_norm": 0.8619927763938904,
      "learning_rate": 9.161107685408375e-06,
      "loss": 0.0826,
      "step": 64870
    },
    {
      "epoch": 3.6343444528471,
      "grad_norm": 1.534385085105896,
      "learning_rate": 9.147093446942095e-06,
      "loss": 0.087,
      "step": 64880
    },
    {
      "epoch": 3.634904629863037,
      "grad_norm": 0.6664586663246155,
      "learning_rate": 9.133079208475811e-06,
      "loss": 0.1214,
      "step": 64890
    },
    {
      "epoch": 3.6354648068789737,
      "grad_norm": 1.742286205291748,
      "learning_rate": 9.119064970009531e-06,
      "loss": 0.1285,
      "step": 64900
    },
    {
      "epoch": 3.636024983894911,
      "grad_norm": 0.6656668186187744,
      "learning_rate": 9.105050731543248e-06,
      "loss": 0.0778,
      "step": 64910
    },
    {
      "epoch": 3.636585160910848,
      "grad_norm": 2.8721373081207275,
      "learning_rate": 9.091036493076967e-06,
      "loss": 0.1078,
      "step": 64920
    },
    {
      "epoch": 3.6371453379267846,
      "grad_norm": 1.3565469980239868,
      "learning_rate": 9.077022254610684e-06,
      "loss": 0.0831,
      "step": 64930
    },
    {
      "epoch": 3.6377055149427218,
      "grad_norm": 0.9809048771858215,
      "learning_rate": 9.063008016144404e-06,
      "loss": 0.1246,
      "step": 64940
    },
    {
      "epoch": 3.638265691958659,
      "grad_norm": 0.6919464468955994,
      "learning_rate": 9.048993777678122e-06,
      "loss": 0.0842,
      "step": 64950
    },
    {
      "epoch": 3.638825868974596,
      "grad_norm": 3.3474624156951904,
      "learning_rate": 9.03497953921184e-06,
      "loss": 0.129,
      "step": 64960
    },
    {
      "epoch": 3.639386045990533,
      "grad_norm": 0.9662774801254272,
      "learning_rate": 9.020965300745558e-06,
      "loss": 0.1063,
      "step": 64970
    },
    {
      "epoch": 3.6399462230064703,
      "grad_norm": 1.8303582668304443,
      "learning_rate": 9.006951062279276e-06,
      "loss": 0.089,
      "step": 64980
    },
    {
      "epoch": 3.640506400022407,
      "grad_norm": 1.9420950412750244,
      "learning_rate": 8.992936823812994e-06,
      "loss": 0.0923,
      "step": 64990
    },
    {
      "epoch": 3.641066577038344,
      "grad_norm": 1.6871976852416992,
      "learning_rate": 8.978922585346712e-06,
      "loss": 0.1516,
      "step": 65000
    },
    {
      "epoch": 3.6416267540542813,
      "grad_norm": 2.384004831314087,
      "learning_rate": 8.96490834688043e-06,
      "loss": 0.1106,
      "step": 65010
    },
    {
      "epoch": 3.642186931070218,
      "grad_norm": 0.9584923386573792,
      "learning_rate": 8.95089410841415e-06,
      "loss": 0.0823,
      "step": 65020
    },
    {
      "epoch": 3.642747108086155,
      "grad_norm": 0.808081328868866,
      "learning_rate": 8.936879869947867e-06,
      "loss": 0.0925,
      "step": 65030
    },
    {
      "epoch": 3.6433072851020922,
      "grad_norm": 1.8212023973464966,
      "learning_rate": 8.922865631481587e-06,
      "loss": 0.0962,
      "step": 65040
    },
    {
      "epoch": 3.6438674621180294,
      "grad_norm": 1.4981390237808228,
      "learning_rate": 8.908851393015303e-06,
      "loss": 0.1166,
      "step": 65050
    },
    {
      "epoch": 3.6444276391339665,
      "grad_norm": 1.1101020574569702,
      "learning_rate": 8.894837154549023e-06,
      "loss": 0.1731,
      "step": 65060
    },
    {
      "epoch": 3.6449878161499036,
      "grad_norm": 2.03389573097229,
      "learning_rate": 8.88082291608274e-06,
      "loss": 0.1488,
      "step": 65070
    },
    {
      "epoch": 3.6455479931658403,
      "grad_norm": 1.417433261871338,
      "learning_rate": 8.866808677616459e-06,
      "loss": 0.1157,
      "step": 65080
    },
    {
      "epoch": 3.6461081701817775,
      "grad_norm": 0.8324845433235168,
      "learning_rate": 8.852794439150176e-06,
      "loss": 0.0876,
      "step": 65090
    },
    {
      "epoch": 3.6466683471977146,
      "grad_norm": 1.4976252317428589,
      "learning_rate": 8.838780200683895e-06,
      "loss": 0.1143,
      "step": 65100
    },
    {
      "epoch": 3.6472285242136513,
      "grad_norm": 2.7024545669555664,
      "learning_rate": 8.824765962217614e-06,
      "loss": 0.0875,
      "step": 65110
    },
    {
      "epoch": 3.6477887012295884,
      "grad_norm": 3.3171308040618896,
      "learning_rate": 8.810751723751332e-06,
      "loss": 0.1409,
      "step": 65120
    },
    {
      "epoch": 3.6483488782455256,
      "grad_norm": 2.285670518875122,
      "learning_rate": 8.79673748528505e-06,
      "loss": 0.075,
      "step": 65130
    },
    {
      "epoch": 3.6489090552614627,
      "grad_norm": 0.5946248769760132,
      "learning_rate": 8.782723246818768e-06,
      "loss": 0.1387,
      "step": 65140
    },
    {
      "epoch": 3.6494692322774,
      "grad_norm": 2.5661258697509766,
      "learning_rate": 8.768709008352486e-06,
      "loss": 0.1051,
      "step": 65150
    },
    {
      "epoch": 3.650029409293337,
      "grad_norm": 1.6257044076919556,
      "learning_rate": 8.754694769886204e-06,
      "loss": 0.1313,
      "step": 65160
    },
    {
      "epoch": 3.6505895863092737,
      "grad_norm": 0.6543262004852295,
      "learning_rate": 8.740680531419922e-06,
      "loss": 0.0735,
      "step": 65170
    },
    {
      "epoch": 3.651149763325211,
      "grad_norm": 1.0571216344833374,
      "learning_rate": 8.726666292953642e-06,
      "loss": 0.1003,
      "step": 65180
    },
    {
      "epoch": 3.651709940341148,
      "grad_norm": 1.1168850660324097,
      "learning_rate": 8.712652054487359e-06,
      "loss": 0.1077,
      "step": 65190
    },
    {
      "epoch": 3.6522701173570846,
      "grad_norm": 0.9108115434646606,
      "learning_rate": 8.698637816021078e-06,
      "loss": 0.1426,
      "step": 65200
    },
    {
      "epoch": 3.6528302943730218,
      "grad_norm": 3.4742071628570557,
      "learning_rate": 8.684623577554797e-06,
      "loss": 0.0922,
      "step": 65210
    },
    {
      "epoch": 3.653390471388959,
      "grad_norm": 3.5660438537597656,
      "learning_rate": 8.670609339088515e-06,
      "loss": 0.1046,
      "step": 65220
    },
    {
      "epoch": 3.653950648404896,
      "grad_norm": 1.9934254884719849,
      "learning_rate": 8.656595100622233e-06,
      "loss": 0.1978,
      "step": 65230
    },
    {
      "epoch": 3.654510825420833,
      "grad_norm": 0.9046182632446289,
      "learning_rate": 8.642580862155951e-06,
      "loss": 0.0986,
      "step": 65240
    },
    {
      "epoch": 3.65507100243677,
      "grad_norm": 0.7828723192214966,
      "learning_rate": 8.628566623689669e-06,
      "loss": 0.0707,
      "step": 65250
    },
    {
      "epoch": 3.655631179452707,
      "grad_norm": 1.4800302982330322,
      "learning_rate": 8.614552385223387e-06,
      "loss": 0.1691,
      "step": 65260
    },
    {
      "epoch": 3.656191356468644,
      "grad_norm": 4.281245708465576,
      "learning_rate": 8.600538146757105e-06,
      "loss": 0.096,
      "step": 65270
    },
    {
      "epoch": 3.6567515334845813,
      "grad_norm": 2.2928683757781982,
      "learning_rate": 8.586523908290823e-06,
      "loss": 0.1074,
      "step": 65280
    },
    {
      "epoch": 3.657311710500518,
      "grad_norm": 4.813806533813477,
      "learning_rate": 8.572509669824543e-06,
      "loss": 0.1149,
      "step": 65290
    },
    {
      "epoch": 3.657871887516455,
      "grad_norm": 0.8095290660858154,
      "learning_rate": 8.55849543135826e-06,
      "loss": 0.1044,
      "step": 65300
    },
    {
      "epoch": 3.6584320645323922,
      "grad_norm": 2.820443630218506,
      "learning_rate": 8.54448119289198e-06,
      "loss": 0.1225,
      "step": 65310
    },
    {
      "epoch": 3.6589922415483294,
      "grad_norm": 0.7414801716804504,
      "learning_rate": 8.530466954425696e-06,
      "loss": 0.1398,
      "step": 65320
    },
    {
      "epoch": 3.6595524185642665,
      "grad_norm": 5.4057393074035645,
      "learning_rate": 8.516452715959416e-06,
      "loss": 0.1225,
      "step": 65330
    },
    {
      "epoch": 3.660112595580203,
      "grad_norm": 0.529542863368988,
      "learning_rate": 8.502438477493134e-06,
      "loss": 0.0858,
      "step": 65340
    },
    {
      "epoch": 3.6606727725961403,
      "grad_norm": 0.8547174334526062,
      "learning_rate": 8.488424239026852e-06,
      "loss": 0.1153,
      "step": 65350
    },
    {
      "epoch": 3.6612329496120775,
      "grad_norm": 2.2547926902770996,
      "learning_rate": 8.47441000056057e-06,
      "loss": 0.1278,
      "step": 65360
    },
    {
      "epoch": 3.6617931266280146,
      "grad_norm": 4.961932182312012,
      "learning_rate": 8.460395762094288e-06,
      "loss": 0.1372,
      "step": 65370
    },
    {
      "epoch": 3.6623533036439513,
      "grad_norm": 2.5777828693389893,
      "learning_rate": 8.446381523628006e-06,
      "loss": 0.0982,
      "step": 65380
    },
    {
      "epoch": 3.6629134806598884,
      "grad_norm": 2.3649837970733643,
      "learning_rate": 8.432367285161725e-06,
      "loss": 0.0814,
      "step": 65390
    },
    {
      "epoch": 3.6634736576758256,
      "grad_norm": 0.8548780679702759,
      "learning_rate": 8.418353046695443e-06,
      "loss": 0.0877,
      "step": 65400
    },
    {
      "epoch": 3.6640338346917627,
      "grad_norm": 4.076712131500244,
      "learning_rate": 8.404338808229163e-06,
      "loss": 0.0982,
      "step": 65410
    },
    {
      "epoch": 3.6645940117077,
      "grad_norm": 3.4873199462890625,
      "learning_rate": 8.390324569762879e-06,
      "loss": 0.0712,
      "step": 65420
    },
    {
      "epoch": 3.6651541887236365,
      "grad_norm": 1.648483157157898,
      "learning_rate": 8.376310331296599e-06,
      "loss": 0.0843,
      "step": 65430
    },
    {
      "epoch": 3.6657143657395737,
      "grad_norm": 2.988308906555176,
      "learning_rate": 8.362296092830315e-06,
      "loss": 0.1414,
      "step": 65440
    },
    {
      "epoch": 3.666274542755511,
      "grad_norm": 0.7009596228599548,
      "learning_rate": 8.348281854364035e-06,
      "loss": 0.1029,
      "step": 65450
    },
    {
      "epoch": 3.666834719771448,
      "grad_norm": 1.3346918821334839,
      "learning_rate": 8.334267615897752e-06,
      "loss": 0.0945,
      "step": 65460
    },
    {
      "epoch": 3.6673948967873846,
      "grad_norm": 0.7888744473457336,
      "learning_rate": 8.320253377431471e-06,
      "loss": 0.1004,
      "step": 65470
    },
    {
      "epoch": 3.6679550738033218,
      "grad_norm": 0.5328219532966614,
      "learning_rate": 8.306239138965188e-06,
      "loss": 0.1253,
      "step": 65480
    },
    {
      "epoch": 3.668515250819259,
      "grad_norm": 1.26650869846344,
      "learning_rate": 8.292224900498908e-06,
      "loss": 0.1282,
      "step": 65490
    },
    {
      "epoch": 3.669075427835196,
      "grad_norm": 0.9794864654541016,
      "learning_rate": 8.278210662032626e-06,
      "loss": 0.1405,
      "step": 65500
    },
    {
      "epoch": 3.669635604851133,
      "grad_norm": 0.7107083201408386,
      "learning_rate": 8.264196423566344e-06,
      "loss": 0.0613,
      "step": 65510
    },
    {
      "epoch": 3.67019578186707,
      "grad_norm": 3.0758676528930664,
      "learning_rate": 8.250182185100062e-06,
      "loss": 0.1452,
      "step": 65520
    },
    {
      "epoch": 3.670755958883007,
      "grad_norm": 2.6859896183013916,
      "learning_rate": 8.23616794663378e-06,
      "loss": 0.1434,
      "step": 65530
    },
    {
      "epoch": 3.671316135898944,
      "grad_norm": 0.6793738007545471,
      "learning_rate": 8.222153708167498e-06,
      "loss": 0.1153,
      "step": 65540
    },
    {
      "epoch": 3.6718763129148813,
      "grad_norm": 3.199476957321167,
      "learning_rate": 8.208139469701216e-06,
      "loss": 0.1236,
      "step": 65550
    },
    {
      "epoch": 3.672436489930818,
      "grad_norm": 1.4989008903503418,
      "learning_rate": 8.194125231234935e-06,
      "loss": 0.0834,
      "step": 65560
    },
    {
      "epoch": 3.672996666946755,
      "grad_norm": 1.5299428701400757,
      "learning_rate": 8.180110992768654e-06,
      "loss": 0.08,
      "step": 65570
    },
    {
      "epoch": 3.673556843962692,
      "grad_norm": 0.6906523704528809,
      "learning_rate": 8.16609675430237e-06,
      "loss": 0.2062,
      "step": 65580
    },
    {
      "epoch": 3.6741170209786294,
      "grad_norm": 1.5848630666732788,
      "learning_rate": 8.15208251583609e-06,
      "loss": 0.1033,
      "step": 65590
    },
    {
      "epoch": 3.6746771979945665,
      "grad_norm": 0.5174250602722168,
      "learning_rate": 8.138068277369807e-06,
      "loss": 0.1249,
      "step": 65600
    },
    {
      "epoch": 3.675237375010503,
      "grad_norm": 3.6599981784820557,
      "learning_rate": 8.124054038903527e-06,
      "loss": 0.089,
      "step": 65610
    },
    {
      "epoch": 3.6757975520264403,
      "grad_norm": 2.036905288696289,
      "learning_rate": 8.110039800437243e-06,
      "loss": 0.0975,
      "step": 65620
    },
    {
      "epoch": 3.6763577290423775,
      "grad_norm": 2.7699081897735596,
      "learning_rate": 8.096025561970963e-06,
      "loss": 0.0777,
      "step": 65630
    },
    {
      "epoch": 3.6769179060583146,
      "grad_norm": 1.6930040121078491,
      "learning_rate": 8.082011323504681e-06,
      "loss": 0.0893,
      "step": 65640
    },
    {
      "epoch": 3.6774780830742513,
      "grad_norm": 2.8899059295654297,
      "learning_rate": 8.0679970850384e-06,
      "loss": 0.1463,
      "step": 65650
    },
    {
      "epoch": 3.6780382600901884,
      "grad_norm": 1.2355149984359741,
      "learning_rate": 8.053982846572118e-06,
      "loss": 0.105,
      "step": 65660
    },
    {
      "epoch": 3.6785984371061256,
      "grad_norm": 0.6680312156677246,
      "learning_rate": 8.039968608105836e-06,
      "loss": 0.1234,
      "step": 65670
    },
    {
      "epoch": 3.6791586141220627,
      "grad_norm": 1.2381826639175415,
      "learning_rate": 8.025954369639554e-06,
      "loss": 0.1241,
      "step": 65680
    },
    {
      "epoch": 3.679718791138,
      "grad_norm": 1.090003252029419,
      "learning_rate": 8.011940131173272e-06,
      "loss": 0.1116,
      "step": 65690
    },
    {
      "epoch": 3.6802789681539365,
      "grad_norm": 1.2334483861923218,
      "learning_rate": 7.99792589270699e-06,
      "loss": 0.1129,
      "step": 65700
    },
    {
      "epoch": 3.6808391451698736,
      "grad_norm": 0.7578545212745667,
      "learning_rate": 7.983911654240708e-06,
      "loss": 0.1091,
      "step": 65710
    },
    {
      "epoch": 3.681399322185811,
      "grad_norm": 0.9348567724227905,
      "learning_rate": 7.969897415774428e-06,
      "loss": 0.1313,
      "step": 65720
    },
    {
      "epoch": 3.6819594992017475,
      "grad_norm": 0.9352220296859741,
      "learning_rate": 7.955883177308146e-06,
      "loss": 0.0766,
      "step": 65730
    },
    {
      "epoch": 3.6825196762176846,
      "grad_norm": 0.9579422473907471,
      "learning_rate": 7.941868938841864e-06,
      "loss": 0.1422,
      "step": 65740
    },
    {
      "epoch": 3.6830798532336217,
      "grad_norm": 1.0740885734558105,
      "learning_rate": 7.927854700375582e-06,
      "loss": 0.0988,
      "step": 65750
    },
    {
      "epoch": 3.683640030249559,
      "grad_norm": 1.7910693883895874,
      "learning_rate": 7.9138404619093e-06,
      "loss": 0.0982,
      "step": 65760
    },
    {
      "epoch": 3.684200207265496,
      "grad_norm": 5.629027366638184,
      "learning_rate": 7.899826223443019e-06,
      "loss": 0.1159,
      "step": 65770
    },
    {
      "epoch": 3.684760384281433,
      "grad_norm": 1.751868486404419,
      "learning_rate": 7.885811984976737e-06,
      "loss": 0.078,
      "step": 65780
    },
    {
      "epoch": 3.68532056129737,
      "grad_norm": 1.0013796091079712,
      "learning_rate": 7.871797746510455e-06,
      "loss": 0.1312,
      "step": 65790
    },
    {
      "epoch": 3.685880738313307,
      "grad_norm": 2.098574638366699,
      "learning_rate": 7.857783508044175e-06,
      "loss": 0.1163,
      "step": 65800
    },
    {
      "epoch": 3.686440915329244,
      "grad_norm": 3.649695873260498,
      "learning_rate": 7.843769269577891e-06,
      "loss": 0.1195,
      "step": 65810
    },
    {
      "epoch": 3.687001092345181,
      "grad_norm": 1.4720484018325806,
      "learning_rate": 7.829755031111611e-06,
      "loss": 0.111,
      "step": 65820
    },
    {
      "epoch": 3.687561269361118,
      "grad_norm": 1.6553382873535156,
      "learning_rate": 7.815740792645327e-06,
      "loss": 0.123,
      "step": 65830
    },
    {
      "epoch": 3.688121446377055,
      "grad_norm": 2.423722267150879,
      "learning_rate": 7.801726554179047e-06,
      "loss": 0.1404,
      "step": 65840
    },
    {
      "epoch": 3.688681623392992,
      "grad_norm": 2.4927520751953125,
      "learning_rate": 7.787712315712764e-06,
      "loss": 0.1102,
      "step": 65850
    },
    {
      "epoch": 3.6892418004089294,
      "grad_norm": 1.6724226474761963,
      "learning_rate": 7.773698077246484e-06,
      "loss": 0.0747,
      "step": 65860
    },
    {
      "epoch": 3.6898019774248665,
      "grad_norm": 0.9433161020278931,
      "learning_rate": 7.7596838387802e-06,
      "loss": 0.0854,
      "step": 65870
    },
    {
      "epoch": 3.690362154440803,
      "grad_norm": 0.6047518253326416,
      "learning_rate": 7.74566960031392e-06,
      "loss": 0.1528,
      "step": 65880
    },
    {
      "epoch": 3.6909223314567403,
      "grad_norm": 5.287929534912109,
      "learning_rate": 7.731655361847638e-06,
      "loss": 0.1574,
      "step": 65890
    },
    {
      "epoch": 3.6914825084726774,
      "grad_norm": 6.403285503387451,
      "learning_rate": 7.717641123381356e-06,
      "loss": 0.1217,
      "step": 65900
    },
    {
      "epoch": 3.692042685488614,
      "grad_norm": 1.3912296295166016,
      "learning_rate": 7.703626884915074e-06,
      "loss": 0.1028,
      "step": 65910
    },
    {
      "epoch": 3.6926028625045513,
      "grad_norm": 0.9078454375267029,
      "learning_rate": 7.689612646448792e-06,
      "loss": 0.0908,
      "step": 65920
    },
    {
      "epoch": 3.6931630395204884,
      "grad_norm": 4.843667507171631,
      "learning_rate": 7.67559840798251e-06,
      "loss": 0.1378,
      "step": 65930
    },
    {
      "epoch": 3.6937232165364255,
      "grad_norm": 0.8778007626533508,
      "learning_rate": 7.661584169516229e-06,
      "loss": 0.107,
      "step": 65940
    },
    {
      "epoch": 3.6942833935523627,
      "grad_norm": 3.8320090770721436,
      "learning_rate": 7.647569931049947e-06,
      "loss": 0.1034,
      "step": 65950
    },
    {
      "epoch": 3.6948435705683,
      "grad_norm": 0.7279869914054871,
      "learning_rate": 7.633555692583667e-06,
      "loss": 0.0969,
      "step": 65960
    },
    {
      "epoch": 3.6954037475842365,
      "grad_norm": 4.3306779861450195,
      "learning_rate": 7.619541454117384e-06,
      "loss": 0.1739,
      "step": 65970
    },
    {
      "epoch": 3.6959639246001736,
      "grad_norm": 1.8919934034347534,
      "learning_rate": 7.605527215651102e-06,
      "loss": 0.0816,
      "step": 65980
    },
    {
      "epoch": 3.696524101616111,
      "grad_norm": 1.6315655708312988,
      "learning_rate": 7.59151297718482e-06,
      "loss": 0.1037,
      "step": 65990
    },
    {
      "epoch": 3.6970842786320475,
      "grad_norm": 2.002967357635498,
      "learning_rate": 7.577498738718539e-06,
      "loss": 0.1634,
      "step": 66000
    },
    {
      "epoch": 3.6976444556479846,
      "grad_norm": 2.7890360355377197,
      "learning_rate": 7.563484500252256e-06,
      "loss": 0.1005,
      "step": 66010
    },
    {
      "epoch": 3.6982046326639217,
      "grad_norm": 2.105226993560791,
      "learning_rate": 7.549470261785975e-06,
      "loss": 0.0993,
      "step": 66020
    },
    {
      "epoch": 3.698764809679859,
      "grad_norm": 6.923276424407959,
      "learning_rate": 7.5354560233196935e-06,
      "loss": 0.1652,
      "step": 66030
    },
    {
      "epoch": 3.699324986695796,
      "grad_norm": 1.722077488899231,
      "learning_rate": 7.521441784853412e-06,
      "loss": 0.0851,
      "step": 66040
    },
    {
      "epoch": 3.699885163711733,
      "grad_norm": 3.239236831665039,
      "learning_rate": 7.5074275463871306e-06,
      "loss": 0.1884,
      "step": 66050
    },
    {
      "epoch": 3.70044534072767,
      "grad_norm": 1.188843011856079,
      "learning_rate": 7.493413307920848e-06,
      "loss": 0.0888,
      "step": 66060
    },
    {
      "epoch": 3.701005517743607,
      "grad_norm": 0.8985309600830078,
      "learning_rate": 7.479399069454567e-06,
      "loss": 0.1477,
      "step": 66070
    },
    {
      "epoch": 3.701565694759544,
      "grad_norm": 2.059220790863037,
      "learning_rate": 7.465384830988284e-06,
      "loss": 0.0945,
      "step": 66080
    },
    {
      "epoch": 3.702125871775481,
      "grad_norm": 1.6775023937225342,
      "learning_rate": 7.451370592522003e-06,
      "loss": 0.0906,
      "step": 66090
    },
    {
      "epoch": 3.702686048791418,
      "grad_norm": 1.4826549291610718,
      "learning_rate": 7.43735635405572e-06,
      "loss": 0.1238,
      "step": 66100
    },
    {
      "epoch": 3.703246225807355,
      "grad_norm": 3.153961181640625,
      "learning_rate": 7.423342115589439e-06,
      "loss": 0.0958,
      "step": 66110
    },
    {
      "epoch": 3.703806402823292,
      "grad_norm": 4.577073574066162,
      "learning_rate": 7.409327877123158e-06,
      "loss": 0.103,
      "step": 66120
    },
    {
      "epoch": 3.7043665798392293,
      "grad_norm": 3.9721269607543945,
      "learning_rate": 7.395313638656876e-06,
      "loss": 0.1311,
      "step": 66130
    },
    {
      "epoch": 3.7049267568551665,
      "grad_norm": 2.8609187602996826,
      "learning_rate": 7.381299400190595e-06,
      "loss": 0.1353,
      "step": 66140
    },
    {
      "epoch": 3.705486933871103,
      "grad_norm": 0.8714966773986816,
      "learning_rate": 7.367285161724312e-06,
      "loss": 0.1509,
      "step": 66150
    },
    {
      "epoch": 3.7060471108870403,
      "grad_norm": 0.7184019684791565,
      "learning_rate": 7.353270923258031e-06,
      "loss": 0.0688,
      "step": 66160
    },
    {
      "epoch": 3.7066072879029774,
      "grad_norm": 0.7730745077133179,
      "learning_rate": 7.339256684791748e-06,
      "loss": 0.0919,
      "step": 66170
    },
    {
      "epoch": 3.707167464918914,
      "grad_norm": 1.2436609268188477,
      "learning_rate": 7.325242446325467e-06,
      "loss": 0.1087,
      "step": 66180
    },
    {
      "epoch": 3.7077276419348513,
      "grad_norm": 0.8915407061576843,
      "learning_rate": 7.311228207859186e-06,
      "loss": 0.0962,
      "step": 66190
    },
    {
      "epoch": 3.7082878189507884,
      "grad_norm": 1.8648877143859863,
      "learning_rate": 7.297213969392903e-06,
      "loss": 0.1013,
      "step": 66200
    },
    {
      "epoch": 3.7088479959667255,
      "grad_norm": 2.2104928493499756,
      "learning_rate": 7.283199730926622e-06,
      "loss": 0.1076,
      "step": 66210
    },
    {
      "epoch": 3.7094081729826627,
      "grad_norm": 3.8916945457458496,
      "learning_rate": 7.26918549246034e-06,
      "loss": 0.0767,
      "step": 66220
    },
    {
      "epoch": 3.7099683499986,
      "grad_norm": 4.139605522155762,
      "learning_rate": 7.255171253994059e-06,
      "loss": 0.1076,
      "step": 66230
    },
    {
      "epoch": 3.7105285270145365,
      "grad_norm": 0.650603711605072,
      "learning_rate": 7.241157015527776e-06,
      "loss": 0.1476,
      "step": 66240
    },
    {
      "epoch": 3.7110887040304736,
      "grad_norm": 1.5122050046920776,
      "learning_rate": 7.227142777061495e-06,
      "loss": 0.1128,
      "step": 66250
    },
    {
      "epoch": 3.7116488810464108,
      "grad_norm": 2.3723526000976562,
      "learning_rate": 7.213128538595212e-06,
      "loss": 0.0875,
      "step": 66260
    },
    {
      "epoch": 3.7122090580623475,
      "grad_norm": 1.3196072578430176,
      "learning_rate": 7.199114300128931e-06,
      "loss": 0.0808,
      "step": 66270
    },
    {
      "epoch": 3.7127692350782846,
      "grad_norm": 0.8565122485160828,
      "learning_rate": 7.18510006166265e-06,
      "loss": 0.1032,
      "step": 66280
    },
    {
      "epoch": 3.7133294120942217,
      "grad_norm": 0.8857130408287048,
      "learning_rate": 7.171085823196367e-06,
      "loss": 0.0937,
      "step": 66290
    },
    {
      "epoch": 3.713889589110159,
      "grad_norm": 1.3729978799819946,
      "learning_rate": 7.157071584730086e-06,
      "loss": 0.0675,
      "step": 66300
    },
    {
      "epoch": 3.714449766126096,
      "grad_norm": 3.5517475605010986,
      "learning_rate": 7.143057346263804e-06,
      "loss": 0.0869,
      "step": 66310
    },
    {
      "epoch": 3.715009943142033,
      "grad_norm": 0.7639012932777405,
      "learning_rate": 7.129043107797523e-06,
      "loss": 0.1261,
      "step": 66320
    },
    {
      "epoch": 3.71557012015797,
      "grad_norm": 1.3910104036331177,
      "learning_rate": 7.11502886933124e-06,
      "loss": 0.0911,
      "step": 66330
    },
    {
      "epoch": 3.716130297173907,
      "grad_norm": 2.0262866020202637,
      "learning_rate": 7.101014630864959e-06,
      "loss": 0.1925,
      "step": 66340
    },
    {
      "epoch": 3.716690474189844,
      "grad_norm": 4.341653347015381,
      "learning_rate": 7.087000392398678e-06,
      "loss": 0.1051,
      "step": 66350
    },
    {
      "epoch": 3.717250651205781,
      "grad_norm": 2.0702126026153564,
      "learning_rate": 7.072986153932395e-06,
      "loss": 0.0866,
      "step": 66360
    },
    {
      "epoch": 3.717810828221718,
      "grad_norm": 1.0189826488494873,
      "learning_rate": 7.058971915466114e-06,
      "loss": 0.0715,
      "step": 66370
    },
    {
      "epoch": 3.718371005237655,
      "grad_norm": 4.103121280670166,
      "learning_rate": 7.0449576769998314e-06,
      "loss": 0.1099,
      "step": 66380
    },
    {
      "epoch": 3.718931182253592,
      "grad_norm": 0.9015589356422424,
      "learning_rate": 7.03094343853355e-06,
      "loss": 0.0888,
      "step": 66390
    },
    {
      "epoch": 3.7194913592695293,
      "grad_norm": 0.5073807239532471,
      "learning_rate": 7.0169292000672685e-06,
      "loss": 0.1014,
      "step": 66400
    },
    {
      "epoch": 3.7200515362854665,
      "grad_norm": 0.7546198964118958,
      "learning_rate": 7.002914961600987e-06,
      "loss": 0.1129,
      "step": 66410
    },
    {
      "epoch": 3.720611713301403,
      "grad_norm": 0.702332615852356,
      "learning_rate": 6.988900723134705e-06,
      "loss": 0.0931,
      "step": 66420
    },
    {
      "epoch": 3.7211718903173403,
      "grad_norm": 4.277688026428223,
      "learning_rate": 6.974886484668424e-06,
      "loss": 0.1627,
      "step": 66430
    },
    {
      "epoch": 3.7217320673332774,
      "grad_norm": 2.1559770107269287,
      "learning_rate": 6.960872246202142e-06,
      "loss": 0.0989,
      "step": 66440
    },
    {
      "epoch": 3.722292244349214,
      "grad_norm": 0.9743653535842896,
      "learning_rate": 6.94685800773586e-06,
      "loss": 0.0798,
      "step": 66450
    },
    {
      "epoch": 3.7228524213651513,
      "grad_norm": 2.0323374271392822,
      "learning_rate": 6.932843769269578e-06,
      "loss": 0.0916,
      "step": 66460
    },
    {
      "epoch": 3.7234125983810884,
      "grad_norm": 3.752315044403076,
      "learning_rate": 6.918829530803296e-06,
      "loss": 0.1036,
      "step": 66470
    },
    {
      "epoch": 3.7239727753970255,
      "grad_norm": 1.024702787399292,
      "learning_rate": 6.904815292337015e-06,
      "loss": 0.1193,
      "step": 66480
    },
    {
      "epoch": 3.7245329524129627,
      "grad_norm": 1.7338519096374512,
      "learning_rate": 6.8908010538707326e-06,
      "loss": 0.0743,
      "step": 66490
    },
    {
      "epoch": 3.7250931294289,
      "grad_norm": 1.09746253490448,
      "learning_rate": 6.8767868154044515e-06,
      "loss": 0.0844,
      "step": 66500
    },
    {
      "epoch": 3.7256533064448365,
      "grad_norm": 3.172370195388794,
      "learning_rate": 6.8627725769381705e-06,
      "loss": 0.1479,
      "step": 66510
    },
    {
      "epoch": 3.7262134834607736,
      "grad_norm": 2.1371188163757324,
      "learning_rate": 6.848758338471888e-06,
      "loss": 0.1127,
      "step": 66520
    },
    {
      "epoch": 3.7267736604767108,
      "grad_norm": 0.814960241317749,
      "learning_rate": 6.834744100005607e-06,
      "loss": 0.1725,
      "step": 66530
    },
    {
      "epoch": 3.7273338374926475,
      "grad_norm": 1.7307837009429932,
      "learning_rate": 6.820729861539324e-06,
      "loss": 0.1502,
      "step": 66540
    },
    {
      "epoch": 3.7278940145085846,
      "grad_norm": 3.091886281967163,
      "learning_rate": 6.806715623073043e-06,
      "loss": 0.0846,
      "step": 66550
    },
    {
      "epoch": 3.7284541915245217,
      "grad_norm": 0.8226361870765686,
      "learning_rate": 6.79270138460676e-06,
      "loss": 0.072,
      "step": 66560
    },
    {
      "epoch": 3.729014368540459,
      "grad_norm": 1.9985088109970093,
      "learning_rate": 6.778687146140479e-06,
      "loss": 0.0954,
      "step": 66570
    },
    {
      "epoch": 3.729574545556396,
      "grad_norm": 0.8897969126701355,
      "learning_rate": 6.764672907674198e-06,
      "loss": 0.1083,
      "step": 66580
    },
    {
      "epoch": 3.7301347225723327,
      "grad_norm": 3.9697515964508057,
      "learning_rate": 6.7506586692079156e-06,
      "loss": 0.0884,
      "step": 66590
    },
    {
      "epoch": 3.73069489958827,
      "grad_norm": 4.1582183837890625,
      "learning_rate": 6.7366444307416346e-06,
      "loss": 0.0831,
      "step": 66600
    },
    {
      "epoch": 3.731255076604207,
      "grad_norm": 1.6672868728637695,
      "learning_rate": 6.722630192275352e-06,
      "loss": 0.1041,
      "step": 66610
    },
    {
      "epoch": 3.731815253620144,
      "grad_norm": 1.8861080408096313,
      "learning_rate": 6.708615953809071e-06,
      "loss": 0.1438,
      "step": 66620
    },
    {
      "epoch": 3.732375430636081,
      "grad_norm": 3.1788394451141357,
      "learning_rate": 6.694601715342788e-06,
      "loss": 0.1211,
      "step": 66630
    },
    {
      "epoch": 3.732935607652018,
      "grad_norm": 0.8910374641418457,
      "learning_rate": 6.680587476876507e-06,
      "loss": 0.0938,
      "step": 66640
    },
    {
      "epoch": 3.733495784667955,
      "grad_norm": 1.7591001987457275,
      "learning_rate": 6.666573238410224e-06,
      "loss": 0.108,
      "step": 66650
    },
    {
      "epoch": 3.734055961683892,
      "grad_norm": 1.4092216491699219,
      "learning_rate": 6.652558999943943e-06,
      "loss": 0.0878,
      "step": 66660
    },
    {
      "epoch": 3.7346161386998293,
      "grad_norm": 2.384026527404785,
      "learning_rate": 6.638544761477662e-06,
      "loss": 0.0844,
      "step": 66670
    },
    {
      "epoch": 3.735176315715766,
      "grad_norm": 2.9923064708709717,
      "learning_rate": 6.62453052301138e-06,
      "loss": 0.1035,
      "step": 66680
    },
    {
      "epoch": 3.735736492731703,
      "grad_norm": 0.811991274356842,
      "learning_rate": 6.610516284545099e-06,
      "loss": 0.1545,
      "step": 66690
    },
    {
      "epoch": 3.7362966697476403,
      "grad_norm": 2.8135035037994385,
      "learning_rate": 6.596502046078816e-06,
      "loss": 0.0941,
      "step": 66700
    },
    {
      "epoch": 3.7368568467635774,
      "grad_norm": 1.6721478700637817,
      "learning_rate": 6.582487807612535e-06,
      "loss": 0.1126,
      "step": 66710
    },
    {
      "epoch": 3.737417023779514,
      "grad_norm": 2.739665985107422,
      "learning_rate": 6.568473569146252e-06,
      "loss": 0.1232,
      "step": 66720
    },
    {
      "epoch": 3.7379772007954513,
      "grad_norm": 5.135771751403809,
      "learning_rate": 6.554459330679971e-06,
      "loss": 0.1267,
      "step": 66730
    },
    {
      "epoch": 3.7385373778113884,
      "grad_norm": 0.5640993714332581,
      "learning_rate": 6.54044509221369e-06,
      "loss": 0.0785,
      "step": 66740
    },
    {
      "epoch": 3.7390975548273255,
      "grad_norm": 4.290271759033203,
      "learning_rate": 6.526430853747407e-06,
      "loss": 0.1968,
      "step": 66750
    },
    {
      "epoch": 3.7396577318432627,
      "grad_norm": 1.0922378301620483,
      "learning_rate": 6.512416615281126e-06,
      "loss": 0.0986,
      "step": 66760
    },
    {
      "epoch": 3.7402179088591994,
      "grad_norm": 2.2496941089630127,
      "learning_rate": 6.498402376814844e-06,
      "loss": 0.1325,
      "step": 66770
    },
    {
      "epoch": 3.7407780858751365,
      "grad_norm": 4.631849765777588,
      "learning_rate": 6.484388138348563e-06,
      "loss": 0.088,
      "step": 66780
    },
    {
      "epoch": 3.7413382628910736,
      "grad_norm": 0.8246093988418579,
      "learning_rate": 6.47037389988228e-06,
      "loss": 0.1345,
      "step": 66790
    },
    {
      "epoch": 3.7418984399070108,
      "grad_norm": 3.144608497619629,
      "learning_rate": 6.456359661415999e-06,
      "loss": 0.0876,
      "step": 66800
    },
    {
      "epoch": 3.7424586169229475,
      "grad_norm": 0.9331252574920654,
      "learning_rate": 6.442345422949716e-06,
      "loss": 0.0753,
      "step": 66810
    },
    {
      "epoch": 3.7430187939388846,
      "grad_norm": 1.4402841329574585,
      "learning_rate": 6.428331184483435e-06,
      "loss": 0.0781,
      "step": 66820
    },
    {
      "epoch": 3.7435789709548217,
      "grad_norm": 1.3487346172332764,
      "learning_rate": 6.414316946017154e-06,
      "loss": 0.1154,
      "step": 66830
    },
    {
      "epoch": 3.744139147970759,
      "grad_norm": 1.1541067361831665,
      "learning_rate": 6.400302707550871e-06,
      "loss": 0.1206,
      "step": 66840
    },
    {
      "epoch": 3.744699324986696,
      "grad_norm": 2.6335361003875732,
      "learning_rate": 6.38628846908459e-06,
      "loss": 0.1009,
      "step": 66850
    },
    {
      "epoch": 3.7452595020026327,
      "grad_norm": 1.013084053993225,
      "learning_rate": 6.3722742306183085e-06,
      "loss": 0.1162,
      "step": 66860
    },
    {
      "epoch": 3.74581967901857,
      "grad_norm": 0.9379423260688782,
      "learning_rate": 6.358259992152027e-06,
      "loss": 0.1016,
      "step": 66870
    },
    {
      "epoch": 3.746379856034507,
      "grad_norm": 3.232611894607544,
      "learning_rate": 6.344245753685745e-06,
      "loss": 0.1516,
      "step": 66880
    },
    {
      "epoch": 3.746940033050444,
      "grad_norm": 2.0980260372161865,
      "learning_rate": 6.330231515219463e-06,
      "loss": 0.1158,
      "step": 66890
    },
    {
      "epoch": 3.747500210066381,
      "grad_norm": 3.186941623687744,
      "learning_rate": 6.316217276753182e-06,
      "loss": 0.0941,
      "step": 66900
    },
    {
      "epoch": 3.748060387082318,
      "grad_norm": 1.1619484424591064,
      "learning_rate": 6.3022030382869e-06,
      "loss": 0.0941,
      "step": 66910
    },
    {
      "epoch": 3.748620564098255,
      "grad_norm": 3.205740451812744,
      "learning_rate": 6.288188799820618e-06,
      "loss": 0.2184,
      "step": 66920
    },
    {
      "epoch": 3.749180741114192,
      "grad_norm": 0.8048990368843079,
      "learning_rate": 6.274174561354336e-06,
      "loss": 0.115,
      "step": 66930
    },
    {
      "epoch": 3.7497409181301293,
      "grad_norm": 1.0194180011749268,
      "learning_rate": 6.260160322888054e-06,
      "loss": 0.1313,
      "step": 66940
    },
    {
      "epoch": 3.750301095146066,
      "grad_norm": 0.9368438720703125,
      "learning_rate": 6.246146084421773e-06,
      "loss": 0.0921,
      "step": 66950
    },
    {
      "epoch": 3.750861272162003,
      "grad_norm": 0.9389623403549194,
      "learning_rate": 6.2321318459554915e-06,
      "loss": 0.096,
      "step": 66960
    },
    {
      "epoch": 3.7514214491779403,
      "grad_norm": 1.2323261499404907,
      "learning_rate": 6.21811760748921e-06,
      "loss": 0.1066,
      "step": 66970
    },
    {
      "epoch": 3.7519816261938774,
      "grad_norm": 2.563431978225708,
      "learning_rate": 6.204103369022928e-06,
      "loss": 0.0765,
      "step": 66980
    },
    {
      "epoch": 3.752541803209814,
      "grad_norm": 4.139644622802734,
      "learning_rate": 6.190089130556646e-06,
      "loss": 0.0969,
      "step": 66990
    },
    {
      "epoch": 3.7531019802257513,
      "grad_norm": 1.1615092754364014,
      "learning_rate": 6.176074892090364e-06,
      "loss": 0.0958,
      "step": 67000
    },
    {
      "epoch": 3.7536621572416884,
      "grad_norm": 1.058720350265503,
      "learning_rate": 6.162060653624083e-06,
      "loss": 0.0882,
      "step": 67010
    },
    {
      "epoch": 3.7542223342576255,
      "grad_norm": 0.6386235356330872,
      "learning_rate": 6.148046415157801e-06,
      "loss": 0.1515,
      "step": 67020
    },
    {
      "epoch": 3.7547825112735627,
      "grad_norm": 0.42423391342163086,
      "learning_rate": 6.134032176691519e-06,
      "loss": 0.0909,
      "step": 67030
    },
    {
      "epoch": 3.7553426882894994,
      "grad_norm": 4.577072620391846,
      "learning_rate": 6.120017938225237e-06,
      "loss": 0.0986,
      "step": 67040
    },
    {
      "epoch": 3.7559028653054365,
      "grad_norm": 0.8348718285560608,
      "learning_rate": 6.1060036997589555e-06,
      "loss": 0.1319,
      "step": 67050
    },
    {
      "epoch": 3.7564630423213736,
      "grad_norm": 1.2722585201263428,
      "learning_rate": 6.091989461292674e-06,
      "loss": 0.0943,
      "step": 67060
    },
    {
      "epoch": 3.7570232193373103,
      "grad_norm": 2.926934003829956,
      "learning_rate": 6.077975222826392e-06,
      "loss": 0.0722,
      "step": 67070
    },
    {
      "epoch": 3.7575833963532475,
      "grad_norm": 2.4372944831848145,
      "learning_rate": 6.06396098436011e-06,
      "loss": 0.0932,
      "step": 67080
    },
    {
      "epoch": 3.7581435733691846,
      "grad_norm": 3.0769360065460205,
      "learning_rate": 6.049946745893829e-06,
      "loss": 0.1336,
      "step": 67090
    },
    {
      "epoch": 3.7587037503851217,
      "grad_norm": 1.6205275058746338,
      "learning_rate": 6.035932507427547e-06,
      "loss": 0.0915,
      "step": 67100
    },
    {
      "epoch": 3.759263927401059,
      "grad_norm": 0.9911646842956543,
      "learning_rate": 6.021918268961265e-06,
      "loss": 0.1681,
      "step": 67110
    },
    {
      "epoch": 3.759824104416996,
      "grad_norm": 1.2746261358261108,
      "learning_rate": 6.007904030494983e-06,
      "loss": 0.0961,
      "step": 67120
    },
    {
      "epoch": 3.7603842814329327,
      "grad_norm": 1.7545506954193115,
      "learning_rate": 5.9938897920287014e-06,
      "loss": 0.1069,
      "step": 67130
    },
    {
      "epoch": 3.76094445844887,
      "grad_norm": 1.0774637460708618,
      "learning_rate": 5.9798755535624196e-06,
      "loss": 0.1214,
      "step": 67140
    },
    {
      "epoch": 3.761504635464807,
      "grad_norm": 1.0392130613327026,
      "learning_rate": 5.965861315096138e-06,
      "loss": 0.0791,
      "step": 67150
    },
    {
      "epoch": 3.7620648124807436,
      "grad_norm": 1.3877241611480713,
      "learning_rate": 5.951847076629856e-06,
      "loss": 0.1498,
      "step": 67160
    },
    {
      "epoch": 3.762624989496681,
      "grad_norm": 0.5850501656532288,
      "learning_rate": 5.937832838163575e-06,
      "loss": 0.0865,
      "step": 67170
    },
    {
      "epoch": 3.763185166512618,
      "grad_norm": 3.1714987754821777,
      "learning_rate": 5.923818599697293e-06,
      "loss": 0.0879,
      "step": 67180
    },
    {
      "epoch": 3.763745343528555,
      "grad_norm": 2.3496999740600586,
      "learning_rate": 5.909804361231011e-06,
      "loss": 0.1405,
      "step": 67190
    },
    {
      "epoch": 3.764305520544492,
      "grad_norm": 2.088791847229004,
      "learning_rate": 5.895790122764729e-06,
      "loss": 0.103,
      "step": 67200
    },
    {
      "epoch": 3.7648656975604293,
      "grad_norm": 5.037848949432373,
      "learning_rate": 5.881775884298447e-06,
      "loss": 0.1336,
      "step": 67210
    },
    {
      "epoch": 3.765425874576366,
      "grad_norm": 5.390398025512695,
      "learning_rate": 5.8677616458321655e-06,
      "loss": 0.1005,
      "step": 67220
    },
    {
      "epoch": 3.765986051592303,
      "grad_norm": 1.071918249130249,
      "learning_rate": 5.853747407365884e-06,
      "loss": 0.1192,
      "step": 67230
    },
    {
      "epoch": 3.7665462286082403,
      "grad_norm": 1.0154632329940796,
      "learning_rate": 5.839733168899602e-06,
      "loss": 0.1284,
      "step": 67240
    },
    {
      "epoch": 3.767106405624177,
      "grad_norm": 0.8505653142929077,
      "learning_rate": 5.825718930433321e-06,
      "loss": 0.102,
      "step": 67250
    },
    {
      "epoch": 3.767666582640114,
      "grad_norm": 0.6237878799438477,
      "learning_rate": 5.811704691967039e-06,
      "loss": 0.1172,
      "step": 67260
    },
    {
      "epoch": 3.7682267596560513,
      "grad_norm": 2.5201478004455566,
      "learning_rate": 5.797690453500757e-06,
      "loss": 0.0871,
      "step": 67270
    },
    {
      "epoch": 3.7687869366719884,
      "grad_norm": 1.2816975116729736,
      "learning_rate": 5.783676215034475e-06,
      "loss": 0.1127,
      "step": 67280
    },
    {
      "epoch": 3.7693471136879255,
      "grad_norm": 2.2541236877441406,
      "learning_rate": 5.769661976568193e-06,
      "loss": 0.0823,
      "step": 67290
    },
    {
      "epoch": 3.7699072907038627,
      "grad_norm": 2.2619590759277344,
      "learning_rate": 5.755647738101911e-06,
      "loss": 0.0881,
      "step": 67300
    },
    {
      "epoch": 3.7704674677197993,
      "grad_norm": 1.112259030342102,
      "learning_rate": 5.7416334996356295e-06,
      "loss": 0.1059,
      "step": 67310
    },
    {
      "epoch": 3.7710276447357365,
      "grad_norm": 1.9350192546844482,
      "learning_rate": 5.727619261169348e-06,
      "loss": 0.0837,
      "step": 67320
    },
    {
      "epoch": 3.7715878217516736,
      "grad_norm": 2.3682544231414795,
      "learning_rate": 5.713605022703067e-06,
      "loss": 0.1234,
      "step": 67330
    },
    {
      "epoch": 3.7721479987676103,
      "grad_norm": 1.8999176025390625,
      "learning_rate": 5.699590784236785e-06,
      "loss": 0.1025,
      "step": 67340
    },
    {
      "epoch": 3.7727081757835474,
      "grad_norm": 3.33843731880188,
      "learning_rate": 5.685576545770503e-06,
      "loss": 0.1432,
      "step": 67350
    },
    {
      "epoch": 3.7732683527994846,
      "grad_norm": 2.52872633934021,
      "learning_rate": 5.671562307304221e-06,
      "loss": 0.1251,
      "step": 67360
    },
    {
      "epoch": 3.7738285298154217,
      "grad_norm": 1.394425630569458,
      "learning_rate": 5.657548068837939e-06,
      "loss": 0.1434,
      "step": 67370
    },
    {
      "epoch": 3.774388706831359,
      "grad_norm": 3.5560007095336914,
      "learning_rate": 5.643533830371658e-06,
      "loss": 0.1789,
      "step": 67380
    },
    {
      "epoch": 3.774948883847296,
      "grad_norm": 1.2138382196426392,
      "learning_rate": 5.629519591905376e-06,
      "loss": 0.1554,
      "step": 67390
    },
    {
      "epoch": 3.7755090608632327,
      "grad_norm": 2.125347375869751,
      "learning_rate": 5.615505353439094e-06,
      "loss": 0.1146,
      "step": 67400
    },
    {
      "epoch": 3.77606923787917,
      "grad_norm": 1.9116324186325073,
      "learning_rate": 5.6014911149728125e-06,
      "loss": 0.1018,
      "step": 67410
    },
    {
      "epoch": 3.776629414895107,
      "grad_norm": 1.3546935319900513,
      "learning_rate": 5.5874768765065315e-06,
      "loss": 0.1063,
      "step": 67420
    },
    {
      "epoch": 3.7771895919110436,
      "grad_norm": 0.7458925843238831,
      "learning_rate": 5.57346263804025e-06,
      "loss": 0.0946,
      "step": 67430
    },
    {
      "epoch": 3.7777497689269808,
      "grad_norm": 2.402090549468994,
      "learning_rate": 5.559448399573968e-06,
      "loss": 0.086,
      "step": 67440
    },
    {
      "epoch": 3.778309945942918,
      "grad_norm": 1.9672471284866333,
      "learning_rate": 5.545434161107686e-06,
      "loss": 0.0891,
      "step": 67450
    },
    {
      "epoch": 3.778870122958855,
      "grad_norm": 1.0077531337738037,
      "learning_rate": 5.531419922641404e-06,
      "loss": 0.1292,
      "step": 67460
    },
    {
      "epoch": 3.779430299974792,
      "grad_norm": 1.3910000324249268,
      "learning_rate": 5.517405684175122e-06,
      "loss": 0.0853,
      "step": 67470
    },
    {
      "epoch": 3.7799904769907293,
      "grad_norm": 0.5323976874351501,
      "learning_rate": 5.503391445708841e-06,
      "loss": 0.0879,
      "step": 67480
    },
    {
      "epoch": 3.780550654006666,
      "grad_norm": 0.8187978863716125,
      "learning_rate": 5.489377207242559e-06,
      "loss": 0.0892,
      "step": 67490
    },
    {
      "epoch": 3.781110831022603,
      "grad_norm": 1.081161379814148,
      "learning_rate": 5.475362968776277e-06,
      "loss": 0.0786,
      "step": 67500
    },
    {
      "epoch": 3.7816710080385403,
      "grad_norm": 2.14298152923584,
      "learning_rate": 5.4613487303099955e-06,
      "loss": 0.1161,
      "step": 67510
    },
    {
      "epoch": 3.782231185054477,
      "grad_norm": 1.1960222721099854,
      "learning_rate": 5.447334491843714e-06,
      "loss": 0.0831,
      "step": 67520
    },
    {
      "epoch": 3.782791362070414,
      "grad_norm": 2.6034669876098633,
      "learning_rate": 5.433320253377432e-06,
      "loss": 0.1828,
      "step": 67530
    },
    {
      "epoch": 3.7833515390863512,
      "grad_norm": 2.001286268234253,
      "learning_rate": 5.41930601491115e-06,
      "loss": 0.1086,
      "step": 67540
    },
    {
      "epoch": 3.7839117161022884,
      "grad_norm": 1.4465501308441162,
      "learning_rate": 5.405291776444868e-06,
      "loss": 0.1258,
      "step": 67550
    },
    {
      "epoch": 3.7844718931182255,
      "grad_norm": 2.888408899307251,
      "learning_rate": 5.391277537978587e-06,
      "loss": 0.1222,
      "step": 67560
    },
    {
      "epoch": 3.7850320701341627,
      "grad_norm": 1.4176396131515503,
      "learning_rate": 5.377263299512305e-06,
      "loss": 0.0889,
      "step": 67570
    },
    {
      "epoch": 3.7855922471500993,
      "grad_norm": 1.764447569847107,
      "learning_rate": 5.363249061046023e-06,
      "loss": 0.117,
      "step": 67580
    },
    {
      "epoch": 3.7861524241660365,
      "grad_norm": 1.7055222988128662,
      "learning_rate": 5.349234822579741e-06,
      "loss": 0.1701,
      "step": 67590
    },
    {
      "epoch": 3.7867126011819736,
      "grad_norm": 1.1995694637298584,
      "learning_rate": 5.3352205841134595e-06,
      "loss": 0.1043,
      "step": 67600
    },
    {
      "epoch": 3.7872727781979103,
      "grad_norm": 3.4267325401306152,
      "learning_rate": 5.321206345647178e-06,
      "loss": 0.0718,
      "step": 67610
    },
    {
      "epoch": 3.7878329552138474,
      "grad_norm": 1.6856456995010376,
      "learning_rate": 5.307192107180896e-06,
      "loss": 0.1012,
      "step": 67620
    },
    {
      "epoch": 3.7883931322297846,
      "grad_norm": 3.763165235519409,
      "learning_rate": 5.293177868714614e-06,
      "loss": 0.1271,
      "step": 67630
    },
    {
      "epoch": 3.7889533092457217,
      "grad_norm": 0.9982907176017761,
      "learning_rate": 5.279163630248333e-06,
      "loss": 0.1585,
      "step": 67640
    },
    {
      "epoch": 3.789513486261659,
      "grad_norm": 3.151691198348999,
      "learning_rate": 5.265149391782051e-06,
      "loss": 0.0748,
      "step": 67650
    },
    {
      "epoch": 3.790073663277596,
      "grad_norm": 0.7776056528091431,
      "learning_rate": 5.251135153315769e-06,
      "loss": 0.0744,
      "step": 67660
    },
    {
      "epoch": 3.7906338402935327,
      "grad_norm": 0.54449862241745,
      "learning_rate": 5.237120914849487e-06,
      "loss": 0.0855,
      "step": 67670
    },
    {
      "epoch": 3.79119401730947,
      "grad_norm": 2.529290199279785,
      "learning_rate": 5.223106676383205e-06,
      "loss": 0.1198,
      "step": 67680
    },
    {
      "epoch": 3.791754194325407,
      "grad_norm": 3.828004837036133,
      "learning_rate": 5.2090924379169236e-06,
      "loss": 0.1088,
      "step": 67690
    },
    {
      "epoch": 3.7923143713413436,
      "grad_norm": 2.2099015712738037,
      "learning_rate": 5.195078199450642e-06,
      "loss": 0.0687,
      "step": 67700
    },
    {
      "epoch": 3.7928745483572808,
      "grad_norm": 4.361266613006592,
      "learning_rate": 5.18106396098436e-06,
      "loss": 0.1165,
      "step": 67710
    },
    {
      "epoch": 3.793434725373218,
      "grad_norm": 1.1561198234558105,
      "learning_rate": 5.167049722518079e-06,
      "loss": 0.0766,
      "step": 67720
    },
    {
      "epoch": 3.793994902389155,
      "grad_norm": 4.009222507476807,
      "learning_rate": 5.153035484051797e-06,
      "loss": 0.1052,
      "step": 67730
    },
    {
      "epoch": 3.794555079405092,
      "grad_norm": 1.3115838766098022,
      "learning_rate": 5.139021245585515e-06,
      "loss": 0.0904,
      "step": 67740
    },
    {
      "epoch": 3.7951152564210293,
      "grad_norm": 0.3925311863422394,
      "learning_rate": 5.125007007119233e-06,
      "loss": 0.0945,
      "step": 67750
    },
    {
      "epoch": 3.795675433436966,
      "grad_norm": 2.2402522563934326,
      "learning_rate": 5.110992768652951e-06,
      "loss": 0.1228,
      "step": 67760
    },
    {
      "epoch": 3.796235610452903,
      "grad_norm": 0.6258916854858398,
      "learning_rate": 5.0969785301866695e-06,
      "loss": 0.0838,
      "step": 67770
    },
    {
      "epoch": 3.7967957874688403,
      "grad_norm": 1.3758544921875,
      "learning_rate": 5.082964291720388e-06,
      "loss": 0.201,
      "step": 67780
    },
    {
      "epoch": 3.797355964484777,
      "grad_norm": 2.48930025100708,
      "learning_rate": 5.068950053254106e-06,
      "loss": 0.1135,
      "step": 67790
    },
    {
      "epoch": 3.797916141500714,
      "grad_norm": 1.534671425819397,
      "learning_rate": 5.054935814787825e-06,
      "loss": 0.0759,
      "step": 67800
    },
    {
      "epoch": 3.7984763185166512,
      "grad_norm": 2.294440269470215,
      "learning_rate": 5.040921576321543e-06,
      "loss": 0.1185,
      "step": 67810
    },
    {
      "epoch": 3.7990364955325884,
      "grad_norm": 1.0805566310882568,
      "learning_rate": 5.026907337855261e-06,
      "loss": 0.0845,
      "step": 67820
    },
    {
      "epoch": 3.7995966725485255,
      "grad_norm": 1.8143277168273926,
      "learning_rate": 5.012893099388979e-06,
      "loss": 0.101,
      "step": 67830
    },
    {
      "epoch": 3.8001568495644626,
      "grad_norm": 4.667812824249268,
      "learning_rate": 4.998878860922697e-06,
      "loss": 0.1366,
      "step": 67840
    },
    {
      "epoch": 3.8007170265803993,
      "grad_norm": 0.4573167860507965,
      "learning_rate": 4.984864622456416e-06,
      "loss": 0.0872,
      "step": 67850
    },
    {
      "epoch": 3.8012772035963365,
      "grad_norm": 1.2289587259292603,
      "learning_rate": 4.970850383990134e-06,
      "loss": 0.1158,
      "step": 67860
    },
    {
      "epoch": 3.8018373806122736,
      "grad_norm": 0.46658575534820557,
      "learning_rate": 4.9568361455238525e-06,
      "loss": 0.0989,
      "step": 67870
    },
    {
      "epoch": 3.8023975576282103,
      "grad_norm": 0.6496896147727966,
      "learning_rate": 4.942821907057571e-06,
      "loss": 0.1141,
      "step": 67880
    },
    {
      "epoch": 3.8029577346441474,
      "grad_norm": 0.7248826026916504,
      "learning_rate": 4.9288076685912896e-06,
      "loss": 0.0839,
      "step": 67890
    },
    {
      "epoch": 3.8035179116600846,
      "grad_norm": 0.8578829169273376,
      "learning_rate": 4.914793430125008e-06,
      "loss": 0.117,
      "step": 67900
    },
    {
      "epoch": 3.8040780886760217,
      "grad_norm": 0.9950056672096252,
      "learning_rate": 4.900779191658726e-06,
      "loss": 0.113,
      "step": 67910
    },
    {
      "epoch": 3.804638265691959,
      "grad_norm": 3.5723910331726074,
      "learning_rate": 4.886764953192444e-06,
      "loss": 0.1876,
      "step": 67920
    },
    {
      "epoch": 3.8051984427078955,
      "grad_norm": 1.3665416240692139,
      "learning_rate": 4.872750714726162e-06,
      "loss": 0.0969,
      "step": 67930
    },
    {
      "epoch": 3.8057586197238327,
      "grad_norm": 1.8066036701202393,
      "learning_rate": 4.85873647625988e-06,
      "loss": 0.1294,
      "step": 67940
    },
    {
      "epoch": 3.80631879673977,
      "grad_norm": 3.331805467605591,
      "learning_rate": 4.844722237793599e-06,
      "loss": 0.1288,
      "step": 67950
    },
    {
      "epoch": 3.806878973755707,
      "grad_norm": 1.053842544555664,
      "learning_rate": 4.830707999327317e-06,
      "loss": 0.1108,
      "step": 67960
    },
    {
      "epoch": 3.8074391507716436,
      "grad_norm": 1.2914936542510986,
      "learning_rate": 4.8166937608610355e-06,
      "loss": 0.088,
      "step": 67970
    },
    {
      "epoch": 3.8079993277875808,
      "grad_norm": 1.451620101928711,
      "learning_rate": 4.802679522394754e-06,
      "loss": 0.0709,
      "step": 67980
    },
    {
      "epoch": 3.808559504803518,
      "grad_norm": 1.2126641273498535,
      "learning_rate": 4.788665283928472e-06,
      "loss": 0.0921,
      "step": 67990
    },
    {
      "epoch": 3.809119681819455,
      "grad_norm": 0.5278725028038025,
      "learning_rate": 4.77465104546219e-06,
      "loss": 0.0557,
      "step": 68000
    },
    {
      "epoch": 3.809679858835392,
      "grad_norm": 5.862438201904297,
      "learning_rate": 4.760636806995908e-06,
      "loss": 0.1325,
      "step": 68010
    },
    {
      "epoch": 3.810240035851329,
      "grad_norm": 0.9005404114723206,
      "learning_rate": 4.746622568529626e-06,
      "loss": 0.1787,
      "step": 68020
    },
    {
      "epoch": 3.810800212867266,
      "grad_norm": 4.277657985687256,
      "learning_rate": 4.732608330063345e-06,
      "loss": 0.1162,
      "step": 68030
    },
    {
      "epoch": 3.811360389883203,
      "grad_norm": 3.364248752593994,
      "learning_rate": 4.718594091597063e-06,
      "loss": 0.08,
      "step": 68040
    },
    {
      "epoch": 3.8119205668991403,
      "grad_norm": 4.169286251068115,
      "learning_rate": 4.704579853130781e-06,
      "loss": 0.1018,
      "step": 68050
    },
    {
      "epoch": 3.812480743915077,
      "grad_norm": 1.5325952768325806,
      "learning_rate": 4.6905656146644995e-06,
      "loss": 0.0786,
      "step": 68060
    },
    {
      "epoch": 3.813040920931014,
      "grad_norm": 2.182380437850952,
      "learning_rate": 4.676551376198218e-06,
      "loss": 0.1049,
      "step": 68070
    },
    {
      "epoch": 3.8136010979469512,
      "grad_norm": 2.8203444480895996,
      "learning_rate": 4.662537137731936e-06,
      "loss": 0.0807,
      "step": 68080
    },
    {
      "epoch": 3.8141612749628884,
      "grad_norm": 1.204668402671814,
      "learning_rate": 4.648522899265654e-06,
      "loss": 0.0757,
      "step": 68090
    },
    {
      "epoch": 3.8147214519788255,
      "grad_norm": 0.5008286833763123,
      "learning_rate": 4.634508660799372e-06,
      "loss": 0.1275,
      "step": 68100
    },
    {
      "epoch": 3.815281628994762,
      "grad_norm": 1.8421260118484497,
      "learning_rate": 4.620494422333091e-06,
      "loss": 0.1063,
      "step": 68110
    },
    {
      "epoch": 3.8158418060106993,
      "grad_norm": 6.199284076690674,
      "learning_rate": 4.606480183866809e-06,
      "loss": 0.1518,
      "step": 68120
    },
    {
      "epoch": 3.8164019830266365,
      "grad_norm": 0.7580214738845825,
      "learning_rate": 4.592465945400527e-06,
      "loss": 0.1607,
      "step": 68130
    },
    {
      "epoch": 3.8169621600425736,
      "grad_norm": 2.9222402572631836,
      "learning_rate": 4.578451706934245e-06,
      "loss": 0.1368,
      "step": 68140
    },
    {
      "epoch": 3.8175223370585103,
      "grad_norm": 1.029563069343567,
      "learning_rate": 4.5644374684679635e-06,
      "loss": 0.1157,
      "step": 68150
    },
    {
      "epoch": 3.8180825140744474,
      "grad_norm": 3.687405586242676,
      "learning_rate": 4.550423230001682e-06,
      "loss": 0.0898,
      "step": 68160
    },
    {
      "epoch": 3.8186426910903846,
      "grad_norm": 1.369454264640808,
      "learning_rate": 4.5364089915354e-06,
      "loss": 0.1092,
      "step": 68170
    },
    {
      "epoch": 3.8192028681063217,
      "grad_norm": 1.1254969835281372,
      "learning_rate": 4.522394753069118e-06,
      "loss": 0.1212,
      "step": 68180
    },
    {
      "epoch": 3.819763045122259,
      "grad_norm": 3.2147624492645264,
      "learning_rate": 4.508380514602837e-06,
      "loss": 0.0963,
      "step": 68190
    },
    {
      "epoch": 3.8203232221381955,
      "grad_norm": 2.2527945041656494,
      "learning_rate": 4.494366276136555e-06,
      "loss": 0.1192,
      "step": 68200
    },
    {
      "epoch": 3.8208833991541327,
      "grad_norm": 2.0975499153137207,
      "learning_rate": 4.480352037670273e-06,
      "loss": 0.1513,
      "step": 68210
    },
    {
      "epoch": 3.82144357617007,
      "grad_norm": 2.7299630641937256,
      "learning_rate": 4.466337799203991e-06,
      "loss": 0.0898,
      "step": 68220
    },
    {
      "epoch": 3.822003753186007,
      "grad_norm": 2.619372844696045,
      "learning_rate": 4.452323560737709e-06,
      "loss": 0.1023,
      "step": 68230
    },
    {
      "epoch": 3.8225639302019436,
      "grad_norm": 0.8709568381309509,
      "learning_rate": 4.4383093222714275e-06,
      "loss": 0.1004,
      "step": 68240
    },
    {
      "epoch": 3.8231241072178808,
      "grad_norm": 0.6267557740211487,
      "learning_rate": 4.424295083805146e-06,
      "loss": 0.0713,
      "step": 68250
    },
    {
      "epoch": 3.823684284233818,
      "grad_norm": 1.1309374570846558,
      "learning_rate": 4.410280845338864e-06,
      "loss": 0.1355,
      "step": 68260
    },
    {
      "epoch": 3.824244461249755,
      "grad_norm": 1.2054972648620605,
      "learning_rate": 4.396266606872583e-06,
      "loss": 0.1153,
      "step": 68270
    },
    {
      "epoch": 3.824804638265692,
      "grad_norm": 3.0395219326019287,
      "learning_rate": 4.382252368406301e-06,
      "loss": 0.0891,
      "step": 68280
    },
    {
      "epoch": 3.825364815281629,
      "grad_norm": 5.038853168487549,
      "learning_rate": 4.368238129940019e-06,
      "loss": 0.1153,
      "step": 68290
    },
    {
      "epoch": 3.825924992297566,
      "grad_norm": 0.40569332242012024,
      "learning_rate": 4.354223891473737e-06,
      "loss": 0.1009,
      "step": 68300
    },
    {
      "epoch": 3.826485169313503,
      "grad_norm": 1.4104591608047485,
      "learning_rate": 4.340209653007455e-06,
      "loss": 0.1397,
      "step": 68310
    },
    {
      "epoch": 3.8270453463294403,
      "grad_norm": 2.213141918182373,
      "learning_rate": 4.3261954145411734e-06,
      "loss": 0.088,
      "step": 68320
    },
    {
      "epoch": 3.827605523345377,
      "grad_norm": 0.9514126777648926,
      "learning_rate": 4.312181176074892e-06,
      "loss": 0.0881,
      "step": 68330
    },
    {
      "epoch": 3.828165700361314,
      "grad_norm": 3.1432273387908936,
      "learning_rate": 4.2981669376086105e-06,
      "loss": 0.085,
      "step": 68340
    },
    {
      "epoch": 3.8287258773772512,
      "grad_norm": 6.986440658569336,
      "learning_rate": 4.284152699142329e-06,
      "loss": 0.1573,
      "step": 68350
    },
    {
      "epoch": 3.8292860543931884,
      "grad_norm": 1.1474264860153198,
      "learning_rate": 4.270138460676047e-06,
      "loss": 0.1228,
      "step": 68360
    },
    {
      "epoch": 3.8298462314091255,
      "grad_norm": 0.7578073143959045,
      "learning_rate": 4.256124222209766e-06,
      "loss": 0.1559,
      "step": 68370
    },
    {
      "epoch": 3.830406408425062,
      "grad_norm": 3.152050495147705,
      "learning_rate": 4.242109983743484e-06,
      "loss": 0.1096,
      "step": 68380
    },
    {
      "epoch": 3.8309665854409993,
      "grad_norm": 2.6590821743011475,
      "learning_rate": 4.228095745277202e-06,
      "loss": 0.0905,
      "step": 68390
    },
    {
      "epoch": 3.8315267624569365,
      "grad_norm": 1.983609914779663,
      "learning_rate": 4.21408150681092e-06,
      "loss": 0.1094,
      "step": 68400
    },
    {
      "epoch": 3.832086939472873,
      "grad_norm": 0.9855937361717224,
      "learning_rate": 4.200067268344638e-06,
      "loss": 0.0832,
      "step": 68410
    },
    {
      "epoch": 3.8326471164888103,
      "grad_norm": 4.936244964599609,
      "learning_rate": 4.186053029878357e-06,
      "loss": 0.1194,
      "step": 68420
    },
    {
      "epoch": 3.8332072935047474,
      "grad_norm": 0.7654221653938293,
      "learning_rate": 4.1720387914120754e-06,
      "loss": 0.1017,
      "step": 68430
    },
    {
      "epoch": 3.8337674705206846,
      "grad_norm": 1.078609585762024,
      "learning_rate": 4.1580245529457936e-06,
      "loss": 0.1323,
      "step": 68440
    },
    {
      "epoch": 3.8343276475366217,
      "grad_norm": 1.1606887578964233,
      "learning_rate": 4.144010314479512e-06,
      "loss": 0.1346,
      "step": 68450
    },
    {
      "epoch": 3.834887824552559,
      "grad_norm": 0.9189954400062561,
      "learning_rate": 4.12999607601323e-06,
      "loss": 0.1195,
      "step": 68460
    },
    {
      "epoch": 3.8354480015684955,
      "grad_norm": 2.8778326511383057,
      "learning_rate": 4.115981837546948e-06,
      "loss": 0.1074,
      "step": 68470
    },
    {
      "epoch": 3.8360081785844327,
      "grad_norm": 1.4003797769546509,
      "learning_rate": 4.101967599080666e-06,
      "loss": 0.0866,
      "step": 68480
    },
    {
      "epoch": 3.83656835560037,
      "grad_norm": 1.533856749534607,
      "learning_rate": 4.087953360614384e-06,
      "loss": 0.1059,
      "step": 68490
    },
    {
      "epoch": 3.8371285326163065,
      "grad_norm": 2.3910584449768066,
      "learning_rate": 4.073939122148103e-06,
      "loss": 0.1033,
      "step": 68500
    },
    {
      "epoch": 3.8376887096322436,
      "grad_norm": 1.733432412147522,
      "learning_rate": 4.059924883681821e-06,
      "loss": 0.1202,
      "step": 68510
    },
    {
      "epoch": 3.8382488866481808,
      "grad_norm": 1.1653809547424316,
      "learning_rate": 4.0459106452155395e-06,
      "loss": 0.1063,
      "step": 68520
    },
    {
      "epoch": 3.838809063664118,
      "grad_norm": 1.4260340929031372,
      "learning_rate": 4.031896406749258e-06,
      "loss": 0.1167,
      "step": 68530
    },
    {
      "epoch": 3.839369240680055,
      "grad_norm": 0.5839911103248596,
      "learning_rate": 4.017882168282976e-06,
      "loss": 0.1052,
      "step": 68540
    },
    {
      "epoch": 3.839929417695992,
      "grad_norm": 1.7573730945587158,
      "learning_rate": 4.003867929816694e-06,
      "loss": 0.0873,
      "step": 68550
    },
    {
      "epoch": 3.840489594711929,
      "grad_norm": 1.8592885732650757,
      "learning_rate": 3.989853691350412e-06,
      "loss": 0.0963,
      "step": 68560
    },
    {
      "epoch": 3.841049771727866,
      "grad_norm": 5.052472114562988,
      "learning_rate": 3.97583945288413e-06,
      "loss": 0.1013,
      "step": 68570
    },
    {
      "epoch": 3.841609948743803,
      "grad_norm": 3.798638105392456,
      "learning_rate": 3.961825214417849e-06,
      "loss": 0.1452,
      "step": 68580
    },
    {
      "epoch": 3.84217012575974,
      "grad_norm": 1.5178759098052979,
      "learning_rate": 3.947810975951567e-06,
      "loss": 0.103,
      "step": 68590
    },
    {
      "epoch": 3.842730302775677,
      "grad_norm": 0.7552253603935242,
      "learning_rate": 3.933796737485285e-06,
      "loss": 0.1019,
      "step": 68600
    },
    {
      "epoch": 3.843290479791614,
      "grad_norm": 3.3688223361968994,
      "learning_rate": 3.9197824990190035e-06,
      "loss": 0.0879,
      "step": 68610
    },
    {
      "epoch": 3.8438506568075512,
      "grad_norm": 1.825105905532837,
      "learning_rate": 3.905768260552722e-06,
      "loss": 0.0961,
      "step": 68620
    },
    {
      "epoch": 3.8444108338234884,
      "grad_norm": 0.8024070262908936,
      "learning_rate": 3.89175402208644e-06,
      "loss": 0.0908,
      "step": 68630
    },
    {
      "epoch": 3.8449710108394255,
      "grad_norm": 2.982635974884033,
      "learning_rate": 3.877739783620158e-06,
      "loss": 0.0831,
      "step": 68640
    },
    {
      "epoch": 3.845531187855362,
      "grad_norm": 2.5569701194763184,
      "learning_rate": 3.863725545153876e-06,
      "loss": 0.1398,
      "step": 68650
    },
    {
      "epoch": 3.8460913648712993,
      "grad_norm": 1.663278579711914,
      "learning_rate": 3.849711306687595e-06,
      "loss": 0.0892,
      "step": 68660
    },
    {
      "epoch": 3.8466515418872365,
      "grad_norm": 2.061786413192749,
      "learning_rate": 3.835697068221313e-06,
      "loss": 0.08,
      "step": 68670
    },
    {
      "epoch": 3.847211718903173,
      "grad_norm": 4.272778511047363,
      "learning_rate": 3.821682829755031e-06,
      "loss": 0.1923,
      "step": 68680
    },
    {
      "epoch": 3.8477718959191103,
      "grad_norm": 4.8217878341674805,
      "learning_rate": 3.8076685912887494e-06,
      "loss": 0.1804,
      "step": 68690
    },
    {
      "epoch": 3.8483320729350474,
      "grad_norm": 3.3164632320404053,
      "learning_rate": 3.793654352822468e-06,
      "loss": 0.1103,
      "step": 68700
    },
    {
      "epoch": 3.8488922499509846,
      "grad_norm": 4.804905414581299,
      "learning_rate": 3.779640114356186e-06,
      "loss": 0.1165,
      "step": 68710
    },
    {
      "epoch": 3.8494524269669217,
      "grad_norm": 1.6965738534927368,
      "learning_rate": 3.765625875889904e-06,
      "loss": 0.0836,
      "step": 68720
    },
    {
      "epoch": 3.850012603982859,
      "grad_norm": 1.0852060317993164,
      "learning_rate": 3.7516116374236227e-06,
      "loss": 0.1264,
      "step": 68730
    },
    {
      "epoch": 3.8505727809987955,
      "grad_norm": 3.911057233810425,
      "learning_rate": 3.7375973989573413e-06,
      "loss": 0.1165,
      "step": 68740
    },
    {
      "epoch": 3.8511329580147327,
      "grad_norm": 1.4105761051177979,
      "learning_rate": 3.7235831604910594e-06,
      "loss": 0.0977,
      "step": 68750
    },
    {
      "epoch": 3.85169313503067,
      "grad_norm": 1.7808501720428467,
      "learning_rate": 3.7095689220247776e-06,
      "loss": 0.1421,
      "step": 68760
    },
    {
      "epoch": 3.8522533120466065,
      "grad_norm": 1.5767643451690674,
      "learning_rate": 3.6955546835584957e-06,
      "loss": 0.0877,
      "step": 68770
    },
    {
      "epoch": 3.8528134890625436,
      "grad_norm": 2.8166706562042236,
      "learning_rate": 3.681540445092214e-06,
      "loss": 0.0781,
      "step": 68780
    },
    {
      "epoch": 3.8533736660784808,
      "grad_norm": 1.5835199356079102,
      "learning_rate": 3.667526206625932e-06,
      "loss": 0.1292,
      "step": 68790
    },
    {
      "epoch": 3.853933843094418,
      "grad_norm": 2.201551914215088,
      "learning_rate": 3.65351196815965e-06,
      "loss": 0.1378,
      "step": 68800
    },
    {
      "epoch": 3.854494020110355,
      "grad_norm": 0.820383608341217,
      "learning_rate": 3.639497729693369e-06,
      "loss": 0.0862,
      "step": 68810
    },
    {
      "epoch": 3.855054197126292,
      "grad_norm": 0.8585058450698853,
      "learning_rate": 3.625483491227087e-06,
      "loss": 0.1281,
      "step": 68820
    },
    {
      "epoch": 3.855614374142229,
      "grad_norm": 2.933654546737671,
      "learning_rate": 3.6114692527608053e-06,
      "loss": 0.1885,
      "step": 68830
    },
    {
      "epoch": 3.856174551158166,
      "grad_norm": 3.139413595199585,
      "learning_rate": 3.5974550142945235e-06,
      "loss": 0.0875,
      "step": 68840
    },
    {
      "epoch": 3.856734728174103,
      "grad_norm": 1.0193942785263062,
      "learning_rate": 3.5834407758282416e-06,
      "loss": 0.0837,
      "step": 68850
    },
    {
      "epoch": 3.85729490519004,
      "grad_norm": 1.087276577949524,
      "learning_rate": 3.5694265373619597e-06,
      "loss": 0.079,
      "step": 68860
    },
    {
      "epoch": 3.857855082205977,
      "grad_norm": 1.166891098022461,
      "learning_rate": 3.555412298895678e-06,
      "loss": 0.0882,
      "step": 68870
    },
    {
      "epoch": 3.858415259221914,
      "grad_norm": 2.575770854949951,
      "learning_rate": 3.541398060429396e-06,
      "loss": 0.0782,
      "step": 68880
    },
    {
      "epoch": 3.858975436237851,
      "grad_norm": 1.4480371475219727,
      "learning_rate": 3.527383821963115e-06,
      "loss": 0.1121,
      "step": 68890
    },
    {
      "epoch": 3.8595356132537884,
      "grad_norm": 3.505284070968628,
      "learning_rate": 3.513369583496833e-06,
      "loss": 0.1276,
      "step": 68900
    },
    {
      "epoch": 3.8600957902697255,
      "grad_norm": 0.7049834728240967,
      "learning_rate": 3.4993553450305512e-06,
      "loss": 0.088,
      "step": 68910
    },
    {
      "epoch": 3.860655967285662,
      "grad_norm": 1.0106419324874878,
      "learning_rate": 3.4853411065642694e-06,
      "loss": 0.1183,
      "step": 68920
    },
    {
      "epoch": 3.8612161443015993,
      "grad_norm": 1.3841198682785034,
      "learning_rate": 3.4713268680979875e-06,
      "loss": 0.1122,
      "step": 68930
    },
    {
      "epoch": 3.8617763213175365,
      "grad_norm": 2.501281976699829,
      "learning_rate": 3.457312629631706e-06,
      "loss": 0.1023,
      "step": 68940
    },
    {
      "epoch": 3.862336498333473,
      "grad_norm": 4.3143391609191895,
      "learning_rate": 3.443298391165424e-06,
      "loss": 0.1086,
      "step": 68950
    },
    {
      "epoch": 3.8628966753494103,
      "grad_norm": 1.3563246726989746,
      "learning_rate": 3.4292841526991423e-06,
      "loss": 0.0788,
      "step": 68960
    },
    {
      "epoch": 3.8634568523653474,
      "grad_norm": 0.8231911063194275,
      "learning_rate": 3.415269914232861e-06,
      "loss": 0.0838,
      "step": 68970
    },
    {
      "epoch": 3.8640170293812846,
      "grad_norm": 0.9782544374465942,
      "learning_rate": 3.4012556757665794e-06,
      "loss": 0.0899,
      "step": 68980
    },
    {
      "epoch": 3.8645772063972217,
      "grad_norm": 1.0278996229171753,
      "learning_rate": 3.3872414373002975e-06,
      "loss": 0.0712,
      "step": 68990
    },
    {
      "epoch": 3.865137383413159,
      "grad_norm": 1.913813829421997,
      "learning_rate": 3.3732271988340157e-06,
      "loss": 0.0822,
      "step": 69000
    },
    {
      "epoch": 3.8656975604290955,
      "grad_norm": 1.4025386571884155,
      "learning_rate": 3.359212960367734e-06,
      "loss": 0.0935,
      "step": 69010
    },
    {
      "epoch": 3.8662577374450326,
      "grad_norm": 7.498987197875977,
      "learning_rate": 3.345198721901452e-06,
      "loss": 0.1395,
      "step": 69020
    },
    {
      "epoch": 3.86681791446097,
      "grad_norm": 1.3529348373413086,
      "learning_rate": 3.33118448343517e-06,
      "loss": 0.1231,
      "step": 69030
    },
    {
      "epoch": 3.8673780914769065,
      "grad_norm": 3.42101788520813,
      "learning_rate": 3.317170244968888e-06,
      "loss": 0.0807,
      "step": 69040
    },
    {
      "epoch": 3.8679382684928436,
      "grad_norm": 1.4563556909561157,
      "learning_rate": 3.303156006502607e-06,
      "loss": 0.2135,
      "step": 69050
    },
    {
      "epoch": 3.8684984455087807,
      "grad_norm": 1.6853950023651123,
      "learning_rate": 3.2891417680363253e-06,
      "loss": 0.1156,
      "step": 69060
    },
    {
      "epoch": 3.869058622524718,
      "grad_norm": 0.5696179270744324,
      "learning_rate": 3.2751275295700434e-06,
      "loss": 0.0687,
      "step": 69070
    },
    {
      "epoch": 3.869618799540655,
      "grad_norm": 1.2636843919754028,
      "learning_rate": 3.2611132911037616e-06,
      "loss": 0.0895,
      "step": 69080
    },
    {
      "epoch": 3.870178976556592,
      "grad_norm": 2.762986183166504,
      "learning_rate": 3.2470990526374797e-06,
      "loss": 0.1509,
      "step": 69090
    },
    {
      "epoch": 3.870739153572529,
      "grad_norm": 1.0687274932861328,
      "learning_rate": 3.233084814171198e-06,
      "loss": 0.1478,
      "step": 69100
    },
    {
      "epoch": 3.871299330588466,
      "grad_norm": 3.6433799266815186,
      "learning_rate": 3.219070575704916e-06,
      "loss": 0.1406,
      "step": 69110
    },
    {
      "epoch": 3.871859507604403,
      "grad_norm": 0.5515134334564209,
      "learning_rate": 3.205056337238634e-06,
      "loss": 0.0794,
      "step": 69120
    },
    {
      "epoch": 3.87241968462034,
      "grad_norm": 1.9584784507751465,
      "learning_rate": 3.191042098772353e-06,
      "loss": 0.1536,
      "step": 69130
    },
    {
      "epoch": 3.872979861636277,
      "grad_norm": 2.3336119651794434,
      "learning_rate": 3.177027860306071e-06,
      "loss": 0.1333,
      "step": 69140
    },
    {
      "epoch": 3.873540038652214,
      "grad_norm": 1.2001594305038452,
      "learning_rate": 3.1630136218397893e-06,
      "loss": 0.1043,
      "step": 69150
    },
    {
      "epoch": 3.874100215668151,
      "grad_norm": 3.434225082397461,
      "learning_rate": 3.1489993833735075e-06,
      "loss": 0.1489,
      "step": 69160
    },
    {
      "epoch": 3.8746603926840884,
      "grad_norm": 1.4670026302337646,
      "learning_rate": 3.1349851449072256e-06,
      "loss": 0.0885,
      "step": 69170
    },
    {
      "epoch": 3.875220569700025,
      "grad_norm": 5.591968536376953,
      "learning_rate": 3.120970906440944e-06,
      "loss": 0.1191,
      "step": 69180
    },
    {
      "epoch": 3.875780746715962,
      "grad_norm": 2.334869623184204,
      "learning_rate": 3.1069566679746623e-06,
      "loss": 0.106,
      "step": 69190
    },
    {
      "epoch": 3.8763409237318993,
      "grad_norm": 2.091590404510498,
      "learning_rate": 3.092942429508381e-06,
      "loss": 0.1596,
      "step": 69200
    },
    {
      "epoch": 3.8769011007478364,
      "grad_norm": 2.1712706089019775,
      "learning_rate": 3.078928191042099e-06,
      "loss": 0.0916,
      "step": 69210
    },
    {
      "epoch": 3.877461277763773,
      "grad_norm": 1.730018138885498,
      "learning_rate": 3.0649139525758175e-06,
      "loss": 0.1034,
      "step": 69220
    },
    {
      "epoch": 3.8780214547797103,
      "grad_norm": 1.2847288846969604,
      "learning_rate": 3.0508997141095357e-06,
      "loss": 0.1512,
      "step": 69230
    },
    {
      "epoch": 3.8785816317956474,
      "grad_norm": 0.9557124972343445,
      "learning_rate": 3.036885475643254e-06,
      "loss": 0.1269,
      "step": 69240
    },
    {
      "epoch": 3.8791418088115845,
      "grad_norm": 3.0652620792388916,
      "learning_rate": 3.022871237176972e-06,
      "loss": 0.0935,
      "step": 69250
    },
    {
      "epoch": 3.8797019858275217,
      "grad_norm": 1.7004401683807373,
      "learning_rate": 3.0088569987106905e-06,
      "loss": 0.1041,
      "step": 69260
    },
    {
      "epoch": 3.8802621628434584,
      "grad_norm": 2.5513205528259277,
      "learning_rate": 2.9948427602444086e-06,
      "loss": 0.1154,
      "step": 69270
    },
    {
      "epoch": 3.8808223398593955,
      "grad_norm": 1.8100552558898926,
      "learning_rate": 2.9808285217781267e-06,
      "loss": 0.0975,
      "step": 69280
    },
    {
      "epoch": 3.8813825168753326,
      "grad_norm": 2.617229700088501,
      "learning_rate": 2.966814283311845e-06,
      "loss": 0.1227,
      "step": 69290
    },
    {
      "epoch": 3.88194269389127,
      "grad_norm": 5.59971284866333,
      "learning_rate": 2.9528000448455634e-06,
      "loss": 0.1497,
      "step": 69300
    },
    {
      "epoch": 3.8825028709072065,
      "grad_norm": 5.161716938018799,
      "learning_rate": 2.9387858063792816e-06,
      "loss": 0.1541,
      "step": 69310
    },
    {
      "epoch": 3.8830630479231436,
      "grad_norm": 1.5588550567626953,
      "learning_rate": 2.9247715679129997e-06,
      "loss": 0.0926,
      "step": 69320
    },
    {
      "epoch": 3.8836232249390807,
      "grad_norm": 0.9941207766532898,
      "learning_rate": 2.910757329446718e-06,
      "loss": 0.0765,
      "step": 69330
    },
    {
      "epoch": 3.884183401955018,
      "grad_norm": 1.703369379043579,
      "learning_rate": 2.8967430909804364e-06,
      "loss": 0.1256,
      "step": 69340
    },
    {
      "epoch": 3.884743578970955,
      "grad_norm": 3.833108901977539,
      "learning_rate": 2.8827288525141545e-06,
      "loss": 0.0998,
      "step": 69350
    },
    {
      "epoch": 3.8853037559868917,
      "grad_norm": 4.087214946746826,
      "learning_rate": 2.8687146140478726e-06,
      "loss": 0.1052,
      "step": 69360
    },
    {
      "epoch": 3.885863933002829,
      "grad_norm": 2.503392219543457,
      "learning_rate": 2.8547003755815908e-06,
      "loss": 0.0988,
      "step": 69370
    },
    {
      "epoch": 3.886424110018766,
      "grad_norm": 1.8214725255966187,
      "learning_rate": 2.8406861371153093e-06,
      "loss": 0.137,
      "step": 69380
    },
    {
      "epoch": 3.886984287034703,
      "grad_norm": 1.3704032897949219,
      "learning_rate": 2.8266718986490275e-06,
      "loss": 0.0859,
      "step": 69390
    },
    {
      "epoch": 3.88754446405064,
      "grad_norm": 1.215925931930542,
      "learning_rate": 2.8126576601827456e-06,
      "loss": 0.1237,
      "step": 69400
    },
    {
      "epoch": 3.888104641066577,
      "grad_norm": 1.652268886566162,
      "learning_rate": 2.798643421716464e-06,
      "loss": 0.0857,
      "step": 69410
    },
    {
      "epoch": 3.888664818082514,
      "grad_norm": 0.9419177770614624,
      "learning_rate": 2.7846291832501823e-06,
      "loss": 0.0966,
      "step": 69420
    },
    {
      "epoch": 3.889224995098451,
      "grad_norm": 3.0506503582000732,
      "learning_rate": 2.770614944783901e-06,
      "loss": 0.1415,
      "step": 69430
    },
    {
      "epoch": 3.8897851721143883,
      "grad_norm": 0.7992985844612122,
      "learning_rate": 2.756600706317619e-06,
      "loss": 0.1215,
      "step": 69440
    },
    {
      "epoch": 3.890345349130325,
      "grad_norm": 0.7835961580276489,
      "learning_rate": 2.742586467851337e-06,
      "loss": 0.1034,
      "step": 69450
    },
    {
      "epoch": 3.890905526146262,
      "grad_norm": 0.9619598388671875,
      "learning_rate": 2.7285722293850556e-06,
      "loss": 0.0905,
      "step": 69460
    },
    {
      "epoch": 3.8914657031621993,
      "grad_norm": 4.152733325958252,
      "learning_rate": 2.7145579909187738e-06,
      "loss": 0.1145,
      "step": 69470
    },
    {
      "epoch": 3.8920258801781364,
      "grad_norm": 2.6441586017608643,
      "learning_rate": 2.700543752452492e-06,
      "loss": 0.122,
      "step": 69480
    },
    {
      "epoch": 3.892586057194073,
      "grad_norm": 1.584090232849121,
      "learning_rate": 2.68652951398621e-06,
      "loss": 0.1839,
      "step": 69490
    },
    {
      "epoch": 3.8931462342100103,
      "grad_norm": 1.8066370487213135,
      "learning_rate": 2.6725152755199286e-06,
      "loss": 0.0842,
      "step": 69500
    },
    {
      "epoch": 3.8937064112259474,
      "grad_norm": 2.227658748626709,
      "learning_rate": 2.6585010370536467e-06,
      "loss": 0.0934,
      "step": 69510
    },
    {
      "epoch": 3.8942665882418845,
      "grad_norm": 6.257554054260254,
      "learning_rate": 2.644486798587365e-06,
      "loss": 0.1478,
      "step": 69520
    },
    {
      "epoch": 3.8948267652578217,
      "grad_norm": 1.6067380905151367,
      "learning_rate": 2.630472560121083e-06,
      "loss": 0.0983,
      "step": 69530
    },
    {
      "epoch": 3.8953869422737584,
      "grad_norm": 2.042677402496338,
      "learning_rate": 2.6164583216548015e-06,
      "loss": 0.0832,
      "step": 69540
    },
    {
      "epoch": 3.8959471192896955,
      "grad_norm": 2.4904568195343018,
      "learning_rate": 2.6024440831885197e-06,
      "loss": 0.1078,
      "step": 69550
    },
    {
      "epoch": 3.8965072963056326,
      "grad_norm": 1.5066542625427246,
      "learning_rate": 2.588429844722238e-06,
      "loss": 0.0748,
      "step": 69560
    },
    {
      "epoch": 3.8970674733215698,
      "grad_norm": 0.9985843300819397,
      "learning_rate": 2.574415606255956e-06,
      "loss": 0.0919,
      "step": 69570
    },
    {
      "epoch": 3.8976276503375065,
      "grad_norm": 0.953597366809845,
      "learning_rate": 2.5604013677896745e-06,
      "loss": 0.1241,
      "step": 69580
    },
    {
      "epoch": 3.8981878273534436,
      "grad_norm": 0.8595107197761536,
      "learning_rate": 2.5463871293233926e-06,
      "loss": 0.0941,
      "step": 69590
    },
    {
      "epoch": 3.8987480043693807,
      "grad_norm": 0.48891112208366394,
      "learning_rate": 2.5323728908571107e-06,
      "loss": 0.1315,
      "step": 69600
    },
    {
      "epoch": 3.899308181385318,
      "grad_norm": 1.0193071365356445,
      "learning_rate": 2.518358652390829e-06,
      "loss": 0.0735,
      "step": 69610
    },
    {
      "epoch": 3.899868358401255,
      "grad_norm": 1.2059838771820068,
      "learning_rate": 2.5043444139245474e-06,
      "loss": 0.14,
      "step": 69620
    },
    {
      "epoch": 3.9004285354171917,
      "grad_norm": 1.2808367013931274,
      "learning_rate": 2.4903301754582656e-06,
      "loss": 0.0899,
      "step": 69630
    },
    {
      "epoch": 3.900988712433129,
      "grad_norm": 1.3790451288223267,
      "learning_rate": 2.4763159369919837e-06,
      "loss": 0.0919,
      "step": 69640
    },
    {
      "epoch": 3.901548889449066,
      "grad_norm": 1.4066933393478394,
      "learning_rate": 2.4623016985257022e-06,
      "loss": 0.1549,
      "step": 69650
    },
    {
      "epoch": 3.902109066465003,
      "grad_norm": 1.631139874458313,
      "learning_rate": 2.4482874600594204e-06,
      "loss": 0.1188,
      "step": 69660
    },
    {
      "epoch": 3.90266924348094,
      "grad_norm": 2.700439453125,
      "learning_rate": 2.434273221593139e-06,
      "loss": 0.0707,
      "step": 69670
    },
    {
      "epoch": 3.903229420496877,
      "grad_norm": 2.872382164001465,
      "learning_rate": 2.420258983126857e-06,
      "loss": 0.139,
      "step": 69680
    },
    {
      "epoch": 3.903789597512814,
      "grad_norm": 2.1925883293151855,
      "learning_rate": 2.4062447446605756e-06,
      "loss": 0.12,
      "step": 69690
    },
    {
      "epoch": 3.904349774528751,
      "grad_norm": 0.6940253973007202,
      "learning_rate": 2.3922305061942938e-06,
      "loss": 0.098,
      "step": 69700
    },
    {
      "epoch": 3.9049099515446883,
      "grad_norm": 2.288180351257324,
      "learning_rate": 2.378216267728012e-06,
      "loss": 0.0888,
      "step": 69710
    },
    {
      "epoch": 3.905470128560625,
      "grad_norm": 1.4093317985534668,
      "learning_rate": 2.36420202926173e-06,
      "loss": 0.1495,
      "step": 69720
    },
    {
      "epoch": 3.906030305576562,
      "grad_norm": 1.7209855318069458,
      "learning_rate": 2.3501877907954486e-06,
      "loss": 0.0838,
      "step": 69730
    },
    {
      "epoch": 3.9065904825924993,
      "grad_norm": 5.056981086730957,
      "learning_rate": 2.3361735523291667e-06,
      "loss": 0.1325,
      "step": 69740
    },
    {
      "epoch": 3.907150659608436,
      "grad_norm": 1.5724668502807617,
      "learning_rate": 2.322159313862885e-06,
      "loss": 0.0837,
      "step": 69750
    },
    {
      "epoch": 3.907710836624373,
      "grad_norm": 1.6094847917556763,
      "learning_rate": 2.308145075396603e-06,
      "loss": 0.1217,
      "step": 69760
    },
    {
      "epoch": 3.9082710136403103,
      "grad_norm": 4.2886128425598145,
      "learning_rate": 2.2941308369303215e-06,
      "loss": 0.0935,
      "step": 69770
    },
    {
      "epoch": 3.9088311906562474,
      "grad_norm": 1.7155028581619263,
      "learning_rate": 2.2801165984640396e-06,
      "loss": 0.1258,
      "step": 69780
    },
    {
      "epoch": 3.9093913676721845,
      "grad_norm": 1.4690641164779663,
      "learning_rate": 2.2661023599977578e-06,
      "loss": 0.1185,
      "step": 69790
    },
    {
      "epoch": 3.9099515446881217,
      "grad_norm": 1.403930902481079,
      "learning_rate": 2.252088121531476e-06,
      "loss": 0.0817,
      "step": 69800
    },
    {
      "epoch": 3.9105117217040584,
      "grad_norm": 0.9098203778266907,
      "learning_rate": 2.2380738830651945e-06,
      "loss": 0.134,
      "step": 69810
    },
    {
      "epoch": 3.9110718987199955,
      "grad_norm": 1.9304982423782349,
      "learning_rate": 2.2240596445989126e-06,
      "loss": 0.0894,
      "step": 69820
    },
    {
      "epoch": 3.9116320757359326,
      "grad_norm": 0.7573782205581665,
      "learning_rate": 2.2100454061326307e-06,
      "loss": 0.0837,
      "step": 69830
    },
    {
      "epoch": 3.9121922527518693,
      "grad_norm": 1.5237663984298706,
      "learning_rate": 2.196031167666349e-06,
      "loss": 0.1647,
      "step": 69840
    },
    {
      "epoch": 3.9127524297678065,
      "grad_norm": 0.5436161160469055,
      "learning_rate": 2.1820169292000674e-06,
      "loss": 0.1074,
      "step": 69850
    },
    {
      "epoch": 3.9133126067837436,
      "grad_norm": 4.254706859588623,
      "learning_rate": 2.1680026907337855e-06,
      "loss": 0.1287,
      "step": 69860
    },
    {
      "epoch": 3.9138727837996807,
      "grad_norm": 4.203883647918701,
      "learning_rate": 2.1539884522675037e-06,
      "loss": 0.0949,
      "step": 69870
    },
    {
      "epoch": 3.914432960815618,
      "grad_norm": 1.6079766750335693,
      "learning_rate": 2.139974213801222e-06,
      "loss": 0.1111,
      "step": 69880
    },
    {
      "epoch": 3.914993137831555,
      "grad_norm": 2.0034689903259277,
      "learning_rate": 2.1259599753349404e-06,
      "loss": 0.2,
      "step": 69890
    },
    {
      "epoch": 3.9155533148474917,
      "grad_norm": 1.1452534198760986,
      "learning_rate": 2.1119457368686585e-06,
      "loss": 0.0834,
      "step": 69900
    },
    {
      "epoch": 3.916113491863429,
      "grad_norm": 0.6447186470031738,
      "learning_rate": 2.097931498402377e-06,
      "loss": 0.1128,
      "step": 69910
    },
    {
      "epoch": 3.916673668879366,
      "grad_norm": 2.3415215015411377,
      "learning_rate": 2.083917259936095e-06,
      "loss": 0.0814,
      "step": 69920
    },
    {
      "epoch": 3.9172338458953027,
      "grad_norm": 0.8343183398246765,
      "learning_rate": 2.0699030214698137e-06,
      "loss": 0.122,
      "step": 69930
    },
    {
      "epoch": 3.91779402291124,
      "grad_norm": 1.0858997106552124,
      "learning_rate": 2.055888783003532e-06,
      "loss": 0.0912,
      "step": 69940
    },
    {
      "epoch": 3.918354199927177,
      "grad_norm": 1.777084469795227,
      "learning_rate": 2.04187454453725e-06,
      "loss": 0.0811,
      "step": 69950
    },
    {
      "epoch": 3.918914376943114,
      "grad_norm": 3.339949369430542,
      "learning_rate": 2.027860306070968e-06,
      "loss": 0.1289,
      "step": 69960
    },
    {
      "epoch": 3.919474553959051,
      "grad_norm": 3.8822641372680664,
      "learning_rate": 2.0138460676046867e-06,
      "loss": 0.1584,
      "step": 69970
    },
    {
      "epoch": 3.9200347309749883,
      "grad_norm": 2.0335910320281982,
      "learning_rate": 1.999831829138405e-06,
      "loss": 0.0757,
      "step": 69980
    },
    {
      "epoch": 3.920594907990925,
      "grad_norm": 3.462514877319336,
      "learning_rate": 1.985817590672123e-06,
      "loss": 0.0908,
      "step": 69990
    },
    {
      "epoch": 3.921155085006862,
      "grad_norm": 2.6229562759399414,
      "learning_rate": 1.971803352205841e-06,
      "loss": 0.1001,
      "step": 70000
    },
    {
      "epoch": 3.9217152620227993,
      "grad_norm": 0.9973483085632324,
      "learning_rate": 1.9577891137395596e-06,
      "loss": 0.088,
      "step": 70010
    },
    {
      "epoch": 3.922275439038736,
      "grad_norm": 1.593148112297058,
      "learning_rate": 1.9437748752732778e-06,
      "loss": 0.1358,
      "step": 70020
    },
    {
      "epoch": 3.922835616054673,
      "grad_norm": 3.1209092140197754,
      "learning_rate": 1.929760636806996e-06,
      "loss": 0.1431,
      "step": 70030
    },
    {
      "epoch": 3.9233957930706103,
      "grad_norm": 6.029901504516602,
      "learning_rate": 1.915746398340714e-06,
      "loss": 0.2072,
      "step": 70040
    },
    {
      "epoch": 3.9239559700865474,
      "grad_norm": 2.5191948413848877,
      "learning_rate": 1.9017321598744326e-06,
      "loss": 0.0981,
      "step": 70050
    },
    {
      "epoch": 3.9245161471024845,
      "grad_norm": 0.7775284647941589,
      "learning_rate": 1.8877179214081507e-06,
      "loss": 0.0857,
      "step": 70060
    },
    {
      "epoch": 3.9250763241184217,
      "grad_norm": 1.9714585542678833,
      "learning_rate": 1.873703682941869e-06,
      "loss": 0.0882,
      "step": 70070
    },
    {
      "epoch": 3.9256365011343584,
      "grad_norm": 2.559994697570801,
      "learning_rate": 1.8596894444755872e-06,
      "loss": 0.12,
      "step": 70080
    },
    {
      "epoch": 3.9261966781502955,
      "grad_norm": 3.705371856689453,
      "learning_rate": 1.8456752060093055e-06,
      "loss": 0.163,
      "step": 70090
    },
    {
      "epoch": 3.9267568551662326,
      "grad_norm": 4.65751838684082,
      "learning_rate": 1.8316609675430239e-06,
      "loss": 0.1573,
      "step": 70100
    },
    {
      "epoch": 3.9273170321821693,
      "grad_norm": 2.3894667625427246,
      "learning_rate": 1.817646729076742e-06,
      "loss": 0.098,
      "step": 70110
    },
    {
      "epoch": 3.9278772091981065,
      "grad_norm": 1.4700655937194824,
      "learning_rate": 1.8036324906104606e-06,
      "loss": 0.0848,
      "step": 70120
    },
    {
      "epoch": 3.9284373862140436,
      "grad_norm": 2.324213743209839,
      "learning_rate": 1.7896182521441787e-06,
      "loss": 0.0957,
      "step": 70130
    },
    {
      "epoch": 3.9289975632299807,
      "grad_norm": 2.396188735961914,
      "learning_rate": 1.7756040136778968e-06,
      "loss": 0.1211,
      "step": 70140
    },
    {
      "epoch": 3.929557740245918,
      "grad_norm": 0.8560657501220703,
      "learning_rate": 1.761589775211615e-06,
      "loss": 0.0976,
      "step": 70150
    },
    {
      "epoch": 3.930117917261855,
      "grad_norm": 4.0938239097595215,
      "learning_rate": 1.7475755367453335e-06,
      "loss": 0.0843,
      "step": 70160
    },
    {
      "epoch": 3.9306780942777917,
      "grad_norm": 5.600588321685791,
      "learning_rate": 1.7335612982790516e-06,
      "loss": 0.0987,
      "step": 70170
    },
    {
      "epoch": 3.931238271293729,
      "grad_norm": 3.5616047382354736,
      "learning_rate": 1.7195470598127698e-06,
      "loss": 0.0788,
      "step": 70180
    },
    {
      "epoch": 3.931798448309666,
      "grad_norm": 0.9193775653839111,
      "learning_rate": 1.7055328213464881e-06,
      "loss": 0.0808,
      "step": 70190
    },
    {
      "epoch": 3.9323586253256027,
      "grad_norm": 2.1527228355407715,
      "learning_rate": 1.6915185828802064e-06,
      "loss": 0.1079,
      "step": 70200
    },
    {
      "epoch": 3.93291880234154,
      "grad_norm": 1.192961573600769,
      "learning_rate": 1.6775043444139248e-06,
      "loss": 0.064,
      "step": 70210
    },
    {
      "epoch": 3.933478979357477,
      "grad_norm": 5.608516216278076,
      "learning_rate": 1.663490105947643e-06,
      "loss": 0.1245,
      "step": 70220
    },
    {
      "epoch": 3.934039156373414,
      "grad_norm": 2.2469589710235596,
      "learning_rate": 1.649475867481361e-06,
      "loss": 0.1006,
      "step": 70230
    },
    {
      "epoch": 3.934599333389351,
      "grad_norm": 2.047834634780884,
      "learning_rate": 1.6354616290150796e-06,
      "loss": 0.1612,
      "step": 70240
    },
    {
      "epoch": 3.9351595104052883,
      "grad_norm": 0.955137312412262,
      "learning_rate": 1.6214473905487977e-06,
      "loss": 0.102,
      "step": 70250
    },
    {
      "epoch": 3.935719687421225,
      "grad_norm": 1.612974762916565,
      "learning_rate": 1.6074331520825159e-06,
      "loss": 0.1131,
      "step": 70260
    },
    {
      "epoch": 3.936279864437162,
      "grad_norm": 2.3680944442749023,
      "learning_rate": 1.593418913616234e-06,
      "loss": 0.0897,
      "step": 70270
    },
    {
      "epoch": 3.9368400414530993,
      "grad_norm": 1.0071220397949219,
      "learning_rate": 1.5794046751499526e-06,
      "loss": 0.0734,
      "step": 70280
    },
    {
      "epoch": 3.937400218469036,
      "grad_norm": 1.252137303352356,
      "learning_rate": 1.5653904366836707e-06,
      "loss": 0.1106,
      "step": 70290
    },
    {
      "epoch": 3.937960395484973,
      "grad_norm": 1.482410192489624,
      "learning_rate": 1.5513761982173888e-06,
      "loss": 0.1757,
      "step": 70300
    },
    {
      "epoch": 3.9385205725009103,
      "grad_norm": 3.4424641132354736,
      "learning_rate": 1.5373619597511072e-06,
      "loss": 0.1103,
      "step": 70310
    },
    {
      "epoch": 3.9390807495168474,
      "grad_norm": 1.2558104991912842,
      "learning_rate": 1.5233477212848255e-06,
      "loss": 0.1326,
      "step": 70320
    },
    {
      "epoch": 3.9396409265327845,
      "grad_norm": 0.4942796528339386,
      "learning_rate": 1.5093334828185438e-06,
      "loss": 0.0819,
      "step": 70330
    },
    {
      "epoch": 3.9402011035487217,
      "grad_norm": 0.8951675891876221,
      "learning_rate": 1.495319244352262e-06,
      "loss": 0.0852,
      "step": 70340
    },
    {
      "epoch": 3.9407612805646584,
      "grad_norm": 2.5600826740264893,
      "learning_rate": 1.4813050058859803e-06,
      "loss": 0.0933,
      "step": 70350
    },
    {
      "epoch": 3.9413214575805955,
      "grad_norm": 3.5743768215179443,
      "learning_rate": 1.4672907674196985e-06,
      "loss": 0.1183,
      "step": 70360
    },
    {
      "epoch": 3.9418816345965326,
      "grad_norm": 0.5453238487243652,
      "learning_rate": 1.4532765289534168e-06,
      "loss": 0.126,
      "step": 70370
    },
    {
      "epoch": 3.9424418116124693,
      "grad_norm": 2.229254961013794,
      "learning_rate": 1.439262290487135e-06,
      "loss": 0.0929,
      "step": 70380
    },
    {
      "epoch": 3.9430019886284065,
      "grad_norm": 1.3254458904266357,
      "learning_rate": 1.4252480520208533e-06,
      "loss": 0.1012,
      "step": 70390
    },
    {
      "epoch": 3.9435621656443436,
      "grad_norm": 5.207257270812988,
      "learning_rate": 1.4112338135545714e-06,
      "loss": 0.2572,
      "step": 70400
    },
    {
      "epoch": 3.9441223426602807,
      "grad_norm": 1.073277235031128,
      "learning_rate": 1.3972195750882897e-06,
      "loss": 0.0738,
      "step": 70410
    },
    {
      "epoch": 3.944682519676218,
      "grad_norm": 1.2622294425964355,
      "learning_rate": 1.3832053366220079e-06,
      "loss": 0.1219,
      "step": 70420
    },
    {
      "epoch": 3.945242696692155,
      "grad_norm": 0.9150616526603699,
      "learning_rate": 1.3691910981557262e-06,
      "loss": 0.1044,
      "step": 70430
    },
    {
      "epoch": 3.9458028737080917,
      "grad_norm": 2.2741355895996094,
      "learning_rate": 1.3551768596894446e-06,
      "loss": 0.0907,
      "step": 70440
    },
    {
      "epoch": 3.946363050724029,
      "grad_norm": 1.2500790357589722,
      "learning_rate": 1.341162621223163e-06,
      "loss": 0.0804,
      "step": 70450
    },
    {
      "epoch": 3.946923227739966,
      "grad_norm": 2.9947526454925537,
      "learning_rate": 1.327148382756881e-06,
      "loss": 0.1035,
      "step": 70460
    },
    {
      "epoch": 3.9474834047559026,
      "grad_norm": 1.6549659967422485,
      "learning_rate": 1.3131341442905994e-06,
      "loss": 0.0947,
      "step": 70470
    },
    {
      "epoch": 3.94804358177184,
      "grad_norm": 1.4919177293777466,
      "learning_rate": 1.2991199058243175e-06,
      "loss": 0.1083,
      "step": 70480
    },
    {
      "epoch": 3.948603758787777,
      "grad_norm": 4.062656879425049,
      "learning_rate": 1.2851056673580359e-06,
      "loss": 0.0941,
      "step": 70490
    },
    {
      "epoch": 3.949163935803714,
      "grad_norm": 1.4021066427230835,
      "learning_rate": 1.271091428891754e-06,
      "loss": 0.0765,
      "step": 70500
    },
    {
      "epoch": 3.949724112819651,
      "grad_norm": 0.8986619710922241,
      "learning_rate": 1.2570771904254723e-06,
      "loss": 0.1574,
      "step": 70510
    },
    {
      "epoch": 3.950284289835588,
      "grad_norm": 4.104448318481445,
      "learning_rate": 1.2430629519591905e-06,
      "loss": 0.1364,
      "step": 70520
    },
    {
      "epoch": 3.950844466851525,
      "grad_norm": 0.8678916096687317,
      "learning_rate": 1.2290487134929088e-06,
      "loss": 0.089,
      "step": 70530
    },
    {
      "epoch": 3.951404643867462,
      "grad_norm": 0.9659202694892883,
      "learning_rate": 1.215034475026627e-06,
      "loss": 0.1292,
      "step": 70540
    },
    {
      "epoch": 3.9519648208833993,
      "grad_norm": 0.7533054351806641,
      "learning_rate": 1.2010202365603453e-06,
      "loss": 0.0792,
      "step": 70550
    },
    {
      "epoch": 3.952524997899336,
      "grad_norm": 3.9800539016723633,
      "learning_rate": 1.1870059980940636e-06,
      "loss": 0.1422,
      "step": 70560
    },
    {
      "epoch": 3.953085174915273,
      "grad_norm": 1.2210463285446167,
      "learning_rate": 1.172991759627782e-06,
      "loss": 0.0867,
      "step": 70570
    },
    {
      "epoch": 3.9536453519312103,
      "grad_norm": 2.673002243041992,
      "learning_rate": 1.1589775211615003e-06,
      "loss": 0.0897,
      "step": 70580
    },
    {
      "epoch": 3.9542055289471474,
      "grad_norm": 1.3588736057281494,
      "learning_rate": 1.1449632826952184e-06,
      "loss": 0.0836,
      "step": 70590
    },
    {
      "epoch": 3.9547657059630845,
      "grad_norm": 1.1039689779281616,
      "learning_rate": 1.1309490442289368e-06,
      "loss": 0.0841,
      "step": 70600
    },
    {
      "epoch": 3.955325882979021,
      "grad_norm": 5.778039455413818,
      "learning_rate": 1.116934805762655e-06,
      "loss": 0.1356,
      "step": 70610
    },
    {
      "epoch": 3.9558860599949583,
      "grad_norm": 4.091336250305176,
      "learning_rate": 1.1029205672963733e-06,
      "loss": 0.1136,
      "step": 70620
    },
    {
      "epoch": 3.9564462370108955,
      "grad_norm": 1.9073747396469116,
      "learning_rate": 1.0889063288300914e-06,
      "loss": 0.0794,
      "step": 70630
    },
    {
      "epoch": 3.9570064140268326,
      "grad_norm": 1.1159205436706543,
      "learning_rate": 1.0748920903638097e-06,
      "loss": 0.0951,
      "step": 70640
    },
    {
      "epoch": 3.9575665910427693,
      "grad_norm": 1.4360390901565552,
      "learning_rate": 1.0608778518975279e-06,
      "loss": 0.1432,
      "step": 70650
    },
    {
      "epoch": 3.9581267680587064,
      "grad_norm": 1.6432323455810547,
      "learning_rate": 1.0468636134312462e-06,
      "loss": 0.0824,
      "step": 70660
    },
    {
      "epoch": 3.9586869450746436,
      "grad_norm": 0.6178684234619141,
      "learning_rate": 1.0328493749649643e-06,
      "loss": 0.142,
      "step": 70670
    },
    {
      "epoch": 3.9592471220905807,
      "grad_norm": 1.507665991783142,
      "learning_rate": 1.0188351364986827e-06,
      "loss": 0.1267,
      "step": 70680
    },
    {
      "epoch": 3.959807299106518,
      "grad_norm": 2.030515193939209,
      "learning_rate": 1.004820898032401e-06,
      "loss": 0.0766,
      "step": 70690
    },
    {
      "epoch": 3.9603674761224545,
      "grad_norm": 0.7663758993148804,
      "learning_rate": 9.908066595661194e-07,
      "loss": 0.1135,
      "step": 70700
    },
    {
      "epoch": 3.9609276531383917,
      "grad_norm": 0.9376887679100037,
      "learning_rate": 9.767924210998375e-07,
      "loss": 0.0811,
      "step": 70710
    },
    {
      "epoch": 3.961487830154329,
      "grad_norm": 2.591324806213379,
      "learning_rate": 9.627781826335558e-07,
      "loss": 0.0648,
      "step": 70720
    },
    {
      "epoch": 3.962048007170266,
      "grad_norm": 1.3865456581115723,
      "learning_rate": 9.48763944167274e-07,
      "loss": 0.0819,
      "step": 70730
    },
    {
      "epoch": 3.9626081841862026,
      "grad_norm": 0.8912171125411987,
      "learning_rate": 9.347497057009923e-07,
      "loss": 0.0872,
      "step": 70740
    },
    {
      "epoch": 3.9631683612021398,
      "grad_norm": 1.6610321998596191,
      "learning_rate": 9.207354672347104e-07,
      "loss": 0.0946,
      "step": 70750
    },
    {
      "epoch": 3.963728538218077,
      "grad_norm": 3.8926796913146973,
      "learning_rate": 9.067212287684288e-07,
      "loss": 0.0976,
      "step": 70760
    },
    {
      "epoch": 3.964288715234014,
      "grad_norm": 2.942253589630127,
      "learning_rate": 8.92706990302147e-07,
      "loss": 0.0941,
      "step": 70770
    },
    {
      "epoch": 3.964848892249951,
      "grad_norm": 1.8797495365142822,
      "learning_rate": 8.786927518358654e-07,
      "loss": 0.1378,
      "step": 70780
    },
    {
      "epoch": 3.965409069265888,
      "grad_norm": 0.47692134976387024,
      "learning_rate": 8.646785133695835e-07,
      "loss": 0.0867,
      "step": 70790
    },
    {
      "epoch": 3.965969246281825,
      "grad_norm": 1.4736546277999878,
      "learning_rate": 8.506642749033018e-07,
      "loss": 0.0683,
      "step": 70800
    },
    {
      "epoch": 3.966529423297762,
      "grad_norm": 3.653200387954712,
      "learning_rate": 8.3665003643702e-07,
      "loss": 0.1757,
      "step": 70810
    },
    {
      "epoch": 3.9670896003136993,
      "grad_norm": 0.7893109321594238,
      "learning_rate": 8.226357979707383e-07,
      "loss": 0.0984,
      "step": 70820
    },
    {
      "epoch": 3.967649777329636,
      "grad_norm": 2.643914222717285,
      "learning_rate": 8.086215595044565e-07,
      "loss": 0.118,
      "step": 70830
    },
    {
      "epoch": 3.968209954345573,
      "grad_norm": 1.5445343255996704,
      "learning_rate": 7.946073210381749e-07,
      "loss": 0.0875,
      "step": 70840
    },
    {
      "epoch": 3.9687701313615102,
      "grad_norm": 5.522687911987305,
      "learning_rate": 7.805930825718931e-07,
      "loss": 0.1655,
      "step": 70850
    },
    {
      "epoch": 3.9693303083774474,
      "grad_norm": 6.680904865264893,
      "learning_rate": 7.665788441056114e-07,
      "loss": 0.1167,
      "step": 70860
    },
    {
      "epoch": 3.9698904853933845,
      "grad_norm": 4.298681259155273,
      "learning_rate": 7.525646056393296e-07,
      "loss": 0.1301,
      "step": 70870
    },
    {
      "epoch": 3.970450662409321,
      "grad_norm": 2.767849922180176,
      "learning_rate": 7.385503671730478e-07,
      "loss": 0.0867,
      "step": 70880
    },
    {
      "epoch": 3.9710108394252583,
      "grad_norm": 1.2183109521865845,
      "learning_rate": 7.245361287067662e-07,
      "loss": 0.095,
      "step": 70890
    },
    {
      "epoch": 3.9715710164411955,
      "grad_norm": 5.176458358764648,
      "learning_rate": 7.105218902404844e-07,
      "loss": 0.1265,
      "step": 70900
    },
    {
      "epoch": 3.9721311934571326,
      "grad_norm": 1.0918216705322266,
      "learning_rate": 6.965076517742027e-07,
      "loss": 0.0984,
      "step": 70910
    },
    {
      "epoch": 3.9726913704730693,
      "grad_norm": 0.4514658451080322,
      "learning_rate": 6.824934133079209e-07,
      "loss": 0.1622,
      "step": 70920
    },
    {
      "epoch": 3.9732515474890064,
      "grad_norm": 1.3300515413284302,
      "learning_rate": 6.684791748416391e-07,
      "loss": 0.1379,
      "step": 70930
    },
    {
      "epoch": 3.9738117245049436,
      "grad_norm": 0.7101684212684631,
      "learning_rate": 6.544649363753574e-07,
      "loss": 0.0874,
      "step": 70940
    },
    {
      "epoch": 3.9743719015208807,
      "grad_norm": 1.1180893182754517,
      "learning_rate": 6.404506979090757e-07,
      "loss": 0.1146,
      "step": 70950
    },
    {
      "epoch": 3.974932078536818,
      "grad_norm": 1.8490607738494873,
      "learning_rate": 6.264364594427939e-07,
      "loss": 0.0885,
      "step": 70960
    },
    {
      "epoch": 3.9754922555527545,
      "grad_norm": 2.3977673053741455,
      "learning_rate": 6.124222209765122e-07,
      "loss": 0.1555,
      "step": 70970
    },
    {
      "epoch": 3.9760524325686917,
      "grad_norm": 0.9830726981163025,
      "learning_rate": 5.984079825102304e-07,
      "loss": 0.1349,
      "step": 70980
    },
    {
      "epoch": 3.976612609584629,
      "grad_norm": 1.829492449760437,
      "learning_rate": 5.843937440439487e-07,
      "loss": 0.0831,
      "step": 70990
    },
    {
      "epoch": 3.9771727866005655,
      "grad_norm": 0.49445274472236633,
      "learning_rate": 5.703795055776669e-07,
      "loss": 0.0992,
      "step": 71000
    },
    {
      "epoch": 3.9777329636165026,
      "grad_norm": 1.256615161895752,
      "learning_rate": 5.563652671113852e-07,
      "loss": 0.1914,
      "step": 71010
    },
    {
      "epoch": 3.9782931406324398,
      "grad_norm": 3.15335750579834,
      "learning_rate": 5.423510286451035e-07,
      "loss": 0.121,
      "step": 71020
    },
    {
      "epoch": 3.978853317648377,
      "grad_norm": 1.2049006223678589,
      "learning_rate": 5.283367901788217e-07,
      "loss": 0.103,
      "step": 71030
    },
    {
      "epoch": 3.979413494664314,
      "grad_norm": 1.8661378622055054,
      "learning_rate": 5.1432255171254e-07,
      "loss": 0.1362,
      "step": 71040
    },
    {
      "epoch": 3.979973671680251,
      "grad_norm": 1.1870615482330322,
      "learning_rate": 5.003083132462582e-07,
      "loss": 0.0907,
      "step": 71050
    },
    {
      "epoch": 3.980533848696188,
      "grad_norm": 2.9555671215057373,
      "learning_rate": 4.862940747799764e-07,
      "loss": 0.0954,
      "step": 71060
    },
    {
      "epoch": 3.981094025712125,
      "grad_norm": 3.2427475452423096,
      "learning_rate": 4.722798363136947e-07,
      "loss": 0.0706,
      "step": 71070
    },
    {
      "epoch": 3.981654202728062,
      "grad_norm": 2.1090290546417236,
      "learning_rate": 4.58265597847413e-07,
      "loss": 0.1262,
      "step": 71080
    },
    {
      "epoch": 3.982214379743999,
      "grad_norm": 4.063536167144775,
      "learning_rate": 4.4425135938113124e-07,
      "loss": 0.1041,
      "step": 71090
    },
    {
      "epoch": 3.982774556759936,
      "grad_norm": 1.1160248517990112,
      "learning_rate": 4.302371209148495e-07,
      "loss": 0.1478,
      "step": 71100
    },
    {
      "epoch": 3.983334733775873,
      "grad_norm": 2.2349603176116943,
      "learning_rate": 4.1622288244856777e-07,
      "loss": 0.1119,
      "step": 71110
    },
    {
      "epoch": 3.9838949107918102,
      "grad_norm": 1.4309642314910889,
      "learning_rate": 4.02208643982286e-07,
      "loss": 0.1352,
      "step": 71120
    },
    {
      "epoch": 3.9844550878077474,
      "grad_norm": 2.1613457202911377,
      "learning_rate": 3.881944055160043e-07,
      "loss": 0.1032,
      "step": 71130
    },
    {
      "epoch": 3.9850152648236845,
      "grad_norm": 1.9002621173858643,
      "learning_rate": 3.7418016704972253e-07,
      "loss": 0.1016,
      "step": 71140
    },
    {
      "epoch": 3.985575441839621,
      "grad_norm": 1.4874542951583862,
      "learning_rate": 3.601659285834408e-07,
      "loss": 0.0788,
      "step": 71150
    },
    {
      "epoch": 3.9861356188555583,
      "grad_norm": 3.169569253921509,
      "learning_rate": 3.4615169011715906e-07,
      "loss": 0.1115,
      "step": 71160
    },
    {
      "epoch": 3.9866957958714955,
      "grad_norm": 1.289158582687378,
      "learning_rate": 3.321374516508773e-07,
      "loss": 0.1152,
      "step": 71170
    },
    {
      "epoch": 3.987255972887432,
      "grad_norm": 1.0700510740280151,
      "learning_rate": 3.181232131845956e-07,
      "loss": 0.1141,
      "step": 71180
    },
    {
      "epoch": 3.9878161499033693,
      "grad_norm": 5.979076862335205,
      "learning_rate": 3.041089747183138e-07,
      "loss": 0.1144,
      "step": 71190
    },
    {
      "epoch": 3.9883763269193064,
      "grad_norm": 1.3023629188537598,
      "learning_rate": 2.9009473625203206e-07,
      "loss": 0.0817,
      "step": 71200
    },
    {
      "epoch": 3.9889365039352436,
      "grad_norm": 1.8939472436904907,
      "learning_rate": 2.7608049778575035e-07,
      "loss": 0.0719,
      "step": 71210
    },
    {
      "epoch": 3.9894966809511807,
      "grad_norm": 3.0668981075286865,
      "learning_rate": 2.620662593194686e-07,
      "loss": 0.1076,
      "step": 71220
    },
    {
      "epoch": 3.990056857967118,
      "grad_norm": 3.53798508644104,
      "learning_rate": 2.480520208531868e-07,
      "loss": 0.1194,
      "step": 71230
    },
    {
      "epoch": 3.9906170349830545,
      "grad_norm": 1.5810017585754395,
      "learning_rate": 2.3403778238690511e-07,
      "loss": 0.1727,
      "step": 71240
    },
    {
      "epoch": 3.9911772119989917,
      "grad_norm": 1.3744548559188843,
      "learning_rate": 2.2002354392062335e-07,
      "loss": 0.112,
      "step": 71250
    },
    {
      "epoch": 3.991737389014929,
      "grad_norm": 4.364106178283691,
      "learning_rate": 2.0600930545434161e-07,
      "loss": 0.1099,
      "step": 71260
    },
    {
      "epoch": 3.9922975660308655,
      "grad_norm": 2.1563589572906494,
      "learning_rate": 1.9199506698805988e-07,
      "loss": 0.1345,
      "step": 71270
    },
    {
      "epoch": 3.9928577430468026,
      "grad_norm": 0.9000837802886963,
      "learning_rate": 1.7798082852177811e-07,
      "loss": 0.0942,
      "step": 71280
    },
    {
      "epoch": 3.9934179200627398,
      "grad_norm": 4.744304656982422,
      "learning_rate": 1.639665900554964e-07,
      "loss": 0.1261,
      "step": 71290
    },
    {
      "epoch": 3.993978097078677,
      "grad_norm": 2.3309857845306396,
      "learning_rate": 1.4995235158921467e-07,
      "loss": 0.123,
      "step": 71300
    },
    {
      "epoch": 3.994538274094614,
      "grad_norm": 0.9124817252159119,
      "learning_rate": 1.359381131229329e-07,
      "loss": 0.0954,
      "step": 71310
    },
    {
      "epoch": 3.995098451110551,
      "grad_norm": 0.44094935059547424,
      "learning_rate": 1.2192387465665117e-07,
      "loss": 0.1045,
      "step": 71320
    },
    {
      "epoch": 3.995658628126488,
      "grad_norm": 2.0915791988372803,
      "learning_rate": 1.0790963619036942e-07,
      "loss": 0.0931,
      "step": 71330
    },
    {
      "epoch": 3.996218805142425,
      "grad_norm": 4.1689252853393555,
      "learning_rate": 9.389539772408768e-08,
      "loss": 0.118,
      "step": 71340
    },
    {
      "epoch": 3.996778982158362,
      "grad_norm": 1.029751181602478,
      "learning_rate": 7.988115925780593e-08,
      "loss": 0.0904,
      "step": 71350
    },
    {
      "epoch": 3.997339159174299,
      "grad_norm": 2.52363920211792,
      "learning_rate": 6.586692079152418e-08,
      "loss": 0.1414,
      "step": 71360
    },
    {
      "epoch": 3.997899336190236,
      "grad_norm": 1.01207435131073,
      "learning_rate": 5.185268232524245e-08,
      "loss": 0.0882,
      "step": 71370
    },
    {
      "epoch": 3.998459513206173,
      "grad_norm": 1.1945979595184326,
      "learning_rate": 3.78384438589607e-08,
      "loss": 0.0889,
      "step": 71380
    },
    {
      "epoch": 3.9990196902221102,
      "grad_norm": 3.4409682750701904,
      "learning_rate": 2.3824205392678963e-08,
      "loss": 0.1292,
      "step": 71390
    },
    {
      "epoch": 3.9995798672380474,
      "grad_norm": 6.079535007476807,
      "learning_rate": 9.80996692639722e-09,
      "loss": 0.2088,
      "step": 71400
    }
  ],
  "logging_steps": 10,
  "max_steps": 71406,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.248404400947331e+19,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
